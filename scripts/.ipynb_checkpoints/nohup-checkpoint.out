Run this job and save the output to output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      96
# test     864
---------  -------------------
['banded', 'blotchy', 'braided', 'bubbly', 'bumpy', 'chequered', 'cobwebbed', 'cracked', 'crosshatched', 'crystalline', 'dotted', 'fibrous', 'flecked', 'freckled', 'frilly', 'gauzy', 'grid', 'grooved', 'honeycombed', 'interlaced', 'knitted', 'lacelike', 'lined', 'marbled']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X banded texture.', 'X X X X blotchy texture.', 'X X X X braided texture.', 'X X X X bubbly texture.', 'X X X X bumpy texture.', 'X X X X chequered texture.', 'X X X X cobwebbed texture.', 'X X X X cracked texture.', 'X X X X crosshatched texture.', 'X X X X crystalline texture.', 'X X X X dotted texture.', 'X X X X fibrous texture.', 'X X X X flecked texture.', 'X X X X freckled texture.', 'X X X X frilly texture.', 'X X X X gauzy texture.', 'X X X X grid texture.', 'X X X X grooved texture.', 'X X X X honeycombed texture.', 'X X X X interlaced texture.', 'X X X X knitted texture.', 'X X X X lacelike texture.', 'X X X X lined texture.', 'X X X X marbled texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([24, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/12] time 0.145 (0.307) data 0.000 (0.155) loss 6.1016 (6.2750) acc 46.8750 (40.6250) lr 1.0000e-05 eta 0:03:02
epoch [1/50] batch [10/12] time 0.144 (0.226) data 0.000 (0.078) loss 6.1953 (6.2152) acc 40.6250 (41.5625) lr 1.0000e-05 eta 0:02:13
epoch [2/50] batch [5/12] time 0.143 (0.294) data 0.000 (0.149) loss 3.7539 (4.8281) acc 71.8750 (51.8750) lr 2.0000e-03 eta 0:02:51
epoch [2/50] batch [10/12] time 0.143 (0.219) data 0.000 (0.075) loss 4.0625 (4.4295) acc 50.0000 (53.1250) lr 2.0000e-03 eta 0:02:06
epoch [3/50] batch [5/12] time 0.143 (0.273) data 0.000 (0.128) loss 3.1289 (3.5293) acc 71.8750 (63.1250) lr 1.9980e-03 eta 0:02:35
epoch [3/50] batch [10/12] time 0.144 (0.208) data 0.000 (0.064) loss 3.4766 (3.3816) acc 56.2500 (64.6875) lr 1.9980e-03 eta 0:01:57
epoch [4/50] batch [5/12] time 0.143 (0.290) data 0.000 (0.145) loss 2.6738 (3.0969) acc 81.2500 (68.1250) lr 1.9921e-03 eta 0:02:41
epoch [4/50] batch [10/12] time 0.143 (0.216) data 0.000 (0.073) loss 2.2520 (2.9227) acc 96.8750 (70.3125) lr 1.9921e-03 eta 0:01:59
epoch [5/50] batch [5/12] time 0.143 (0.285) data 0.000 (0.141) loss 2.6230 (2.5258) acc 81.2500 (77.5000) lr 1.9823e-03 eta 0:02:35
epoch [5/50] batch [10/12] time 0.147 (0.215) data 0.000 (0.070) loss 2.3477 (2.6244) acc 84.3750 (75.6250) lr 1.9823e-03 eta 0:01:56
epoch [6/50] batch [5/12] time 0.144 (0.295) data 0.000 (0.150) loss 2.4414 (2.3973) acc 75.0000 (79.3750) lr 1.9686e-03 eta 0:02:37
epoch [6/50] batch [10/12] time 0.144 (0.219) data 0.000 (0.075) loss 2.5918 (2.4479) acc 75.0000 (77.1875) lr 1.9686e-03 eta 0:01:56
epoch [7/50] batch [5/12] time 0.137 (0.304) data 0.000 (0.162) loss 2.5781 (2.4879) acc 78.1250 (79.3750) lr 1.9511e-03 eta 0:02:39
epoch [7/50] batch [10/12] time 0.137 (0.220) data 0.000 (0.081) loss 2.2754 (2.4143) acc 90.6250 (80.0000) lr 1.9511e-03 eta 0:01:54
epoch [8/50] batch [5/12] time 0.144 (0.304) data 0.000 (0.159) loss 2.2871 (2.3027) acc 78.1250 (83.7500) lr 1.9298e-03 eta 0:02:35
epoch [8/50] batch [10/12] time 0.137 (0.223) data 0.000 (0.080) loss 1.8525 (2.2063) acc 90.6250 (85.0000) lr 1.9298e-03 eta 0:01:52
epoch [9/50] batch [5/12] time 0.147 (0.281) data 0.000 (0.134) loss 2.0820 (2.1074) acc 90.6250 (88.1250) lr 1.9048e-03 eta 0:02:20
epoch [9/50] batch [10/12] time 0.145 (0.213) data 0.000 (0.067) loss 2.3047 (2.1125) acc 78.1250 (85.6250) lr 1.9048e-03 eta 0:01:45
epoch [10/50] batch [5/12] time 0.143 (0.285) data 0.000 (0.141) loss 2.0898 (2.0338) acc 84.3750 (83.7500) lr 1.8763e-03 eta 0:02:18
epoch [10/50] batch [10/12] time 0.142 (0.213) data 0.000 (0.071) loss 2.0684 (2.0173) acc 84.3750 (84.3750) lr 1.8763e-03 eta 0:01:42
epoch [11/50] batch [5/12] time 0.145 (0.284) data 0.000 (0.139) loss 1.8486 (2.0146) acc 87.5000 (86.2500) lr 1.8443e-03 eta 0:02:14
epoch [11/50] batch [10/12] time 0.147 (0.215) data 0.000 (0.070) loss 1.7979 (1.9638) acc 93.7500 (87.1875) lr 1.8443e-03 eta 0:01:41
epoch [12/50] batch [5/12] time 0.143 (0.306) data 0.000 (0.162) loss 2.2090 (2.0656) acc 75.0000 (83.7500) lr 1.8090e-03 eta 0:02:21
epoch [12/50] batch [10/12] time 0.142 (0.225) data 0.000 (0.081) loss 1.6934 (1.9880) acc 100.0000 (86.8750) lr 1.8090e-03 eta 0:01:42
epoch [13/50] batch [5/12] time 0.144 (0.281) data 0.000 (0.136) loss 1.9941 (1.8873) acc 90.6250 (93.7500) lr 1.7705e-03 eta 0:02:06
epoch [13/50] batch [10/12] time 0.143 (0.212) data 0.000 (0.068) loss 1.9189 (1.9339) acc 81.2500 (90.6250) lr 1.7705e-03 eta 0:01:34
epoch [14/50] batch [5/12] time 0.143 (0.284) data 0.000 (0.140) loss 1.9092 (1.9129) acc 93.7500 (86.8750) lr 1.7290e-03 eta 0:02:04
epoch [14/50] batch [10/12] time 0.144 (0.213) data 0.000 (0.070) loss 1.8984 (1.8479) acc 87.5000 (87.8125) lr 1.7290e-03 eta 0:01:32
epoch [15/50] batch [5/12] time 0.145 (0.307) data 0.000 (0.162) loss 1.6904 (1.7410) acc 90.6250 (91.8750) lr 1.6845e-03 eta 0:02:11
epoch [15/50] batch [10/12] time 0.143 (0.225) data 0.000 (0.081) loss 2.1250 (1.8185) acc 81.2500 (90.3125) lr 1.6845e-03 eta 0:01:34
epoch [16/50] batch [5/12] time 0.142 (0.280) data 0.000 (0.137) loss 1.9102 (1.7434) acc 93.7500 (94.3750) lr 1.6374e-03 eta 0:01:56
epoch [16/50] batch [10/12] time 0.144 (0.212) data 0.000 (0.069) loss 1.9492 (1.7599) acc 93.7500 (94.0625) lr 1.6374e-03 eta 0:01:26
epoch [17/50] batch [5/12] time 0.145 (0.304) data 0.000 (0.157) loss 1.4980 (1.6428) acc 96.8750 (93.7500) lr 1.5878e-03 eta 0:02:02
epoch [17/50] batch [10/12] time 0.144 (0.225) data 0.000 (0.079) loss 1.5537 (1.7150) acc 96.8750 (91.5625) lr 1.5878e-03 eta 0:01:29
epoch [18/50] batch [5/12] time 0.143 (0.285) data 0.000 (0.141) loss 1.5908 (1.7760) acc 93.7500 (89.3750) lr 1.5358e-03 eta 0:01:51
epoch [18/50] batch [10/12] time 0.144 (0.214) data 0.000 (0.071) loss 1.5215 (1.7238) acc 96.8750 (91.8750) lr 1.5358e-03 eta 0:01:22
epoch [19/50] batch [5/12] time 0.143 (0.290) data 0.000 (0.146) loss 1.8193 (1.7154) acc 93.7500 (93.7500) lr 1.4818e-03 eta 0:01:49
epoch [19/50] batch [10/12] time 0.143 (0.216) data 0.000 (0.073) loss 1.6543 (1.6899) acc 90.6250 (94.3750) lr 1.4818e-03 eta 0:01:20
epoch [20/50] batch [5/12] time 0.142 (0.293) data 0.000 (0.149) loss 1.8428 (1.7508) acc 84.3750 (88.7500) lr 1.4258e-03 eta 0:01:47
epoch [20/50] batch [10/12] time 0.143 (0.218) data 0.000 (0.074) loss 1.5869 (1.6832) acc 96.8750 (91.5625) lr 1.4258e-03 eta 0:01:18
epoch [21/50] batch [5/12] time 0.142 (0.303) data 0.000 (0.159) loss 1.7334 (1.7299) acc 96.8750 (92.5000) lr 1.3681e-03 eta 0:01:47
epoch [21/50] batch [10/12] time 0.142 (0.223) data 0.000 (0.080) loss 1.6416 (1.6417) acc 96.8750 (94.0625) lr 1.3681e-03 eta 0:01:18
epoch [22/50] batch [5/12] time 0.144 (0.288) data 0.000 (0.144) loss 1.5977 (1.6855) acc 96.8750 (95.6250) lr 1.3090e-03 eta 0:01:38
epoch [22/50] batch [10/12] time 0.144 (0.216) data 0.000 (0.072) loss 1.7373 (1.6670) acc 93.7500 (94.6875) lr 1.3090e-03 eta 0:01:12
epoch [23/50] batch [5/12] time 0.143 (0.289) data 0.000 (0.145) loss 1.7051 (1.6342) acc 93.7500 (94.3750) lr 1.2487e-03 eta 0:01:35
epoch [23/50] batch [10/12] time 0.143 (0.216) data 0.000 (0.072) loss 1.3848 (1.6531) acc 96.8750 (94.0625) lr 1.2487e-03 eta 0:01:10
epoch [24/50] batch [5/12] time 0.144 (0.284) data 0.000 (0.139) loss 1.3203 (1.5434) acc 100.0000 (98.1250) lr 1.1874e-03 eta 0:01:30
epoch [24/50] batch [10/12] time 0.145 (0.214) data 0.000 (0.070) loss 1.6074 (1.5861) acc 93.7500 (97.1875) lr 1.1874e-03 eta 0:01:07
epoch [25/50] batch [5/12] time 0.143 (0.298) data 0.000 (0.155) loss 1.6045 (1.5055) acc 100.0000 (98.7500) lr 1.1253e-03 eta 0:01:31
epoch [25/50] batch [10/12] time 0.144 (0.221) data 0.000 (0.078) loss 1.6113 (1.5599) acc 96.8750 (95.9375) lr 1.1253e-03 eta 0:01:06
epoch [26/50] batch [5/12] time 0.143 (0.275) data 0.000 (0.131) loss 1.6504 (1.5416) acc 90.6250 (95.6250) lr 1.0628e-03 eta 0:01:21
epoch [26/50] batch [10/12] time 0.143 (0.209) data 0.000 (0.066) loss 1.4521 (1.6066) acc 93.7500 (95.9375) lr 1.0628e-03 eta 0:01:00
epoch [27/50] batch [5/12] time 0.144 (0.288) data 0.000 (0.143) loss 1.4756 (1.6115) acc 96.8750 (96.2500) lr 1.0000e-03 eta 0:01:21
epoch [27/50] batch [10/12] time 0.143 (0.216) data 0.000 (0.071) loss 1.4170 (1.5546) acc 96.8750 (96.5625) lr 1.0000e-03 eta 0:01:00
epoch [28/50] batch [5/12] time 0.144 (0.304) data 0.000 (0.161) loss 1.4404 (1.4953) acc 100.0000 (97.5000) lr 9.3721e-04 eta 0:01:22
epoch [28/50] batch [10/12] time 0.143 (0.224) data 0.000 (0.080) loss 1.6143 (1.5255) acc 90.6250 (96.5625) lr 9.3721e-04 eta 0:00:59
epoch [29/50] batch [5/12] time 0.144 (0.283) data 0.000 (0.140) loss 1.4766 (1.6184) acc 96.8750 (95.0000) lr 8.7467e-04 eta 0:01:13
epoch [29/50] batch [10/12] time 0.144 (0.214) data 0.000 (0.070) loss 1.5654 (1.5477) acc 100.0000 (96.2500) lr 8.7467e-04 eta 0:00:54
epoch [30/50] batch [5/12] time 0.144 (0.290) data 0.000 (0.144) loss 1.6348 (1.5604) acc 93.7500 (94.3750) lr 8.1262e-04 eta 0:01:11
epoch [30/50] batch [10/12] time 0.144 (0.217) data 0.000 (0.072) loss 1.4316 (1.5508) acc 100.0000 (94.6875) lr 8.1262e-04 eta 0:00:52
epoch [31/50] batch [5/12] time 0.143 (0.306) data 0.000 (0.161) loss 1.8418 (1.5775) acc 93.7500 (94.3750) lr 7.5131e-04 eta 0:01:11
epoch [31/50] batch [10/12] time 0.145 (0.225) data 0.000 (0.080) loss 1.4883 (1.5402) acc 96.8750 (95.3125) lr 7.5131e-04 eta 0:00:51
epoch [32/50] batch [5/12] time 0.144 (0.290) data 0.000 (0.146) loss 1.7900 (1.6439) acc 87.5000 (93.7500) lr 6.9098e-04 eta 0:01:04
epoch [32/50] batch [10/12] time 0.144 (0.217) data 0.000 (0.073) loss 1.5742 (1.5425) acc 96.8750 (96.2500) lr 6.9098e-04 eta 0:00:47
epoch [33/50] batch [5/12] time 0.138 (0.283) data 0.000 (0.144) loss 1.5527 (1.4707) acc 96.8750 (95.6250) lr 6.3188e-04 eta 0:00:59
epoch [33/50] batch [10/12] time 0.138 (0.210) data 0.000 (0.072) loss 1.6592 (1.5017) acc 96.8750 (94.6875) lr 6.3188e-04 eta 0:00:43
epoch [34/50] batch [5/12] time 0.144 (0.289) data 0.000 (0.144) loss 1.4287 (1.5092) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:00:57
epoch [34/50] batch [10/12] time 0.143 (0.217) data 0.000 (0.072) loss 1.6543 (1.5532) acc 93.7500 (96.8750) lr 5.7422e-04 eta 0:00:42
epoch [35/50] batch [5/12] time 0.143 (0.298) data 0.000 (0.153) loss 1.4570 (1.4883) acc 93.7500 (95.0000) lr 5.1825e-04 eta 0:00:55
epoch [35/50] batch [10/12] time 0.144 (0.221) data 0.000 (0.076) loss 1.4277 (1.4737) acc 100.0000 (96.8750) lr 5.1825e-04 eta 0:00:40
epoch [36/50] batch [5/12] time 0.145 (0.289) data 0.000 (0.143) loss 1.4951 (1.6447) acc 96.8750 (95.0000) lr 4.6417e-04 eta 0:00:50
epoch [36/50] batch [10/12] time 0.138 (0.213) data 0.000 (0.071) loss 1.3984 (1.5173) acc 96.8750 (96.5625) lr 4.6417e-04 eta 0:00:36
epoch [37/50] batch [5/12] time 0.143 (0.280) data 0.000 (0.136) loss 1.4766 (1.4219) acc 100.0000 (100.0000) lr 4.1221e-04 eta 0:00:45
epoch [37/50] batch [10/12] time 0.143 (0.212) data 0.000 (0.068) loss 1.4316 (1.5000) acc 93.7500 (97.1875) lr 4.1221e-04 eta 0:00:33
epoch [38/50] batch [5/12] time 0.143 (0.289) data 0.000 (0.145) loss 1.5615 (1.6137) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:43
epoch [38/50] batch [10/12] time 0.143 (0.216) data 0.000 (0.073) loss 1.8672 (1.5772) acc 87.5000 (93.4375) lr 3.6258e-04 eta 0:00:31
epoch [39/50] batch [5/12] time 0.144 (0.298) data 0.000 (0.152) loss 1.4648 (1.5486) acc 93.7500 (95.0000) lr 3.1545e-04 eta 0:00:41
epoch [39/50] batch [10/12] time 0.145 (0.222) data 0.000 (0.076) loss 1.3848 (1.5089) acc 100.0000 (96.8750) lr 3.1545e-04 eta 0:00:29
epoch [40/50] batch [5/12] time 0.143 (0.282) data 0.000 (0.137) loss 1.4531 (1.4438) acc 100.0000 (96.8750) lr 2.7103e-04 eta 0:00:35
epoch [40/50] batch [10/12] time 0.143 (0.213) data 0.000 (0.069) loss 1.4697 (1.4592) acc 93.7500 (97.5000) lr 2.7103e-04 eta 0:00:25
epoch [41/50] batch [5/12] time 0.145 (0.288) data 0.000 (0.142) loss 1.5107 (1.4828) acc 96.8750 (98.1250) lr 2.2949e-04 eta 0:00:33
epoch [41/50] batch [10/12] time 0.144 (0.223) data 0.000 (0.071) loss 1.4668 (1.5288) acc 96.8750 (97.1875) lr 2.2949e-04 eta 0:00:24
epoch [42/50] batch [5/12] time 0.147 (0.301) data 0.000 (0.155) loss 1.3184 (1.5213) acc 100.0000 (98.1250) lr 1.9098e-04 eta 0:00:30
epoch [42/50] batch [10/12] time 0.145 (0.223) data 0.000 (0.077) loss 1.8359 (1.5342) acc 93.7500 (97.5000) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/12] time 0.143 (0.285) data 0.000 (0.140) loss 1.3721 (1.5063) acc 100.0000 (96.8750) lr 1.5567e-04 eta 0:00:25
epoch [43/50] batch [10/12] time 0.144 (0.214) data 0.000 (0.070) loss 1.7363 (1.4831) acc 96.8750 (97.8125) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/12] time 0.143 (0.292) data 0.000 (0.145) loss 1.4082 (1.5236) acc 96.8750 (96.8750) lr 1.2369e-04 eta 0:00:23
epoch [44/50] batch [10/12] time 0.144 (0.218) data 0.000 (0.072) loss 1.3223 (1.4661) acc 100.0000 (98.1250) lr 1.2369e-04 eta 0:00:16
epoch [45/50] batch [5/12] time 0.143 (0.303) data 0.000 (0.159) loss 1.3535 (1.5217) acc 96.8750 (95.6250) lr 9.5173e-05 eta 0:00:20
epoch [45/50] batch [10/12] time 0.145 (0.224) data 0.000 (0.080) loss 1.3926 (1.4793) acc 100.0000 (97.5000) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/12] time 0.143 (0.296) data 0.000 (0.151) loss 1.6387 (1.4584) acc 96.8750 (98.1250) lr 7.0224e-05 eta 0:00:16
epoch [46/50] batch [10/12] time 0.144 (0.220) data 0.000 (0.076) loss 1.5176 (1.4889) acc 96.8750 (97.5000) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/12] time 0.146 (0.306) data 0.000 (0.159) loss 1.5322 (1.4438) acc 93.7500 (96.8750) lr 4.8943e-05 eta 0:00:13
epoch [47/50] batch [10/12] time 0.143 (0.225) data 0.000 (0.079) loss 1.5410 (1.5536) acc 96.8750 (96.5625) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/12] time 0.145 (0.294) data 0.000 (0.148) loss 1.2793 (1.4949) acc 96.8750 (99.3750) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [10/12] time 0.146 (0.219) data 0.000 (0.074) loss 1.5527 (1.4623) acc 96.8750 (99.0625) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/12] time 0.143 (0.281) data 0.000 (0.137) loss 1.6768 (1.4654) acc 96.8750 (96.8750) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [10/12] time 0.144 (0.212) data 0.000 (0.069) loss 1.4980 (1.4683) acc 100.0000 (97.8125) lr 1.7713e-05 eta 0:00:02
epoch [50/50] batch [5/12] time 0.189 (0.303) data 0.000 (0.149) loss 1.3945 (1.3631) acc 100.0000 (98.1250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [10/12] time 0.146 (0.224) data 0.000 (0.075) loss 1.6299 (1.4384) acc 90.6250 (96.8750) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:12<00:12, 12.55s/it]100%|██████████| 2/2 [00:15<00:00,  6.84s/it]100%|██████████| 2/2 [00:15<00:00,  7.76s/it]
=> result
* total: 864
* correct: 728
* accuracy: 84.3%
* error: 15.7%
* macro_f1: 84.1%
Elapsed: 0:02:24
Run this job and save the output to output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      96
# test     864
---------  -------------------
['banded', 'blotchy', 'braided', 'bubbly', 'bumpy', 'chequered', 'cobwebbed', 'cracked', 'crosshatched', 'crystalline', 'dotted', 'fibrous', 'flecked', 'freckled', 'frilly', 'gauzy', 'grid', 'grooved', 'honeycombed', 'interlaced', 'knitted', 'lacelike', 'lined', 'marbled']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X banded texture.', 'X X X X blotchy texture.', 'X X X X braided texture.', 'X X X X bubbly texture.', 'X X X X bumpy texture.', 'X X X X chequered texture.', 'X X X X cobwebbed texture.', 'X X X X cracked texture.', 'X X X X crosshatched texture.', 'X X X X crystalline texture.', 'X X X X dotted texture.', 'X X X X fibrous texture.', 'X X X X flecked texture.', 'X X X X freckled texture.', 'X X X X frilly texture.', 'X X X X gauzy texture.', 'X X X X grid texture.', 'X X X X grooved texture.', 'X X X X honeycombed texture.', 'X X X X interlaced texture.', 'X X X X knitted texture.', 'X X X X lacelike texture.', 'X X X X lined texture.', 'X X X X marbled texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([24, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/12] time 0.143 (0.300) data 0.000 (0.147) loss 5.5156 (5.7742) acc 50.0000 (44.3750) lr 1.0000e-05 eta 0:02:58
epoch [1/50] batch [10/12] time 0.144 (0.222) data 0.000 (0.074) loss 6.0859 (5.8352) acc 43.7500 (45.6250) lr 1.0000e-05 eta 0:02:11
epoch [2/50] batch [5/12] time 0.144 (0.298) data 0.000 (0.151) loss 4.3125 (4.7094) acc 56.2500 (56.2500) lr 2.0000e-03 eta 0:02:53
epoch [2/50] batch [10/12] time 0.144 (0.221) data 0.000 (0.076) loss 4.2148 (4.3164) acc 46.8750 (57.5000) lr 2.0000e-03 eta 0:02:07
epoch [3/50] batch [5/12] time 0.145 (0.286) data 0.000 (0.141) loss 3.5898 (3.4406) acc 68.7500 (66.2500) lr 1.9980e-03 eta 0:02:43
epoch [3/50] batch [10/12] time 0.146 (0.216) data 0.000 (0.071) loss 2.6719 (3.1973) acc 71.8750 (68.1250) lr 1.9980e-03 eta 0:02:01
epoch [4/50] batch [5/12] time 0.144 (0.274) data 0.000 (0.128) loss 2.7578 (2.8512) acc 71.8750 (73.7500) lr 1.9921e-03 eta 0:02:33
epoch [4/50] batch [10/12] time 0.144 (0.209) data 0.000 (0.064) loss 2.2871 (2.7678) acc 84.3750 (75.6250) lr 1.9921e-03 eta 0:01:55
epoch [5/50] batch [5/12] time 0.143 (0.276) data 0.000 (0.131) loss 2.3750 (2.5137) acc 81.2500 (76.8750) lr 1.9823e-03 eta 0:02:31
epoch [5/50] batch [10/12] time 0.144 (0.210) data 0.000 (0.066) loss 2.2598 (2.4541) acc 84.3750 (80.0000) lr 1.9823e-03 eta 0:01:53
epoch [6/50] batch [5/12] time 0.143 (0.284) data 0.000 (0.139) loss 2.3789 (2.2848) acc 78.1250 (83.7500) lr 1.9686e-03 eta 0:02:32
epoch [6/50] batch [10/12] time 0.143 (0.214) data 0.000 (0.070) loss 2.6758 (2.3494) acc 78.1250 (82.1875) lr 1.9686e-03 eta 0:01:53
epoch [7/50] batch [5/12] time 0.145 (0.295) data 0.000 (0.149) loss 2.3555 (2.2574) acc 81.2500 (85.6250) lr 1.9511e-03 eta 0:02:34
epoch [7/50] batch [10/12] time 0.143 (0.220) data 0.000 (0.075) loss 2.4648 (2.2372) acc 84.3750 (86.2500) lr 1.9511e-03 eta 0:01:54
epoch [8/50] batch [5/12] time 0.143 (0.282) data 0.000 (0.137) loss 2.1836 (2.2111) acc 84.3750 (83.1250) lr 1.9298e-03 eta 0:02:24
epoch [8/50] batch [10/12] time 0.145 (0.213) data 0.000 (0.069) loss 2.4062 (2.1548) acc 81.2500 (85.6250) lr 1.9298e-03 eta 0:01:47
epoch [9/50] batch [5/12] time 0.144 (0.282) data 0.000 (0.137) loss 1.8848 (2.1197) acc 100.0000 (87.5000) lr 1.9048e-03 eta 0:02:20
epoch [9/50] batch [10/12] time 0.144 (0.214) data 0.000 (0.068) loss 1.6680 (2.0524) acc 90.6250 (88.4375) lr 1.9048e-03 eta 0:01:45
epoch [10/50] batch [5/12] time 0.145 (0.269) data 0.000 (0.123) loss 1.9297 (2.0178) acc 87.5000 (88.7500) lr 1.8763e-03 eta 0:02:11
epoch [10/50] batch [10/12] time 0.146 (0.207) data 0.000 (0.062) loss 2.2031 (1.9575) acc 81.2500 (89.6875) lr 1.8763e-03 eta 0:01:39
epoch [11/50] batch [5/12] time 0.145 (0.272) data 0.000 (0.127) loss 2.0312 (1.9088) acc 78.1250 (86.8750) lr 1.8443e-03 eta 0:02:09
epoch [11/50] batch [10/12] time 0.143 (0.208) data 0.000 (0.063) loss 2.0527 (2.0149) acc 90.6250 (85.6250) lr 1.8443e-03 eta 0:01:37
epoch [12/50] batch [5/12] time 0.145 (0.286) data 0.000 (0.140) loss 1.7021 (1.9109) acc 93.7500 (91.2500) lr 1.8090e-03 eta 0:02:12
epoch [12/50] batch [10/12] time 0.144 (0.215) data 0.000 (0.070) loss 2.1230 (1.9329) acc 81.2500 (89.0625) lr 1.8090e-03 eta 0:01:38
epoch [13/50] batch [5/12] time 0.146 (0.295) data 0.000 (0.150) loss 1.8760 (1.7270) acc 93.7500 (92.5000) lr 1.7705e-03 eta 0:02:12
epoch [13/50] batch [10/12] time 0.145 (0.220) data 0.000 (0.075) loss 1.5967 (1.8438) acc 100.0000 (90.3125) lr 1.7705e-03 eta 0:01:38
epoch [14/50] batch [5/12] time 0.146 (0.282) data 0.000 (0.135) loss 2.0020 (1.7973) acc 96.8750 (94.3750) lr 1.7290e-03 eta 0:02:03
epoch [14/50] batch [10/12] time 0.146 (0.213) data 0.000 (0.068) loss 1.9824 (1.7862) acc 87.5000 (93.4375) lr 1.7290e-03 eta 0:01:32
epoch [15/50] batch [5/12] time 0.144 (0.273) data 0.000 (0.127) loss 2.0020 (1.7744) acc 84.3750 (91.8750) lr 1.6845e-03 eta 0:01:56
epoch [15/50] batch [10/12] time 0.145 (0.209) data 0.000 (0.064) loss 1.6562 (1.7763) acc 96.8750 (91.8750) lr 1.6845e-03 eta 0:01:28
epoch [16/50] batch [5/12] time 0.144 (0.281) data 0.000 (0.136) loss 1.6348 (1.6846) acc 93.7500 (91.8750) lr 1.6374e-03 eta 0:01:56
epoch [16/50] batch [10/12] time 0.145 (0.213) data 0.000 (0.068) loss 1.5625 (1.6894) acc 96.8750 (91.5625) lr 1.6374e-03 eta 0:01:27
epoch [17/50] batch [5/12] time 0.145 (0.305) data 0.000 (0.158) loss 1.6582 (1.6621) acc 96.8750 (93.7500) lr 1.5878e-03 eta 0:02:02
epoch [17/50] batch [10/12] time 0.145 (0.225) data 0.000 (0.079) loss 1.7910 (1.6731) acc 90.6250 (91.8750) lr 1.5878e-03 eta 0:01:29
epoch [18/50] batch [5/12] time 0.145 (0.276) data 0.000 (0.130) loss 1.6934 (1.7365) acc 90.6250 (93.1250) lr 1.5358e-03 eta 0:01:47
epoch [18/50] batch [10/12] time 0.145 (0.210) data 0.000 (0.065) loss 1.6416 (1.7111) acc 87.5000 (92.5000) lr 1.5358e-03 eta 0:01:21
epoch [19/50] batch [5/12] time 0.146 (0.305) data 0.000 (0.160) loss 2.0410 (1.6549) acc 81.2500 (93.7500) lr 1.4818e-03 eta 0:01:55
epoch [19/50] batch [10/12] time 0.146 (0.225) data 0.000 (0.080) loss 1.6309 (1.6996) acc 84.3750 (91.5625) lr 1.4818e-03 eta 0:01:24
epoch [20/50] batch [5/12] time 0.145 (0.281) data 0.000 (0.134) loss 1.7373 (1.7328) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:01:42
epoch [20/50] batch [10/12] time 0.145 (0.213) data 0.000 (0.067) loss 1.3330 (1.6307) acc 96.8750 (93.1250) lr 1.4258e-03 eta 0:01:16
epoch [21/50] batch [5/12] time 0.149 (0.292) data 0.000 (0.145) loss 1.6709 (1.6467) acc 96.8750 (94.3750) lr 1.3681e-03 eta 0:01:43
epoch [21/50] batch [10/12] time 0.148 (0.219) data 0.000 (0.073) loss 1.6377 (1.5811) acc 93.7500 (94.3750) lr 1.3681e-03 eta 0:01:16
epoch [22/50] batch [5/12] time 0.144 (0.271) data 0.000 (0.126) loss 1.7070 (1.5969) acc 90.6250 (95.0000) lr 1.3090e-03 eta 0:01:32
epoch [22/50] batch [10/12] time 0.144 (0.208) data 0.000 (0.063) loss 1.7676 (1.6211) acc 84.3750 (92.8125) lr 1.3090e-03 eta 0:01:10
epoch [23/50] batch [5/12] time 0.145 (0.302) data 0.000 (0.155) loss 1.4287 (1.5533) acc 96.8750 (96.2500) lr 1.2487e-03 eta 0:01:39
epoch [23/50] batch [10/12] time 0.146 (0.223) data 0.000 (0.078) loss 1.4414 (1.5908) acc 100.0000 (95.0000) lr 1.2487e-03 eta 0:01:12
epoch [24/50] batch [5/12] time 0.145 (0.291) data 0.000 (0.145) loss 1.5264 (1.5666) acc 93.7500 (95.6250) lr 1.1874e-03 eta 0:01:32
epoch [24/50] batch [10/12] time 0.146 (0.219) data 0.000 (0.073) loss 1.6777 (1.5390) acc 84.3750 (94.6875) lr 1.1874e-03 eta 0:01:08
epoch [25/50] batch [5/12] time 0.146 (0.271) data 0.000 (0.125) loss 1.5547 (1.4867) acc 87.5000 (95.0000) lr 1.1253e-03 eta 0:01:23
epoch [25/50] batch [10/12] time 0.145 (0.208) data 0.000 (0.062) loss 1.5264 (1.5541) acc 96.8750 (94.3750) lr 1.1253e-03 eta 0:01:02
epoch [26/50] batch [5/12] time 0.148 (0.284) data 0.000 (0.138) loss 1.3779 (1.4541) acc 96.8750 (96.8750) lr 1.0628e-03 eta 0:01:23
epoch [26/50] batch [10/12] time 0.147 (0.216) data 0.000 (0.069) loss 1.4678 (1.5094) acc 96.8750 (96.5625) lr 1.0628e-03 eta 0:01:02
epoch [27/50] batch [5/12] time 0.144 (0.269) data 0.000 (0.125) loss 1.5693 (1.4205) acc 93.7500 (96.2500) lr 1.0000e-03 eta 0:01:16
epoch [27/50] batch [10/12] time 0.144 (0.206) data 0.000 (0.063) loss 1.6904 (1.5035) acc 84.3750 (94.6875) lr 1.0000e-03 eta 0:00:57
epoch [28/50] batch [5/12] time 0.145 (0.286) data 0.000 (0.141) loss 1.7559 (1.4984) acc 96.8750 (97.5000) lr 9.3721e-04 eta 0:01:17
epoch [28/50] batch [10/12] time 0.145 (0.216) data 0.000 (0.071) loss 1.3828 (1.5375) acc 100.0000 (96.5625) lr 9.3721e-04 eta 0:00:57
epoch [29/50] batch [5/12] time 0.144 (0.273) data 0.000 (0.127) loss 1.6641 (1.4316) acc 90.6250 (96.8750) lr 8.7467e-04 eta 0:01:10
epoch [29/50] batch [10/12] time 0.144 (0.209) data 0.000 (0.063) loss 1.4414 (1.4703) acc 96.8750 (96.2500) lr 8.7467e-04 eta 0:00:53
epoch [30/50] batch [5/12] time 0.145 (0.300) data 0.000 (0.154) loss 1.6494 (1.4787) acc 96.8750 (97.5000) lr 8.1262e-04 eta 0:01:14
epoch [30/50] batch [10/12] time 0.144 (0.222) data 0.000 (0.077) loss 1.3008 (1.4724) acc 100.0000 (96.2500) lr 8.1262e-04 eta 0:00:53
epoch [31/50] batch [5/12] time 0.145 (0.280) data 0.000 (0.134) loss 1.8086 (1.5900) acc 90.6250 (95.0000) lr 7.5131e-04 eta 0:01:05
epoch [31/50] batch [10/12] time 0.145 (0.212) data 0.000 (0.067) loss 1.4277 (1.5322) acc 96.8750 (95.6250) lr 7.5131e-04 eta 0:00:48
epoch [32/50] batch [5/12] time 0.145 (0.301) data 0.000 (0.155) loss 1.4287 (1.4555) acc 100.0000 (96.8750) lr 6.9098e-04 eta 0:01:07
epoch [32/50] batch [10/12] time 0.144 (0.223) data 0.000 (0.078) loss 1.3633 (1.4675) acc 96.8750 (96.2500) lr 6.9098e-04 eta 0:00:48
epoch [33/50] batch [5/12] time 0.144 (0.274) data 0.000 (0.129) loss 1.5195 (1.4557) acc 96.8750 (96.8750) lr 6.3188e-04 eta 0:00:57
epoch [33/50] batch [10/12] time 0.145 (0.209) data 0.000 (0.065) loss 1.3750 (1.4251) acc 100.0000 (97.5000) lr 6.3188e-04 eta 0:00:43
epoch [34/50] batch [5/12] time 0.145 (0.280) data 0.000 (0.133) loss 1.1484 (1.4502) acc 96.8750 (96.2500) lr 5.7422e-04 eta 0:00:55
epoch [34/50] batch [10/12] time 0.148 (0.213) data 0.000 (0.067) loss 1.3018 (1.4407) acc 96.8750 (96.5625) lr 5.7422e-04 eta 0:00:41
epoch [35/50] batch [5/12] time 0.144 (0.267) data 0.000 (0.122) loss 1.3496 (1.5207) acc 100.0000 (96.2500) lr 5.1825e-04 eta 0:00:49
epoch [35/50] batch [10/12] time 0.144 (0.205) data 0.000 (0.061) loss 1.4531 (1.4788) acc 93.7500 (96.2500) lr 5.1825e-04 eta 0:00:37
epoch [36/50] batch [5/12] time 0.143 (0.271) data 0.000 (0.126) loss 1.3926 (1.5033) acc 93.7500 (94.3750) lr 4.6417e-04 eta 0:00:47
epoch [36/50] batch [10/12] time 0.145 (0.207) data 0.000 (0.063) loss 1.4697 (1.4627) acc 93.7500 (95.0000) lr 4.6417e-04 eta 0:00:35
epoch [37/50] batch [5/12] time 0.143 (0.271) data 0.000 (0.127) loss 1.5654 (1.4979) acc 90.6250 (95.6250) lr 4.1221e-04 eta 0:00:44
epoch [37/50] batch [10/12] time 0.143 (0.207) data 0.000 (0.063) loss 1.4238 (1.4237) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:00:32
epoch [38/50] batch [5/12] time 0.143 (0.294) data 0.000 (0.149) loss 1.3076 (1.4209) acc 96.8750 (95.6250) lr 3.6258e-04 eta 0:00:44
epoch [38/50] batch [10/12] time 0.146 (0.219) data 0.002 (0.075) loss 1.4541 (1.4457) acc 96.8750 (95.9375) lr 3.6258e-04 eta 0:00:31
epoch [39/50] batch [5/12] time 0.143 (0.267) data 0.000 (0.123) loss 1.3682 (1.3850) acc 100.0000 (97.5000) lr 3.1545e-04 eta 0:00:37
epoch [39/50] batch [10/12] time 0.144 (0.206) data 0.000 (0.062) loss 1.4717 (1.3600) acc 90.6250 (96.8750) lr 3.1545e-04 eta 0:00:27
epoch [40/50] batch [5/12] time 0.143 (0.273) data 0.000 (0.128) loss 1.6055 (1.3986) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:00:34
epoch [40/50] batch [10/12] time 0.143 (0.208) data 0.000 (0.064) loss 1.2988 (1.4008) acc 100.0000 (96.8750) lr 2.7103e-04 eta 0:00:25
epoch [41/50] batch [5/12] time 0.142 (0.264) data 0.000 (0.120) loss 1.5703 (1.4502) acc 96.8750 (98.7500) lr 2.2949e-04 eta 0:00:30
epoch [41/50] batch [10/12] time 0.144 (0.204) data 0.000 (0.060) loss 1.4727 (1.4388) acc 96.8750 (97.8125) lr 2.2949e-04 eta 0:00:22
epoch [42/50] batch [5/12] time 0.143 (0.274) data 0.000 (0.130) loss 1.4424 (1.4441) acc 100.0000 (97.5000) lr 1.9098e-04 eta 0:00:28
epoch [42/50] batch [10/12] time 0.142 (0.209) data 0.000 (0.065) loss 1.4004 (1.3946) acc 100.0000 (97.8125) lr 1.9098e-04 eta 0:00:20
epoch [43/50] batch [5/12] time 0.144 (0.267) data 0.000 (0.121) loss 1.2852 (1.4352) acc 100.0000 (98.7500) lr 1.5567e-04 eta 0:00:24
epoch [43/50] batch [10/12] time 0.142 (0.206) data 0.000 (0.061) loss 1.5928 (1.4201) acc 93.7500 (98.1250) lr 1.5567e-04 eta 0:00:17
epoch [44/50] batch [5/12] time 0.146 (0.278) data 0.000 (0.132) loss 1.3887 (1.4740) acc 100.0000 (96.2500) lr 1.2369e-04 eta 0:00:21
epoch [44/50] batch [10/12] time 0.147 (0.212) data 0.000 (0.066) loss 1.4521 (1.4738) acc 100.0000 (97.5000) lr 1.2369e-04 eta 0:00:15
epoch [45/50] batch [5/12] time 0.145 (0.287) data 0.000 (0.141) loss 2.0234 (1.4555) acc 90.6250 (96.8750) lr 9.5173e-05 eta 0:00:19
epoch [45/50] batch [10/12] time 0.144 (0.215) data 0.000 (0.071) loss 1.4111 (1.3919) acc 100.0000 (97.8125) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/12] time 0.143 (0.279) data 0.000 (0.135) loss 1.3926 (1.4062) acc 96.8750 (96.2500) lr 7.0224e-05 eta 0:00:15
epoch [46/50] batch [10/12] time 0.147 (0.212) data 0.000 (0.068) loss 1.5527 (1.4055) acc 96.8750 (97.1875) lr 7.0224e-05 eta 0:00:10
epoch [47/50] batch [5/12] time 0.142 (0.277) data 0.000 (0.133) loss 1.5225 (1.3850) acc 90.6250 (96.2500) lr 4.8943e-05 eta 0:00:11
epoch [47/50] batch [10/12] time 0.145 (0.210) data 0.000 (0.066) loss 1.5674 (1.4378) acc 96.8750 (96.2500) lr 4.8943e-05 eta 0:00:07
epoch [48/50] batch [5/12] time 0.143 (0.270) data 0.000 (0.126) loss 1.4795 (1.3893) acc 100.0000 (99.3750) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [10/12] time 0.145 (0.207) data 0.000 (0.063) loss 1.0928 (1.3726) acc 100.0000 (98.7500) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/12] time 0.144 (0.287) data 0.000 (0.143) loss 1.4004 (1.3711) acc 100.0000 (98.1250) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [10/12] time 0.143 (0.215) data 0.000 (0.071) loss 1.5059 (1.3975) acc 96.8750 (97.8125) lr 1.7713e-05 eta 0:00:03
epoch [50/50] batch [5/12] time 0.143 (0.277) data 0.000 (0.133) loss 1.2490 (1.3004) acc 100.0000 (98.7500) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/12] time 0.143 (0.210) data 0.000 (0.067) loss 1.4346 (1.3942) acc 100.0000 (97.8125) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:06<00:06,  6.06s/it]100%|██████████| 2/2 [00:06<00:00,  3.02s/it]100%|██████████| 2/2 [00:07<00:00,  3.53s/it]
=> result
* total: 864
* correct: 729
* accuracy: 84.4%
* error: 15.6%
* macro_f1: 84.3%
Elapsed: 0:02:13
Run this job and save the output to output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  24
# train_x  384
# val      96
# test     864
---------  -------------------
['banded', 'blotchy', 'braided', 'bubbly', 'bumpy', 'chequered', 'cobwebbed', 'cracked', 'crosshatched', 'crystalline', 'dotted', 'fibrous', 'flecked', 'freckled', 'frilly', 'gauzy', 'grid', 'grooved', 'honeycombed', 'interlaced', 'knitted', 'lacelike', 'lined', 'marbled']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X banded texture.', 'X X X X blotchy texture.', 'X X X X braided texture.', 'X X X X bubbly texture.', 'X X X X bumpy texture.', 'X X X X chequered texture.', 'X X X X cobwebbed texture.', 'X X X X cracked texture.', 'X X X X crosshatched texture.', 'X X X X crystalline texture.', 'X X X X dotted texture.', 'X X X X fibrous texture.', 'X X X X flecked texture.', 'X X X X freckled texture.', 'X X X X frilly texture.', 'X X X X gauzy texture.', 'X X X X grid texture.', 'X X X X grooved texture.', 'X X X X honeycombed texture.', 'X X X X interlaced texture.', 'X X X X knitted texture.', 'X X X X lacelike texture.', 'X X X X lined texture.', 'X X X X marbled texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([24, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/12] time 0.145 (1.577) data 0.000 (0.158) loss 6.1875 (6.2945) acc 40.6250 (41.2500) lr 1.0000e-05 eta 0:15:38
epoch [1/50] batch [10/12] time 0.142 (0.861) data 0.000 (0.079) loss 6.2266 (6.3090) acc 37.5000 (40.9375) lr 1.0000e-05 eta 0:08:27
epoch [2/50] batch [5/12] time 0.141 (0.320) data 0.000 (0.176) loss 5.0977 (5.3070) acc 40.6250 (47.5000) lr 2.0000e-03 eta 0:03:06
epoch [2/50] batch [10/12] time 0.142 (0.231) data 0.000 (0.088) loss 3.9727 (4.8617) acc 62.5000 (48.4375) lr 2.0000e-03 eta 0:02:13
epoch [3/50] batch [5/12] time 0.143 (0.295) data 0.000 (0.151) loss 3.1035 (3.2953) acc 75.0000 (70.6250) lr 1.9980e-03 eta 0:02:48
epoch [3/50] batch [10/12] time 0.143 (0.219) data 0.000 (0.075) loss 3.6758 (3.4410) acc 62.5000 (65.3125) lr 1.9980e-03 eta 0:02:03
epoch [4/50] batch [5/12] time 0.145 (0.305) data 0.000 (0.160) loss 2.9102 (3.0129) acc 75.0000 (73.1250) lr 1.9921e-03 eta 0:02:50
epoch [4/50] batch [10/12] time 0.144 (0.224) data 0.000 (0.080) loss 2.7129 (2.9406) acc 71.8750 (73.7500) lr 1.9921e-03 eta 0:02:04
epoch [5/50] batch [5/12] time 0.147 (0.287) data 0.000 (0.139) loss 2.3066 (2.7207) acc 87.5000 (78.7500) lr 1.9823e-03 eta 0:02:36
epoch [5/50] batch [10/12] time 0.147 (0.218) data 0.000 (0.070) loss 2.6523 (2.7049) acc 84.3750 (76.2500) lr 1.9823e-03 eta 0:01:58
epoch [6/50] batch [5/12] time 0.147 (0.296) data 0.000 (0.148) loss 2.5703 (2.5621) acc 78.1250 (81.8750) lr 1.9686e-03 eta 0:02:38
epoch [6/50] batch [10/12] time 0.147 (0.221) data 0.000 (0.074) loss 2.5117 (2.4867) acc 78.1250 (80.0000) lr 1.9686e-03 eta 0:01:57
epoch [7/50] batch [5/12] time 0.144 (0.294) data 0.000 (0.149) loss 2.2148 (2.4016) acc 84.3750 (80.6250) lr 1.9511e-03 eta 0:02:33
epoch [7/50] batch [10/12] time 0.144 (0.219) data 0.000 (0.075) loss 2.4336 (2.3770) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:01:53
epoch [8/50] batch [5/12] time 0.143 (0.294) data 0.000 (0.150) loss 2.5430 (2.2160) acc 78.1250 (82.5000) lr 1.9298e-03 eta 0:02:30
epoch [8/50] batch [10/12] time 0.144 (0.219) data 0.000 (0.075) loss 2.0293 (2.2186) acc 90.6250 (85.3125) lr 1.9298e-03 eta 0:01:50
epoch [9/50] batch [5/12] time 0.145 (0.319) data 0.000 (0.173) loss 1.8613 (2.1191) acc 90.6250 (84.3750) lr 1.9048e-03 eta 0:02:39
epoch [9/50] batch [10/12] time 0.144 (0.232) data 0.000 (0.087) loss 2.0039 (2.0998) acc 90.6250 (85.9375) lr 1.9048e-03 eta 0:01:54
epoch [10/50] batch [5/12] time 0.142 (0.288) data 0.000 (0.144) loss 2.2969 (1.9838) acc 84.3750 (88.7500) lr 1.8763e-03 eta 0:02:20
epoch [10/50] batch [10/12] time 0.145 (0.216) data 0.000 (0.072) loss 1.9561 (1.9210) acc 84.3750 (90.9375) lr 1.8763e-03 eta 0:01:44
epoch [11/50] batch [5/12] time 0.145 (0.289) data 0.000 (0.145) loss 1.9717 (1.9205) acc 87.5000 (88.7500) lr 1.8443e-03 eta 0:02:17
epoch [11/50] batch [10/12] time 0.145 (0.217) data 0.000 (0.073) loss 1.9971 (1.9680) acc 87.5000 (89.0625) lr 1.8443e-03 eta 0:01:41
epoch [12/50] batch [5/12] time 0.146 (0.293) data 0.000 (0.148) loss 2.0625 (2.0352) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:02:15
epoch [12/50] batch [10/12] time 0.144 (0.218) data 0.000 (0.074) loss 1.9307 (1.9585) acc 87.5000 (90.0000) lr 1.8090e-03 eta 0:01:39
epoch [13/50] batch [5/12] time 0.144 (0.323) data 0.000 (0.179) loss 2.0859 (1.9033) acc 87.5000 (90.6250) lr 1.7705e-03 eta 0:02:25
epoch [13/50] batch [10/12] time 0.145 (0.233) data 0.000 (0.089) loss 1.7754 (1.8455) acc 96.8750 (91.2500) lr 1.7705e-03 eta 0:01:44
epoch [14/50] batch [5/12] time 0.145 (0.299) data 0.000 (0.155) loss 1.5430 (1.7629) acc 93.7500 (91.8750) lr 1.7290e-03 eta 0:02:11
epoch [14/50] batch [10/12] time 0.141 (0.221) data 0.000 (0.078) loss 1.6504 (1.7797) acc 90.6250 (90.3125) lr 1.7290e-03 eta 0:01:35
epoch [15/50] batch [5/12] time 0.143 (0.309) data 0.000 (0.164) loss 1.6914 (1.7543) acc 93.7500 (90.6250) lr 1.6845e-03 eta 0:02:11
epoch [15/50] batch [10/12] time 0.145 (0.226) data 0.000 (0.082) loss 1.6221 (1.7684) acc 96.8750 (90.3125) lr 1.6845e-03 eta 0:01:35
epoch [16/50] batch [5/12] time 0.142 (0.298) data 0.000 (0.153) loss 1.4326 (1.5635) acc 96.8750 (96.2500) lr 1.6374e-03 eta 0:02:03
epoch [16/50] batch [10/12] time 0.143 (0.220) data 0.000 (0.077) loss 2.2227 (1.7144) acc 81.2500 (93.1250) lr 1.6374e-03 eta 0:01:30
epoch [17/50] batch [5/12] time 0.144 (0.293) data 0.000 (0.149) loss 1.6211 (1.8236) acc 96.8750 (89.3750) lr 1.5878e-03 eta 0:01:58
epoch [17/50] batch [10/12] time 0.146 (0.218) data 0.000 (0.075) loss 2.1094 (1.7232) acc 81.2500 (91.2500) lr 1.5878e-03 eta 0:01:26
epoch [18/50] batch [5/12] time 0.145 (0.303) data 0.000 (0.159) loss 1.9863 (1.7416) acc 90.6250 (92.5000) lr 1.5358e-03 eta 0:01:58
epoch [18/50] batch [10/12] time 0.146 (0.223) data 0.000 (0.079) loss 1.4980 (1.6466) acc 100.0000 (94.6875) lr 1.5358e-03 eta 0:01:26
epoch [19/50] batch [5/12] time 0.145 (0.295) data 0.000 (0.151) loss 1.6816 (1.6855) acc 93.7500 (93.1250) lr 1.4818e-03 eta 0:01:51
epoch [19/50] batch [10/12] time 0.146 (0.219) data 0.000 (0.075) loss 1.5293 (1.6228) acc 96.8750 (93.4375) lr 1.4818e-03 eta 0:01:22
epoch [20/50] batch [5/12] time 0.144 (0.293) data 0.000 (0.147) loss 1.5742 (1.6875) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:01:47
epoch [20/50] batch [10/12] time 0.146 (0.219) data 0.000 (0.074) loss 1.5459 (1.6762) acc 93.7500 (94.0625) lr 1.4258e-03 eta 0:01:19
epoch [21/50] batch [5/12] time 0.143 (0.292) data 0.000 (0.148) loss 1.8027 (1.6645) acc 90.6250 (95.6250) lr 1.3681e-03 eta 0:01:43
epoch [21/50] batch [10/12] time 0.144 (0.217) data 0.000 (0.074) loss 1.5947 (1.5997) acc 93.7500 (94.6875) lr 1.3681e-03 eta 0:01:16
epoch [22/50] batch [5/12] time 0.143 (0.292) data 0.000 (0.147) loss 1.6406 (1.6441) acc 90.6250 (93.1250) lr 1.3090e-03 eta 0:01:40
epoch [22/50] batch [10/12] time 0.142 (0.218) data 0.000 (0.074) loss 1.4395 (1.6296) acc 96.8750 (94.6875) lr 1.3090e-03 eta 0:01:13
epoch [23/50] batch [5/12] time 0.143 (0.304) data 0.000 (0.160) loss 1.2773 (1.5494) acc 96.8750 (96.2500) lr 1.2487e-03 eta 0:01:40
epoch [23/50] batch [10/12] time 0.143 (0.224) data 0.000 (0.080) loss 1.6543 (1.6209) acc 96.8750 (95.0000) lr 1.2487e-03 eta 0:01:12
epoch [24/50] batch [5/12] time 0.143 (0.309) data 0.000 (0.165) loss 1.3125 (1.5754) acc 96.8750 (93.7500) lr 1.1874e-03 eta 0:01:38
epoch [24/50] batch [10/12] time 0.142 (0.226) data 0.000 (0.083) loss 1.5459 (1.5754) acc 96.8750 (94.0625) lr 1.1874e-03 eta 0:01:10
epoch [25/50] batch [5/12] time 0.144 (0.292) data 0.000 (0.147) loss 1.7422 (1.5680) acc 90.6250 (94.3750) lr 1.1253e-03 eta 0:01:29
epoch [25/50] batch [10/12] time 0.143 (0.218) data 0.000 (0.073) loss 1.3457 (1.5133) acc 93.7500 (95.6250) lr 1.1253e-03 eta 0:01:05
epoch [26/50] batch [5/12] time 0.146 (0.305) data 0.000 (0.159) loss 1.4844 (1.4686) acc 96.8750 (96.2500) lr 1.0628e-03 eta 0:01:29
epoch [26/50] batch [10/12] time 0.143 (0.224) data 0.000 (0.080) loss 1.4023 (1.5145) acc 100.0000 (95.9375) lr 1.0628e-03 eta 0:01:04
epoch [27/50] batch [5/12] time 0.145 (0.301) data 0.000 (0.157) loss 1.6484 (1.5895) acc 93.7500 (95.6250) lr 1.0000e-03 eta 0:01:25
epoch [27/50] batch [10/12] time 0.145 (0.222) data 0.000 (0.078) loss 1.4375 (1.5484) acc 100.0000 (96.5625) lr 1.0000e-03 eta 0:01:01
epoch [28/50] batch [5/12] time 0.145 (0.299) data 0.000 (0.154) loss 1.6758 (1.5682) acc 93.7500 (95.0000) lr 9.3721e-04 eta 0:01:20
epoch [28/50] batch [10/12] time 0.146 (0.221) data 0.000 (0.077) loss 1.7666 (1.5714) acc 90.6250 (94.3750) lr 9.3721e-04 eta 0:00:58
epoch [29/50] batch [5/12] time 0.144 (0.306) data 0.000 (0.160) loss 1.6631 (1.5217) acc 90.6250 (95.0000) lr 8.7467e-04 eta 0:01:19
epoch [29/50] batch [10/12] time 0.145 (0.225) data 0.000 (0.080) loss 1.6562 (1.5553) acc 93.7500 (95.6250) lr 8.7467e-04 eta 0:00:57
epoch [30/50] batch [5/12] time 0.144 (0.294) data 0.000 (0.150) loss 1.4912 (1.4793) acc 93.7500 (96.2500) lr 8.1262e-04 eta 0:01:12
epoch [30/50] batch [10/12] time 0.146 (0.219) data 0.000 (0.075) loss 1.3135 (1.4853) acc 100.0000 (96.8750) lr 8.1262e-04 eta 0:00:53
epoch [31/50] batch [5/12] time 0.145 (0.297) data 0.000 (0.152) loss 1.6680 (1.4262) acc 100.0000 (98.7500) lr 7.5131e-04 eta 0:01:09
epoch [31/50] batch [10/12] time 0.144 (0.220) data 0.000 (0.076) loss 1.5518 (1.4513) acc 96.8750 (98.1250) lr 7.5131e-04 eta 0:00:50
epoch [32/50] batch [5/12] time 0.144 (0.304) data 0.000 (0.159) loss 1.4639 (1.5037) acc 100.0000 (96.8750) lr 6.9098e-04 eta 0:01:07
epoch [32/50] batch [10/12] time 0.144 (0.224) data 0.000 (0.080) loss 1.2949 (1.4863) acc 100.0000 (97.5000) lr 6.9098e-04 eta 0:00:48
epoch [33/50] batch [5/12] time 0.146 (0.290) data 0.000 (0.143) loss 1.6592 (1.4746) acc 90.6250 (96.8750) lr 6.3188e-04 eta 0:01:01
epoch [33/50] batch [10/12] time 0.146 (0.218) data 0.000 (0.072) loss 1.5908 (1.5148) acc 100.0000 (96.2500) lr 6.3188e-04 eta 0:00:44
epoch [34/50] batch [5/12] time 0.143 (0.285) data 0.000 (0.140) loss 1.5068 (1.5152) acc 90.6250 (95.0000) lr 5.7422e-04 eta 0:00:56
epoch [34/50] batch [10/12] time 0.143 (0.214) data 0.000 (0.070) loss 1.3057 (1.4734) acc 100.0000 (96.5625) lr 5.7422e-04 eta 0:00:41
epoch [35/50] batch [5/12] time 0.143 (0.303) data 0.000 (0.159) loss 1.4209 (1.4053) acc 100.0000 (98.1250) lr 5.1825e-04 eta 0:00:56
epoch [35/50] batch [10/12] time 0.144 (0.223) data 0.000 (0.079) loss 1.3828 (1.4814) acc 96.8750 (96.5625) lr 5.1825e-04 eta 0:00:40
epoch [36/50] batch [5/12] time 0.143 (0.297) data 0.000 (0.153) loss 1.5098 (1.5441) acc 100.0000 (94.3750) lr 4.6417e-04 eta 0:00:52
epoch [36/50] batch [10/12] time 0.142 (0.220) data 0.000 (0.077) loss 1.3555 (1.4586) acc 96.8750 (95.9375) lr 4.6417e-04 eta 0:00:37
epoch [37/50] batch [5/12] time 0.145 (0.304) data 0.000 (0.160) loss 1.2314 (1.4412) acc 96.8750 (97.5000) lr 4.1221e-04 eta 0:00:49
epoch [37/50] batch [10/12] time 0.144 (0.224) data 0.000 (0.080) loss 1.4727 (1.4525) acc 96.8750 (97.8125) lr 4.1221e-04 eta 0:00:35
epoch [38/50] batch [5/12] time 0.146 (0.295) data 0.000 (0.149) loss 1.3027 (1.4857) acc 96.8750 (96.2500) lr 3.6258e-04 eta 0:00:44
epoch [38/50] batch [10/12] time 0.143 (0.220) data 0.000 (0.075) loss 1.4062 (1.4886) acc 96.8750 (96.5625) lr 3.6258e-04 eta 0:00:32
epoch [39/50] batch [5/12] time 0.144 (0.305) data 0.000 (0.160) loss 1.5664 (1.4703) acc 100.0000 (98.7500) lr 3.1545e-04 eta 0:00:42
epoch [39/50] batch [10/12] time 0.144 (0.224) data 0.000 (0.080) loss 1.3867 (1.4646) acc 100.0000 (98.4375) lr 3.1545e-04 eta 0:00:30
epoch [40/50] batch [5/12] time 0.144 (0.294) data 0.000 (0.149) loss 1.3008 (1.3770) acc 100.0000 (98.1250) lr 2.7103e-04 eta 0:00:37
epoch [40/50] batch [10/12] time 0.143 (0.218) data 0.000 (0.074) loss 1.2715 (1.3913) acc 96.8750 (97.8125) lr 2.7103e-04 eta 0:00:26
epoch [41/50] batch [5/12] time 0.143 (0.295) data 0.000 (0.150) loss 1.2549 (1.4023) acc 100.0000 (98.1250) lr 2.2949e-04 eta 0:00:33
epoch [41/50] batch [10/12] time 0.143 (0.219) data 0.000 (0.075) loss 1.6758 (1.3822) acc 93.7500 (98.1250) lr 2.2949e-04 eta 0:00:24
epoch [42/50] batch [5/12] time 0.144 (0.298) data 0.000 (0.153) loss 1.5078 (1.4785) acc 96.8750 (97.5000) lr 1.9098e-04 eta 0:00:30
epoch [42/50] batch [10/12] time 0.144 (0.221) data 0.000 (0.077) loss 1.2920 (1.4412) acc 100.0000 (97.8125) lr 1.9098e-04 eta 0:00:21
epoch [43/50] batch [5/12] time 0.144 (0.292) data 0.000 (0.148) loss 1.2842 (1.4781) acc 96.8750 (96.2500) lr 1.5567e-04 eta 0:00:26
epoch [43/50] batch [10/12] time 0.144 (0.218) data 0.000 (0.074) loss 1.5166 (1.4771) acc 96.8750 (96.8750) lr 1.5567e-04 eta 0:00:18
epoch [44/50] batch [5/12] time 0.142 (0.297) data 0.000 (0.152) loss 1.3652 (1.4318) acc 93.7500 (97.5000) lr 1.2369e-04 eta 0:00:23
epoch [44/50] batch [10/12] time 0.145 (0.221) data 0.000 (0.076) loss 1.2793 (1.3935) acc 100.0000 (96.5625) lr 1.2369e-04 eta 0:00:16
epoch [45/50] batch [5/12] time 0.143 (0.286) data 0.000 (0.141) loss 1.2949 (1.4297) acc 96.8750 (97.5000) lr 9.5173e-05 eta 0:00:19
epoch [45/50] batch [10/12] time 0.142 (0.215) data 0.000 (0.071) loss 1.4131 (1.4392) acc 96.8750 (97.5000) lr 9.5173e-05 eta 0:00:13
epoch [46/50] batch [5/12] time 0.147 (0.298) data 0.000 (0.150) loss 1.4600 (1.5344) acc 100.0000 (96.2500) lr 7.0224e-05 eta 0:00:16
epoch [46/50] batch [10/12] time 0.147 (0.223) data 0.000 (0.075) loss 1.1973 (1.4340) acc 100.0000 (96.8750) lr 7.0224e-05 eta 0:00:11
epoch [47/50] batch [5/12] time 0.147 (0.291) data 0.000 (0.143) loss 1.3652 (1.3582) acc 96.8750 (98.1250) lr 4.8943e-05 eta 0:00:12
epoch [47/50] batch [10/12] time 0.138 (0.218) data 0.000 (0.071) loss 1.6338 (1.3759) acc 93.7500 (97.8125) lr 4.8943e-05 eta 0:00:08
epoch [48/50] batch [5/12] time 0.145 (0.289) data 0.000 (0.144) loss 1.3613 (1.4998) acc 100.0000 (95.0000) lr 3.1417e-05 eta 0:00:08
epoch [48/50] batch [10/12] time 0.145 (0.217) data 0.000 (0.072) loss 1.5020 (1.4776) acc 100.0000 (96.5625) lr 3.1417e-05 eta 0:00:05
epoch [49/50] batch [5/12] time 0.143 (0.288) data 0.000 (0.143) loss 1.3184 (1.3938) acc 96.8750 (96.2500) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [10/12] time 0.144 (0.216) data 0.000 (0.072) loss 1.6289 (1.4393) acc 93.7500 (97.5000) lr 1.7713e-05 eta 0:00:03
epoch [50/50] batch [5/12] time 0.143 (0.298) data 0.000 (0.154) loss 1.2090 (1.4629) acc 100.0000 (98.7500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [10/12] time 0.145 (0.221) data 0.000 (0.077) loss 1.4580 (1.4069) acc 100.0000 (98.4375) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:05<00:05,  5.32s/it]100%|██████████| 2/2 [00:06<00:00,  2.71s/it]100%|██████████| 2/2 [00:06<00:00,  3.15s/it]
=> result
* total: 864
* correct: 721
* accuracy: 83.4%
* error: 16.6%
* macro_f1: 83.3%
Elapsed: 0:02:23
Run this job and save the output to output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      92
# test     828
---------  -------------------
['matted', 'meshed', 'paisley', 'perforated', 'pitted', 'pleated', 'polka-dotted', 'porous', 'potholed', 'scaly', 'smeared', 'spiralled', 'sprinkled', 'stained', 'stratified', 'striped', 'studded', 'swirly', 'veined', 'waffled', 'woven', 'wrinkled', 'zigzagged']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X matted texture.', 'X X X X meshed texture.', 'X X X X paisley texture.', 'X X X X perforated texture.', 'X X X X pitted texture.', 'X X X X pleated texture.', 'X X X X polka-dotted texture.', 'X X X X porous texture.', 'X X X X potholed texture.', 'X X X X scaly texture.', 'X X X X smeared texture.', 'X X X X spiralled texture.', 'X X X X sprinkled texture.', 'X X X X stained texture.', 'X X X X stratified texture.', 'X X X X striped texture.', 'X X X X studded texture.', 'X X X X swirly texture.', 'X X X X veined texture.', 'X X X X waffled texture.', 'X X X X woven texture.', 'X X X X wrinkled texture.', 'X X X X zigzagged texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([23, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:05<00:05,  5.19s/it]100%|██████████| 2/2 [00:05<00:00,  2.61s/it]100%|██████████| 2/2 [00:06<00:00,  3.04s/it]
=> result
* total: 828
* correct: 507
* accuracy: 61.2%
* error: 38.8%
* macro_f1: 60.3%
Run this job and save the output to output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      92
# test     828
---------  -------------------
['matted', 'meshed', 'paisley', 'perforated', 'pitted', 'pleated', 'polka-dotted', 'porous', 'potholed', 'scaly', 'smeared', 'spiralled', 'sprinkled', 'stained', 'stratified', 'striped', 'studded', 'swirly', 'veined', 'waffled', 'woven', 'wrinkled', 'zigzagged']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X matted texture.', 'X X X X meshed texture.', 'X X X X paisley texture.', 'X X X X perforated texture.', 'X X X X pitted texture.', 'X X X X pleated texture.', 'X X X X polka-dotted texture.', 'X X X X porous texture.', 'X X X X potholed texture.', 'X X X X scaly texture.', 'X X X X smeared texture.', 'X X X X spiralled texture.', 'X X X X sprinkled texture.', 'X X X X stained texture.', 'X X X X stratified texture.', 'X X X X striped texture.', 'X X X X studded texture.', 'X X X X swirly texture.', 'X X X X veined texture.', 'X X X X waffled texture.', 'X X X X woven texture.', 'X X X X wrinkled texture.', 'X X X X zigzagged texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([23, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:05<00:05,  5.27s/it]100%|██████████| 2/2 [00:06<00:00,  2.64s/it]100%|██████████| 2/2 [00:06<00:00,  3.08s/it]
=> result
* total: 828
* correct: 516
* accuracy: 62.3%
* error: 37.7%
* macro_f1: 61.5%
Run this job and save the output to output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/dtd.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: DescribableTextures
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: DescribableTextures
Reading split from /data/yht/data/cl/data/dtd/split_zhou_DescribableTextures.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/dtd/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------------
Dataset    DescribableTextures
# classes  23
# train_x  368
# val      92
# test     828
---------  -------------------
['matted', 'meshed', 'paisley', 'perforated', 'pitted', 'pleated', 'polka-dotted', 'porous', 'potholed', 'scaly', 'smeared', 'spiralled', 'sprinkled', 'stained', 'stratified', 'striped', 'studded', 'swirly', 'veined', 'waffled', 'woven', 'wrinkled', 'zigzagged']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X matted texture.', 'X X X X meshed texture.', 'X X X X paisley texture.', 'X X X X perforated texture.', 'X X X X pitted texture.', 'X X X X pleated texture.', 'X X X X polka-dotted texture.', 'X X X X porous texture.', 'X X X X potholed texture.', 'X X X X scaly texture.', 'X X X X smeared texture.', 'X X X X spiralled texture.', 'X X X X sprinkled texture.', 'X X X X stained texture.', 'X X X X stratified texture.', 'X X X X striped texture.', 'X X X X studded texture.', 'X X X X swirly texture.', 'X X X X veined texture.', 'X X X X waffled texture.', 'X X X X woven texture.', 'X X X X wrinkled texture.', 'X X X X zigzagged texture.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([23, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/dtd/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:08<00:08,  8.73s/it]100%|██████████| 2/2 [00:09<00:00,  4.07s/it]100%|██████████| 2/2 [00:09<00:00,  4.81s/it]
=> result
* total: 828
* correct: 450
* accuracy: 54.3%
* error: 45.7%
* macro_f1: 51.8%
Run this job and save the output to output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     4,200
---------  -------
['Annual Crop Land', 'Forest', 'Herbaceous Vegetation Land', 'Highway or Road', 'Industrial Buildings']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Annual Crop Land.', 'X X X X Forest.', 'X X X X Herbaceous Vegetation Land.', 'X X X X Highway or Road.', 'X X X X Industrial Buildings.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([5, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [1/3] time 0.782 (0.782) data 0.621 (0.621) loss 5.5430 (5.5430) acc 46.8750 (46.8750) lr 1.0000e-05 eta 0:01:56
epoch [1/50] batch [2/3] time 0.136 (0.459) data 0.000 (0.310) loss 5.6172 (5.5801) acc 40.6250 (43.7500) lr 1.0000e-05 eta 0:01:07
epoch [1/50] batch [3/3] time 0.082 (0.333) data 0.000 (0.207) loss 5.5664 (5.5755) acc 56.2500 (47.9167) lr 2.0000e-03 eta 0:00:48
epoch [2/50] batch [1/3] time 0.715 (0.715) data 0.574 (0.574) loss 5.7656 (5.7656) acc 43.7500 (43.7500) lr 2.0000e-03 eta 0:01:44
epoch [2/50] batch [2/3] time 0.139 (0.427) data 0.000 (0.287) loss 4.4961 (5.1309) acc 71.8750 (57.8125) lr 2.0000e-03 eta 0:01:01
epoch [2/50] batch [3/3] time 0.082 (0.312) data 0.000 (0.191) loss 3.7734 (4.6784) acc 75.0000 (63.5417) lr 1.9980e-03 eta 0:00:44
epoch [3/50] batch [1/3] time 0.730 (0.730) data 0.588 (0.588) loss 3.7031 (3.7031) acc 62.5000 (62.5000) lr 1.9980e-03 eta 0:01:44
epoch [3/50] batch [2/3] time 0.139 (0.434) data 0.000 (0.294) loss 3.6270 (3.6650) acc 59.3750 (60.9375) lr 1.9980e-03 eta 0:01:01
epoch [3/50] batch [3/3] time 0.083 (0.317) data 0.000 (0.196) loss 2.5957 (3.3086) acc 75.0000 (65.6250) lr 1.9921e-03 eta 0:00:44
epoch [4/50] batch [1/3] time 0.738 (0.738) data 0.595 (0.595) loss 2.7266 (2.7266) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:01:43
epoch [4/50] batch [2/3] time 0.138 (0.438) data 0.000 (0.298) loss 2.6172 (2.6719) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:01:00
epoch [4/50] batch [3/3] time 0.084 (0.320) data 0.000 (0.199) loss 1.9404 (2.4281) acc 100.0000 (83.3333) lr 1.9823e-03 eta 0:00:44
epoch [5/50] batch [1/3] time 0.723 (0.723) data 0.582 (0.582) loss 2.2949 (2.2949) acc 75.0000 (75.0000) lr 1.9823e-03 eta 0:01:38
epoch [5/50] batch [2/3] time 0.138 (0.430) data 0.000 (0.291) loss 2.0605 (2.1777) acc 84.3750 (79.6875) lr 1.9823e-03 eta 0:00:58
epoch [5/50] batch [3/3] time 0.083 (0.314) data 0.000 (0.194) loss 2.0352 (2.1302) acc 93.7500 (84.3750) lr 1.9686e-03 eta 0:00:42
epoch [6/50] batch [1/3] time 0.739 (0.739) data 0.601 (0.601) loss 2.0332 (2.0332) acc 75.0000 (75.0000) lr 1.9686e-03 eta 0:01:39
epoch [6/50] batch [2/3] time 0.137 (0.438) data 0.000 (0.301) loss 1.9473 (1.9902) acc 84.3750 (79.6875) lr 1.9686e-03 eta 0:00:58
epoch [6/50] batch [3/3] time 0.083 (0.320) data 0.000 (0.201) loss 2.3223 (2.1009) acc 68.7500 (76.0417) lr 1.9511e-03 eta 0:00:42
epoch [7/50] batch [1/3] time 0.744 (0.744) data 0.601 (0.601) loss 2.0723 (2.0723) acc 75.0000 (75.0000) lr 1.9511e-03 eta 0:01:37
epoch [7/50] batch [2/3] time 0.137 (0.441) data 0.000 (0.301) loss 1.8066 (1.9395) acc 78.1250 (76.5625) lr 1.9511e-03 eta 0:00:57
epoch [7/50] batch [3/3] time 0.084 (0.322) data 0.000 (0.200) loss 1.5352 (1.8047) acc 93.7500 (82.2917) lr 1.9298e-03 eta 0:00:41
epoch [8/50] batch [1/3] time 0.873 (0.873) data 0.731 (0.731) loss 1.9492 (1.9492) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:01:51
epoch [8/50] batch [2/3] time 0.140 (0.506) data 0.000 (0.366) loss 1.6865 (1.8179) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:01:04
epoch [8/50] batch [3/3] time 0.084 (0.366) data 0.000 (0.244) loss 1.7861 (1.8073) acc 81.2500 (81.2500) lr 1.9048e-03 eta 0:00:46
epoch [9/50] batch [1/3] time 1.021 (1.021) data 0.879 (0.879) loss 1.4961 (1.4961) acc 90.6250 (90.6250) lr 1.9048e-03 eta 0:02:07
epoch [9/50] batch [2/3] time 0.136 (0.579) data 0.000 (0.439) loss 1.2988 (1.3975) acc 87.5000 (89.0625) lr 1.9048e-03 eta 0:01:11
epoch [9/50] batch [3/3] time 0.087 (0.415) data 0.000 (0.293) loss 1.7393 (1.5114) acc 87.5000 (88.5417) lr 1.8763e-03 eta 0:00:50
epoch [10/50] batch [1/3] time 0.729 (0.729) data 0.587 (0.587) loss 1.3516 (1.3516) acc 90.6250 (90.6250) lr 1.8763e-03 eta 0:01:28
epoch [10/50] batch [2/3] time 0.142 (0.435) data 0.000 (0.294) loss 1.7559 (1.5537) acc 78.1250 (84.3750) lr 1.8763e-03 eta 0:00:52
epoch [10/50] batch [3/3] time 0.084 (0.318) data 0.000 (0.196) loss 1.2881 (1.4652) acc 93.7500 (87.5000) lr 1.8443e-03 eta 0:00:38
epoch [11/50] batch [1/3] time 0.770 (0.770) data 0.626 (0.626) loss 1.2344 (1.2344) acc 90.6250 (90.6250) lr 1.8443e-03 eta 0:01:31
epoch [11/50] batch [2/3] time 0.138 (0.454) data 0.000 (0.313) loss 1.4551 (1.3447) acc 87.5000 (89.0625) lr 1.8443e-03 eta 0:00:53
epoch [11/50] batch [3/3] time 0.082 (0.330) data 0.000 (0.209) loss 1.1113 (1.2669) acc 93.7500 (90.6250) lr 1.8090e-03 eta 0:00:38
epoch [12/50] batch [1/3] time 0.722 (0.722) data 0.582 (0.582) loss 1.1631 (1.1631) acc 93.7500 (93.7500) lr 1.8090e-03 eta 0:01:23
epoch [12/50] batch [2/3] time 0.138 (0.430) data 0.000 (0.291) loss 1.3809 (1.2720) acc 87.5000 (90.6250) lr 1.8090e-03 eta 0:00:49
epoch [12/50] batch [3/3] time 0.083 (0.314) data 0.000 (0.194) loss 1.1230 (1.2223) acc 93.7500 (91.6667) lr 1.7705e-03 eta 0:00:35
epoch [13/50] batch [1/3] time 0.790 (0.790) data 0.649 (0.649) loss 1.3770 (1.3770) acc 87.5000 (87.5000) lr 1.7705e-03 eta 0:01:29
epoch [13/50] batch [2/3] time 0.137 (0.463) data 0.000 (0.324) loss 1.4219 (1.3994) acc 78.1250 (82.8125) lr 1.7705e-03 eta 0:00:51
epoch [13/50] batch [3/3] time 0.089 (0.339) data 0.001 (0.216) loss 1.1289 (1.3092) acc 93.7500 (86.4583) lr 1.7290e-03 eta 0:00:37
epoch [14/50] batch [1/3] time 0.810 (0.810) data 0.667 (0.667) loss 1.2383 (1.2383) acc 90.6250 (90.6250) lr 1.7290e-03 eta 0:01:29
epoch [14/50] batch [2/3] time 0.140 (0.475) data 0.001 (0.334) loss 1.3652 (1.3018) acc 81.2500 (85.9375) lr 1.7290e-03 eta 0:00:51
epoch [14/50] batch [3/3] time 0.084 (0.344) data 0.000 (0.223) loss 1.1484 (1.2507) acc 87.5000 (86.4583) lr 1.6845e-03 eta 0:00:37
epoch [15/50] batch [1/3] time 0.761 (0.761) data 0.621 (0.621) loss 1.2432 (1.2432) acc 87.5000 (87.5000) lr 1.6845e-03 eta 0:01:21
epoch [15/50] batch [2/3] time 0.138 (0.450) data 0.000 (0.311) loss 0.9565 (1.0999) acc 96.8750 (92.1875) lr 1.6845e-03 eta 0:00:47
epoch [15/50] batch [3/3] time 0.085 (0.328) data 0.000 (0.207) loss 1.3086 (1.1694) acc 75.0000 (86.4583) lr 1.6374e-03 eta 0:00:34
epoch [16/50] batch [1/3] time 0.809 (0.809) data 0.666 (0.666) loss 1.3574 (1.3574) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:01:24
epoch [16/50] batch [2/3] time 0.138 (0.473) data 0.000 (0.333) loss 1.0215 (1.1895) acc 84.3750 (82.8125) lr 1.6374e-03 eta 0:00:48
epoch [16/50] batch [3/3] time 0.086 (0.344) data 0.000 (0.222) loss 1.1777 (1.1855) acc 100.0000 (88.5417) lr 1.5878e-03 eta 0:00:35
epoch [17/50] batch [1/3] time 0.764 (0.764) data 0.623 (0.623) loss 1.3066 (1.3066) acc 81.2500 (81.2500) lr 1.5878e-03 eta 0:01:17
epoch [17/50] batch [2/3] time 0.138 (0.451) data 0.000 (0.312) loss 0.9272 (1.1169) acc 93.7500 (87.5000) lr 1.5878e-03 eta 0:00:45
epoch [17/50] batch [3/3] time 0.083 (0.328) data 0.000 (0.208) loss 0.8516 (1.0285) acc 100.0000 (91.6667) lr 1.5358e-03 eta 0:00:32
epoch [18/50] batch [1/3] time 0.666 (0.666) data 0.526 (0.526) loss 1.1074 (1.1074) acc 90.6250 (90.6250) lr 1.5358e-03 eta 0:01:05
epoch [18/50] batch [2/3] time 0.140 (0.403) data 0.000 (0.263) loss 1.0186 (1.0630) acc 90.6250 (90.6250) lr 1.5358e-03 eta 0:00:39
epoch [18/50] batch [3/3] time 0.084 (0.297) data 0.000 (0.175) loss 1.3096 (1.1452) acc 87.5000 (89.5833) lr 1.4818e-03 eta 0:00:28
epoch [19/50] batch [1/3] time 0.668 (0.668) data 0.528 (0.528) loss 1.1494 (1.1494) acc 87.5000 (87.5000) lr 1.4818e-03 eta 0:01:03
epoch [19/50] batch [2/3] time 0.138 (0.403) data 0.000 (0.264) loss 1.1992 (1.1743) acc 93.7500 (90.6250) lr 1.4818e-03 eta 0:00:37
epoch [19/50] batch [3/3] time 0.088 (0.298) data 0.000 (0.176) loss 1.3398 (1.2295) acc 87.5000 (89.5833) lr 1.4258e-03 eta 0:00:27
epoch [20/50] batch [1/3] time 0.723 (0.723) data 0.579 (0.579) loss 1.2158 (1.2158) acc 84.3750 (84.3750) lr 1.4258e-03 eta 0:01:06
epoch [20/50] batch [2/3] time 0.138 (0.431) data 0.000 (0.290) loss 1.1016 (1.1587) acc 90.6250 (87.5000) lr 1.4258e-03 eta 0:00:39
epoch [20/50] batch [3/3] time 0.083 (0.315) data 0.000 (0.193) loss 1.0742 (1.1305) acc 87.5000 (87.5000) lr 1.3681e-03 eta 0:00:28
epoch [21/50] batch [1/3] time 0.759 (0.759) data 0.617 (0.617) loss 0.7832 (0.7832) acc 96.8750 (96.8750) lr 1.3681e-03 eta 0:01:07
epoch [21/50] batch [2/3] time 0.139 (0.449) data 0.000 (0.309) loss 1.0684 (0.9258) acc 84.3750 (90.6250) lr 1.3681e-03 eta 0:00:39
epoch [21/50] batch [3/3] time 0.084 (0.327) data 0.000 (0.206) loss 1.0547 (0.9688) acc 87.5000 (89.5833) lr 1.3090e-03 eta 0:00:28
epoch [22/50] batch [1/3] time 0.787 (0.787) data 0.643 (0.643) loss 1.2715 (1.2715) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:01:07
epoch [22/50] batch [2/3] time 0.139 (0.463) data 0.000 (0.322) loss 1.2041 (1.2378) acc 90.6250 (89.0625) lr 1.3090e-03 eta 0:00:39
epoch [22/50] batch [3/3] time 0.084 (0.337) data 0.000 (0.215) loss 1.0342 (1.1699) acc 100.0000 (92.7083) lr 1.2487e-03 eta 0:00:28
epoch [23/50] batch [1/3] time 0.658 (0.658) data 0.515 (0.515) loss 1.1406 (1.1406) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:00:54
epoch [23/50] batch [2/3] time 0.139 (0.398) data 0.000 (0.257) loss 0.9141 (1.0273) acc 93.7500 (90.6250) lr 1.2487e-03 eta 0:00:32
epoch [23/50] batch [3/3] time 0.087 (0.295) data 0.000 (0.172) loss 1.2344 (1.0964) acc 81.2500 (87.5000) lr 1.1874e-03 eta 0:00:23
epoch [24/50] batch [1/3] time 0.761 (0.761) data 0.616 (0.616) loss 0.9883 (0.9883) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:01:00
epoch [24/50] batch [2/3] time 0.141 (0.451) data 0.001 (0.308) loss 1.1846 (1.0864) acc 84.3750 (87.5000) lr 1.1874e-03 eta 0:00:35
epoch [24/50] batch [3/3] time 0.085 (0.329) data 0.000 (0.206) loss 0.7246 (0.9658) acc 100.0000 (91.6667) lr 1.1253e-03 eta 0:00:25
epoch [25/50] batch [1/3] time 0.823 (0.823) data 0.679 (0.679) loss 0.9272 (0.9272) acc 96.8750 (96.8750) lr 1.1253e-03 eta 0:01:03
epoch [25/50] batch [2/3] time 0.140 (0.481) data 0.000 (0.340) loss 1.3486 (1.1379) acc 81.2500 (89.0625) lr 1.1253e-03 eta 0:00:36
epoch [25/50] batch [3/3] time 0.090 (0.351) data 0.000 (0.226) loss 1.3574 (1.2111) acc 81.2500 (86.4583) lr 1.0628e-03 eta 0:00:26
epoch [26/50] batch [1/3] time 0.801 (0.801) data 0.660 (0.660) loss 1.2734 (1.2734) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:00:59
epoch [26/50] batch [2/3] time 0.140 (0.470) data 0.000 (0.330) loss 1.0430 (1.1582) acc 81.2500 (84.3750) lr 1.0628e-03 eta 0:00:34
epoch [26/50] batch [3/3] time 0.083 (0.341) data 0.000 (0.220) loss 0.7598 (1.0254) acc 100.0000 (89.5833) lr 1.0000e-03 eta 0:00:24
epoch [27/50] batch [1/3] time 0.741 (0.741) data 0.599 (0.599) loss 1.0078 (1.0078) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:00:52
epoch [27/50] batch [2/3] time 0.140 (0.440) data 0.000 (0.300) loss 0.9150 (0.9614) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:00:30
epoch [27/50] batch [3/3] time 0.092 (0.324) data 0.000 (0.200) loss 0.5737 (0.8322) acc 100.0000 (93.7500) lr 9.3721e-04 eta 0:00:22
epoch [28/50] batch [1/3] time 0.833 (0.833) data 0.683 (0.683) loss 0.8589 (0.8589) acc 93.7500 (93.7500) lr 9.3721e-04 eta 0:00:56
epoch [28/50] batch [2/3] time 0.141 (0.487) data 0.000 (0.342) loss 1.0215 (0.9402) acc 96.8750 (95.3125) lr 9.3721e-04 eta 0:00:32
epoch [28/50] batch [3/3] time 0.085 (0.353) data 0.000 (0.228) loss 1.1162 (0.9989) acc 93.7500 (94.7917) lr 8.7467e-04 eta 0:00:23
epoch [29/50] batch [1/3] time 0.789 (0.789) data 0.646 (0.646) loss 0.8262 (0.8262) acc 93.7500 (93.7500) lr 8.7467e-04 eta 0:00:51
epoch [29/50] batch [2/3] time 0.142 (0.466) data 0.000 (0.323) loss 0.9595 (0.8928) acc 96.8750 (95.3125) lr 8.7467e-04 eta 0:00:29
epoch [29/50] batch [3/3] time 0.084 (0.339) data 0.000 (0.216) loss 0.7178 (0.8345) acc 100.0000 (96.8750) lr 8.1262e-04 eta 0:00:21
epoch [30/50] batch [1/3] time 0.803 (0.803) data 0.659 (0.659) loss 1.2402 (1.2402) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:00:49
epoch [30/50] batch [2/3] time 0.141 (0.472) data 0.001 (0.330) loss 0.9072 (1.0737) acc 93.7500 (90.6250) lr 8.1262e-04 eta 0:00:28
epoch [30/50] batch [3/3] time 0.095 (0.346) data 0.000 (0.220) loss 1.0732 (1.0736) acc 93.7500 (91.6667) lr 7.5131e-04 eta 0:00:20
epoch [31/50] batch [1/3] time 0.785 (0.785) data 0.642 (0.642) loss 1.1055 (1.1055) acc 87.5000 (87.5000) lr 7.5131e-04 eta 0:00:46
epoch [31/50] batch [2/3] time 0.137 (0.461) data 0.001 (0.321) loss 0.9395 (1.0225) acc 90.6250 (89.0625) lr 7.5131e-04 eta 0:00:26
epoch [31/50] batch [3/3] time 0.092 (0.338) data 0.000 (0.214) loss 1.2012 (1.0820) acc 81.2500 (86.4583) lr 6.9098e-04 eta 0:00:19
epoch [32/50] batch [1/3] time 0.803 (0.803) data 0.661 (0.661) loss 0.8936 (0.8936) acc 96.8750 (96.8750) lr 6.9098e-04 eta 0:00:44
epoch [32/50] batch [2/3] time 0.139 (0.471) data 0.000 (0.331) loss 0.8584 (0.8760) acc 93.7500 (95.3125) lr 6.9098e-04 eta 0:00:25
epoch [32/50] batch [3/3] time 0.088 (0.343) data 0.000 (0.220) loss 0.7656 (0.8392) acc 100.0000 (96.8750) lr 6.3188e-04 eta 0:00:18
epoch [33/50] batch [1/3] time 0.794 (0.794) data 0.653 (0.653) loss 0.9497 (0.9497) acc 93.7500 (93.7500) lr 6.3188e-04 eta 0:00:42
epoch [33/50] batch [2/3] time 0.139 (0.466) data 0.000 (0.326) loss 1.0752 (1.0125) acc 87.5000 (90.6250) lr 6.3188e-04 eta 0:00:24
epoch [33/50] batch [3/3] time 0.084 (0.339) data 0.000 (0.218) loss 0.9951 (1.0067) acc 87.5000 (89.5833) lr 5.7422e-04 eta 0:00:17
epoch [34/50] batch [1/3] time 0.811 (0.811) data 0.670 (0.670) loss 0.7949 (0.7949) acc 87.5000 (87.5000) lr 5.7422e-04 eta 0:00:40
epoch [34/50] batch [2/3] time 0.139 (0.475) data 0.000 (0.335) loss 1.0557 (0.9253) acc 90.6250 (89.0625) lr 5.7422e-04 eta 0:00:23
epoch [34/50] batch [3/3] time 0.091 (0.347) data 0.000 (0.223) loss 0.8027 (0.8844) acc 93.7500 (90.6250) lr 5.1825e-04 eta 0:00:16
epoch [35/50] batch [1/3] time 0.788 (0.788) data 0.646 (0.646) loss 0.7422 (0.7422) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:00:37
epoch [35/50] batch [2/3] time 0.140 (0.464) data 0.001 (0.323) loss 1.0898 (0.9160) acc 87.5000 (92.1875) lr 5.1825e-04 eta 0:00:21
epoch [35/50] batch [3/3] time 0.089 (0.339) data 0.000 (0.216) loss 0.8374 (0.8898) acc 93.7500 (92.7083) lr 4.6417e-04 eta 0:00:15
epoch [36/50] batch [1/3] time 0.720 (0.720) data 0.577 (0.577) loss 1.2178 (1.2178) acc 84.3750 (84.3750) lr 4.6417e-04 eta 0:00:31
epoch [36/50] batch [2/3] time 0.141 (0.430) data 0.000 (0.289) loss 1.1406 (1.1792) acc 84.3750 (84.3750) lr 4.6417e-04 eta 0:00:18
epoch [36/50] batch [3/3] time 0.084 (0.315) data 0.000 (0.193) loss 0.8350 (1.0645) acc 87.5000 (85.4167) lr 4.1221e-04 eta 0:00:13
epoch [37/50] batch [1/3] time 0.754 (0.754) data 0.611 (0.611) loss 0.9868 (0.9868) acc 87.5000 (87.5000) lr 4.1221e-04 eta 0:00:30
epoch [37/50] batch [2/3] time 0.141 (0.447) data 0.001 (0.306) loss 0.9170 (0.9519) acc 90.6250 (89.0625) lr 4.1221e-04 eta 0:00:17
epoch [37/50] batch [3/3] time 0.085 (0.326) data 0.000 (0.204) loss 0.7153 (0.8730) acc 100.0000 (92.7083) lr 3.6258e-04 eta 0:00:12
epoch [38/50] batch [1/3] time 0.804 (0.804) data 0.661 (0.661) loss 0.9414 (0.9414) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:30
epoch [38/50] batch [2/3] time 0.139 (0.472) data 0.000 (0.331) loss 0.9375 (0.9395) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:17
epoch [38/50] batch [3/3] time 0.084 (0.342) data 0.000 (0.220) loss 0.8818 (0.9202) acc 100.0000 (95.8333) lr 3.1545e-04 eta 0:00:12
epoch [39/50] batch [1/3] time 0.774 (0.774) data 0.633 (0.633) loss 0.8975 (0.8975) acc 90.6250 (90.6250) lr 3.1545e-04 eta 0:00:27
epoch [39/50] batch [2/3] time 0.139 (0.456) data 0.000 (0.317) loss 1.0605 (0.9790) acc 93.7500 (92.1875) lr 3.1545e-04 eta 0:00:15
epoch [39/50] batch [3/3] time 0.084 (0.332) data 0.000 (0.211) loss 1.0010 (0.9863) acc 93.7500 (92.7083) lr 2.7103e-04 eta 0:00:10
epoch [40/50] batch [1/3] time 0.816 (0.816) data 0.672 (0.672) loss 0.8765 (0.8765) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:26
epoch [40/50] batch [2/3] time 0.141 (0.478) data 0.000 (0.336) loss 0.7969 (0.8367) acc 93.7500 (92.1875) lr 2.7103e-04 eta 0:00:14
epoch [40/50] batch [3/3] time 0.098 (0.352) data 0.000 (0.224) loss 1.0322 (0.9019) acc 100.0000 (94.7917) lr 2.2949e-04 eta 0:00:10
epoch [41/50] batch [1/3] time 0.790 (0.790) data 0.649 (0.649) loss 0.8032 (0.8032) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:00:22
epoch [41/50] batch [2/3] time 0.141 (0.466) data 0.000 (0.325) loss 0.9365 (0.8699) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:00:13
epoch [41/50] batch [3/3] time 0.085 (0.339) data 0.000 (0.216) loss 1.0332 (0.9243) acc 87.5000 (91.6667) lr 1.9098e-04 eta 0:00:09
epoch [42/50] batch [1/3] time 0.806 (0.806) data 0.664 (0.664) loss 0.7559 (0.7559) acc 100.0000 (100.0000) lr 1.9098e-04 eta 0:00:20
epoch [42/50] batch [2/3] time 0.142 (0.474) data 0.000 (0.332) loss 0.8896 (0.8228) acc 93.7500 (96.8750) lr 1.9098e-04 eta 0:00:11
epoch [42/50] batch [3/3] time 0.084 (0.344) data 0.000 (0.222) loss 0.8696 (0.8384) acc 93.7500 (95.8333) lr 1.5567e-04 eta 0:00:08
epoch [43/50] batch [1/3] time 0.763 (0.763) data 0.621 (0.621) loss 0.9795 (0.9795) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:17
epoch [43/50] batch [2/3] time 0.140 (0.451) data 0.000 (0.310) loss 0.9600 (0.9697) acc 84.3750 (87.5000) lr 1.5567e-04 eta 0:00:09
epoch [43/50] batch [3/3] time 0.093 (0.332) data 0.000 (0.207) loss 1.0928 (1.0107) acc 75.0000 (83.3333) lr 1.2369e-04 eta 0:00:06
epoch [44/50] batch [1/3] time 0.714 (0.714) data 0.570 (0.570) loss 1.0371 (1.0371) acc 90.6250 (90.6250) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [2/3] time 0.141 (0.427) data 0.001 (0.285) loss 0.8369 (0.9370) acc 93.7500 (92.1875) lr 1.2369e-04 eta 0:00:08
epoch [44/50] batch [3/3] time 0.091 (0.315) data 0.000 (0.190) loss 0.8535 (0.9092) acc 87.5000 (90.6250) lr 9.5173e-05 eta 0:00:05
epoch [45/50] batch [1/3] time 0.839 (0.839) data 0.697 (0.697) loss 0.8833 (0.8833) acc 93.7500 (93.7500) lr 9.5173e-05 eta 0:00:14
epoch [45/50] batch [2/3] time 0.140 (0.490) data 0.000 (0.349) loss 0.9512 (0.9172) acc 90.6250 (92.1875) lr 9.5173e-05 eta 0:00:07
epoch [45/50] batch [3/3] time 0.084 (0.354) data 0.000 (0.232) loss 1.0762 (0.9702) acc 93.7500 (92.7083) lr 7.0224e-05 eta 0:00:05
epoch [46/50] batch [1/3] time 0.864 (0.864) data 0.723 (0.723) loss 0.9741 (0.9741) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [2/3] time 0.139 (0.502) data 0.000 (0.362) loss 0.8750 (0.9246) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:00:06
epoch [46/50] batch [3/3] time 0.083 (0.362) data 0.000 (0.241) loss 0.6582 (0.8358) acc 100.0000 (95.8333) lr 4.8943e-05 eta 0:00:04
epoch [47/50] batch [1/3] time 0.787 (0.787) data 0.643 (0.643) loss 1.0039 (1.0039) acc 87.5000 (87.5000) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [2/3] time 0.139 (0.463) data 0.000 (0.322) loss 1.0508 (1.0273) acc 93.7500 (90.6250) lr 4.8943e-05 eta 0:00:04
epoch [47/50] batch [3/3] time 0.085 (0.337) data 0.000 (0.214) loss 0.7935 (0.9494) acc 100.0000 (93.7500) lr 3.1417e-05 eta 0:00:03
epoch [48/50] batch [1/3] time 0.823 (0.823) data 0.682 (0.682) loss 0.8301 (0.8301) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [2/3] time 0.139 (0.481) data 0.000 (0.341) loss 0.8950 (0.8625) acc 100.0000 (93.7500) lr 3.1417e-05 eta 0:00:03
epoch [48/50] batch [3/3] time 0.085 (0.349) data 0.000 (0.227) loss 0.8809 (0.8687) acc 100.0000 (95.8333) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [1/3] time 0.790 (0.790) data 0.648 (0.648) loss 0.9624 (0.9624) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [2/3] time 0.140 (0.465) data 0.000 (0.324) loss 0.9717 (0.9670) acc 93.7500 (92.1875) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [3/3] time 0.083 (0.338) data 0.000 (0.216) loss 0.7056 (0.8799) acc 100.0000 (94.7917) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [1/3] time 0.671 (0.671) data 0.529 (0.529) loss 0.8291 (0.8291) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [2/3] time 0.139 (0.405) data 0.000 (0.264) loss 0.9990 (0.9141) acc 87.5000 (89.0625) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [3/3] time 0.094 (0.301) data 0.000 (0.176) loss 0.8105 (0.8796) acc 93.7500 (90.6250) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:07<01:03,  7.92s/it] 22%|██▏       | 2/9 [00:09<00:27,  3.91s/it] 33%|███▎      | 3/9 [00:10<00:15,  2.63s/it] 44%|████▍     | 4/9 [00:11<00:10,  2.03s/it] 56%|█████▌    | 5/9 [00:12<00:06,  1.70s/it] 67%|██████▋   | 6/9 [00:13<00:04,  1.50s/it] 78%|███████▊  | 7/9 [00:14<00:02,  1.37s/it] 89%|████████▉ | 8/9 [00:15<00:01,  1.29s/it]100%|██████████| 9/9 [00:16<00:00,  1.04s/it]100%|██████████| 9/9 [00:16<00:00,  1.81s/it]
=> result
* total: 4,200
* correct: 3,757
* accuracy: 89.5%
* error: 10.5%
* macro_f1: 89.5%
Elapsed: 0:01:12
Run this job and save the output to output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     4,200
---------  -------
['Annual Crop Land', 'Forest', 'Herbaceous Vegetation Land', 'Highway or Road', 'Industrial Buildings']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Annual Crop Land.', 'X X X X Forest.', 'X X X X Herbaceous Vegetation Land.', 'X X X X Highway or Road.', 'X X X X Industrial Buildings.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([5, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [1/3] time 0.875 (0.875) data 0.684 (0.684) loss 5.5469 (5.5469) acc 28.1250 (28.1250) lr 1.0000e-05 eta 0:02:10
epoch [1/50] batch [2/3] time 0.142 (0.508) data 0.000 (0.342) loss 5.3125 (5.4297) acc 56.2500 (42.1875) lr 1.0000e-05 eta 0:01:15
epoch [1/50] batch [3/3] time 0.083 (0.366) data 0.000 (0.228) loss 5.5820 (5.4805) acc 25.0000 (36.4583) lr 2.0000e-03 eta 0:00:53
epoch [2/50] batch [1/3] time 0.741 (0.741) data 0.600 (0.600) loss 5.3086 (5.3086) acc 50.0000 (50.0000) lr 2.0000e-03 eta 0:01:48
epoch [2/50] batch [2/3] time 0.139 (0.440) data 0.000 (0.300) loss 4.4023 (4.8555) acc 59.3750 (54.6875) lr 2.0000e-03 eta 0:01:03
epoch [2/50] batch [3/3] time 0.084 (0.322) data 0.000 (0.200) loss 4.5781 (4.7630) acc 56.2500 (55.2083) lr 1.9980e-03 eta 0:00:46
epoch [3/50] batch [1/3] time 0.853 (0.853) data 0.713 (0.713) loss 4.0469 (4.0469) acc 43.7500 (43.7500) lr 1.9980e-03 eta 0:02:02
epoch [3/50] batch [2/3] time 0.140 (0.497) data 0.000 (0.357) loss 3.7422 (3.8945) acc 53.1250 (48.4375) lr 1.9980e-03 eta 0:01:10
epoch [3/50] batch [3/3] time 0.083 (0.359) data 0.000 (0.238) loss 2.9648 (3.5846) acc 68.7500 (55.2083) lr 1.9921e-03 eta 0:00:50
epoch [4/50] batch [1/3] time 0.669 (0.669) data 0.528 (0.528) loss 2.9961 (2.9961) acc 53.1250 (53.1250) lr 1.9921e-03 eta 0:01:33
epoch [4/50] batch [2/3] time 0.139 (0.404) data 0.000 (0.264) loss 2.8203 (2.9082) acc 62.5000 (57.8125) lr 1.9921e-03 eta 0:00:56
epoch [4/50] batch [3/3] time 0.083 (0.297) data 0.000 (0.176) loss 2.7676 (2.8613) acc 56.2500 (57.2917) lr 1.9823e-03 eta 0:00:40
epoch [5/50] batch [1/3] time 0.783 (0.783) data 0.643 (0.643) loss 2.5703 (2.5703) acc 59.3750 (59.3750) lr 1.9823e-03 eta 0:01:47
epoch [5/50] batch [2/3] time 0.138 (0.460) data 0.000 (0.322) loss 2.1992 (2.3848) acc 75.0000 (67.1875) lr 1.9823e-03 eta 0:01:02
epoch [5/50] batch [3/3] time 0.083 (0.334) data 0.000 (0.215) loss 2.0762 (2.2819) acc 68.7500 (67.7083) lr 1.9686e-03 eta 0:00:45
epoch [6/50] batch [1/3] time 0.782 (0.782) data 0.640 (0.640) loss 2.1797 (2.1797) acc 87.5000 (87.5000) lr 1.9686e-03 eta 0:01:44
epoch [6/50] batch [2/3] time 0.139 (0.461) data 0.000 (0.320) loss 1.9980 (2.0889) acc 81.2500 (84.3750) lr 1.9686e-03 eta 0:01:01
epoch [6/50] batch [3/3] time 0.082 (0.335) data 0.000 (0.214) loss 2.4531 (2.2103) acc 56.2500 (75.0000) lr 1.9511e-03 eta 0:00:44
epoch [7/50] batch [1/3] time 0.726 (0.726) data 0.586 (0.586) loss 2.0273 (2.0273) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:01:35
epoch [7/50] batch [2/3] time 0.139 (0.432) data 0.000 (0.293) loss 1.8203 (1.9238) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:00:56
epoch [7/50] batch [3/3] time 0.082 (0.316) data 0.000 (0.195) loss 1.9209 (1.9229) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:00:40
epoch [8/50] batch [1/3] time 0.681 (0.681) data 0.541 (0.541) loss 2.0430 (2.0430) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:01:27
epoch [8/50] batch [2/3] time 0.139 (0.410) data 0.000 (0.271) loss 1.7832 (1.9131) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:00:52
epoch [8/50] batch [3/3] time 0.084 (0.301) data 0.000 (0.180) loss 1.8633 (1.8965) acc 87.5000 (83.3333) lr 1.9048e-03 eta 0:00:37
epoch [9/50] batch [1/3] time 0.670 (0.670) data 0.527 (0.527) loss 1.7695 (1.7695) acc 75.0000 (75.0000) lr 1.9048e-03 eta 0:01:23
epoch [9/50] batch [2/3] time 0.140 (0.405) data 0.000 (0.264) loss 1.6758 (1.7227) acc 68.7500 (71.8750) lr 1.9048e-03 eta 0:00:50
epoch [9/50] batch [3/3] time 0.083 (0.298) data 0.000 (0.176) loss 1.5752 (1.6735) acc 93.7500 (79.1667) lr 1.8763e-03 eta 0:00:36
epoch [10/50] batch [1/3] time 0.704 (0.704) data 0.564 (0.564) loss 1.6660 (1.6660) acc 75.0000 (75.0000) lr 1.8763e-03 eta 0:01:25
epoch [10/50] batch [2/3] time 0.138 (0.421) data 0.000 (0.282) loss 1.7041 (1.6851) acc 78.1250 (76.5625) lr 1.8763e-03 eta 0:00:50
epoch [10/50] batch [3/3] time 0.084 (0.309) data 0.000 (0.188) loss 1.6680 (1.6794) acc 87.5000 (80.2083) lr 1.8443e-03 eta 0:00:37
epoch [11/50] batch [1/3] time 0.711 (0.711) data 0.573 (0.573) loss 1.5342 (1.5342) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:01:24
epoch [11/50] batch [2/3] time 0.140 (0.425) data 0.000 (0.286) loss 1.7500 (1.6421) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:00:50
epoch [11/50] batch [3/3] time 0.084 (0.312) data 0.000 (0.191) loss 1.4238 (1.5693) acc 87.5000 (83.3333) lr 1.8090e-03 eta 0:00:36
epoch [12/50] batch [1/3] time 0.817 (0.817) data 0.675 (0.675) loss 1.4004 (1.4004) acc 93.7500 (93.7500) lr 1.8090e-03 eta 0:01:34
epoch [12/50] batch [2/3] time 0.138 (0.477) data 0.000 (0.337) loss 1.5156 (1.4580) acc 81.2500 (87.5000) lr 1.8090e-03 eta 0:00:54
epoch [12/50] batch [3/3] time 0.083 (0.346) data 0.000 (0.225) loss 1.5195 (1.4785) acc 93.7500 (89.5833) lr 1.7705e-03 eta 0:00:39
epoch [13/50] batch [1/3] time 0.675 (0.675) data 0.535 (0.535) loss 1.6094 (1.6094) acc 81.2500 (81.2500) lr 1.7705e-03 eta 0:01:16
epoch [13/50] batch [2/3] time 0.140 (0.408) data 0.000 (0.267) loss 1.4932 (1.5513) acc 78.1250 (79.6875) lr 1.7705e-03 eta 0:00:45
epoch [13/50] batch [3/3] time 0.083 (0.299) data 0.000 (0.178) loss 1.4414 (1.5146) acc 81.2500 (80.2083) lr 1.7290e-03 eta 0:00:33
epoch [14/50] batch [1/3] time 0.831 (0.831) data 0.689 (0.689) loss 1.1777 (1.1777) acc 93.7500 (93.7500) lr 1.7290e-03 eta 0:01:31
epoch [14/50] batch [2/3] time 0.139 (0.485) data 0.000 (0.345) loss 1.3223 (1.2500) acc 87.5000 (90.6250) lr 1.7290e-03 eta 0:00:52
epoch [14/50] batch [3/3] time 0.108 (0.359) data 0.000 (0.230) loss 1.3066 (1.2689) acc 81.2500 (87.5000) lr 1.6845e-03 eta 0:00:38
epoch [15/50] batch [1/3] time 0.709 (0.709) data 0.571 (0.571) loss 1.3691 (1.3691) acc 90.6250 (90.6250) lr 1.6845e-03 eta 0:01:15
epoch [15/50] batch [2/3] time 0.137 (0.423) data 0.000 (0.285) loss 1.4941 (1.4316) acc 81.2500 (85.9375) lr 1.6845e-03 eta 0:00:44
epoch [15/50] batch [3/3] time 0.093 (0.313) data 0.000 (0.190) loss 1.3350 (1.3994) acc 93.7500 (88.5417) lr 1.6374e-03 eta 0:00:32
epoch [16/50] batch [1/3] time 0.756 (0.756) data 0.618 (0.618) loss 1.4814 (1.4814) acc 81.2500 (81.2500) lr 1.6374e-03 eta 0:01:18
epoch [16/50] batch [2/3] time 0.140 (0.448) data 0.000 (0.309) loss 1.3037 (1.3926) acc 90.6250 (85.9375) lr 1.6374e-03 eta 0:00:46
epoch [16/50] batch [3/3] time 0.082 (0.326) data 0.000 (0.206) loss 1.5303 (1.4385) acc 75.0000 (82.2917) lr 1.5878e-03 eta 0:00:33
epoch [17/50] batch [1/3] time 0.748 (0.748) data 0.606 (0.606) loss 1.2754 (1.2754) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:01:15
epoch [17/50] batch [2/3] time 0.139 (0.443) data 0.000 (0.303) loss 1.2734 (1.2744) acc 84.3750 (87.5000) lr 1.5878e-03 eta 0:00:44
epoch [17/50] batch [3/3] time 0.085 (0.324) data 0.000 (0.202) loss 1.2012 (1.2500) acc 93.7500 (89.5833) lr 1.5358e-03 eta 0:00:32
epoch [18/50] batch [1/3] time 0.768 (0.768) data 0.623 (0.623) loss 1.6270 (1.6270) acc 78.1250 (78.1250) lr 1.5358e-03 eta 0:01:15
epoch [18/50] batch [2/3] time 0.139 (0.453) data 0.000 (0.312) loss 1.2988 (1.4629) acc 93.7500 (85.9375) lr 1.5358e-03 eta 0:00:43
epoch [18/50] batch [3/3] time 0.083 (0.330) data 0.000 (0.208) loss 1.1484 (1.3581) acc 93.7500 (88.5417) lr 1.4818e-03 eta 0:00:31
epoch [19/50] batch [1/3] time 0.738 (0.738) data 0.597 (0.597) loss 1.3770 (1.3770) acc 90.6250 (90.6250) lr 1.4818e-03 eta 0:01:10
epoch [19/50] batch [2/3] time 0.138 (0.438) data 0.000 (0.299) loss 1.5352 (1.4561) acc 81.2500 (85.9375) lr 1.4818e-03 eta 0:00:41
epoch [19/50] batch [3/3] time 0.082 (0.319) data 0.000 (0.199) loss 1.4258 (1.4460) acc 93.7500 (88.5417) lr 1.4258e-03 eta 0:00:29
epoch [20/50] batch [1/3] time 0.666 (0.666) data 0.523 (0.523) loss 1.0879 (1.0879) acc 93.7500 (93.7500) lr 1.4258e-03 eta 0:01:01
epoch [20/50] batch [2/3] time 0.138 (0.402) data 0.000 (0.262) loss 1.1777 (1.1328) acc 96.8750 (95.3125) lr 1.4258e-03 eta 0:00:36
epoch [20/50] batch [3/3] time 0.083 (0.296) data 0.000 (0.174) loss 1.4824 (1.2493) acc 87.5000 (92.7083) lr 1.3681e-03 eta 0:00:26
epoch [21/50] batch [1/3] time 0.722 (0.722) data 0.580 (0.580) loss 1.4482 (1.4482) acc 90.6250 (90.6250) lr 1.3681e-03 eta 0:01:04
epoch [21/50] batch [2/3] time 0.139 (0.431) data 0.000 (0.290) loss 1.2764 (1.3623) acc 90.6250 (90.6250) lr 1.3681e-03 eta 0:00:37
epoch [21/50] batch [3/3] time 0.083 (0.315) data 0.000 (0.194) loss 1.2227 (1.3158) acc 93.7500 (91.6667) lr 1.3090e-03 eta 0:00:27
epoch [22/50] batch [1/3] time 0.726 (0.726) data 0.586 (0.586) loss 1.0352 (1.0352) acc 93.7500 (93.7500) lr 1.3090e-03 eta 0:01:02
epoch [22/50] batch [2/3] time 0.138 (0.432) data 0.001 (0.294) loss 1.0449 (1.0400) acc 87.5000 (90.6250) lr 1.3090e-03 eta 0:00:36
epoch [22/50] batch [3/3] time 0.083 (0.316) data 0.000 (0.196) loss 0.9048 (0.9950) acc 100.0000 (93.7500) lr 1.2487e-03 eta 0:00:26
epoch [23/50] batch [1/3] time 0.729 (0.729) data 0.588 (0.588) loss 1.4297 (1.4297) acc 84.3750 (84.3750) lr 1.2487e-03 eta 0:01:00
epoch [23/50] batch [2/3] time 0.141 (0.435) data 0.000 (0.294) loss 1.5762 (1.5029) acc 78.1250 (81.2500) lr 1.2487e-03 eta 0:00:35
epoch [23/50] batch [3/3] time 0.084 (0.318) data 0.000 (0.196) loss 1.0176 (1.3411) acc 93.7500 (85.4167) lr 1.1874e-03 eta 0:00:25
epoch [24/50] batch [1/3] time 0.794 (0.794) data 0.651 (0.651) loss 1.2676 (1.2676) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:01:03
epoch [24/50] batch [2/3] time 0.140 (0.467) data 0.000 (0.325) loss 1.2383 (1.2529) acc 93.7500 (92.1875) lr 1.1874e-03 eta 0:00:36
epoch [24/50] batch [3/3] time 0.085 (0.340) data 0.000 (0.217) loss 1.1250 (1.2103) acc 87.5000 (90.6250) lr 1.1253e-03 eta 0:00:26
epoch [25/50] batch [1/3] time 0.660 (0.660) data 0.518 (0.518) loss 1.2969 (1.2969) acc 81.2500 (81.2500) lr 1.1253e-03 eta 0:00:50
epoch [25/50] batch [2/3] time 0.139 (0.399) data 0.000 (0.259) loss 1.1426 (1.2197) acc 90.6250 (85.9375) lr 1.1253e-03 eta 0:00:30
epoch [25/50] batch [3/3] time 0.083 (0.294) data 0.000 (0.173) loss 1.2266 (1.2220) acc 87.5000 (86.4583) lr 1.0628e-03 eta 0:00:22
epoch [26/50] batch [1/3] time 0.700 (0.700) data 0.557 (0.557) loss 1.0938 (1.0938) acc 96.8750 (96.8750) lr 1.0628e-03 eta 0:00:51
epoch [26/50] batch [2/3] time 0.137 (0.418) data 0.000 (0.279) loss 1.1699 (1.1318) acc 87.5000 (92.1875) lr 1.0628e-03 eta 0:00:30
epoch [26/50] batch [3/3] time 0.083 (0.307) data 0.000 (0.186) loss 0.9609 (1.0749) acc 87.5000 (90.6250) lr 1.0000e-03 eta 0:00:22
epoch [27/50] batch [1/3] time 0.661 (0.661) data 0.516 (0.516) loss 1.0508 (1.0508) acc 96.8750 (96.8750) lr 1.0000e-03 eta 0:00:46
epoch [27/50] batch [2/3] time 0.140 (0.401) data 0.000 (0.258) loss 1.2227 (1.1367) acc 90.6250 (93.7500) lr 1.0000e-03 eta 0:00:28
epoch [27/50] batch [3/3] time 0.083 (0.295) data 0.000 (0.172) loss 1.2109 (1.1615) acc 81.2500 (89.5833) lr 9.3721e-04 eta 0:00:20
epoch [28/50] batch [1/3] time 0.660 (0.660) data 0.517 (0.517) loss 1.3047 (1.3047) acc 84.3750 (84.3750) lr 9.3721e-04 eta 0:00:44
epoch [28/50] batch [2/3] time 0.137 (0.398) data 0.000 (0.259) loss 1.2773 (1.2910) acc 84.3750 (84.3750) lr 9.3721e-04 eta 0:00:26
epoch [28/50] batch [3/3] time 0.082 (0.293) data 0.000 (0.173) loss 1.3730 (1.3184) acc 81.2500 (83.3333) lr 8.7467e-04 eta 0:00:19
epoch [29/50] batch [1/3] time 0.812 (0.812) data 0.672 (0.672) loss 1.0488 (1.0488) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:00:52
epoch [29/50] batch [2/3] time 0.138 (0.475) data 0.000 (0.336) loss 1.0674 (1.0581) acc 96.8750 (96.8750) lr 8.7467e-04 eta 0:00:30
epoch [29/50] batch [3/3] time 0.082 (0.344) data 0.000 (0.224) loss 1.3662 (1.1608) acc 81.2500 (91.6667) lr 8.1262e-04 eta 0:00:21
epoch [30/50] batch [1/3] time 0.711 (0.711) data 0.569 (0.569) loss 0.9834 (0.9834) acc 96.8750 (96.8750) lr 8.1262e-04 eta 0:00:44
epoch [30/50] batch [2/3] time 0.139 (0.425) data 0.000 (0.284) loss 1.2432 (1.1133) acc 84.3750 (90.6250) lr 8.1262e-04 eta 0:00:25
epoch [30/50] batch [3/3] time 0.082 (0.311) data 0.000 (0.190) loss 1.2324 (1.1530) acc 87.5000 (89.5833) lr 7.5131e-04 eta 0:00:18
epoch [31/50] batch [1/3] time 0.720 (0.720) data 0.579 (0.579) loss 1.1660 (1.1660) acc 93.7500 (93.7500) lr 7.5131e-04 eta 0:00:42
epoch [31/50] batch [2/3] time 0.138 (0.429) data 0.000 (0.290) loss 1.2705 (1.2183) acc 87.5000 (90.6250) lr 7.5131e-04 eta 0:00:24
epoch [31/50] batch [3/3] time 0.083 (0.314) data 0.000 (0.193) loss 1.5645 (1.3337) acc 68.7500 (83.3333) lr 6.9098e-04 eta 0:00:17
epoch [32/50] batch [1/3] time 0.688 (0.688) data 0.546 (0.546) loss 1.2031 (1.2031) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:00:38
epoch [32/50] batch [2/3] time 0.137 (0.412) data 0.000 (0.273) loss 1.0830 (1.1431) acc 96.8750 (93.7500) lr 6.9098e-04 eta 0:00:22
epoch [32/50] batch [3/3] time 0.081 (0.302) data 0.000 (0.182) loss 1.2861 (1.1908) acc 93.7500 (93.7500) lr 6.3188e-04 eta 0:00:16
epoch [33/50] batch [1/3] time 0.790 (0.790) data 0.650 (0.650) loss 0.9668 (0.9668) acc 100.0000 (100.0000) lr 6.3188e-04 eta 0:00:41
epoch [33/50] batch [2/3] time 0.138 (0.464) data 0.000 (0.325) loss 1.0723 (1.0195) acc 87.5000 (93.7500) lr 6.3188e-04 eta 0:00:24
epoch [33/50] batch [3/3] time 0.083 (0.337) data 0.000 (0.217) loss 1.4004 (1.1465) acc 87.5000 (91.6667) lr 5.7422e-04 eta 0:00:17
epoch [34/50] batch [1/3] time 0.864 (0.864) data 0.723 (0.723) loss 0.9556 (0.9556) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:00:43
epoch [34/50] batch [2/3] time 0.139 (0.502) data 0.000 (0.362) loss 1.0283 (0.9919) acc 93.7500 (95.3125) lr 5.7422e-04 eta 0:00:24
epoch [34/50] batch [3/3] time 0.083 (0.362) data 0.000 (0.241) loss 1.2188 (1.0675) acc 93.7500 (94.7917) lr 5.1825e-04 eta 0:00:17
epoch [35/50] batch [1/3] time 0.888 (0.888) data 0.747 (0.747) loss 1.0020 (1.0020) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:00:41
epoch [35/50] batch [2/3] time 0.139 (0.513) data 0.000 (0.373) loss 0.9702 (0.9861) acc 100.0000 (98.4375) lr 5.1825e-04 eta 0:00:23
epoch [35/50] batch [3/3] time 0.084 (0.370) data 0.000 (0.249) loss 1.0098 (0.9940) acc 93.7500 (96.8750) lr 4.6417e-04 eta 0:00:16
epoch [36/50] batch [1/3] time 0.733 (0.733) data 0.591 (0.591) loss 0.8604 (0.8604) acc 96.8750 (96.8750) lr 4.6417e-04 eta 0:00:32
epoch [36/50] batch [2/3] time 0.139 (0.436) data 0.000 (0.296) loss 1.1182 (0.9893) acc 84.3750 (90.6250) lr 4.6417e-04 eta 0:00:18
epoch [36/50] batch [3/3] time 0.084 (0.319) data 0.000 (0.197) loss 1.1816 (1.0534) acc 93.7500 (91.6667) lr 4.1221e-04 eta 0:00:13
epoch [37/50] batch [1/3] time 0.728 (0.728) data 0.585 (0.585) loss 1.1279 (1.1279) acc 90.6250 (90.6250) lr 4.1221e-04 eta 0:00:29
epoch [37/50] batch [2/3] time 0.140 (0.434) data 0.000 (0.293) loss 1.0254 (1.0767) acc 87.5000 (89.0625) lr 4.1221e-04 eta 0:00:17
epoch [37/50] batch [3/3] time 0.083 (0.317) data 0.000 (0.195) loss 0.9951 (1.0495) acc 93.7500 (90.6250) lr 3.6258e-04 eta 0:00:12
epoch [38/50] batch [1/3] time 0.712 (0.712) data 0.570 (0.570) loss 1.1338 (1.1338) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:27
epoch [38/50] batch [2/3] time 0.140 (0.426) data 0.000 (0.285) loss 0.9551 (1.0444) acc 96.8750 (95.3125) lr 3.6258e-04 eta 0:00:15
epoch [38/50] batch [3/3] time 0.083 (0.312) data 0.000 (0.190) loss 1.3027 (1.1305) acc 75.0000 (88.5417) lr 3.1545e-04 eta 0:00:11
epoch [39/50] batch [1/3] time 0.816 (0.816) data 0.675 (0.675) loss 1.1875 (1.1875) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:00:28
epoch [39/50] batch [2/3] time 0.140 (0.478) data 0.000 (0.338) loss 1.1240 (1.1558) acc 84.3750 (89.0625) lr 3.1545e-04 eta 0:00:16
epoch [39/50] batch [3/3] time 0.083 (0.347) data 0.000 (0.225) loss 1.3037 (1.2051) acc 87.5000 (88.5417) lr 2.7103e-04 eta 0:00:11
epoch [40/50] batch [1/3] time 0.915 (0.915) data 0.774 (0.774) loss 1.1797 (1.1797) acc 90.6250 (90.6250) lr 2.7103e-04 eta 0:00:29
epoch [40/50] batch [2/3] time 0.139 (0.527) data 0.000 (0.387) loss 0.9038 (1.0417) acc 96.8750 (93.7500) lr 2.7103e-04 eta 0:00:16
epoch [40/50] batch [3/3] time 0.083 (0.379) data 0.000 (0.258) loss 1.1777 (1.0871) acc 93.7500 (93.7500) lr 2.2949e-04 eta 0:00:11
epoch [41/50] batch [1/3] time 0.731 (0.731) data 0.589 (0.589) loss 0.9946 (0.9946) acc 100.0000 (100.0000) lr 2.2949e-04 eta 0:00:21
epoch [41/50] batch [2/3] time 0.139 (0.435) data 0.000 (0.295) loss 1.2188 (1.1067) acc 90.6250 (95.3125) lr 2.2949e-04 eta 0:00:12
epoch [41/50] batch [3/3] time 0.084 (0.318) data 0.000 (0.197) loss 0.9941 (1.0692) acc 100.0000 (96.8750) lr 1.9098e-04 eta 0:00:08
epoch [42/50] batch [1/3] time 0.672 (0.672) data 0.530 (0.530) loss 1.2080 (1.2080) acc 84.3750 (84.3750) lr 1.9098e-04 eta 0:00:17
epoch [42/50] batch [2/3] time 0.140 (0.406) data 0.000 (0.265) loss 1.1318 (1.1699) acc 90.6250 (87.5000) lr 1.9098e-04 eta 0:00:10
epoch [42/50] batch [3/3] time 0.083 (0.298) data 0.000 (0.177) loss 1.1289 (1.1562) acc 87.5000 (87.5000) lr 1.5567e-04 eta 0:00:07
epoch [43/50] batch [1/3] time 0.660 (0.660) data 0.519 (0.519) loss 0.9648 (0.9648) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:15
epoch [43/50] batch [2/3] time 0.139 (0.400) data 0.000 (0.260) loss 1.0068 (0.9858) acc 100.0000 (95.3125) lr 1.5567e-04 eta 0:00:08
epoch [43/50] batch [3/3] time 0.084 (0.294) data 0.000 (0.173) loss 1.2197 (1.0638) acc 93.7500 (94.7917) lr 1.2369e-04 eta 0:00:06
epoch [44/50] batch [1/3] time 0.710 (0.710) data 0.571 (0.571) loss 1.0723 (1.0723) acc 90.6250 (90.6250) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [2/3] time 0.139 (0.425) data 0.000 (0.285) loss 1.0537 (1.0630) acc 90.6250 (90.6250) lr 1.2369e-04 eta 0:00:08
epoch [44/50] batch [3/3] time 0.082 (0.311) data 0.000 (0.190) loss 0.9736 (1.0332) acc 93.7500 (91.6667) lr 9.5173e-05 eta 0:00:05
epoch [45/50] batch [1/3] time 0.687 (0.687) data 0.547 (0.547) loss 0.9932 (0.9932) acc 100.0000 (100.0000) lr 9.5173e-05 eta 0:00:11
epoch [45/50] batch [2/3] time 0.139 (0.413) data 0.000 (0.273) loss 1.1367 (1.0649) acc 93.7500 (96.8750) lr 9.5173e-05 eta 0:00:06
epoch [45/50] batch [3/3] time 0.086 (0.304) data 0.000 (0.182) loss 0.8521 (0.9940) acc 100.0000 (97.9167) lr 7.0224e-05 eta 0:00:04
epoch [46/50] batch [1/3] time 0.712 (0.712) data 0.571 (0.571) loss 1.2061 (1.2061) acc 84.3750 (84.3750) lr 7.0224e-05 eta 0:00:09
epoch [46/50] batch [2/3] time 0.140 (0.426) data 0.000 (0.286) loss 1.1309 (1.1685) acc 90.6250 (87.5000) lr 7.0224e-05 eta 0:00:05
epoch [46/50] batch [3/3] time 0.084 (0.312) data 0.000 (0.191) loss 0.9980 (1.1117) acc 100.0000 (91.6667) lr 4.8943e-05 eta 0:00:03
epoch [47/50] batch [1/3] time 0.718 (0.718) data 0.572 (0.572) loss 0.9590 (0.9590) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:07
epoch [47/50] batch [2/3] time 0.140 (0.429) data 0.000 (0.286) loss 0.9727 (0.9658) acc 96.8750 (96.8750) lr 4.8943e-05 eta 0:00:04
epoch [47/50] batch [3/3] time 0.083 (0.314) data 0.000 (0.191) loss 1.4219 (1.1178) acc 81.2500 (91.6667) lr 3.1417e-05 eta 0:00:02
epoch [48/50] batch [1/3] time 0.803 (0.803) data 0.662 (0.662) loss 1.2031 (1.2031) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [2/3] time 0.139 (0.471) data 0.000 (0.331) loss 1.1699 (1.1865) acc 87.5000 (87.5000) lr 3.1417e-05 eta 0:00:03
epoch [48/50] batch [3/3] time 0.096 (0.346) data 0.000 (0.221) loss 1.3086 (1.2272) acc 87.5000 (87.5000) lr 1.7713e-05 eta 0:00:02
epoch [49/50] batch [1/3] time 0.737 (0.737) data 0.591 (0.591) loss 1.2578 (1.2578) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [2/3] time 0.139 (0.438) data 0.000 (0.296) loss 1.1348 (1.1963) acc 84.3750 (87.5000) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [3/3] time 0.084 (0.320) data 0.000 (0.197) loss 1.1113 (1.1680) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [1/3] time 0.779 (0.779) data 0.637 (0.637) loss 1.2256 (1.2256) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [2/3] time 0.138 (0.458) data 0.000 (0.319) loss 1.1650 (1.1953) acc 93.7500 (90.6250) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [3/3] time 0.082 (0.333) data 0.000 (0.213) loss 0.8989 (1.0965) acc 100.0000 (93.7500) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:07,  8.48s/it] 22%|██▏       | 2/9 [00:09<00:29,  4.15s/it] 33%|███▎      | 3/9 [00:10<00:16,  2.76s/it] 44%|████▍     | 4/9 [00:11<00:10,  2.11s/it] 56%|█████▌    | 5/9 [00:12<00:07,  1.75s/it] 67%|██████▋   | 6/9 [00:14<00:04,  1.53s/it] 78%|███████▊  | 7/9 [00:15<00:02,  1.40s/it] 89%|████████▉ | 8/9 [00:16<00:01,  1.31s/it]100%|██████████| 9/9 [00:16<00:00,  1.06s/it]100%|██████████| 9/9 [00:16<00:00,  1.88s/it]
=> result
* total: 4,200
* correct: 3,827
* accuracy: 91.1%
* error: 8.9%
* macro_f1: 91.2%
Elapsed: 0:01:11
Run this job and save the output to output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     4,200
---------  -------
['Annual Crop Land', 'Forest', 'Herbaceous Vegetation Land', 'Highway or Road', 'Industrial Buildings']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Annual Crop Land.', 'X X X X Forest.', 'X X X X Herbaceous Vegetation Land.', 'X X X X Highway or Road.', 'X X X X Industrial Buildings.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([5, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [1/3] time 0.877 (0.877) data 0.707 (0.707) loss 5.4297 (5.4297) acc 56.2500 (56.2500) lr 1.0000e-05 eta 0:02:10
epoch [1/50] batch [2/3] time 0.140 (0.509) data 0.000 (0.354) loss 5.8828 (5.6562) acc 34.3750 (45.3125) lr 1.0000e-05 eta 0:01:15
epoch [1/50] batch [3/3] time 0.083 (0.367) data 0.000 (0.236) loss 5.9648 (5.7591) acc 25.0000 (38.5417) lr 2.0000e-03 eta 0:00:53
epoch [2/50] batch [1/3] time 0.795 (0.795) data 0.654 (0.654) loss 5.5625 (5.5625) acc 43.7500 (43.7500) lr 2.0000e-03 eta 0:01:56
epoch [2/50] batch [2/3] time 0.138 (0.467) data 0.000 (0.327) loss 4.2422 (4.9023) acc 62.5000 (53.1250) lr 2.0000e-03 eta 0:01:07
epoch [2/50] batch [3/3] time 0.085 (0.340) data 0.000 (0.218) loss 4.3945 (4.7331) acc 62.5000 (56.2500) lr 1.9980e-03 eta 0:00:48
epoch [3/50] batch [1/3] time 0.943 (0.943) data 0.801 (0.801) loss 3.8281 (3.8281) acc 75.0000 (75.0000) lr 1.9980e-03 eta 0:02:14
epoch [3/50] batch [2/3] time 0.137 (0.540) data 0.000 (0.401) loss 3.6660 (3.7471) acc 71.8750 (73.4375) lr 1.9980e-03 eta 0:01:16
epoch [3/50] batch [3/3] time 0.082 (0.387) data 0.000 (0.267) loss 3.2969 (3.5970) acc 68.7500 (71.8750) lr 1.9921e-03 eta 0:00:54
epoch [4/50] batch [1/3] time 0.783 (0.783) data 0.643 (0.643) loss 2.7578 (2.7578) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:01:49
epoch [4/50] batch [2/3] time 0.139 (0.461) data 0.001 (0.322) loss 2.7891 (2.7734) acc 75.0000 (75.0000) lr 1.9921e-03 eta 0:01:04
epoch [4/50] batch [3/3] time 0.083 (0.335) data 0.000 (0.215) loss 2.1836 (2.5768) acc 87.5000 (79.1667) lr 1.9823e-03 eta 0:00:46
epoch [5/50] batch [1/3] time 0.778 (0.778) data 0.638 (0.638) loss 2.5664 (2.5664) acc 65.6250 (65.6250) lr 1.9823e-03 eta 0:01:46
epoch [5/50] batch [2/3] time 0.138 (0.458) data 0.000 (0.319) loss 2.0273 (2.2969) acc 87.5000 (76.5625) lr 1.9823e-03 eta 0:01:02
epoch [5/50] batch [3/3] time 0.082 (0.333) data 0.000 (0.213) loss 2.0293 (2.2077) acc 81.2500 (78.1250) lr 1.9686e-03 eta 0:00:44
epoch [6/50] batch [1/3] time 0.805 (0.805) data 0.664 (0.664) loss 2.0000 (2.0000) acc 75.0000 (75.0000) lr 1.9686e-03 eta 0:01:47
epoch [6/50] batch [2/3] time 0.139 (0.472) data 0.000 (0.332) loss 1.7852 (1.8926) acc 87.5000 (81.2500) lr 1.9686e-03 eta 0:01:02
epoch [6/50] batch [3/3] time 0.083 (0.342) data 0.000 (0.222) loss 2.0098 (1.9316) acc 81.2500 (81.2500) lr 1.9511e-03 eta 0:00:45
epoch [7/50] batch [1/3] time 0.857 (0.857) data 0.718 (0.718) loss 2.1973 (2.1973) acc 59.3750 (59.3750) lr 1.9511e-03 eta 0:01:52
epoch [7/50] batch [2/3] time 0.138 (0.498) data 0.000 (0.359) loss 1.7158 (1.9565) acc 81.2500 (70.3125) lr 1.9511e-03 eta 0:01:04
epoch [7/50] batch [3/3] time 0.090 (0.362) data 0.000 (0.239) loss 1.8828 (1.9320) acc 81.2500 (73.9583) lr 1.9298e-03 eta 0:00:46
epoch [8/50] batch [1/3] time 0.809 (0.809) data 0.670 (0.670) loss 1.7734 (1.7734) acc 81.2500 (81.2500) lr 1.9298e-03 eta 0:01:43
epoch [8/50] batch [2/3] time 0.139 (0.474) data 0.000 (0.335) loss 1.5840 (1.6787) acc 87.5000 (84.3750) lr 1.9298e-03 eta 0:01:00
epoch [8/50] batch [3/3] time 0.082 (0.344) data 0.000 (0.223) loss 1.5605 (1.6393) acc 81.2500 (83.3333) lr 1.9048e-03 eta 0:00:43
epoch [9/50] batch [1/3] time 0.777 (0.777) data 0.638 (0.638) loss 1.8926 (1.8926) acc 81.2500 (81.2500) lr 1.9048e-03 eta 0:01:37
epoch [9/50] batch [2/3] time 0.138 (0.458) data 0.000 (0.319) loss 1.6348 (1.7637) acc 81.2500 (81.2500) lr 1.9048e-03 eta 0:00:56
epoch [9/50] batch [3/3] time 0.087 (0.334) data 0.000 (0.213) loss 1.7812 (1.7695) acc 81.2500 (81.2500) lr 1.8763e-03 eta 0:00:41
epoch [10/50] batch [1/3] time 0.820 (0.820) data 0.681 (0.681) loss 1.7734 (1.7734) acc 78.1250 (78.1250) lr 1.8763e-03 eta 0:01:40
epoch [10/50] batch [2/3] time 0.139 (0.480) data 0.000 (0.341) loss 1.5098 (1.6416) acc 90.6250 (84.3750) lr 1.8763e-03 eta 0:00:58
epoch [10/50] batch [3/3] time 0.082 (0.347) data 0.000 (0.227) loss 1.4971 (1.5934) acc 81.2500 (83.3333) lr 1.8443e-03 eta 0:00:41
epoch [11/50] batch [1/3] time 0.872 (0.872) data 0.732 (0.732) loss 1.6553 (1.6553) acc 81.2500 (81.2500) lr 1.8443e-03 eta 0:01:43
epoch [11/50] batch [2/3] time 0.139 (0.506) data 0.001 (0.366) loss 1.2383 (1.4468) acc 87.5000 (84.3750) lr 1.8443e-03 eta 0:00:59
epoch [11/50] batch [3/3] time 0.083 (0.365) data 0.000 (0.244) loss 1.5557 (1.4831) acc 75.0000 (81.2500) lr 1.8090e-03 eta 0:00:42
epoch [12/50] batch [1/3] time 0.776 (0.776) data 0.637 (0.637) loss 1.6621 (1.6621) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:01:30
epoch [12/50] batch [2/3] time 0.138 (0.457) data 0.000 (0.318) loss 1.4707 (1.5664) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:00:52
epoch [12/50] batch [3/3] time 0.082 (0.332) data 0.000 (0.212) loss 1.3789 (1.5039) acc 93.7500 (89.5833) lr 1.7705e-03 eta 0:00:37
epoch [13/50] batch [1/3] time 0.775 (0.775) data 0.629 (0.629) loss 1.4111 (1.4111) acc 78.1250 (78.1250) lr 1.7705e-03 eta 0:01:27
epoch [13/50] batch [2/3] time 0.139 (0.457) data 0.000 (0.315) loss 1.3008 (1.3560) acc 87.5000 (82.8125) lr 1.7705e-03 eta 0:00:51
epoch [13/50] batch [3/3] time 0.083 (0.332) data 0.000 (0.210) loss 1.3838 (1.3652) acc 81.2500 (82.2917) lr 1.7290e-03 eta 0:00:36
epoch [14/50] batch [1/3] time 0.790 (0.790) data 0.650 (0.650) loss 1.6602 (1.6602) acc 81.2500 (81.2500) lr 1.7290e-03 eta 0:01:26
epoch [14/50] batch [2/3] time 0.137 (0.464) data 0.000 (0.325) loss 1.6514 (1.6558) acc 78.1250 (79.6875) lr 1.7290e-03 eta 0:00:50
epoch [14/50] batch [3/3] time 0.083 (0.337) data 0.000 (0.217) loss 1.0850 (1.4655) acc 93.7500 (84.3750) lr 1.6845e-03 eta 0:00:36
epoch [15/50] batch [1/3] time 0.840 (0.840) data 0.701 (0.701) loss 1.4805 (1.4805) acc 84.3750 (84.3750) lr 1.6845e-03 eta 0:01:29
epoch [15/50] batch [2/3] time 0.137 (0.489) data 0.000 (0.351) loss 1.1855 (1.3330) acc 93.7500 (89.0625) lr 1.6845e-03 eta 0:00:51
epoch [15/50] batch [3/3] time 0.082 (0.353) data 0.000 (0.234) loss 1.3750 (1.3470) acc 93.7500 (90.6250) lr 1.6374e-03 eta 0:00:37
epoch [16/50] batch [1/3] time 0.953 (0.953) data 0.808 (0.808) loss 1.0605 (1.0605) acc 96.8750 (96.8750) lr 1.6374e-03 eta 0:01:39
epoch [16/50] batch [2/3] time 0.510 (0.732) data 0.370 (0.589) loss 1.4023 (1.2314) acc 84.3750 (90.6250) lr 1.6374e-03 eta 0:01:15
epoch [16/50] batch [3/3] time 0.082 (0.515) data 0.000 (0.393) loss 1.0586 (1.1738) acc 100.0000 (93.7500) lr 1.5878e-03 eta 0:00:52
epoch [17/50] batch [1/3] time 0.814 (0.814) data 0.672 (0.672) loss 1.6689 (1.6689) acc 78.1250 (78.1250) lr 1.5878e-03 eta 0:01:22
epoch [17/50] batch [2/3] time 0.138 (0.476) data 0.000 (0.336) loss 1.2412 (1.4551) acc 87.5000 (82.8125) lr 1.5878e-03 eta 0:00:47
epoch [17/50] batch [3/3] time 0.082 (0.345) data 0.000 (0.224) loss 1.2256 (1.3786) acc 87.5000 (84.3750) lr 1.5358e-03 eta 0:00:34
epoch [18/50] batch [1/3] time 0.783 (0.783) data 0.643 (0.643) loss 1.3555 (1.3555) acc 90.6250 (90.6250) lr 1.5358e-03 eta 0:01:16
epoch [18/50] batch [2/3] time 0.138 (0.460) data 0.000 (0.322) loss 1.3652 (1.3604) acc 84.3750 (87.5000) lr 1.5358e-03 eta 0:00:44
epoch [18/50] batch [3/3] time 0.082 (0.334) data 0.000 (0.214) loss 1.1621 (1.2943) acc 87.5000 (87.5000) lr 1.4818e-03 eta 0:00:32
epoch [19/50] batch [1/3] time 0.778 (0.778) data 0.636 (0.636) loss 1.5293 (1.5293) acc 78.1250 (78.1250) lr 1.4818e-03 eta 0:01:13
epoch [19/50] batch [2/3] time 0.140 (0.459) data 0.001 (0.318) loss 1.1582 (1.3438) acc 84.3750 (81.2500) lr 1.4818e-03 eta 0:00:43
epoch [19/50] batch [3/3] time 0.083 (0.334) data 0.000 (0.212) loss 1.5996 (1.4290) acc 68.7500 (77.0833) lr 1.4258e-03 eta 0:00:31
epoch [20/50] batch [1/3] time 0.750 (0.750) data 0.609 (0.609) loss 1.1816 (1.1816) acc 90.6250 (90.6250) lr 1.4258e-03 eta 0:01:09
epoch [20/50] batch [2/3] time 0.139 (0.445) data 0.000 (0.305) loss 1.2949 (1.2383) acc 87.5000 (89.0625) lr 1.4258e-03 eta 0:00:40
epoch [20/50] batch [3/3] time 0.090 (0.326) data 0.000 (0.203) loss 1.4512 (1.3092) acc 81.2500 (86.4583) lr 1.3681e-03 eta 0:00:29
epoch [21/50] batch [1/3] time 0.710 (0.710) data 0.570 (0.570) loss 1.3008 (1.3008) acc 84.3750 (84.3750) lr 1.3681e-03 eta 0:01:03
epoch [21/50] batch [2/3] time 0.137 (0.424) data 0.000 (0.285) loss 1.3213 (1.3110) acc 90.6250 (87.5000) lr 1.3681e-03 eta 0:00:37
epoch [21/50] batch [3/3] time 0.082 (0.310) data 0.000 (0.190) loss 1.0928 (1.2383) acc 87.5000 (87.5000) lr 1.3090e-03 eta 0:00:26
epoch [22/50] batch [1/3] time 0.699 (0.699) data 0.558 (0.558) loss 1.1992 (1.1992) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:01:00
epoch [22/50] batch [2/3] time 0.140 (0.419) data 0.000 (0.279) loss 1.2598 (1.2295) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:00:35
epoch [22/50] batch [3/3] time 0.081 (0.307) data 0.000 (0.186) loss 1.5566 (1.3385) acc 81.2500 (87.5000) lr 1.2487e-03 eta 0:00:25
epoch [23/50] batch [1/3] time 0.870 (0.870) data 0.728 (0.728) loss 1.2559 (1.2559) acc 90.6250 (90.6250) lr 1.2487e-03 eta 0:01:12
epoch [23/50] batch [2/3] time 0.137 (0.503) data 0.000 (0.364) loss 1.1094 (1.1826) acc 87.5000 (89.0625) lr 1.2487e-03 eta 0:00:41
epoch [23/50] batch [3/3] time 0.083 (0.363) data 0.000 (0.243) loss 1.0410 (1.1354) acc 87.5000 (88.5417) lr 1.1874e-03 eta 0:00:29
epoch [24/50] batch [1/3] time 0.741 (0.741) data 0.595 (0.595) loss 1.1426 (1.1426) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:00:59
epoch [24/50] batch [2/3] time 0.138 (0.439) data 0.000 (0.298) loss 1.0527 (1.0977) acc 90.6250 (90.6250) lr 1.1874e-03 eta 0:00:34
epoch [24/50] batch [3/3] time 0.083 (0.321) data 0.000 (0.198) loss 0.9541 (1.0498) acc 93.7500 (91.6667) lr 1.1253e-03 eta 0:00:25
epoch [25/50] batch [1/3] time 0.709 (0.709) data 0.569 (0.569) loss 1.0127 (1.0127) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:00:54
epoch [25/50] batch [2/3] time 0.138 (0.423) data 0.000 (0.285) loss 1.0146 (1.0137) acc 93.7500 (93.7500) lr 1.1253e-03 eta 0:00:32
epoch [25/50] batch [3/3] time 0.082 (0.309) data 0.000 (0.190) loss 0.6436 (0.8903) acc 100.0000 (95.8333) lr 1.0628e-03 eta 0:00:23
epoch [26/50] batch [1/3] time 0.715 (0.715) data 0.573 (0.573) loss 1.1504 (1.1504) acc 87.5000 (87.5000) lr 1.0628e-03 eta 0:00:52
epoch [26/50] batch [2/3] time 0.139 (0.427) data 0.000 (0.287) loss 1.2734 (1.2119) acc 90.6250 (89.0625) lr 1.0628e-03 eta 0:00:31
epoch [26/50] batch [3/3] time 0.083 (0.312) data 0.000 (0.191) loss 0.9463 (1.1234) acc 93.7500 (90.6250) lr 1.0000e-03 eta 0:00:22
epoch [27/50] batch [1/3] time 0.688 (0.688) data 0.548 (0.548) loss 1.0244 (1.0244) acc 90.6250 (90.6250) lr 1.0000e-03 eta 0:00:48
epoch [27/50] batch [2/3] time 0.139 (0.414) data 0.000 (0.274) loss 1.0059 (1.0151) acc 93.7500 (92.1875) lr 1.0000e-03 eta 0:00:28
epoch [27/50] batch [3/3] time 0.083 (0.303) data 0.000 (0.183) loss 1.0352 (1.0218) acc 93.7500 (92.7083) lr 9.3721e-04 eta 0:00:20
epoch [28/50] batch [1/3] time 0.694 (0.694) data 0.552 (0.552) loss 1.1172 (1.1172) acc 93.7500 (93.7500) lr 9.3721e-04 eta 0:00:47
epoch [28/50] batch [2/3] time 0.139 (0.416) data 0.000 (0.276) loss 1.2803 (1.1987) acc 81.2500 (87.5000) lr 9.3721e-04 eta 0:00:27
epoch [28/50] batch [3/3] time 0.086 (0.306) data 0.000 (0.184) loss 1.0459 (1.1478) acc 93.7500 (89.5833) lr 8.7467e-04 eta 0:00:20
epoch [29/50] batch [1/3] time 0.706 (0.706) data 0.564 (0.564) loss 1.2852 (1.2852) acc 84.3750 (84.3750) lr 8.7467e-04 eta 0:00:45
epoch [29/50] batch [2/3] time 0.259 (0.482) data 0.000 (0.282) loss 1.0898 (1.1875) acc 90.6250 (87.5000) lr 8.7467e-04 eta 0:00:30
epoch [29/50] batch [3/3] time 0.085 (0.350) data 0.000 (0.188) loss 0.9785 (1.1178) acc 93.7500 (89.5833) lr 8.1262e-04 eta 0:00:22
epoch [30/50] batch [1/3] time 0.711 (0.711) data 0.568 (0.568) loss 1.1230 (1.1230) acc 93.7500 (93.7500) lr 8.1262e-04 eta 0:00:44
epoch [30/50] batch [2/3] time 0.140 (0.426) data 0.001 (0.284) loss 0.9551 (1.0391) acc 96.8750 (95.3125) lr 8.1262e-04 eta 0:00:25
epoch [30/50] batch [3/3] time 0.083 (0.311) data 0.000 (0.190) loss 0.9053 (0.9945) acc 87.5000 (92.7083) lr 7.5131e-04 eta 0:00:18
epoch [31/50] batch [1/3] time 0.742 (0.742) data 0.601 (0.601) loss 1.3428 (1.3428) acc 84.3750 (84.3750) lr 7.5131e-04 eta 0:00:43
epoch [31/50] batch [2/3] time 0.139 (0.440) data 0.000 (0.301) loss 0.9863 (1.1646) acc 87.5000 (85.9375) lr 7.5131e-04 eta 0:00:25
epoch [31/50] batch [3/3] time 0.082 (0.321) data 0.000 (0.200) loss 0.9126 (1.0806) acc 100.0000 (90.6250) lr 6.9098e-04 eta 0:00:18
epoch [32/50] batch [1/3] time 0.728 (0.728) data 0.586 (0.586) loss 1.1328 (1.1328) acc 100.0000 (100.0000) lr 6.9098e-04 eta 0:00:40
epoch [32/50] batch [2/3] time 0.138 (0.433) data 0.000 (0.293) loss 1.1016 (1.1172) acc 90.6250 (95.3125) lr 6.9098e-04 eta 0:00:23
epoch [32/50] batch [3/3] time 0.083 (0.316) data 0.000 (0.195) loss 0.9424 (1.0589) acc 100.0000 (96.8750) lr 6.3188e-04 eta 0:00:17
epoch [33/50] batch [1/3] time 0.731 (0.731) data 0.588 (0.588) loss 1.3975 (1.3975) acc 84.3750 (84.3750) lr 6.3188e-04 eta 0:00:38
epoch [33/50] batch [2/3] time 0.142 (0.437) data 0.000 (0.294) loss 0.9136 (1.1555) acc 93.7500 (89.0625) lr 6.3188e-04 eta 0:00:22
epoch [33/50] batch [3/3] time 0.084 (0.319) data 0.000 (0.196) loss 0.9194 (1.0768) acc 100.0000 (92.7083) lr 5.7422e-04 eta 0:00:16
epoch [34/50] batch [1/3] time 0.876 (0.876) data 0.735 (0.735) loss 1.2256 (1.2256) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:00:43
epoch [34/50] batch [2/3] time 0.141 (0.509) data 0.000 (0.368) loss 1.1699 (1.1978) acc 90.6250 (93.7500) lr 5.7422e-04 eta 0:00:24
epoch [34/50] batch [3/3] time 0.090 (0.369) data 0.000 (0.245) loss 0.8359 (1.0771) acc 93.7500 (93.7500) lr 5.1825e-04 eta 0:00:17
epoch [35/50] batch [1/3] time 0.797 (0.797) data 0.651 (0.651) loss 1.3213 (1.3213) acc 84.3750 (84.3750) lr 5.1825e-04 eta 0:00:37
epoch [35/50] batch [2/3] time 0.144 (0.471) data 0.001 (0.326) loss 1.0986 (1.2100) acc 93.7500 (89.0625) lr 5.1825e-04 eta 0:00:21
epoch [35/50] batch [3/3] time 0.088 (0.343) data 0.000 (0.217) loss 0.7930 (1.0710) acc 100.0000 (92.7083) lr 4.6417e-04 eta 0:00:15
epoch [36/50] batch [1/3] time 0.801 (0.801) data 0.654 (0.654) loss 1.2900 (1.2900) acc 84.3750 (84.3750) lr 4.6417e-04 eta 0:00:35
epoch [36/50] batch [2/3] time 0.147 (0.474) data 0.001 (0.327) loss 0.8550 (1.0725) acc 96.8750 (90.6250) lr 4.6417e-04 eta 0:00:20
epoch [36/50] batch [3/3] time 0.089 (0.346) data 0.000 (0.218) loss 1.3984 (1.1812) acc 81.2500 (87.5000) lr 4.1221e-04 eta 0:00:14
epoch [37/50] batch [1/3] time 0.692 (0.692) data 0.545 (0.545) loss 1.0566 (1.0566) acc 90.6250 (90.6250) lr 4.1221e-04 eta 0:00:28
epoch [37/50] batch [2/3] time 0.161 (0.426) data 0.000 (0.273) loss 1.2852 (1.1709) acc 84.3750 (87.5000) lr 4.1221e-04 eta 0:00:17
epoch [37/50] batch [3/3] time 0.086 (0.313) data 0.000 (0.182) loss 0.7866 (1.0428) acc 93.7500 (89.5833) lr 3.6258e-04 eta 0:00:12
epoch [38/50] batch [1/3] time 0.705 (0.705) data 0.564 (0.564) loss 1.0879 (1.0879) acc 93.7500 (93.7500) lr 3.6258e-04 eta 0:00:26
epoch [38/50] batch [2/3] time 0.140 (0.423) data 0.001 (0.282) loss 0.8760 (0.9819) acc 96.8750 (95.3125) lr 3.6258e-04 eta 0:00:15
epoch [38/50] batch [3/3] time 0.160 (0.335) data 0.000 (0.188) loss 0.8623 (0.9421) acc 93.7500 (94.7917) lr 3.1545e-04 eta 0:00:12
epoch [39/50] batch [1/3] time 0.815 (0.815) data 0.673 (0.673) loss 0.9805 (0.9805) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:00:28
epoch [39/50] batch [2/3] time 0.140 (0.477) data 0.000 (0.336) loss 1.1191 (1.0498) acc 93.7500 (93.7500) lr 3.1545e-04 eta 0:00:16
epoch [39/50] batch [3/3] time 0.084 (0.346) data 0.000 (0.224) loss 1.7441 (1.2812) acc 68.7500 (85.4167) lr 2.7103e-04 eta 0:00:11
epoch [40/50] batch [1/3] time 0.763 (0.763) data 0.599 (0.599) loss 0.8428 (0.8428) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:00:24
epoch [40/50] batch [2/3] time 0.140 (0.451) data 0.000 (0.300) loss 1.1982 (1.0205) acc 84.3750 (90.6250) lr 2.7103e-04 eta 0:00:13
epoch [40/50] batch [3/3] time 0.083 (0.328) data 0.000 (0.200) loss 0.7837 (0.9416) acc 100.0000 (93.7500) lr 2.2949e-04 eta 0:00:09
epoch [41/50] batch [1/3] time 0.757 (0.757) data 0.614 (0.614) loss 0.9385 (0.9385) acc 96.8750 (96.8750) lr 2.2949e-04 eta 0:00:21
epoch [41/50] batch [2/3] time 0.137 (0.447) data 0.000 (0.307) loss 1.1572 (1.0479) acc 87.5000 (92.1875) lr 2.2949e-04 eta 0:00:12
epoch [41/50] batch [3/3] time 0.083 (0.326) data 0.000 (0.205) loss 0.8877 (0.9945) acc 93.7500 (92.7083) lr 1.9098e-04 eta 0:00:08
epoch [42/50] batch [1/3] time 0.735 (0.735) data 0.594 (0.594) loss 0.7529 (0.7529) acc 96.8750 (96.8750) lr 1.9098e-04 eta 0:00:19
epoch [42/50] batch [2/3] time 0.161 (0.448) data 0.000 (0.297) loss 1.1836 (0.9683) acc 90.6250 (93.7500) lr 1.9098e-04 eta 0:00:11
epoch [42/50] batch [3/3] time 0.086 (0.327) data 0.000 (0.198) loss 1.1777 (1.0381) acc 93.7500 (93.7500) lr 1.5567e-04 eta 0:00:07
epoch [43/50] batch [1/3] time 0.898 (0.898) data 0.756 (0.756) loss 0.9727 (0.9727) acc 96.8750 (96.8750) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [2/3] time 0.142 (0.520) data 0.000 (0.378) loss 1.0625 (1.0176) acc 87.5000 (92.1875) lr 1.5567e-04 eta 0:00:11
epoch [43/50] batch [3/3] time 0.084 (0.375) data 0.000 (0.252) loss 1.1963 (1.0771) acc 100.0000 (94.7917) lr 1.2369e-04 eta 0:00:07
epoch [44/50] batch [1/3] time 0.708 (0.708) data 0.566 (0.566) loss 1.1523 (1.1523) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [2/3] time 0.138 (0.423) data 0.000 (0.283) loss 0.8794 (1.0159) acc 90.6250 (92.1875) lr 1.2369e-04 eta 0:00:08
epoch [44/50] batch [3/3] time 0.087 (0.311) data 0.000 (0.189) loss 0.9233 (0.9850) acc 87.5000 (90.6250) lr 9.5173e-05 eta 0:00:05
epoch [45/50] batch [1/3] time 0.909 (0.909) data 0.769 (0.769) loss 1.0723 (1.0723) acc 87.5000 (87.5000) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [2/3] time 0.140 (0.525) data 0.000 (0.384) loss 0.9902 (1.0312) acc 100.0000 (93.7500) lr 9.5173e-05 eta 0:00:08
epoch [45/50] batch [3/3] time 0.083 (0.378) data 0.000 (0.256) loss 0.8296 (0.9640) acc 93.7500 (93.7500) lr 7.0224e-05 eta 0:00:05
epoch [46/50] batch [1/3] time 0.865 (0.865) data 0.722 (0.722) loss 1.1045 (1.1045) acc 90.6250 (90.6250) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [2/3] time 0.139 (0.502) data 0.000 (0.361) loss 1.1914 (1.1479) acc 87.5000 (89.0625) lr 7.0224e-05 eta 0:00:06
epoch [46/50] batch [3/3] time 0.086 (0.363) data 0.000 (0.241) loss 0.9683 (1.0881) acc 81.2500 (86.4583) lr 4.8943e-05 eta 0:00:04
epoch [47/50] batch [1/3] time 0.735 (0.735) data 0.594 (0.594) loss 0.9902 (0.9902) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [2/3] time 0.141 (0.438) data 0.001 (0.297) loss 0.8477 (0.9189) acc 93.7500 (93.7500) lr 4.8943e-05 eta 0:00:04
epoch [47/50] batch [3/3] time 0.085 (0.320) data 0.000 (0.198) loss 0.8149 (0.8843) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:00:02
epoch [48/50] batch [1/3] time 0.680 (0.680) data 0.540 (0.540) loss 0.9961 (0.9961) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:05
epoch [48/50] batch [2/3] time 0.139 (0.409) data 0.000 (0.270) loss 0.8809 (0.9385) acc 93.7500 (95.3125) lr 3.1417e-05 eta 0:00:02
epoch [48/50] batch [3/3] time 0.082 (0.300) data 0.000 (0.180) loss 1.0596 (0.9788) acc 93.7500 (94.7917) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [1/3] time 0.678 (0.678) data 0.535 (0.535) loss 1.2227 (1.2227) acc 84.3750 (84.3750) lr 1.7713e-05 eta 0:00:03
epoch [49/50] batch [2/3] time 0.139 (0.408) data 0.000 (0.268) loss 0.8965 (1.0596) acc 93.7500 (89.0625) lr 1.7713e-05 eta 0:00:01
epoch [49/50] batch [3/3] time 0.083 (0.300) data 0.000 (0.179) loss 1.0137 (1.0443) acc 93.7500 (90.6250) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [1/3] time 0.679 (0.679) data 0.541 (0.541) loss 1.0410 (1.0410) acc 87.5000 (87.5000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [2/3] time 0.138 (0.408) data 0.000 (0.271) loss 0.8369 (0.9390) acc 100.0000 (93.7500) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [3/3] time 0.084 (0.300) data 0.000 (0.181) loss 1.1211 (0.9997) acc 93.7500 (93.7500) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:04<00:35,  4.45s/it] 22%|██▏       | 2/9 [00:05<00:17,  2.49s/it] 33%|███▎      | 3/9 [00:06<00:11,  1.86s/it] 44%|████▍     | 4/9 [00:07<00:07,  1.56s/it] 56%|█████▌    | 5/9 [00:08<00:05,  1.40s/it] 67%|██████▋   | 6/9 [00:10<00:03,  1.30s/it] 78%|███████▊  | 7/9 [00:11<00:02,  1.24s/it] 89%|████████▉ | 8/9 [00:12<00:01,  1.20s/it]100%|██████████| 9/9 [00:12<00:00,  1.02it/s]100%|██████████| 9/9 [00:12<00:00,  1.43s/it]
=> result
* total: 4,200
* correct: 3,871
* accuracy: 92.2%
* error: 7.8%
* macro_f1: 92.2%
Elapsed: 0:01:10
Run this job and save the output to output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     3,900
---------  -------
['Pasture Land', 'Permanent Crop Land', 'Residential Buildings', 'River', 'Sea or Lake']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Pasture Land.', 'X X X X Permanent Crop Land.', 'X X X X Residential Buildings.', 'X X X X River.', 'X X X X Sea or Lake.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([5, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:04<00:30,  4.29s/it] 25%|██▌       | 2/8 [00:05<00:14,  2.42s/it] 38%|███▊      | 3/8 [00:06<00:09,  1.83s/it] 50%|█████     | 4/8 [00:07<00:06,  1.55s/it] 62%|██████▎   | 5/8 [00:08<00:04,  1.39s/it] 75%|███████▌  | 6/8 [00:09<00:02,  1.30s/it] 88%|████████▊ | 7/8 [00:10<00:01,  1.24s/it]100%|██████████| 8/8 [00:11<00:00,  1.15s/it]100%|██████████| 8/8 [00:12<00:00,  1.51s/it]
=> result
* total: 3,900
* correct: 3,147
* accuracy: 80.7%
* error: 19.3%
* macro_f1: 79.2%
Run this job and save the output to output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     3,900
---------  -------
['Pasture Land', 'Permanent Crop Land', 'Residential Buildings', 'River', 'Sea or Lake']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Pasture Land.', 'X X X X Permanent Crop Land.', 'X X X X Residential Buildings.', 'X X X X River.', 'X X X X Sea or Lake.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([5, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:03<00:23,  3.32s/it] 25%|██▌       | 2/8 [00:04<00:12,  2.02s/it] 38%|███▊      | 3/8 [00:05<00:08,  1.61s/it] 50%|█████     | 4/8 [00:06<00:05,  1.41s/it] 62%|██████▎   | 5/8 [00:07<00:03,  1.30s/it] 75%|███████▌  | 6/8 [00:08<00:02,  1.24s/it] 88%|████████▊ | 7/8 [00:09<00:01,  1.20s/it]100%|██████████| 8/8 [00:10<00:00,  1.12s/it]100%|██████████| 8/8 [00:11<00:00,  1.38s/it]
=> result
* total: 3,900
* correct: 2,542
* accuracy: 65.2%
* error: 34.8%
* macro_f1: 62.4%
Run this job and save the output to output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/eurosat.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: EuroSAT
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: EuroSAT
Reading split from /data/yht/data/cl/data/eurosat/split_zhou_EuroSAT.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/eurosat/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    EuroSAT
# classes  5
# train_x  80
# val      20
# test     3,900
---------  -------
['Pasture Land', 'Permanent Crop Land', 'Residential Buildings', 'River', 'Sea or Lake']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Pasture Land.', 'X X X X Permanent Crop Land.', 'X X X X Residential Buildings.', 'X X X X River.', 'X X X X Sea or Lake.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([5, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/eurosat/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/8 [00:00<?, ?it/s] 12%|█▎        | 1/8 [00:04<00:29,  4.22s/it] 25%|██▌       | 2/8 [00:05<00:14,  2.39s/it] 38%|███▊      | 3/8 [00:06<00:09,  1.80s/it] 50%|█████     | 4/8 [00:07<00:06,  1.53s/it] 62%|██████▎   | 5/8 [00:08<00:04,  1.38s/it] 75%|███████▌  | 6/8 [00:09<00:02,  1.29s/it] 88%|████████▊ | 7/8 [00:10<00:01,  1.23s/it]100%|██████████| 8/8 [00:11<00:00,  1.14s/it]100%|██████████| 8/8 [00:11<00:00,  1.50s/it]
=> result
* total: 3,900
* correct: 3,123
* accuracy: 80.1%
* error: 19.9%
* macro_f1: 78.4%
Run this job and save the output to output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,666
---------  ------------
['707-320', '727-200', '737-200', '737-300', '737-400', '737-500', '737-600', '737-700', '737-800', '737-900', '747-100', '747-200', '747-300', '747-400', '757-200', '757-300', '767-200', '767-300', '767-400', '777-200', '777-300', 'A300B4', 'A310', 'A318', 'A319', 'A320', 'A321', 'A330-200', 'A330-300', 'A340-200', 'A340-300', 'A340-500', 'A340-600', 'A380', 'ATR-42', 'ATR-72', 'An-12', 'BAE 146-200', 'BAE 146-300', 'BAE-125', 'Beechcraft 1900', 'Boeing 717', 'C-130', 'C-47', 'CRJ-200', 'CRJ-700', 'CRJ-900', 'Cessna 172', 'Cessna 208', 'Cessna 525']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 707-320, a type of aircraft.', 'X X X X 727-200, a type of aircraft.', 'X X X X 737-200, a type of aircraft.', 'X X X X 737-300, a type of aircraft.', 'X X X X 737-400, a type of aircraft.', 'X X X X 737-500, a type of aircraft.', 'X X X X 737-600, a type of aircraft.', 'X X X X 737-700, a type of aircraft.', 'X X X X 737-800, a type of aircraft.', 'X X X X 737-900, a type of aircraft.', 'X X X X 747-100, a type of aircraft.', 'X X X X 747-200, a type of aircraft.', 'X X X X 747-300, a type of aircraft.', 'X X X X 747-400, a type of aircraft.', 'X X X X 757-200, a type of aircraft.', 'X X X X 757-300, a type of aircraft.', 'X X X X 767-200, a type of aircraft.', 'X X X X 767-300, a type of aircraft.', 'X X X X 767-400, a type of aircraft.', 'X X X X 777-200, a type of aircraft.', 'X X X X 777-300, a type of aircraft.', 'X X X X A300B4, a type of aircraft.', 'X X X X A310, a type of aircraft.', 'X X X X A318, a type of aircraft.', 'X X X X A319, a type of aircraft.', 'X X X X A320, a type of aircraft.', 'X X X X A321, a type of aircraft.', 'X X X X A330-200, a type of aircraft.', 'X X X X A330-300, a type of aircraft.', 'X X X X A340-200, a type of aircraft.', 'X X X X A340-300, a type of aircraft.', 'X X X X A340-500, a type of aircraft.', 'X X X X A340-600, a type of aircraft.', 'X X X X A380, a type of aircraft.', 'X X X X ATR-42, a type of aircraft.', 'X X X X ATR-72, a type of aircraft.', 'X X X X An-12, a type of aircraft.', 'X X X X BAE 146-200, a type of aircraft.', 'X X X X BAE 146-300, a type of aircraft.', 'X X X X BAE-125, a type of aircraft.', 'X X X X Beechcraft 1900, a type of aircraft.', 'X X X X Boeing 717, a type of aircraft.', 'X X X X C-130, a type of aircraft.', 'X X X X C-47, a type of aircraft.', 'X X X X CRJ-200, a type of aircraft.', 'X X X X CRJ-700, a type of aircraft.', 'X X X X CRJ-900, a type of aircraft.', 'X X X X Cessna 172, a type of aircraft.', 'X X X X Cessna 208, a type of aircraft.', 'X X X X Cessna 525, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/25] time 0.154 (0.377) data 0.000 (0.215) loss 8.9688 (9.0312) acc 28.1250 (12.5000) lr 1.0000e-05 eta 0:07:48
epoch [1/50] batch [10/25] time 0.157 (0.266) data 0.000 (0.108) loss 8.8281 (8.9000) acc 6.2500 (11.8750) lr 1.0000e-05 eta 0:05:29
epoch [1/50] batch [15/25] time 0.152 (0.228) data 0.000 (0.072) loss 8.8750 (8.9234) acc 3.1250 (10.6250) lr 1.0000e-05 eta 0:04:41
epoch [1/50] batch [20/25] time 0.153 (0.210) data 0.000 (0.054) loss 8.4688 (8.8426) acc 18.7500 (11.8750) lr 1.0000e-05 eta 0:04:17
epoch [1/50] batch [25/25] time 0.153 (0.198) data 0.000 (0.043) loss 8.5000 (8.8081) acc 12.5000 (11.5000) lr 2.0000e-03 eta 0:04:02
epoch [2/50] batch [5/25] time 0.153 (0.370) data 0.000 (0.216) loss 6.0391 (6.7062) acc 21.8750 (25.6250) lr 2.0000e-03 eta 0:07:30
epoch [2/50] batch [10/25] time 0.153 (0.261) data 0.000 (0.108) loss 6.2266 (6.4129) acc 9.3750 (23.4375) lr 2.0000e-03 eta 0:05:17
epoch [2/50] batch [15/25] time 0.150 (0.225) data 0.000 (0.072) loss 6.4180 (6.2180) acc 21.8750 (24.7917) lr 2.0000e-03 eta 0:04:31
epoch [2/50] batch [20/25] time 0.150 (0.206) data 0.000 (0.054) loss 5.5586 (6.1012) acc 37.5000 (26.0938) lr 2.0000e-03 eta 0:04:08
epoch [2/50] batch [25/25] time 0.157 (0.195) data 0.000 (0.043) loss 5.0508 (5.9708) acc 37.5000 (26.5000) lr 1.9980e-03 eta 0:03:54
epoch [3/50] batch [5/25] time 0.153 (0.374) data 0.000 (0.220) loss 5.7109 (5.3836) acc 21.8750 (26.2500) lr 1.9980e-03 eta 0:07:27
epoch [3/50] batch [10/25] time 0.153 (0.264) data 0.000 (0.110) loss 5.5469 (5.3730) acc 25.0000 (26.8750) lr 1.9980e-03 eta 0:05:14
epoch [3/50] batch [15/25] time 0.153 (0.227) data 0.001 (0.074) loss 5.4023 (5.3885) acc 31.2500 (26.8750) lr 1.9980e-03 eta 0:04:29
epoch [3/50] batch [20/25] time 0.155 (0.209) data 0.000 (0.055) loss 5.3945 (5.3842) acc 34.3750 (26.8750) lr 1.9980e-03 eta 0:04:06
epoch [3/50] batch [25/25] time 0.152 (0.197) data 0.000 (0.044) loss 5.1016 (5.3702) acc 28.1250 (27.0000) lr 1.9921e-03 eta 0:03:51
epoch [4/50] batch [5/25] time 0.152 (0.360) data 0.000 (0.205) loss 5.1562 (4.9430) acc 21.8750 (31.8750) lr 1.9921e-03 eta 0:07:00
epoch [4/50] batch [10/25] time 0.153 (0.257) data 0.000 (0.103) loss 4.6719 (4.9922) acc 37.5000 (33.1250) lr 1.9921e-03 eta 0:04:59
epoch [4/50] batch [15/25] time 0.151 (0.222) data 0.000 (0.068) loss 4.7734 (4.9789) acc 37.5000 (32.5000) lr 1.9921e-03 eta 0:04:17
epoch [4/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.051) loss 5.2969 (5.0520) acc 12.5000 (30.3125) lr 1.9921e-03 eta 0:03:56
epoch [4/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.041) loss 4.8984 (5.0986) acc 34.3750 (28.8750) lr 1.9823e-03 eta 0:03:43
epoch [5/50] batch [5/25] time 0.153 (0.372) data 0.000 (0.219) loss 5.1445 (4.9313) acc 21.8750 (30.6250) lr 1.9823e-03 eta 0:07:06
epoch [5/50] batch [10/25] time 0.154 (0.263) data 0.000 (0.109) loss 5.0781 (4.8773) acc 25.0000 (31.5625) lr 1.9823e-03 eta 0:04:59
epoch [5/50] batch [15/25] time 0.151 (0.226) data 0.000 (0.073) loss 5.1133 (4.8872) acc 37.5000 (32.5000) lr 1.9823e-03 eta 0:04:16
epoch [5/50] batch [20/25] time 0.151 (0.207) data 0.000 (0.055) loss 4.9453 (4.9033) acc 28.1250 (32.3438) lr 1.9823e-03 eta 0:03:54
epoch [5/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.044) loss 5.0586 (4.9428) acc 37.5000 (32.3750) lr 1.9686e-03 eta 0:03:40
epoch [6/50] batch [5/25] time 0.154 (0.386) data 0.000 (0.231) loss 4.7148 (4.7148) acc 37.5000 (30.6250) lr 1.9686e-03 eta 0:07:11
epoch [6/50] batch [10/25] time 0.154 (0.269) data 0.000 (0.116) loss 4.5938 (4.7395) acc 37.5000 (31.5625) lr 1.9686e-03 eta 0:05:00
epoch [6/50] batch [15/25] time 0.151 (0.230) data 0.000 (0.077) loss 4.7578 (4.7638) acc 34.3750 (31.4583) lr 1.9686e-03 eta 0:04:15
epoch [6/50] batch [20/25] time 0.151 (0.210) data 0.000 (0.058) loss 4.7344 (4.7639) acc 43.7500 (32.3438) lr 1.9686e-03 eta 0:03:52
epoch [6/50] batch [25/25] time 0.151 (0.199) data 0.000 (0.046) loss 4.6250 (4.7902) acc 25.0000 (32.2500) lr 1.9511e-03 eta 0:03:38
epoch [7/50] batch [5/25] time 0.155 (0.367) data 0.000 (0.212) loss 5.1328 (4.6453) acc 31.2500 (33.1250) lr 1.9511e-03 eta 0:06:42
epoch [7/50] batch [10/25] time 0.154 (0.261) data 0.000 (0.106) loss 5.3281 (4.6727) acc 25.0000 (31.8750) lr 1.9511e-03 eta 0:04:44
epoch [7/50] batch [15/25] time 0.152 (0.225) data 0.000 (0.071) loss 4.9297 (4.7352) acc 21.8750 (30.4167) lr 1.9511e-03 eta 0:04:03
epoch [7/50] batch [20/25] time 0.152 (0.207) data 0.000 (0.053) loss 4.5352 (4.7709) acc 40.6250 (30.7812) lr 1.9511e-03 eta 0:03:43
epoch [7/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.043) loss 4.9297 (4.7566) acc 31.2500 (31.6250) lr 1.9298e-03 eta 0:03:30
epoch [8/50] batch [5/25] time 0.171 (0.384) data 0.001 (0.226) loss 4.6133 (4.5164) acc 34.3750 (33.7500) lr 1.9298e-03 eta 0:06:51
epoch [8/50] batch [10/25] time 0.154 (0.270) data 0.000 (0.113) loss 4.7188 (4.6543) acc 37.5000 (35.0000) lr 1.9298e-03 eta 0:04:47
epoch [8/50] batch [15/25] time 0.152 (0.231) data 0.000 (0.076) loss 4.5352 (4.5725) acc 43.7500 (36.6667) lr 1.9298e-03 eta 0:04:04
epoch [8/50] batch [20/25] time 0.152 (0.211) data 0.000 (0.057) loss 4.8867 (4.6099) acc 34.3750 (35.6250) lr 1.9298e-03 eta 0:03:42
epoch [8/50] batch [25/25] time 0.152 (0.199) data 0.000 (0.045) loss 4.6172 (4.6213) acc 34.3750 (35.2500) lr 1.9048e-03 eta 0:03:29
epoch [9/50] batch [5/25] time 0.154 (0.377) data 0.000 (0.223) loss 5.0820 (4.6742) acc 37.5000 (37.5000) lr 1.9048e-03 eta 0:06:33
epoch [9/50] batch [10/25] time 0.153 (0.266) data 0.000 (0.111) loss 4.4648 (4.5520) acc 34.3750 (37.5000) lr 1.9048e-03 eta 0:04:36
epoch [9/50] batch [15/25] time 0.152 (0.228) data 0.000 (0.074) loss 4.9961 (4.6159) acc 28.1250 (36.2500) lr 1.9048e-03 eta 0:03:55
epoch [9/50] batch [20/25] time 0.152 (0.209) data 0.000 (0.056) loss 4.5391 (4.6436) acc 31.2500 (35.7812) lr 1.9048e-03 eta 0:03:35
epoch [9/50] batch [25/25] time 0.152 (0.197) data 0.000 (0.045) loss 4.2031 (4.6223) acc 46.8750 (35.6250) lr 1.8763e-03 eta 0:03:22
epoch [10/50] batch [5/25] time 0.159 (0.385) data 0.000 (0.228) loss 3.9395 (4.3910) acc 43.7500 (38.1250) lr 1.8763e-03 eta 0:06:32
epoch [10/50] batch [10/25] time 0.155 (0.271) data 0.000 (0.114) loss 4.7461 (4.5678) acc 34.3750 (35.0000) lr 1.8763e-03 eta 0:04:34
epoch [10/50] batch [15/25] time 0.153 (0.232) data 0.000 (0.076) loss 5.0625 (4.5811) acc 31.2500 (34.3750) lr 1.8763e-03 eta 0:03:53
epoch [10/50] batch [20/25] time 0.151 (0.212) data 0.000 (0.057) loss 4.8047 (4.6558) acc 31.2500 (33.5938) lr 1.8763e-03 eta 0:03:32
epoch [10/50] batch [25/25] time 0.153 (0.200) data 0.000 (0.046) loss 4.0234 (4.6176) acc 34.3750 (34.7500) lr 1.8443e-03 eta 0:03:19
epoch [11/50] batch [5/25] time 0.154 (0.377) data 0.000 (0.223) loss 4.3594 (4.3758) acc 37.5000 (40.6250) lr 1.8443e-03 eta 0:06:15
epoch [11/50] batch [10/25] time 0.154 (0.266) data 0.000 (0.112) loss 4.7734 (4.4852) acc 18.7500 (35.3125) lr 1.8443e-03 eta 0:04:22
epoch [11/50] batch [15/25] time 0.152 (0.228) data 0.000 (0.074) loss 4.7656 (4.4977) acc 28.1250 (35.2083) lr 1.8443e-03 eta 0:03:44
epoch [11/50] batch [20/25] time 0.153 (0.209) data 0.000 (0.056) loss 4.9375 (4.5488) acc 31.2500 (35.3125) lr 1.8443e-03 eta 0:03:25
epoch [11/50] batch [25/25] time 0.153 (0.198) data 0.000 (0.045) loss 4.0664 (4.5403) acc 46.8750 (35.8750) lr 1.8090e-03 eta 0:03:12
epoch [12/50] batch [5/25] time 0.153 (0.383) data 0.000 (0.229) loss 4.2812 (4.4891) acc 46.8750 (37.5000) lr 1.8090e-03 eta 0:06:11
epoch [12/50] batch [10/25] time 0.153 (0.268) data 0.000 (0.115) loss 4.2930 (4.4068) acc 40.6250 (40.3125) lr 1.8090e-03 eta 0:04:18
epoch [12/50] batch [15/25] time 0.152 (0.229) data 0.000 (0.076) loss 3.9688 (4.3863) acc 53.1250 (38.9583) lr 1.8090e-03 eta 0:03:40
epoch [12/50] batch [20/25] time 0.151 (0.210) data 0.000 (0.057) loss 4.1992 (4.4114) acc 37.5000 (38.4375) lr 1.8090e-03 eta 0:03:20
epoch [12/50] batch [25/25] time 0.151 (0.198) data 0.000 (0.046) loss 4.3945 (4.4384) acc 34.3750 (37.5000) lr 1.7705e-03 eta 0:03:08
epoch [13/50] batch [5/25] time 0.154 (0.378) data 0.000 (0.224) loss 4.4219 (4.1832) acc 34.3750 (43.7500) lr 1.7705e-03 eta 0:05:56
epoch [13/50] batch [10/25] time 0.153 (0.265) data 0.000 (0.112) loss 4.5156 (4.2412) acc 37.5000 (42.8125) lr 1.7705e-03 eta 0:04:09
epoch [13/50] batch [15/25] time 0.152 (0.228) data 0.000 (0.075) loss 4.9375 (4.3007) acc 21.8750 (42.5000) lr 1.7705e-03 eta 0:03:32
epoch [13/50] batch [20/25] time 0.152 (0.209) data 0.000 (0.056) loss 4.5430 (4.3636) acc 37.5000 (40.6250) lr 1.7705e-03 eta 0:03:14
epoch [13/50] batch [25/25] time 0.152 (0.197) data 0.000 (0.045) loss 4.5156 (4.3773) acc 40.6250 (40.7500) lr 1.7290e-03 eta 0:03:02
epoch [14/50] batch [5/25] time 0.154 (0.358) data 0.000 (0.203) loss 4.2383 (4.3445) acc 46.8750 (43.1250) lr 1.7290e-03 eta 0:05:29
epoch [14/50] batch [10/25] time 0.154 (0.257) data 0.000 (0.102) loss 4.7031 (4.4051) acc 31.2500 (42.1875) lr 1.7290e-03 eta 0:03:54
epoch [14/50] batch [15/25] time 0.152 (0.222) data 0.000 (0.068) loss 4.4805 (4.3883) acc 37.5000 (40.0000) lr 1.7290e-03 eta 0:03:22
epoch [14/50] batch [20/25] time 0.153 (0.205) data 0.000 (0.051) loss 4.5078 (4.4396) acc 34.3750 (37.5000) lr 1.7290e-03 eta 0:03:05
epoch [14/50] batch [25/25] time 0.154 (0.195) data 0.000 (0.041) loss 4.5859 (4.4341) acc 37.5000 (38.6250) lr 1.6845e-03 eta 0:02:55
epoch [15/50] batch [5/25] time 0.154 (0.379) data 0.000 (0.225) loss 4.4922 (4.3883) acc 43.7500 (39.3750) lr 1.6845e-03 eta 0:05:39
epoch [15/50] batch [10/25] time 0.153 (0.267) data 0.000 (0.113) loss 4.5820 (4.4723) acc 40.6250 (38.4375) lr 1.6845e-03 eta 0:03:57
epoch [15/50] batch [15/25] time 0.152 (0.228) data 0.000 (0.075) loss 4.6875 (4.5367) acc 53.1250 (37.0833) lr 1.6845e-03 eta 0:03:22
epoch [15/50] batch [20/25] time 0.154 (0.209) data 0.000 (0.056) loss 3.8672 (4.4359) acc 50.0000 (39.0625) lr 1.6845e-03 eta 0:03:04
epoch [15/50] batch [25/25] time 0.152 (0.198) data 0.000 (0.045) loss 3.9609 (4.4491) acc 46.8750 (38.3750) lr 1.6374e-03 eta 0:02:53
epoch [16/50] batch [5/25] time 0.154 (0.363) data 0.000 (0.209) loss 4.6484 (4.1914) acc 40.6250 (44.3750) lr 1.6374e-03 eta 0:05:15
epoch [16/50] batch [10/25] time 0.153 (0.258) data 0.000 (0.105) loss 4.1875 (4.1682) acc 40.6250 (43.7500) lr 1.6374e-03 eta 0:03:43
epoch [16/50] batch [15/25] time 0.153 (0.223) data 0.000 (0.070) loss 4.2344 (4.2079) acc 34.3750 (41.8750) lr 1.6374e-03 eta 0:03:11
epoch [16/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.052) loss 4.4688 (4.3210) acc 34.3750 (39.2188) lr 1.6374e-03 eta 0:02:55
epoch [16/50] batch [25/25] time 0.154 (0.195) data 0.000 (0.042) loss 4.9453 (4.3721) acc 34.3750 (38.1250) lr 1.5878e-03 eta 0:02:45
epoch [17/50] batch [5/25] time 0.155 (0.375) data 0.000 (0.219) loss 3.9590 (4.2598) acc 53.1250 (40.0000) lr 1.5878e-03 eta 0:05:16
epoch [17/50] batch [10/25] time 0.156 (0.264) data 0.000 (0.110) loss 4.1094 (4.3525) acc 43.7500 (38.1250) lr 1.5878e-03 eta 0:03:42
epoch [17/50] batch [15/25] time 0.152 (0.227) data 0.000 (0.073) loss 4.7422 (4.3060) acc 37.5000 (38.9583) lr 1.5878e-03 eta 0:03:09
epoch [17/50] batch [20/25] time 0.152 (0.208) data 0.000 (0.055) loss 4.5469 (4.3474) acc 40.6250 (39.5312) lr 1.5878e-03 eta 0:02:52
epoch [17/50] batch [25/25] time 0.153 (0.197) data 0.000 (0.044) loss 3.9043 (4.3505) acc 59.3750 (39.6250) lr 1.5358e-03 eta 0:02:42
epoch [18/50] batch [5/25] time 0.154 (0.404) data 0.000 (0.249) loss 4.3984 (4.3969) acc 40.6250 (38.1250) lr 1.5358e-03 eta 0:05:30
epoch [18/50] batch [10/25] time 0.156 (0.279) data 0.000 (0.125) loss 4.4453 (4.3373) acc 46.8750 (39.0625) lr 1.5358e-03 eta 0:03:47
epoch [18/50] batch [15/25] time 0.152 (0.237) data 0.000 (0.083) loss 4.0156 (4.3064) acc 46.8750 (41.8750) lr 1.5358e-03 eta 0:03:12
epoch [18/50] batch [20/25] time 0.152 (0.216) data 0.000 (0.062) loss 4.4219 (4.3292) acc 50.0000 (42.6562) lr 1.5358e-03 eta 0:02:53
epoch [18/50] batch [25/25] time 0.153 (0.203) data 0.000 (0.050) loss 4.5625 (4.3309) acc 28.1250 (41.7500) lr 1.4818e-03 eta 0:02:42
epoch [19/50] batch [5/25] time 0.158 (0.380) data 0.000 (0.223) loss 4.5391 (3.9945) acc 43.7500 (44.3750) lr 1.4818e-03 eta 0:05:01
epoch [19/50] batch [10/25] time 0.160 (0.268) data 0.000 (0.112) loss 4.1367 (4.1443) acc 46.8750 (41.2500) lr 1.4818e-03 eta 0:03:31
epoch [19/50] batch [15/25] time 0.153 (0.229) data 0.000 (0.075) loss 4.3438 (4.1910) acc 43.7500 (40.8333) lr 1.4818e-03 eta 0:03:00
epoch [19/50] batch [20/25] time 0.152 (0.210) data 0.000 (0.056) loss 4.6172 (4.2665) acc 28.1250 (39.6875) lr 1.4818e-03 eta 0:02:43
epoch [19/50] batch [25/25] time 0.152 (0.198) data 0.000 (0.045) loss 4.5781 (4.2577) acc 34.3750 (40.1250) lr 1.4258e-03 eta 0:02:33
epoch [20/50] batch [5/25] time 0.154 (0.358) data 0.000 (0.203) loss 4.9375 (4.2094) acc 43.7500 (49.3750) lr 1.4258e-03 eta 0:04:35
epoch [20/50] batch [10/25] time 0.153 (0.256) data 0.000 (0.102) loss 4.0820 (4.1688) acc 50.0000 (49.0625) lr 1.4258e-03 eta 0:03:15
epoch [20/50] batch [15/25] time 0.155 (0.223) data 0.000 (0.068) loss 4.1836 (4.2609) acc 37.5000 (47.0833) lr 1.4258e-03 eta 0:02:49
epoch [20/50] batch [20/25] time 0.151 (0.205) data 0.000 (0.051) loss 4.0547 (4.2516) acc 40.6250 (45.1562) lr 1.4258e-03 eta 0:02:34
epoch [20/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.041) loss 4.4570 (4.2682) acc 34.3750 (43.6250) lr 1.3681e-03 eta 0:02:25
epoch [21/50] batch [5/25] time 0.155 (0.369) data 0.000 (0.215) loss 4.5703 (4.3148) acc 31.2500 (38.7500) lr 1.3681e-03 eta 0:04:34
epoch [21/50] batch [10/25] time 0.156 (0.262) data 0.000 (0.107) loss 3.9941 (4.2734) acc 43.7500 (40.6250) lr 1.3681e-03 eta 0:03:13
epoch [21/50] batch [15/25] time 0.152 (0.225) data 0.000 (0.072) loss 4.0977 (4.2958) acc 43.7500 (39.5833) lr 1.3681e-03 eta 0:02:45
epoch [21/50] batch [20/25] time 0.152 (0.207) data 0.000 (0.054) loss 4.0742 (4.3127) acc 43.7500 (40.4688) lr 1.3681e-03 eta 0:02:31
epoch [21/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.043) loss 4.2852 (4.3164) acc 43.7500 (41.6250) lr 1.3090e-03 eta 0:02:22
epoch [22/50] batch [5/25] time 0.153 (0.377) data 0.000 (0.222) loss 5.0430 (4.2391) acc 28.1250 (41.2500) lr 1.3090e-03 eta 0:04:31
epoch [22/50] batch [10/25] time 0.155 (0.265) data 0.001 (0.111) loss 3.5938 (4.1113) acc 53.1250 (43.1250) lr 1.3090e-03 eta 0:03:09
epoch [22/50] batch [15/25] time 0.153 (0.228) data 0.000 (0.074) loss 3.9922 (4.0974) acc 50.0000 (43.9583) lr 1.3090e-03 eta 0:02:41
epoch [22/50] batch [20/25] time 0.154 (0.209) data 0.000 (0.056) loss 4.4453 (4.1376) acc 46.8750 (42.1875) lr 1.3090e-03 eta 0:02:27
epoch [22/50] batch [25/25] time 0.153 (0.198) data 0.000 (0.045) loss 4.0664 (4.1427) acc 40.6250 (42.6250) lr 1.2487e-03 eta 0:02:18
epoch [23/50] batch [5/25] time 0.155 (0.363) data 0.000 (0.208) loss 4.2891 (4.2703) acc 56.2500 (41.8750) lr 1.2487e-03 eta 0:04:12
epoch [23/50] batch [10/25] time 0.154 (0.259) data 0.000 (0.104) loss 4.4609 (4.1437) acc 40.6250 (44.0625) lr 1.2487e-03 eta 0:02:58
epoch [23/50] batch [15/25] time 0.152 (0.223) data 0.000 (0.070) loss 4.0078 (4.0768) acc 59.3750 (46.4583) lr 1.2487e-03 eta 0:02:32
epoch [23/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.052) loss 4.0781 (4.1391) acc 50.0000 (45.1562) lr 1.2487e-03 eta 0:02:19
epoch [23/50] batch [25/25] time 0.152 (0.195) data 0.000 (0.042) loss 4.2188 (4.1717) acc 46.8750 (45.6250) lr 1.1874e-03 eta 0:02:11
epoch [24/50] batch [5/25] time 0.153 (0.372) data 0.000 (0.218) loss 4.3945 (4.1813) acc 40.6250 (43.7500) lr 1.1874e-03 eta 0:04:09
epoch [24/50] batch [10/25] time 0.153 (0.263) data 0.000 (0.109) loss 3.9277 (4.1006) acc 50.0000 (45.0000) lr 1.1874e-03 eta 0:02:54
epoch [24/50] batch [15/25] time 0.152 (0.226) data 0.000 (0.073) loss 4.5781 (4.0987) acc 40.6250 (45.2083) lr 1.1874e-03 eta 0:02:29
epoch [24/50] batch [20/25] time 0.152 (0.208) data 0.000 (0.055) loss 4.3867 (4.1252) acc 43.7500 (46.2500) lr 1.1874e-03 eta 0:02:16
epoch [24/50] batch [25/25] time 0.152 (0.197) data 0.000 (0.044) loss 4.3672 (4.1078) acc 34.3750 (46.3750) lr 1.1253e-03 eta 0:02:07
epoch [25/50] batch [5/25] time 0.155 (0.375) data 0.000 (0.219) loss 4.0938 (3.9211) acc 43.7500 (46.2500) lr 1.1253e-03 eta 0:04:01
epoch [25/50] batch [10/25] time 0.156 (0.267) data 0.000 (0.110) loss 4.0938 (4.0115) acc 50.0000 (46.2500) lr 1.1253e-03 eta 0:02:50
epoch [25/50] batch [15/25] time 0.152 (0.229) data 0.000 (0.073) loss 4.1797 (4.1069) acc 40.6250 (44.5833) lr 1.1253e-03 eta 0:02:25
epoch [25/50] batch [20/25] time 0.152 (0.209) data 0.000 (0.055) loss 4.0859 (4.0897) acc 46.8750 (44.0625) lr 1.1253e-03 eta 0:02:11
epoch [25/50] batch [25/25] time 0.152 (0.198) data 0.000 (0.044) loss 3.8164 (4.1293) acc 59.3750 (44.5000) lr 1.0628e-03 eta 0:02:03
epoch [26/50] batch [5/25] time 0.155 (0.387) data 0.000 (0.232) loss 4.2188 (4.1508) acc 46.8750 (51.8750) lr 1.0628e-03 eta 0:04:00
epoch [26/50] batch [10/25] time 0.155 (0.271) data 0.000 (0.116) loss 3.7363 (4.0307) acc 50.0000 (52.1875) lr 1.0628e-03 eta 0:02:46
epoch [26/50] batch [15/25] time 0.155 (0.232) data 0.000 (0.078) loss 4.1797 (4.0809) acc 34.3750 (47.7083) lr 1.0628e-03 eta 0:02:21
epoch [26/50] batch [20/25] time 0.153 (0.213) data 0.000 (0.058) loss 4.6719 (4.1008) acc 31.2500 (46.4062) lr 1.0628e-03 eta 0:02:08
epoch [26/50] batch [25/25] time 0.152 (0.201) data 0.000 (0.047) loss 4.4453 (4.1392) acc 37.5000 (46.1250) lr 1.0000e-03 eta 0:02:00
epoch [27/50] batch [5/25] time 0.155 (0.377) data 0.000 (0.222) loss 3.7285 (3.8785) acc 40.6250 (48.7500) lr 1.0000e-03 eta 0:03:44
epoch [27/50] batch [10/25] time 0.155 (0.266) data 0.000 (0.111) loss 4.0781 (3.9150) acc 37.5000 (45.9375) lr 1.0000e-03 eta 0:02:36
epoch [27/50] batch [15/25] time 0.152 (0.228) data 0.000 (0.074) loss 4.2031 (4.0661) acc 43.7500 (44.5833) lr 1.0000e-03 eta 0:02:13
epoch [27/50] batch [20/25] time 0.152 (0.209) data 0.000 (0.056) loss 4.3633 (4.0622) acc 43.7500 (45.7812) lr 1.0000e-03 eta 0:02:01
epoch [27/50] batch [25/25] time 0.152 (0.198) data 0.000 (0.045) loss 3.8867 (4.0598) acc 53.1250 (46.6250) lr 9.3721e-04 eta 0:01:53
epoch [28/50] batch [5/25] time 0.157 (0.353) data 0.000 (0.196) loss 3.7383 (4.0859) acc 50.0000 (46.8750) lr 9.3721e-04 eta 0:03:21
epoch [28/50] batch [10/25] time 0.153 (0.254) data 0.000 (0.098) loss 4.3867 (3.9990) acc 37.5000 (46.5625) lr 9.3721e-04 eta 0:02:23
epoch [28/50] batch [15/25] time 0.152 (0.220) data 0.000 (0.066) loss 4.1836 (4.0298) acc 43.7500 (46.8750) lr 9.3721e-04 eta 0:02:03
epoch [28/50] batch [20/25] time 0.152 (0.203) data 0.000 (0.049) loss 4.4766 (4.0500) acc 37.5000 (46.0938) lr 9.3721e-04 eta 0:01:52
epoch [28/50] batch [25/25] time 0.152 (0.193) data 0.000 (0.039) loss 4.2266 (4.1023) acc 46.8750 (46.2500) lr 8.7467e-04 eta 0:01:46
epoch [29/50] batch [5/25] time 0.155 (0.371) data 0.000 (0.216) loss 4.1719 (4.2301) acc 46.8750 (45.6250) lr 8.7467e-04 eta 0:03:21
epoch [29/50] batch [10/25] time 0.153 (0.262) data 0.000 (0.108) loss 3.9648 (4.0545) acc 46.8750 (45.9375) lr 8.7467e-04 eta 0:02:21
epoch [29/50] batch [15/25] time 0.152 (0.226) data 0.000 (0.072) loss 4.0859 (4.0905) acc 46.8750 (46.4583) lr 8.7467e-04 eta 0:02:00
epoch [29/50] batch [20/25] time 0.152 (0.207) data 0.000 (0.054) loss 3.9219 (4.1253) acc 53.1250 (45.1562) lr 8.7467e-04 eta 0:01:49
epoch [29/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.043) loss 4.4062 (4.1182) acc 40.6250 (45.5000) lr 8.1262e-04 eta 0:01:43
epoch [30/50] batch [5/25] time 0.154 (0.360) data 0.000 (0.205) loss 4.2031 (4.0645) acc 40.6250 (46.8750) lr 8.1262e-04 eta 0:03:07
epoch [30/50] batch [10/25] time 0.155 (0.258) data 0.000 (0.102) loss 4.2617 (3.9543) acc 40.6250 (48.7500) lr 8.1262e-04 eta 0:02:12
epoch [30/50] batch [15/25] time 0.154 (0.223) data 0.000 (0.068) loss 4.0352 (3.9987) acc 43.7500 (48.5417) lr 8.1262e-04 eta 0:01:53
epoch [30/50] batch [20/25] time 0.152 (0.206) data 0.000 (0.051) loss 4.4492 (4.0688) acc 43.7500 (48.2812) lr 8.1262e-04 eta 0:01:43
epoch [30/50] batch [25/25] time 0.152 (0.195) data 0.000 (0.041) loss 3.9395 (4.0438) acc 43.7500 (48.3750) lr 7.5131e-04 eta 0:01:37
epoch [31/50] batch [5/25] time 0.154 (0.346) data 0.000 (0.192) loss 4.0703 (4.1379) acc 37.5000 (47.5000) lr 7.5131e-04 eta 0:02:51
epoch [31/50] batch [10/25] time 0.154 (0.251) data 0.000 (0.096) loss 4.0312 (4.1975) acc 53.1250 (47.5000) lr 7.5131e-04 eta 0:02:02
epoch [31/50] batch [15/25] time 0.152 (0.218) data 0.000 (0.064) loss 3.6992 (4.0996) acc 62.5000 (47.2917) lr 7.5131e-04 eta 0:01:45
epoch [31/50] batch [20/25] time 0.152 (0.202) data 0.000 (0.048) loss 3.6738 (4.0979) acc 50.0000 (46.4062) lr 7.5131e-04 eta 0:01:36
epoch [31/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.038) loss 3.6250 (4.0837) acc 62.5000 (47.5000) lr 6.9098e-04 eta 0:01:31
epoch [32/50] batch [5/25] time 0.154 (0.359) data 0.000 (0.205) loss 4.6406 (3.8672) acc 40.6250 (55.0000) lr 6.9098e-04 eta 0:02:48
epoch [32/50] batch [10/25] time 0.155 (0.257) data 0.000 (0.103) loss 4.3828 (4.0025) acc 56.2500 (52.1875) lr 6.9098e-04 eta 0:01:59
epoch [32/50] batch [15/25] time 0.152 (0.222) data 0.000 (0.068) loss 3.8418 (4.0875) acc 71.8750 (49.5833) lr 6.9098e-04 eta 0:01:41
epoch [32/50] batch [20/25] time 0.152 (0.204) data 0.000 (0.051) loss 4.1172 (4.1658) acc 50.0000 (47.9688) lr 6.9098e-04 eta 0:01:32
epoch [32/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.041) loss 3.7656 (4.1692) acc 56.2500 (46.7500) lr 6.3188e-04 eta 0:01:27
epoch [33/50] batch [5/25] time 0.153 (0.342) data 0.000 (0.189) loss 4.3438 (3.9480) acc 37.5000 (40.6250) lr 6.3188e-04 eta 0:02:32
epoch [33/50] batch [10/25] time 0.152 (0.248) data 0.000 (0.095) loss 3.6797 (3.9900) acc 56.2500 (44.6875) lr 6.3188e-04 eta 0:01:49
epoch [33/50] batch [15/25] time 0.152 (0.216) data 0.000 (0.063) loss 3.6523 (3.9457) acc 53.1250 (47.9167) lr 6.3188e-04 eta 0:01:33
epoch [33/50] batch [20/25] time 0.152 (0.200) data 0.000 (0.047) loss 4.5586 (4.0481) acc 37.5000 (46.5625) lr 6.3188e-04 eta 0:01:26
epoch [33/50] batch [25/25] time 0.152 (0.190) data 0.000 (0.038) loss 3.7227 (4.0127) acc 62.5000 (48.7500) lr 5.7422e-04 eta 0:01:20
epoch [34/50] batch [5/25] time 0.155 (0.360) data 0.000 (0.205) loss 3.5039 (3.9773) acc 56.2500 (50.0000) lr 5.7422e-04 eta 0:02:31
epoch [34/50] batch [10/25] time 0.154 (0.257) data 0.000 (0.103) loss 4.2344 (3.9738) acc 59.3750 (50.9375) lr 5.7422e-04 eta 0:01:46
epoch [34/50] batch [15/25] time 0.152 (0.222) data 0.000 (0.069) loss 4.1484 (4.0083) acc 50.0000 (48.9583) lr 5.7422e-04 eta 0:01:31
epoch [34/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.051) loss 4.4141 (4.0075) acc 34.3750 (47.6562) lr 5.7422e-04 eta 0:01:22
epoch [34/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.041) loss 3.9492 (4.0304) acc 43.7500 (47.3750) lr 5.1825e-04 eta 0:01:17
epoch [35/50] batch [5/25] time 0.157 (0.351) data 0.000 (0.195) loss 3.7930 (3.9867) acc 56.2500 (46.8750) lr 5.1825e-04 eta 0:02:18
epoch [35/50] batch [10/25] time 0.155 (0.253) data 0.000 (0.098) loss 3.8945 (4.0539) acc 53.1250 (46.5625) lr 5.1825e-04 eta 0:01:38
epoch [35/50] batch [15/25] time 0.152 (0.220) data 0.000 (0.065) loss 3.2266 (3.9410) acc 68.7500 (49.1667) lr 5.1825e-04 eta 0:01:24
epoch [35/50] batch [20/25] time 0.152 (0.203) data 0.000 (0.049) loss 4.0938 (4.0425) acc 50.0000 (48.2812) lr 5.1825e-04 eta 0:01:17
epoch [35/50] batch [25/25] time 0.151 (0.193) data 0.000 (0.039) loss 4.2578 (3.9841) acc 50.0000 (50.0000) lr 4.6417e-04 eta 0:01:12
epoch [36/50] batch [5/25] time 0.155 (0.353) data 0.000 (0.197) loss 3.7148 (3.9426) acc 56.2500 (53.1250) lr 4.6417e-04 eta 0:02:10
epoch [36/50] batch [10/25] time 0.155 (0.254) data 0.000 (0.098) loss 4.2812 (4.0299) acc 40.6250 (48.7500) lr 4.6417e-04 eta 0:01:32
epoch [36/50] batch [15/25] time 0.152 (0.220) data 0.000 (0.066) loss 4.4297 (4.0626) acc 46.8750 (47.7083) lr 4.6417e-04 eta 0:01:19
epoch [36/50] batch [20/25] time 0.153 (0.203) data 0.000 (0.049) loss 3.6094 (3.9839) acc 56.2500 (48.9062) lr 4.6417e-04 eta 0:01:12
epoch [36/50] batch [25/25] time 0.154 (0.193) data 0.000 (0.039) loss 4.2109 (4.0088) acc 40.6250 (49.5000) lr 4.1221e-04 eta 0:01:07
epoch [37/50] batch [5/25] time 0.156 (0.349) data 0.001 (0.193) loss 4.1914 (3.9844) acc 46.8750 (54.3750) lr 4.1221e-04 eta 0:02:00
epoch [37/50] batch [10/25] time 0.153 (0.252) data 0.000 (0.097) loss 3.9492 (4.1172) acc 46.8750 (48.7500) lr 4.1221e-04 eta 0:01:25
epoch [37/50] batch [15/25] time 0.153 (0.219) data 0.000 (0.064) loss 3.3359 (3.9332) acc 65.6250 (51.2500) lr 4.1221e-04 eta 0:01:13
epoch [37/50] batch [20/25] time 0.152 (0.202) data 0.000 (0.048) loss 3.9844 (3.9499) acc 46.8750 (51.7188) lr 4.1221e-04 eta 0:01:06
epoch [37/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.039) loss 4.3125 (3.9805) acc 50.0000 (51.1250) lr 3.6258e-04 eta 0:01:02
epoch [38/50] batch [5/25] time 0.154 (0.365) data 0.000 (0.210) loss 3.8301 (4.0605) acc 65.6250 (50.0000) lr 3.6258e-04 eta 0:01:56
epoch [38/50] batch [10/25] time 0.154 (0.260) data 0.000 (0.105) loss 3.8105 (3.9645) acc 46.8750 (49.6875) lr 3.6258e-04 eta 0:01:21
epoch [38/50] batch [15/25] time 0.153 (0.225) data 0.000 (0.070) loss 4.4453 (3.9966) acc 50.0000 (49.7917) lr 3.6258e-04 eta 0:01:09
epoch [38/50] batch [20/25] time 0.153 (0.207) data 0.000 (0.053) loss 3.8223 (3.9325) acc 40.6250 (50.6250) lr 3.6258e-04 eta 0:01:03
epoch [38/50] batch [25/25] time 0.154 (0.196) data 0.000 (0.042) loss 4.1875 (3.9505) acc 46.8750 (50.5000) lr 3.1545e-04 eta 0:00:58
epoch [39/50] batch [5/25] time 0.155 (0.363) data 0.000 (0.208) loss 3.9062 (3.8109) acc 56.2500 (57.5000) lr 3.1545e-04 eta 0:01:47
epoch [39/50] batch [10/25] time 0.155 (0.259) data 0.000 (0.104) loss 4.3594 (3.9967) acc 46.8750 (53.7500) lr 3.1545e-04 eta 0:01:15
epoch [39/50] batch [15/25] time 0.152 (0.224) data 0.000 (0.070) loss 3.8164 (4.0009) acc 59.3750 (52.2917) lr 3.1545e-04 eta 0:01:03
epoch [39/50] batch [20/25] time 0.152 (0.206) data 0.000 (0.052) loss 3.4805 (3.9412) acc 62.5000 (52.1875) lr 3.1545e-04 eta 0:00:57
epoch [39/50] batch [25/25] time 0.152 (0.195) data 0.000 (0.042) loss 3.6367 (3.9434) acc 53.1250 (51.8750) lr 2.7103e-04 eta 0:00:53
epoch [40/50] batch [5/25] time 0.158 (0.339) data 0.004 (0.184) loss 3.7969 (3.5273) acc 53.1250 (56.8750) lr 2.7103e-04 eta 0:01:31
epoch [40/50] batch [10/25] time 0.154 (0.247) data 0.000 (0.092) loss 4.1328 (3.7242) acc 53.1250 (57.1875) lr 2.7103e-04 eta 0:01:05
epoch [40/50] batch [15/25] time 0.153 (0.215) data 0.000 (0.061) loss 3.4414 (3.7973) acc 56.2500 (55.4167) lr 2.7103e-04 eta 0:00:55
epoch [40/50] batch [20/25] time 0.153 (0.200) data 0.000 (0.046) loss 4.4727 (3.9013) acc 28.1250 (52.5000) lr 2.7103e-04 eta 0:00:50
epoch [40/50] batch [25/25] time 0.154 (0.190) data 0.000 (0.037) loss 3.9648 (3.9160) acc 65.6250 (52.3750) lr 2.2949e-04 eta 0:00:47
epoch [41/50] batch [5/25] time 0.154 (0.346) data 0.000 (0.191) loss 3.8633 (4.0203) acc 53.1250 (55.0000) lr 2.2949e-04 eta 0:01:24
epoch [41/50] batch [10/25] time 0.158 (0.250) data 0.000 (0.096) loss 4.6250 (4.0166) acc 40.6250 (50.9375) lr 2.2949e-04 eta 0:00:59
epoch [41/50] batch [15/25] time 0.152 (0.218) data 0.000 (0.064) loss 3.6133 (3.9978) acc 56.2500 (51.6667) lr 2.2949e-04 eta 0:00:51
epoch [41/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 4.3008 (4.0309) acc 50.0000 (50.9375) lr 2.2949e-04 eta 0:00:46
epoch [41/50] batch [25/25] time 0.153 (0.192) data 0.000 (0.038) loss 3.4453 (3.9917) acc 59.3750 (51.5000) lr 1.9098e-04 eta 0:00:43
epoch [42/50] batch [5/25] time 0.155 (0.384) data 0.000 (0.230) loss 3.8477 (3.9512) acc 59.3750 (56.2500) lr 1.9098e-04 eta 0:01:24
epoch [42/50] batch [10/25] time 0.156 (0.270) data 0.000 (0.115) loss 4.3984 (3.9234) acc 40.6250 (55.3125) lr 1.9098e-04 eta 0:00:58
epoch [42/50] batch [15/25] time 0.155 (0.232) data 0.000 (0.077) loss 3.9883 (3.9608) acc 59.3750 (55.0000) lr 1.9098e-04 eta 0:00:48
epoch [42/50] batch [20/25] time 0.152 (0.212) data 0.000 (0.058) loss 4.4805 (4.0108) acc 37.5000 (52.9688) lr 1.9098e-04 eta 0:00:43
epoch [42/50] batch [25/25] time 0.152 (0.200) data 0.000 (0.046) loss 4.0781 (3.9934) acc 40.6250 (53.0000) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [5/25] time 0.154 (0.367) data 0.000 (0.212) loss 3.4766 (3.8988) acc 68.7500 (53.7500) lr 1.5567e-04 eta 0:01:11
epoch [43/50] batch [10/25] time 0.154 (0.261) data 0.000 (0.106) loss 4.3438 (3.9492) acc 46.8750 (52.8125) lr 1.5567e-04 eta 0:00:49
epoch [43/50] batch [15/25] time 0.152 (0.225) data 0.000 (0.071) loss 4.2656 (3.9854) acc 46.8750 (52.5000) lr 1.5567e-04 eta 0:00:41
epoch [43/50] batch [20/25] time 0.154 (0.207) data 0.000 (0.053) loss 3.5195 (3.9241) acc 65.6250 (53.2812) lr 1.5567e-04 eta 0:00:37
epoch [43/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.043) loss 3.8770 (3.9480) acc 59.3750 (52.7500) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [5/25] time 0.154 (0.377) data 0.000 (0.221) loss 4.3594 (4.0727) acc 34.3750 (51.2500) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [10/25] time 0.155 (0.267) data 0.000 (0.111) loss 4.1875 (4.0625) acc 43.7500 (50.6250) lr 1.2369e-04 eta 0:00:44
epoch [44/50] batch [15/25] time 0.154 (0.229) data 0.000 (0.074) loss 3.9570 (3.9122) acc 53.1250 (53.3333) lr 1.2369e-04 eta 0:00:36
epoch [44/50] batch [20/25] time 0.152 (0.210) data 0.000 (0.056) loss 4.0352 (3.9545) acc 56.2500 (52.3438) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [25/25] time 0.154 (0.199) data 0.000 (0.045) loss 3.7773 (3.8972) acc 59.3750 (54.2500) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [5/25] time 0.155 (0.376) data 0.000 (0.221) loss 3.9102 (4.0184) acc 62.5000 (54.3750) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [10/25] time 0.153 (0.266) data 0.000 (0.111) loss 4.3516 (4.0014) acc 40.6250 (52.5000) lr 9.5173e-05 eta 0:00:37
epoch [45/50] batch [15/25] time 0.154 (0.228) data 0.000 (0.074) loss 4.0977 (4.0181) acc 40.6250 (51.0417) lr 9.5173e-05 eta 0:00:30
epoch [45/50] batch [20/25] time 0.152 (0.209) data 0.000 (0.055) loss 3.9141 (3.9888) acc 46.8750 (50.7812) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [25/25] time 0.153 (0.198) data 0.000 (0.044) loss 3.6445 (3.9959) acc 53.1250 (50.7500) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [5/25] time 0.154 (0.366) data 0.000 (0.211) loss 3.6934 (4.0984) acc 46.8750 (48.1250) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [10/25] time 0.155 (0.261) data 0.000 (0.106) loss 3.7344 (4.1199) acc 62.5000 (50.3125) lr 7.0224e-05 eta 0:00:29
epoch [46/50] batch [15/25] time 0.157 (0.231) data 0.000 (0.071) loss 4.2188 (4.0367) acc 53.1250 (51.6667) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [20/25] time 0.152 (0.211) data 0.000 (0.053) loss 3.8965 (3.9784) acc 59.3750 (53.7500) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [25/25] time 0.154 (0.200) data 0.000 (0.043) loss 3.9922 (3.9591) acc 53.1250 (53.6250) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [5/25] time 0.155 (0.356) data 0.000 (0.201) loss 3.9375 (3.8762) acc 59.3750 (56.8750) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [10/25] time 0.152 (0.256) data 0.000 (0.101) loss 3.6758 (3.7711) acc 62.5000 (58.7500) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [15/25] time 0.152 (0.221) data 0.000 (0.067) loss 4.1328 (3.8745) acc 46.8750 (55.0000) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/25] time 0.152 (0.204) data 0.000 (0.051) loss 4.2969 (3.8717) acc 37.5000 (53.7500) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [25/25] time 0.152 (0.193) data 0.000 (0.041) loss 3.8809 (3.9048) acc 46.8750 (53.1250) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [5/25] time 0.155 (0.345) data 0.000 (0.192) loss 3.9766 (4.0766) acc 50.0000 (49.3750) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [10/25] time 0.154 (0.250) data 0.000 (0.096) loss 4.1523 (4.0334) acc 46.8750 (47.8125) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [15/25] time 0.152 (0.218) data 0.000 (0.064) loss 3.5547 (3.9615) acc 59.3750 (51.6667) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/25] time 0.153 (0.201) data 0.000 (0.048) loss 3.7617 (3.9649) acc 56.2500 (50.9375) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [25/25] time 0.153 (0.191) data 0.000 (0.038) loss 3.2930 (3.9356) acc 71.8750 (51.8750) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [5/25] time 0.157 (0.365) data 0.000 (0.210) loss 3.9668 (3.9168) acc 59.3750 (52.5000) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [10/25] time 0.154 (0.260) data 0.000 (0.105) loss 3.6680 (3.8424) acc 65.6250 (55.9375) lr 1.7713e-05 eta 0:00:10
epoch [49/50] batch [15/25] time 0.152 (0.224) data 0.000 (0.070) loss 3.4922 (3.8449) acc 56.2500 (55.8333) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/25] time 0.152 (0.206) data 0.000 (0.053) loss 4.0859 (3.8726) acc 62.5000 (56.0938) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [25/25] time 0.152 (0.195) data 0.000 (0.042) loss 3.7090 (3.8737) acc 46.8750 (54.7500) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [5/25] time 0.154 (0.370) data 0.000 (0.214) loss 3.3281 (3.8441) acc 68.7500 (55.6250) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/25] time 0.153 (0.262) data 0.000 (0.107) loss 3.4609 (3.7602) acc 68.7500 (57.5000) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/25] time 0.156 (0.226) data 0.000 (0.072) loss 4.1953 (3.8760) acc 59.3750 (56.2500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/25] time 0.153 (0.208) data 0.000 (0.054) loss 3.9023 (3.9128) acc 46.8750 (55.6250) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/25] time 0.153 (0.197) data 0.000 (0.043) loss 3.8633 (3.9083) acc 62.5000 (55.0000) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:31, 10.35s/it] 50%|█████     | 2/4 [00:11<00:09,  4.93s/it] 75%|███████▌  | 3/4 [00:12<00:03,  3.20s/it]100%|██████████| 4/4 [00:13<00:00,  2.11s/it]100%|██████████| 4/4 [00:13<00:00,  3.29s/it]
=> result
* total: 1,666
* correct: 736
* accuracy: 44.2%
* error: 55.8%
* macro_f1: 42.0%
Elapsed: 0:04:24
Run this job and save the output to output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,666
---------  ------------
['707-320', '727-200', '737-200', '737-300', '737-400', '737-500', '737-600', '737-700', '737-800', '737-900', '747-100', '747-200', '747-300', '747-400', '757-200', '757-300', '767-200', '767-300', '767-400', '777-200', '777-300', 'A300B4', 'A310', 'A318', 'A319', 'A320', 'A321', 'A330-200', 'A330-300', 'A340-200', 'A340-300', 'A340-500', 'A340-600', 'A380', 'ATR-42', 'ATR-72', 'An-12', 'BAE 146-200', 'BAE 146-300', 'BAE-125', 'Beechcraft 1900', 'Boeing 717', 'C-130', 'C-47', 'CRJ-200', 'CRJ-700', 'CRJ-900', 'Cessna 172', 'Cessna 208', 'Cessna 525']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 707-320, a type of aircraft.', 'X X X X 727-200, a type of aircraft.', 'X X X X 737-200, a type of aircraft.', 'X X X X 737-300, a type of aircraft.', 'X X X X 737-400, a type of aircraft.', 'X X X X 737-500, a type of aircraft.', 'X X X X 737-600, a type of aircraft.', 'X X X X 737-700, a type of aircraft.', 'X X X X 737-800, a type of aircraft.', 'X X X X 737-900, a type of aircraft.', 'X X X X 747-100, a type of aircraft.', 'X X X X 747-200, a type of aircraft.', 'X X X X 747-300, a type of aircraft.', 'X X X X 747-400, a type of aircraft.', 'X X X X 757-200, a type of aircraft.', 'X X X X 757-300, a type of aircraft.', 'X X X X 767-200, a type of aircraft.', 'X X X X 767-300, a type of aircraft.', 'X X X X 767-400, a type of aircraft.', 'X X X X 777-200, a type of aircraft.', 'X X X X 777-300, a type of aircraft.', 'X X X X A300B4, a type of aircraft.', 'X X X X A310, a type of aircraft.', 'X X X X A318, a type of aircraft.', 'X X X X A319, a type of aircraft.', 'X X X X A320, a type of aircraft.', 'X X X X A321, a type of aircraft.', 'X X X X A330-200, a type of aircraft.', 'X X X X A330-300, a type of aircraft.', 'X X X X A340-200, a type of aircraft.', 'X X X X A340-300, a type of aircraft.', 'X X X X A340-500, a type of aircraft.', 'X X X X A340-600, a type of aircraft.', 'X X X X A380, a type of aircraft.', 'X X X X ATR-42, a type of aircraft.', 'X X X X ATR-72, a type of aircraft.', 'X X X X An-12, a type of aircraft.', 'X X X X BAE 146-200, a type of aircraft.', 'X X X X BAE 146-300, a type of aircraft.', 'X X X X BAE-125, a type of aircraft.', 'X X X X Beechcraft 1900, a type of aircraft.', 'X X X X Boeing 717, a type of aircraft.', 'X X X X C-130, a type of aircraft.', 'X X X X C-47, a type of aircraft.', 'X X X X CRJ-200, a type of aircraft.', 'X X X X CRJ-700, a type of aircraft.', 'X X X X CRJ-900, a type of aircraft.', 'X X X X Cessna 172, a type of aircraft.', 'X X X X Cessna 208, a type of aircraft.', 'X X X X Cessna 525, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/25] time 0.153 (0.370) data 0.001 (0.205) loss 8.1484 (8.4938) acc 12.5000 (13.1250) lr 1.0000e-05 eta 0:07:40
epoch [1/50] batch [10/25] time 0.155 (0.262) data 0.000 (0.103) loss 8.1250 (8.3672) acc 18.7500 (16.2500) lr 1.0000e-05 eta 0:05:24
epoch [1/50] batch [15/25] time 0.153 (0.225) data 0.000 (0.069) loss 8.1719 (8.3115) acc 21.8750 (18.1250) lr 1.0000e-05 eta 0:04:38
epoch [1/50] batch [20/25] time 0.152 (0.207) data 0.000 (0.051) loss 8.2969 (8.2381) acc 18.7500 (18.5938) lr 1.0000e-05 eta 0:04:14
epoch [1/50] batch [25/25] time 0.151 (0.196) data 0.000 (0.041) loss 7.9141 (8.1723) acc 18.7500 (18.3750) lr 2.0000e-03 eta 0:03:59
epoch [2/50] batch [5/25] time 0.153 (0.347) data 0.000 (0.193) loss 5.6484 (6.9062) acc 43.7500 (17.5000) lr 2.0000e-03 eta 0:07:03
epoch [2/50] batch [10/25] time 0.152 (0.250) data 0.000 (0.097) loss 6.1094 (6.4195) acc 21.8750 (20.9375) lr 2.0000e-03 eta 0:05:04
epoch [2/50] batch [15/25] time 0.152 (0.218) data 0.000 (0.065) loss 5.8281 (6.2661) acc 31.2500 (22.7083) lr 2.0000e-03 eta 0:04:23
epoch [2/50] batch [20/25] time 0.151 (0.201) data 0.000 (0.048) loss 5.7969 (6.1211) acc 21.8750 (23.4375) lr 2.0000e-03 eta 0:04:02
epoch [2/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.039) loss 5.6094 (6.0278) acc 31.2500 (24.0000) lr 1.9980e-03 eta 0:03:49
epoch [3/50] batch [5/25] time 0.153 (0.355) data 0.000 (0.202) loss 5.5391 (5.4031) acc 34.3750 (28.7500) lr 1.9980e-03 eta 0:07:04
epoch [3/50] batch [10/25] time 0.152 (0.253) data 0.000 (0.101) loss 4.7852 (5.4000) acc 28.1250 (25.9375) lr 1.9980e-03 eta 0:05:01
epoch [3/50] batch [15/25] time 0.151 (0.219) data 0.000 (0.067) loss 5.3203 (5.3206) acc 28.1250 (27.0833) lr 1.9980e-03 eta 0:04:19
epoch [3/50] batch [20/25] time 0.153 (0.202) data 0.000 (0.051) loss 5.3203 (5.3146) acc 15.6250 (27.6562) lr 1.9980e-03 eta 0:03:58
epoch [3/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.041) loss 5.0625 (5.3011) acc 31.2500 (28.0000) lr 1.9921e-03 eta 0:03:45
epoch [4/50] batch [5/25] time 0.153 (0.347) data 0.000 (0.193) loss 5.2773 (4.9875) acc 25.0000 (33.7500) lr 1.9921e-03 eta 0:06:45
epoch [4/50] batch [10/25] time 0.153 (0.250) data 0.000 (0.097) loss 4.3242 (5.0145) acc 43.7500 (33.7500) lr 1.9921e-03 eta 0:04:51
epoch [4/50] batch [15/25] time 0.153 (0.217) data 0.000 (0.065) loss 4.8281 (5.0542) acc 53.1250 (32.9167) lr 1.9921e-03 eta 0:04:12
epoch [4/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.049) loss 4.7266 (5.0488) acc 25.0000 (30.7812) lr 1.9921e-03 eta 0:03:52
epoch [4/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.039) loss 5.0391 (5.0203) acc 34.3750 (31.2500) lr 1.9823e-03 eta 0:03:40
epoch [5/50] batch [5/25] time 0.153 (0.343) data 0.000 (0.190) loss 4.4570 (4.8211) acc 37.5000 (31.8750) lr 1.9823e-03 eta 0:06:32
epoch [5/50] batch [10/25] time 0.153 (0.248) data 0.000 (0.095) loss 4.8594 (4.9980) acc 21.8750 (28.1250) lr 1.9823e-03 eta 0:04:42
epoch [5/50] batch [15/25] time 0.154 (0.217) data 0.000 (0.064) loss 5.5742 (5.0542) acc 18.7500 (26.6667) lr 1.9823e-03 eta 0:04:05
epoch [5/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 4.9453 (5.0545) acc 28.1250 (28.1250) lr 1.9823e-03 eta 0:03:46
epoch [5/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.038) loss 4.9844 (5.0439) acc 31.2500 (28.8750) lr 1.9686e-03 eta 0:03:34
epoch [6/50] batch [5/25] time 0.151 (0.356) data 0.000 (0.203) loss 4.3281 (4.5617) acc 34.3750 (32.5000) lr 1.9686e-03 eta 0:06:39
epoch [6/50] batch [10/25] time 0.155 (0.256) data 0.000 (0.102) loss 5.2227 (4.6957) acc 15.6250 (30.3125) lr 1.9686e-03 eta 0:04:45
epoch [6/50] batch [15/25] time 0.153 (0.222) data 0.000 (0.068) loss 4.7109 (4.7633) acc 34.3750 (31.6667) lr 1.9686e-03 eta 0:04:06
epoch [6/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.051) loss 4.9844 (4.7514) acc 28.1250 (31.2500) lr 1.9686e-03 eta 0:03:46
epoch [6/50] batch [25/25] time 0.151 (0.194) data 0.000 (0.041) loss 5.1641 (4.8008) acc 25.0000 (31.1250) lr 1.9511e-03 eta 0:03:33
epoch [7/50] batch [5/25] time 0.155 (0.341) data 0.000 (0.186) loss 4.8594 (4.8469) acc 28.1250 (30.0000) lr 1.9511e-03 eta 0:06:13
epoch [7/50] batch [10/25] time 0.153 (0.247) data 0.000 (0.093) loss 4.3164 (4.7059) acc 40.6250 (33.4375) lr 1.9511e-03 eta 0:04:29
epoch [7/50] batch [15/25] time 0.154 (0.216) data 0.000 (0.062) loss 4.3594 (4.6781) acc 37.5000 (33.5417) lr 1.9511e-03 eta 0:03:54
epoch [7/50] batch [20/25] time 0.152 (0.200) data 0.000 (0.047) loss 4.7891 (4.7092) acc 28.1250 (32.5000) lr 1.9511e-03 eta 0:03:36
epoch [7/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.038) loss 4.5898 (4.7462) acc 34.3750 (32.3750) lr 1.9298e-03 eta 0:03:25
epoch [8/50] batch [5/25] time 0.154 (0.371) data 0.000 (0.217) loss 4.4375 (4.4594) acc 37.5000 (41.2500) lr 1.9298e-03 eta 0:06:37
epoch [8/50] batch [10/25] time 0.153 (0.263) data 0.000 (0.109) loss 4.3359 (4.6488) acc 37.5000 (34.6875) lr 1.9298e-03 eta 0:04:40
epoch [8/50] batch [15/25] time 0.151 (0.226) data 0.000 (0.073) loss 4.9375 (4.6701) acc 43.7500 (34.7917) lr 1.9298e-03 eta 0:03:59
epoch [8/50] batch [20/25] time 0.152 (0.207) data 0.000 (0.055) loss 5.0547 (4.6777) acc 28.1250 (33.5938) lr 1.9298e-03 eta 0:03:38
epoch [8/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.044) loss 4.6406 (4.6720) acc 43.7500 (34.2500) lr 1.9048e-03 eta 0:03:26
epoch [9/50] batch [5/25] time 0.153 (0.351) data 0.000 (0.197) loss 4.4023 (4.6570) acc 37.5000 (30.0000) lr 1.9048e-03 eta 0:06:06
epoch [9/50] batch [10/25] time 0.154 (0.253) data 0.000 (0.098) loss 4.3281 (4.6617) acc 46.8750 (30.9375) lr 1.9048e-03 eta 0:04:22
epoch [9/50] batch [15/25] time 0.152 (0.219) data 0.000 (0.066) loss 3.9375 (4.5786) acc 53.1250 (33.5417) lr 1.9048e-03 eta 0:03:46
epoch [9/50] batch [20/25] time 0.152 (0.202) data 0.000 (0.049) loss 3.9023 (4.5418) acc 53.1250 (35.3125) lr 1.9048e-03 eta 0:03:28
epoch [9/50] batch [25/25] time 0.151 (0.192) data 0.000 (0.039) loss 4.2383 (4.5420) acc 46.8750 (36.1250) lr 1.8763e-03 eta 0:03:16
epoch [10/50] batch [5/25] time 0.151 (0.383) data 0.000 (0.229) loss 4.4375 (4.4820) acc 43.7500 (38.7500) lr 1.8763e-03 eta 0:06:30
epoch [10/50] batch [10/25] time 0.154 (0.269) data 0.000 (0.115) loss 4.2578 (4.3793) acc 50.0000 (40.6250) lr 1.8763e-03 eta 0:04:32
epoch [10/50] batch [15/25] time 0.152 (0.230) data 0.000 (0.077) loss 4.9258 (4.4529) acc 31.2500 (39.5833) lr 1.8763e-03 eta 0:03:52
epoch [10/50] batch [20/25] time 0.152 (0.211) data 0.000 (0.058) loss 4.3047 (4.5342) acc 53.1250 (38.4375) lr 1.8763e-03 eta 0:03:31
epoch [10/50] batch [25/25] time 0.152 (0.199) data 0.000 (0.046) loss 4.9336 (4.5405) acc 28.1250 (38.3750) lr 1.8443e-03 eta 0:03:18
epoch [11/50] batch [5/25] time 0.154 (0.354) data 0.000 (0.200) loss 4.5117 (4.5398) acc 34.3750 (37.5000) lr 1.8443e-03 eta 0:05:52
epoch [11/50] batch [10/25] time 0.153 (0.254) data 0.000 (0.100) loss 4.1172 (4.5094) acc 53.1250 (36.5625) lr 1.8443e-03 eta 0:04:11
epoch [11/50] batch [15/25] time 0.152 (0.220) data 0.000 (0.067) loss 4.6836 (4.5138) acc 31.2500 (36.6667) lr 1.8443e-03 eta 0:03:36
epoch [11/50] batch [20/25] time 0.152 (0.203) data 0.000 (0.050) loss 4.7930 (4.5369) acc 40.6250 (37.0312) lr 1.8443e-03 eta 0:03:18
epoch [11/50] batch [25/25] time 0.151 (0.193) data 0.000 (0.040) loss 4.7891 (4.5744) acc 34.3750 (36.2500) lr 1.8090e-03 eta 0:03:07
epoch [12/50] batch [5/25] time 0.155 (0.394) data 0.000 (0.239) loss 3.8105 (4.3121) acc 56.2500 (41.2500) lr 1.8090e-03 eta 0:06:21
epoch [12/50] batch [10/25] time 0.155 (0.274) data 0.000 (0.120) loss 5.2734 (4.3736) acc 34.3750 (38.4375) lr 1.8090e-03 eta 0:04:24
epoch [12/50] batch [15/25] time 0.153 (0.234) data 0.000 (0.080) loss 4.6016 (4.4322) acc 53.1250 (37.9167) lr 1.8090e-03 eta 0:03:44
epoch [12/50] batch [20/25] time 0.152 (0.214) data 0.000 (0.060) loss 4.7812 (4.4782) acc 31.2500 (37.3438) lr 1.8090e-03 eta 0:03:23
epoch [12/50] batch [25/25] time 0.155 (0.202) data 0.000 (0.048) loss 4.2461 (4.4745) acc 46.8750 (37.6250) lr 1.7705e-03 eta 0:03:11
epoch [13/50] batch [5/25] time 0.157 (0.369) data 0.000 (0.213) loss 4.2812 (4.3609) acc 28.1250 (33.1250) lr 1.7705e-03 eta 0:05:49
epoch [13/50] batch [10/25] time 0.153 (0.262) data 0.000 (0.107) loss 4.4414 (4.4285) acc 37.5000 (37.1875) lr 1.7705e-03 eta 0:04:06
epoch [13/50] batch [15/25] time 0.151 (0.225) data 0.000 (0.071) loss 4.2266 (4.3862) acc 43.7500 (37.5000) lr 1.7705e-03 eta 0:03:30
epoch [13/50] batch [20/25] time 0.152 (0.207) data 0.000 (0.054) loss 4.4922 (4.4352) acc 53.1250 (37.8125) lr 1.7705e-03 eta 0:03:12
epoch [13/50] batch [25/25] time 0.153 (0.196) data 0.000 (0.043) loss 4.9453 (4.4400) acc 31.2500 (37.6250) lr 1.7290e-03 eta 0:03:01
epoch [14/50] batch [5/25] time 0.154 (0.351) data 0.000 (0.196) loss 4.2344 (4.2652) acc 40.6250 (42.5000) lr 1.7290e-03 eta 0:05:22
epoch [14/50] batch [10/25] time 0.154 (0.252) data 0.000 (0.098) loss 4.6133 (4.2725) acc 28.1250 (40.3125) lr 1.7290e-03 eta 0:03:51
epoch [14/50] batch [15/25] time 0.155 (0.220) data 0.000 (0.065) loss 4.0078 (4.3001) acc 46.8750 (39.3750) lr 1.7290e-03 eta 0:03:20
epoch [14/50] batch [20/25] time 0.154 (0.204) data 0.000 (0.049) loss 3.7109 (4.2735) acc 56.2500 (39.5312) lr 1.7290e-03 eta 0:03:04
epoch [14/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.039) loss 5.0586 (4.3584) acc 25.0000 (38.8750) lr 1.6845e-03 eta 0:02:54
epoch [15/50] batch [5/25] time 0.153 (0.366) data 0.000 (0.213) loss 4.3477 (4.1730) acc 31.2500 (40.6250) lr 1.6845e-03 eta 0:05:27
epoch [15/50] batch [10/25] time 0.158 (0.260) data 0.000 (0.106) loss 4.7148 (4.3658) acc 40.6250 (40.0000) lr 1.6845e-03 eta 0:03:51
epoch [15/50] batch [15/25] time 0.152 (0.224) data 0.000 (0.071) loss 4.0156 (4.3480) acc 53.1250 (41.6667) lr 1.6845e-03 eta 0:03:18
epoch [15/50] batch [20/25] time 0.152 (0.206) data 0.000 (0.053) loss 4.4219 (4.3827) acc 37.5000 (40.3125) lr 1.6845e-03 eta 0:03:01
epoch [15/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.043) loss 5.0312 (4.4388) acc 28.1250 (39.1250) lr 1.6374e-03 eta 0:02:51
epoch [16/50] batch [5/25] time 0.153 (0.343) data 0.000 (0.189) loss 4.2539 (4.4656) acc 34.3750 (36.2500) lr 1.6374e-03 eta 0:04:58
epoch [16/50] batch [10/25] time 0.153 (0.248) data 0.000 (0.095) loss 4.1445 (4.4258) acc 50.0000 (36.8750) lr 1.6374e-03 eta 0:03:34
epoch [16/50] batch [15/25] time 0.152 (0.216) data 0.000 (0.063) loss 4.7891 (4.3745) acc 40.6250 (40.2083) lr 1.6374e-03 eta 0:03:06
epoch [16/50] batch [20/25] time 0.153 (0.200) data 0.000 (0.047) loss 4.9219 (4.4098) acc 21.8750 (39.6875) lr 1.6374e-03 eta 0:02:51
epoch [16/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.038) loss 4.6367 (4.4030) acc 34.3750 (38.6250) lr 1.5878e-03 eta 0:02:42
epoch [17/50] batch [5/25] time 0.153 (0.374) data 0.000 (0.220) loss 3.7695 (4.2711) acc 46.8750 (43.1250) lr 1.5878e-03 eta 0:05:15
epoch [17/50] batch [10/25] time 0.152 (0.264) data 0.000 (0.110) loss 4.6797 (4.3078) acc 28.1250 (39.6875) lr 1.5878e-03 eta 0:03:41
epoch [17/50] batch [15/25] time 0.152 (0.226) data 0.000 (0.074) loss 4.5078 (4.2964) acc 34.3750 (38.5417) lr 1.5878e-03 eta 0:03:09
epoch [17/50] batch [20/25] time 0.152 (0.208) data 0.000 (0.055) loss 4.8203 (4.3580) acc 25.0000 (39.2188) lr 1.5878e-03 eta 0:02:52
epoch [17/50] batch [25/25] time 0.153 (0.197) data 0.000 (0.044) loss 4.1367 (4.3667) acc 43.7500 (38.6250) lr 1.5358e-03 eta 0:02:42
epoch [18/50] batch [5/25] time 0.157 (0.392) data 0.000 (0.237) loss 4.1094 (4.1797) acc 43.7500 (43.1250) lr 1.5358e-03 eta 0:05:21
epoch [18/50] batch [10/25] time 0.154 (0.273) data 0.000 (0.119) loss 4.4023 (4.1613) acc 40.6250 (45.0000) lr 1.5358e-03 eta 0:03:42
epoch [18/50] batch [15/25] time 0.154 (0.233) data 0.000 (0.079) loss 4.4492 (4.2159) acc 37.5000 (42.9167) lr 1.5358e-03 eta 0:03:09
epoch [18/50] batch [20/25] time 0.154 (0.213) data 0.000 (0.060) loss 4.9219 (4.3328) acc 34.3750 (40.7812) lr 1.5358e-03 eta 0:02:51
epoch [18/50] batch [25/25] time 0.154 (0.202) data 0.001 (0.048) loss 4.3984 (4.3398) acc 43.7500 (41.7500) lr 1.4818e-03 eta 0:02:41
epoch [19/50] batch [5/25] time 0.153 (0.357) data 0.000 (0.202) loss 4.4766 (4.1492) acc 46.8750 (50.0000) lr 1.4818e-03 eta 0:04:43
epoch [19/50] batch [10/25] time 0.153 (0.255) data 0.000 (0.101) loss 4.5859 (4.3656) acc 28.1250 (41.8750) lr 1.4818e-03 eta 0:03:21
epoch [19/50] batch [15/25] time 0.152 (0.221) data 0.000 (0.067) loss 4.3359 (4.3362) acc 40.6250 (41.6667) lr 1.4818e-03 eta 0:02:53
epoch [19/50] batch [20/25] time 0.152 (0.204) data 0.000 (0.051) loss 4.3633 (4.3064) acc 43.7500 (41.0938) lr 1.4818e-03 eta 0:02:39
epoch [19/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.040) loss 4.4805 (4.2957) acc 34.3750 (40.5000) lr 1.4258e-03 eta 0:02:30
epoch [20/50] batch [5/25] time 0.158 (0.365) data 0.000 (0.210) loss 4.2891 (4.1773) acc 31.2500 (45.6250) lr 1.4258e-03 eta 0:04:41
epoch [20/50] batch [10/25] time 0.153 (0.260) data 0.000 (0.105) loss 4.7031 (4.2500) acc 31.2500 (43.7500) lr 1.4258e-03 eta 0:03:18
epoch [20/50] batch [15/25] time 0.153 (0.224) data 0.000 (0.070) loss 4.4141 (4.2320) acc 37.5000 (42.2917) lr 1.4258e-03 eta 0:02:50
epoch [20/50] batch [20/25] time 0.154 (0.206) data 0.000 (0.053) loss 4.3125 (4.2945) acc 34.3750 (39.8438) lr 1.4258e-03 eta 0:02:35
epoch [20/50] batch [25/25] time 0.152 (0.195) data 0.000 (0.042) loss 4.1758 (4.2363) acc 40.6250 (41.1250) lr 1.3681e-03 eta 0:02:26
epoch [21/50] batch [5/25] time 0.155 (0.341) data 0.000 (0.187) loss 4.1523 (4.2234) acc 53.1250 (44.3750) lr 1.3681e-03 eta 0:04:14
epoch [21/50] batch [10/25] time 0.154 (0.248) data 0.000 (0.094) loss 3.9570 (4.2863) acc 46.8750 (42.5000) lr 1.3681e-03 eta 0:03:03
epoch [21/50] batch [15/25] time 0.152 (0.216) data 0.000 (0.062) loss 3.7812 (4.2643) acc 56.2500 (41.8750) lr 1.3681e-03 eta 0:02:38
epoch [21/50] batch [20/25] time 0.154 (0.201) data 0.000 (0.047) loss 3.9434 (4.2794) acc 43.7500 (42.3438) lr 1.3681e-03 eta 0:02:26
epoch [21/50] batch [25/25] time 0.153 (0.191) data 0.000 (0.038) loss 4.0312 (4.2720) acc 50.0000 (42.6250) lr 1.3090e-03 eta 0:02:18
epoch [22/50] batch [5/25] time 0.155 (0.365) data 0.000 (0.210) loss 5.0117 (4.2898) acc 34.3750 (40.0000) lr 1.3090e-03 eta 0:04:22
epoch [22/50] batch [10/25] time 0.155 (0.261) data 0.000 (0.105) loss 3.8359 (4.1492) acc 50.0000 (41.2500) lr 1.3090e-03 eta 0:03:06
epoch [22/50] batch [15/25] time 0.153 (0.225) data 0.000 (0.070) loss 4.1328 (4.2271) acc 31.2500 (40.2083) lr 1.3090e-03 eta 0:02:39
epoch [22/50] batch [20/25] time 0.153 (0.207) data 0.000 (0.053) loss 3.7461 (4.2785) acc 50.0000 (38.5938) lr 1.3090e-03 eta 0:02:25
epoch [22/50] batch [25/25] time 0.153 (0.196) data 0.000 (0.042) loss 4.1992 (4.2905) acc 50.0000 (39.0000) lr 1.2487e-03 eta 0:02:17
epoch [23/50] batch [5/25] time 0.155 (0.385) data 0.000 (0.230) loss 4.4023 (4.3312) acc 46.8750 (41.8750) lr 1.2487e-03 eta 0:04:27
epoch [23/50] batch [10/25] time 0.154 (0.270) data 0.000 (0.115) loss 4.2031 (4.1619) acc 40.6250 (42.8125) lr 1.2487e-03 eta 0:03:06
epoch [23/50] batch [15/25] time 0.154 (0.231) data 0.000 (0.077) loss 4.1719 (4.1249) acc 40.6250 (43.5417) lr 1.2487e-03 eta 0:02:38
epoch [23/50] batch [20/25] time 0.154 (0.212) data 0.000 (0.058) loss 4.8672 (4.2573) acc 18.7500 (41.4062) lr 1.2487e-03 eta 0:02:23
epoch [23/50] batch [25/25] time 0.153 (0.200) data 0.000 (0.046) loss 4.1602 (4.2407) acc 46.8750 (42.3750) lr 1.1874e-03 eta 0:02:14
epoch [24/50] batch [5/25] time 0.155 (0.371) data 0.000 (0.216) loss 3.5938 (4.1609) acc 59.3750 (45.6250) lr 1.1874e-03 eta 0:04:08
epoch [24/50] batch [10/25] time 0.156 (0.263) data 0.000 (0.108) loss 4.3164 (4.1402) acc 34.3750 (44.6875) lr 1.1874e-03 eta 0:02:54
epoch [24/50] batch [15/25] time 0.155 (0.227) data 0.000 (0.072) loss 4.7539 (4.1168) acc 34.3750 (44.1667) lr 1.1874e-03 eta 0:02:30
epoch [24/50] batch [20/25] time 0.155 (0.209) data 0.000 (0.054) loss 4.4023 (4.2081) acc 50.0000 (43.9062) lr 1.1874e-03 eta 0:02:17
epoch [24/50] batch [25/25] time 0.155 (0.199) data 0.000 (0.043) loss 4.6758 (4.2434) acc 31.2500 (43.8750) lr 1.1253e-03 eta 0:02:09
epoch [25/50] batch [5/25] time 0.157 (0.354) data 0.000 (0.197) loss 3.8496 (3.9051) acc 59.3750 (54.3750) lr 1.1253e-03 eta 0:03:48
epoch [25/50] batch [10/25] time 0.156 (0.255) data 0.000 (0.099) loss 4.4883 (4.0262) acc 43.7500 (47.5000) lr 1.1253e-03 eta 0:02:43
epoch [25/50] batch [15/25] time 0.155 (0.222) data 0.000 (0.066) loss 4.7852 (4.1490) acc 28.1250 (44.1667) lr 1.1253e-03 eta 0:02:20
epoch [25/50] batch [20/25] time 0.157 (0.205) data 0.000 (0.049) loss 4.0312 (4.1738) acc 53.1250 (45.6250) lr 1.1253e-03 eta 0:02:09
epoch [25/50] batch [25/25] time 0.157 (0.195) data 0.000 (0.040) loss 4.3125 (4.1944) acc 53.1250 (45.2500) lr 1.0628e-03 eta 0:02:02
epoch [26/50] batch [5/25] time 0.156 (0.381) data 0.000 (0.225) loss 4.3281 (3.9879) acc 37.5000 (41.8750) lr 1.0628e-03 eta 0:03:56
epoch [26/50] batch [10/25] time 0.154 (0.268) data 0.000 (0.113) loss 4.2578 (4.0900) acc 50.0000 (41.2500) lr 1.0628e-03 eta 0:02:44
epoch [26/50] batch [15/25] time 0.152 (0.230) data 0.000 (0.075) loss 4.4375 (4.1087) acc 34.3750 (41.6667) lr 1.0628e-03 eta 0:02:20
epoch [26/50] batch [20/25] time 0.153 (0.210) data 0.000 (0.056) loss 3.7324 (4.0964) acc 43.7500 (42.8125) lr 1.0628e-03 eta 0:02:07
epoch [26/50] batch [25/25] time 0.153 (0.199) data 0.000 (0.045) loss 4.2969 (4.1369) acc 31.2500 (41.6250) lr 1.0000e-03 eta 0:01:59
epoch [27/50] batch [5/25] time 0.153 (0.347) data 0.000 (0.193) loss 3.9961 (4.0672) acc 40.6250 (44.3750) lr 1.0000e-03 eta 0:03:26
epoch [27/50] batch [10/25] time 0.153 (0.251) data 0.000 (0.097) loss 4.7109 (4.0932) acc 37.5000 (45.9375) lr 1.0000e-03 eta 0:02:27
epoch [27/50] batch [15/25] time 0.152 (0.218) data 0.000 (0.065) loss 4.0391 (4.0547) acc 43.7500 (45.8333) lr 1.0000e-03 eta 0:02:07
epoch [27/50] batch [20/25] time 0.152 (0.202) data 0.000 (0.048) loss 4.0547 (4.0386) acc 37.5000 (45.6250) lr 1.0000e-03 eta 0:01:56
epoch [27/50] batch [25/25] time 0.153 (0.192) data 0.000 (0.039) loss 4.4219 (4.1035) acc 34.3750 (44.6250) lr 9.3721e-04 eta 0:01:50
epoch [28/50] batch [5/25] time 0.154 (0.332) data 0.000 (0.178) loss 4.2891 (4.1621) acc 50.0000 (46.8750) lr 9.3721e-04 eta 0:03:09
epoch [28/50] batch [10/25] time 0.154 (0.244) data 0.000 (0.089) loss 4.5117 (4.1543) acc 40.6250 (46.2500) lr 9.3721e-04 eta 0:02:17
epoch [28/50] batch [15/25] time 0.152 (0.214) data 0.000 (0.059) loss 3.6602 (4.0755) acc 56.2500 (47.2917) lr 9.3721e-04 eta 0:01:59
epoch [28/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.045) loss 3.5723 (4.0403) acc 62.5000 (49.2188) lr 9.3721e-04 eta 0:01:50
epoch [28/50] batch [25/25] time 0.152 (0.189) data 0.000 (0.036) loss 4.0938 (4.0984) acc 50.0000 (48.2500) lr 8.7467e-04 eta 0:01:44
epoch [29/50] batch [5/25] time 0.155 (0.339) data 0.000 (0.184) loss 3.7031 (4.0555) acc 50.0000 (46.2500) lr 8.7467e-04 eta 0:03:04
epoch [29/50] batch [10/25] time 0.153 (0.247) data 0.000 (0.092) loss 4.3984 (4.0336) acc 21.8750 (46.8750) lr 8.7467e-04 eta 0:02:13
epoch [29/50] batch [15/25] time 0.153 (0.215) data 0.000 (0.061) loss 4.2500 (4.0027) acc 43.7500 (45.6250) lr 8.7467e-04 eta 0:01:55
epoch [29/50] batch [20/25] time 0.153 (0.200) data 0.000 (0.046) loss 4.4258 (4.0812) acc 43.7500 (45.3125) lr 8.7467e-04 eta 0:01:45
epoch [29/50] batch [25/25] time 0.152 (0.190) data 0.000 (0.037) loss 4.3125 (4.0400) acc 50.0000 (47.1250) lr 8.1262e-04 eta 0:01:39
epoch [30/50] batch [5/25] time 0.155 (0.360) data 0.000 (0.205) loss 3.5898 (3.7766) acc 68.7500 (57.5000) lr 8.1262e-04 eta 0:03:07
epoch [30/50] batch [10/25] time 0.155 (0.258) data 0.000 (0.103) loss 4.7617 (3.9482) acc 43.7500 (54.6875) lr 8.1262e-04 eta 0:02:12
epoch [30/50] batch [15/25] time 0.153 (0.223) data 0.000 (0.068) loss 4.1875 (3.9677) acc 50.0000 (52.2917) lr 8.1262e-04 eta 0:01:53
epoch [30/50] batch [20/25] time 0.153 (0.205) data 0.000 (0.051) loss 3.6445 (4.0334) acc 56.2500 (52.0312) lr 8.1262e-04 eta 0:01:43
epoch [30/50] batch [25/25] time 0.153 (0.195) data 0.000 (0.041) loss 3.7461 (4.0644) acc 46.8750 (50.3750) lr 7.5131e-04 eta 0:01:37
epoch [31/50] batch [5/25] time 0.155 (0.345) data 0.000 (0.190) loss 4.5000 (4.3980) acc 28.1250 (38.1250) lr 7.5131e-04 eta 0:02:50
epoch [31/50] batch [10/25] time 0.153 (0.250) data 0.000 (0.095) loss 3.8945 (4.1268) acc 56.2500 (44.6875) lr 7.5131e-04 eta 0:02:02
epoch [31/50] batch [15/25] time 0.152 (0.217) data 0.000 (0.063) loss 4.0586 (4.1452) acc 43.7500 (45.0000) lr 7.5131e-04 eta 0:01:45
epoch [31/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 3.7188 (4.1325) acc 65.6250 (45.6250) lr 7.5131e-04 eta 0:01:36
epoch [31/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.038) loss 3.5215 (4.0862) acc 78.1250 (47.2500) lr 6.9098e-04 eta 0:01:30
epoch [32/50] batch [5/25] time 0.154 (0.363) data 0.000 (0.209) loss 3.8047 (4.0770) acc 50.0000 (43.7500) lr 6.9098e-04 eta 0:02:50
epoch [32/50] batch [10/25] time 0.155 (0.259) data 0.000 (0.105) loss 4.4609 (4.0568) acc 43.7500 (46.2500) lr 6.9098e-04 eta 0:02:00
epoch [32/50] batch [15/25] time 0.152 (0.223) data 0.000 (0.070) loss 4.0898 (4.1293) acc 43.7500 (45.4167) lr 6.9098e-04 eta 0:01:42
epoch [32/50] batch [20/25] time 0.153 (0.206) data 0.000 (0.052) loss 4.0586 (4.0802) acc 50.0000 (46.5625) lr 6.9098e-04 eta 0:01:33
epoch [32/50] batch [25/25] time 0.152 (0.195) data 0.000 (0.042) loss 4.6562 (4.1209) acc 37.5000 (46.3750) lr 6.3188e-04 eta 0:01:27
epoch [33/50] batch [5/25] time 0.154 (0.338) data 0.000 (0.184) loss 4.2578 (4.0402) acc 34.3750 (48.7500) lr 6.3188e-04 eta 0:02:30
epoch [33/50] batch [10/25] time 0.153 (0.246) data 0.000 (0.092) loss 4.2812 (4.0490) acc 46.8750 (48.1250) lr 6.3188e-04 eta 0:01:48
epoch [33/50] batch [15/25] time 0.152 (0.215) data 0.000 (0.061) loss 4.6016 (4.0818) acc 31.2500 (47.5000) lr 6.3188e-04 eta 0:01:33
epoch [33/50] batch [20/25] time 0.154 (0.200) data 0.000 (0.046) loss 4.0312 (4.0395) acc 46.8750 (47.6562) lr 6.3188e-04 eta 0:01:25
epoch [33/50] batch [25/25] time 0.154 (0.191) data 0.000 (0.037) loss 4.4609 (4.0418) acc 50.0000 (47.8750) lr 5.7422e-04 eta 0:01:21
epoch [34/50] batch [5/25] time 0.154 (0.351) data 0.000 (0.196) loss 3.1875 (3.8449) acc 65.6250 (54.3750) lr 5.7422e-04 eta 0:02:27
epoch [34/50] batch [10/25] time 0.155 (0.252) data 0.000 (0.098) loss 3.6406 (3.8611) acc 56.2500 (52.5000) lr 5.7422e-04 eta 0:01:44
epoch [34/50] batch [15/25] time 0.154 (0.220) data 0.000 (0.065) loss 4.4062 (3.9230) acc 43.7500 (51.0417) lr 5.7422e-04 eta 0:01:30
epoch [34/50] batch [20/25] time 0.152 (0.203) data 0.000 (0.049) loss 3.7266 (3.9817) acc 53.1250 (50.3125) lr 5.7422e-04 eta 0:01:22
epoch [34/50] batch [25/25] time 0.152 (0.193) data 0.000 (0.039) loss 4.2930 (4.0045) acc 34.3750 (50.0000) lr 5.1825e-04 eta 0:01:17
epoch [35/50] batch [5/25] time 0.153 (0.366) data 0.000 (0.211) loss 4.0430 (4.0168) acc 43.7500 (48.7500) lr 5.1825e-04 eta 0:02:24
epoch [35/50] batch [10/25] time 0.152 (0.260) data 0.000 (0.106) loss 4.3516 (4.0311) acc 40.6250 (49.6875) lr 5.1825e-04 eta 0:01:41
epoch [35/50] batch [15/25] time 0.153 (0.224) data 0.000 (0.071) loss 4.2422 (4.0375) acc 50.0000 (50.0000) lr 5.1825e-04 eta 0:01:26
epoch [35/50] batch [20/25] time 0.152 (0.206) data 0.000 (0.053) loss 4.1289 (4.0686) acc 53.1250 (49.5312) lr 5.1825e-04 eta 0:01:18
epoch [35/50] batch [25/25] time 0.152 (0.196) data 0.000 (0.042) loss 3.3945 (3.9708) acc 65.6250 (51.7500) lr 4.6417e-04 eta 0:01:13
epoch [36/50] batch [5/25] time 0.154 (0.350) data 0.000 (0.195) loss 3.8711 (3.9586) acc 62.5000 (51.2500) lr 4.6417e-04 eta 0:02:09
epoch [36/50] batch [10/25] time 0.153 (0.251) data 0.000 (0.098) loss 3.7617 (3.9135) acc 50.0000 (48.7500) lr 4.6417e-04 eta 0:01:31
epoch [36/50] batch [15/25] time 0.152 (0.218) data 0.000 (0.065) loss 3.8359 (3.9452) acc 46.8750 (47.7083) lr 4.6417e-04 eta 0:01:18
epoch [36/50] batch [20/25] time 0.152 (0.202) data 0.000 (0.049) loss 3.8828 (3.9800) acc 43.7500 (48.2812) lr 4.6417e-04 eta 0:01:11
epoch [36/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.039) loss 4.3086 (3.9758) acc 31.2500 (48.6250) lr 4.1221e-04 eta 0:01:07
epoch [37/50] batch [5/25] time 0.154 (0.337) data 0.000 (0.183) loss 4.1758 (3.9422) acc 59.3750 (55.0000) lr 4.1221e-04 eta 0:01:56
epoch [37/50] batch [10/25] time 0.156 (0.245) data 0.000 (0.092) loss 4.0742 (4.0057) acc 46.8750 (55.0000) lr 4.1221e-04 eta 0:01:23
epoch [37/50] batch [15/25] time 0.153 (0.214) data 0.000 (0.061) loss 3.9688 (4.0033) acc 43.7500 (51.8750) lr 4.1221e-04 eta 0:01:11
epoch [37/50] batch [20/25] time 0.152 (0.199) data 0.000 (0.046) loss 4.0547 (3.9832) acc 40.6250 (50.9375) lr 4.1221e-04 eta 0:01:05
epoch [37/50] batch [25/25] time 0.153 (0.189) data 0.000 (0.037) loss 3.7617 (4.0055) acc 43.7500 (48.7500) lr 3.6258e-04 eta 0:01:01
epoch [38/50] batch [5/25] time 0.154 (0.361) data 0.000 (0.205) loss 3.7617 (3.9988) acc 56.2500 (50.0000) lr 3.6258e-04 eta 0:01:55
epoch [38/50] batch [10/25] time 0.155 (0.257) data 0.000 (0.103) loss 4.1602 (4.0514) acc 50.0000 (48.7500) lr 3.6258e-04 eta 0:01:21
epoch [38/50] batch [15/25] time 0.152 (0.223) data 0.000 (0.069) loss 4.0898 (3.9604) acc 46.8750 (50.6250) lr 3.6258e-04 eta 0:01:09
epoch [38/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.052) loss 3.4160 (3.9510) acc 68.7500 (51.2500) lr 3.6258e-04 eta 0:01:02
epoch [38/50] batch [25/25] time 0.152 (0.195) data 0.000 (0.041) loss 3.8359 (3.9759) acc 53.1250 (51.2500) lr 3.1545e-04 eta 0:00:58
epoch [39/50] batch [5/25] time 0.154 (0.353) data 0.000 (0.198) loss 3.5664 (4.1492) acc 65.6250 (51.2500) lr 3.1545e-04 eta 0:01:44
epoch [39/50] batch [10/25] time 0.153 (0.254) data 0.000 (0.099) loss 3.7930 (4.0217) acc 46.8750 (49.6875) lr 3.1545e-04 eta 0:01:13
epoch [39/50] batch [15/25] time 0.153 (0.220) data 0.000 (0.066) loss 3.7656 (3.9944) acc 43.7500 (50.2083) lr 3.1545e-04 eta 0:01:02
epoch [39/50] batch [20/25] time 0.152 (0.203) data 0.000 (0.050) loss 4.2188 (4.0606) acc 46.8750 (48.7500) lr 3.1545e-04 eta 0:00:56
epoch [39/50] batch [25/25] time 0.153 (0.193) data 0.000 (0.040) loss 3.9219 (4.0432) acc 59.3750 (49.6250) lr 2.7103e-04 eta 0:00:53
epoch [40/50] batch [5/25] time 0.154 (0.344) data 0.000 (0.190) loss 3.5977 (3.7734) acc 56.2500 (58.7500) lr 2.7103e-04 eta 0:01:32
epoch [40/50] batch [10/25] time 0.154 (0.249) data 0.000 (0.095) loss 4.0352 (4.0346) acc 53.1250 (50.0000) lr 2.7103e-04 eta 0:01:06
epoch [40/50] batch [15/25] time 0.152 (0.217) data 0.000 (0.063) loss 3.6562 (4.0366) acc 59.3750 (50.0000) lr 2.7103e-04 eta 0:00:56
epoch [40/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 3.6758 (3.9785) acc 65.6250 (52.3438) lr 2.7103e-04 eta 0:00:51
epoch [40/50] batch [25/25] time 0.153 (0.191) data 0.000 (0.038) loss 4.2969 (3.9586) acc 37.5000 (52.1250) lr 2.2949e-04 eta 0:00:47
epoch [41/50] batch [5/25] time 0.155 (0.359) data 0.000 (0.203) loss 3.8359 (3.7270) acc 46.8750 (51.2500) lr 2.2949e-04 eta 0:01:28
epoch [41/50] batch [10/25] time 0.154 (0.257) data 0.000 (0.102) loss 3.3535 (3.8102) acc 65.6250 (51.2500) lr 2.2949e-04 eta 0:01:01
epoch [41/50] batch [15/25] time 0.153 (0.223) data 0.000 (0.068) loss 4.0312 (3.8702) acc 40.6250 (51.4583) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.051) loss 4.2969 (3.8976) acc 34.3750 (51.0938) lr 2.2949e-04 eta 0:00:47
epoch [41/50] batch [25/25] time 0.154 (0.195) data 0.000 (0.041) loss 4.2617 (3.9392) acc 43.7500 (50.7500) lr 1.9098e-04 eta 0:00:43
epoch [42/50] batch [5/25] time 0.155 (0.341) data 0.000 (0.185) loss 3.6758 (3.5953) acc 46.8750 (58.7500) lr 1.9098e-04 eta 0:01:14
epoch [42/50] batch [10/25] time 0.155 (0.248) data 0.000 (0.093) loss 3.9336 (3.7633) acc 53.1250 (56.2500) lr 1.9098e-04 eta 0:00:53
epoch [42/50] batch [15/25] time 0.152 (0.216) data 0.000 (0.062) loss 3.8750 (3.8328) acc 59.3750 (53.9583) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [20/25] time 0.154 (0.200) data 0.000 (0.047) loss 4.1133 (3.9227) acc 46.8750 (52.0312) lr 1.9098e-04 eta 0:00:41
epoch [42/50] batch [25/25] time 0.154 (0.191) data 0.000 (0.037) loss 3.7402 (3.9352) acc 65.6250 (52.7500) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [5/25] time 0.155 (0.337) data 0.000 (0.180) loss 4.0352 (3.8777) acc 40.6250 (50.0000) lr 1.5567e-04 eta 0:01:05
epoch [43/50] batch [10/25] time 0.155 (0.246) data 0.000 (0.090) loss 3.4531 (3.8863) acc 50.0000 (51.5625) lr 1.5567e-04 eta 0:00:46
epoch [43/50] batch [15/25] time 0.152 (0.215) data 0.000 (0.060) loss 4.2148 (3.9185) acc 37.5000 (50.0000) lr 1.5567e-04 eta 0:00:39
epoch [43/50] batch [20/25] time 0.152 (0.199) data 0.000 (0.045) loss 4.0977 (3.8955) acc 50.0000 (51.0938) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/25] time 0.152 (0.190) data 0.000 (0.036) loss 4.2188 (3.9362) acc 46.8750 (50.7500) lr 1.2369e-04 eta 0:00:33
epoch [44/50] batch [5/25] time 0.153 (0.342) data 0.000 (0.188) loss 4.0391 (3.7723) acc 50.0000 (55.6250) lr 1.2369e-04 eta 0:00:58
epoch [44/50] batch [10/25] time 0.155 (0.248) data 0.000 (0.094) loss 3.7930 (3.8955) acc 62.5000 (53.4375) lr 1.2369e-04 eta 0:00:40
epoch [44/50] batch [15/25] time 0.152 (0.216) data 0.000 (0.063) loss 3.8594 (3.9100) acc 59.3750 (52.7083) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [20/25] time 0.152 (0.200) data 0.000 (0.047) loss 4.0078 (3.9730) acc 46.8750 (50.3125) lr 1.2369e-04 eta 0:00:31
epoch [44/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.038) loss 3.6094 (3.9807) acc 65.6250 (49.7500) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [5/25] time 0.155 (0.336) data 0.000 (0.181) loss 4.3984 (4.1383) acc 43.7500 (47.5000) lr 9.5173e-05 eta 0:00:48
epoch [45/50] batch [10/25] time 0.156 (0.245) data 0.000 (0.091) loss 3.3340 (3.9908) acc 65.6250 (51.2500) lr 9.5173e-05 eta 0:00:34
epoch [45/50] batch [15/25] time 0.153 (0.215) data 0.000 (0.061) loss 3.8418 (3.9430) acc 46.8750 (52.0833) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [20/25] time 0.152 (0.199) data 0.000 (0.045) loss 3.8398 (3.9275) acc 56.2500 (51.8750) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/25] time 0.156 (0.190) data 0.000 (0.036) loss 3.5566 (3.9150) acc 59.3750 (52.3750) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [5/25] time 0.155 (0.321) data 0.000 (0.166) loss 3.9707 (4.0199) acc 53.1250 (53.7500) lr 7.0224e-05 eta 0:00:38
epoch [46/50] batch [10/25] time 0.154 (0.238) data 0.000 (0.083) loss 4.1445 (3.9475) acc 53.1250 (54.3750) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [15/25] time 0.152 (0.209) data 0.000 (0.055) loss 3.8281 (3.8958) acc 59.3750 (53.3333) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [20/25] time 0.152 (0.195) data 0.000 (0.042) loss 3.4023 (3.9249) acc 62.5000 (53.4375) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/25] time 0.152 (0.186) data 0.000 (0.033) loss 3.8340 (3.8764) acc 53.1250 (53.5000) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [5/25] time 0.154 (0.360) data 0.000 (0.204) loss 3.8770 (3.8332) acc 43.7500 (55.6250) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [10/25] time 0.154 (0.258) data 0.000 (0.102) loss 3.9492 (3.7936) acc 50.0000 (57.8125) lr 4.8943e-05 eta 0:00:23
epoch [47/50] batch [15/25] time 0.152 (0.223) data 0.000 (0.068) loss 3.8125 (3.7884) acc 50.0000 (57.5000) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.051) loss 4.3438 (3.7962) acc 34.3750 (55.9375) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [25/25] time 0.153 (0.195) data 0.000 (0.041) loss 3.7754 (3.8603) acc 65.6250 (54.5000) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [5/25] time 0.154 (0.339) data 0.000 (0.184) loss 3.7676 (4.0059) acc 59.3750 (53.1250) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [10/25] time 0.156 (0.247) data 0.000 (0.092) loss 3.8867 (3.9119) acc 46.8750 (53.7500) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [15/25] time 0.152 (0.216) data 0.000 (0.061) loss 4.1953 (3.9203) acc 37.5000 (53.5417) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/25] time 0.152 (0.200) data 0.000 (0.046) loss 3.9570 (3.9071) acc 43.7500 (53.1250) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/25] time 0.152 (0.190) data 0.000 (0.037) loss 3.8398 (3.9320) acc 46.8750 (52.2500) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [5/25] time 0.154 (0.328) data 0.000 (0.175) loss 4.0078 (4.1484) acc 40.6250 (49.3750) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [10/25] time 0.154 (0.242) data 0.000 (0.088) loss 3.5703 (4.0100) acc 56.2500 (49.3750) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/25] time 0.152 (0.212) data 0.000 (0.058) loss 3.6484 (3.9165) acc 75.0000 (52.7083) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/25] time 0.152 (0.197) data 0.000 (0.044) loss 3.8574 (3.8677) acc 53.1250 (53.5938) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/25] time 0.152 (0.188) data 0.000 (0.035) loss 4.0000 (3.8532) acc 53.1250 (54.3750) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [5/25] time 0.155 (0.356) data 0.000 (0.200) loss 4.4727 (4.0453) acc 34.3750 (50.6250) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [10/25] time 0.158 (0.256) data 0.000 (0.100) loss 3.8945 (3.9133) acc 46.8750 (50.9375) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/25] time 0.157 (0.222) data 0.000 (0.067) loss 4.1055 (3.9180) acc 53.1250 (52.0833) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/25] time 0.152 (0.205) data 0.000 (0.050) loss 3.7891 (3.9136) acc 62.5000 (52.0312) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.040) loss 3.9570 (3.9153) acc 59.3750 (52.5000) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:31, 10.56s/it] 50%|█████     | 2/4 [00:11<00:10,  5.01s/it] 75%|███████▌  | 3/4 [00:12<00:03,  3.24s/it]100%|██████████| 4/4 [00:13<00:00,  2.13s/it]100%|██████████| 4/4 [00:13<00:00,  3.35s/it]
=> result
* total: 1,666
* correct: 712
* accuracy: 42.7%
* error: 57.3%
* macro_f1: 40.9%
Elapsed: 0:04:21
Run this job and save the output to output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,666
---------  ------------
['707-320', '727-200', '737-200', '737-300', '737-400', '737-500', '737-600', '737-700', '737-800', '737-900', '747-100', '747-200', '747-300', '747-400', '757-200', '757-300', '767-200', '767-300', '767-400', '777-200', '777-300', 'A300B4', 'A310', 'A318', 'A319', 'A320', 'A321', 'A330-200', 'A330-300', 'A340-200', 'A340-300', 'A340-500', 'A340-600', 'A380', 'ATR-42', 'ATR-72', 'An-12', 'BAE 146-200', 'BAE 146-300', 'BAE-125', 'Beechcraft 1900', 'Boeing 717', 'C-130', 'C-47', 'CRJ-200', 'CRJ-700', 'CRJ-900', 'Cessna 172', 'Cessna 208', 'Cessna 525']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 707-320, a type of aircraft.', 'X X X X 727-200, a type of aircraft.', 'X X X X 737-200, a type of aircraft.', 'X X X X 737-300, a type of aircraft.', 'X X X X 737-400, a type of aircraft.', 'X X X X 737-500, a type of aircraft.', 'X X X X 737-600, a type of aircraft.', 'X X X X 737-700, a type of aircraft.', 'X X X X 737-800, a type of aircraft.', 'X X X X 737-900, a type of aircraft.', 'X X X X 747-100, a type of aircraft.', 'X X X X 747-200, a type of aircraft.', 'X X X X 747-300, a type of aircraft.', 'X X X X 747-400, a type of aircraft.', 'X X X X 757-200, a type of aircraft.', 'X X X X 757-300, a type of aircraft.', 'X X X X 767-200, a type of aircraft.', 'X X X X 767-300, a type of aircraft.', 'X X X X 767-400, a type of aircraft.', 'X X X X 777-200, a type of aircraft.', 'X X X X 777-300, a type of aircraft.', 'X X X X A300B4, a type of aircraft.', 'X X X X A310, a type of aircraft.', 'X X X X A318, a type of aircraft.', 'X X X X A319, a type of aircraft.', 'X X X X A320, a type of aircraft.', 'X X X X A321, a type of aircraft.', 'X X X X A330-200, a type of aircraft.', 'X X X X A330-300, a type of aircraft.', 'X X X X A340-200, a type of aircraft.', 'X X X X A340-300, a type of aircraft.', 'X X X X A340-500, a type of aircraft.', 'X X X X A340-600, a type of aircraft.', 'X X X X A380, a type of aircraft.', 'X X X X ATR-42, a type of aircraft.', 'X X X X ATR-72, a type of aircraft.', 'X X X X An-12, a type of aircraft.', 'X X X X BAE 146-200, a type of aircraft.', 'X X X X BAE 146-300, a type of aircraft.', 'X X X X BAE-125, a type of aircraft.', 'X X X X Beechcraft 1900, a type of aircraft.', 'X X X X Boeing 717, a type of aircraft.', 'X X X X C-130, a type of aircraft.', 'X X X X C-47, a type of aircraft.', 'X X X X CRJ-200, a type of aircraft.', 'X X X X CRJ-700, a type of aircraft.', 'X X X X CRJ-900, a type of aircraft.', 'X X X X Cessna 172, a type of aircraft.', 'X X X X Cessna 208, a type of aircraft.', 'X X X X Cessna 525, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/25] time 0.152 (0.359) data 0.000 (0.201) loss 7.3438 (7.3297) acc 25.0000 (23.7500) lr 1.0000e-05 eta 0:07:26
epoch [1/50] batch [10/25] time 0.151 (0.256) data 0.000 (0.101) loss 7.4102 (7.3957) acc 12.5000 (21.8750) lr 1.0000e-05 eta 0:05:17
epoch [1/50] batch [15/25] time 0.151 (0.221) data 0.000 (0.067) loss 7.3398 (7.4161) acc 15.6250 (19.7917) lr 1.0000e-05 eta 0:04:33
epoch [1/50] batch [20/25] time 0.151 (0.204) data 0.000 (0.050) loss 7.5625 (7.3771) acc 15.6250 (19.0625) lr 1.0000e-05 eta 0:04:10
epoch [1/50] batch [25/25] time 0.151 (0.193) data 0.000 (0.040) loss 7.4727 (7.3634) acc 21.8750 (19.5000) lr 2.0000e-03 eta 0:03:56
epoch [2/50] batch [5/25] time 0.153 (0.367) data 0.000 (0.213) loss 6.2266 (6.3773) acc 25.0000 (26.8750) lr 2.0000e-03 eta 0:07:27
epoch [2/50] batch [10/25] time 0.154 (0.260) data 0.000 (0.107) loss 6.0195 (6.2305) acc 21.8750 (24.3750) lr 2.0000e-03 eta 0:05:15
epoch [2/50] batch [15/25] time 0.152 (0.224) data 0.000 (0.071) loss 5.5508 (6.0167) acc 28.1250 (25.6250) lr 2.0000e-03 eta 0:04:30
epoch [2/50] batch [20/25] time 0.157 (0.207) data 0.000 (0.053) loss 5.8203 (5.9525) acc 18.7500 (25.0000) lr 2.0000e-03 eta 0:04:08
epoch [2/50] batch [25/25] time 0.155 (0.196) data 0.000 (0.043) loss 5.5391 (5.8691) acc 28.1250 (26.1250) lr 1.9980e-03 eta 0:03:55
epoch [3/50] batch [5/25] time 0.153 (0.335) data 0.000 (0.181) loss 5.2500 (5.4109) acc 34.3750 (28.1250) lr 1.9980e-03 eta 0:06:40
epoch [3/50] batch [10/25] time 0.153 (0.244) data 0.000 (0.091) loss 5.1016 (5.3074) acc 28.1250 (27.1875) lr 1.9980e-03 eta 0:04:50
epoch [3/50] batch [15/25] time 0.151 (0.213) data 0.000 (0.061) loss 4.6797 (5.2826) acc 43.7500 (28.1250) lr 1.9980e-03 eta 0:04:12
epoch [3/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.045) loss 4.9883 (5.2650) acc 40.6250 (28.7500) lr 1.9980e-03 eta 0:03:53
epoch [3/50] batch [25/25] time 0.151 (0.189) data 0.000 (0.036) loss 4.5391 (5.2205) acc 53.1250 (30.7500) lr 1.9921e-03 eta 0:03:41
epoch [4/50] batch [5/25] time 0.153 (0.350) data 0.000 (0.197) loss 4.7070 (5.1266) acc 31.2500 (26.2500) lr 1.9921e-03 eta 0:06:50
epoch [4/50] batch [10/25] time 0.152 (0.252) data 0.000 (0.099) loss 4.9609 (5.1020) acc 40.6250 (30.3125) lr 1.9921e-03 eta 0:04:53
epoch [4/50] batch [15/25] time 0.153 (0.218) data 0.000 (0.066) loss 4.9141 (5.0505) acc 31.2500 (32.0833) lr 1.9921e-03 eta 0:04:13
epoch [4/50] batch [20/25] time 0.153 (0.202) data 0.000 (0.049) loss 4.7344 (5.0166) acc 40.6250 (32.1875) lr 1.9921e-03 eta 0:03:53
epoch [4/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.040) loss 5.1523 (5.0448) acc 28.1250 (32.0000) lr 1.9823e-03 eta 0:03:40
epoch [5/50] batch [5/25] time 0.153 (0.352) data 0.000 (0.199) loss 4.3438 (4.8039) acc 43.7500 (35.6250) lr 1.9823e-03 eta 0:06:42
epoch [5/50] batch [10/25] time 0.153 (0.252) data 0.000 (0.099) loss 4.9766 (4.8094) acc 31.2500 (35.3125) lr 1.9823e-03 eta 0:04:47
epoch [5/50] batch [15/25] time 0.152 (0.219) data 0.000 (0.066) loss 4.7734 (4.7779) acc 37.5000 (33.7500) lr 1.9823e-03 eta 0:04:08
epoch [5/50] batch [20/25] time 0.151 (0.202) data 0.000 (0.050) loss 4.9336 (4.8514) acc 25.0000 (32.3438) lr 1.9823e-03 eta 0:03:48
epoch [5/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.040) loss 4.9141 (4.8603) acc 37.5000 (32.3750) lr 1.9686e-03 eta 0:03:36
epoch [6/50] batch [5/25] time 0.153 (0.335) data 0.000 (0.182) loss 5.0859 (4.6508) acc 34.3750 (35.6250) lr 1.9686e-03 eta 0:06:15
epoch [6/50] batch [10/25] time 0.152 (0.244) data 0.000 (0.091) loss 4.8750 (4.7492) acc 31.2500 (34.0625) lr 1.9686e-03 eta 0:04:31
epoch [6/50] batch [15/25] time 0.151 (0.213) data 0.000 (0.061) loss 4.4648 (4.7042) acc 43.7500 (35.8333) lr 1.9686e-03 eta 0:03:56
epoch [6/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.046) loss 4.1172 (4.6840) acc 43.7500 (35.6250) lr 1.9686e-03 eta 0:03:38
epoch [6/50] batch [25/25] time 0.152 (0.189) data 0.000 (0.037) loss 5.4219 (4.7220) acc 31.2500 (34.3750) lr 1.9511e-03 eta 0:03:27
epoch [7/50] batch [5/25] time 0.153 (0.378) data 0.000 (0.224) loss 5.0625 (4.5766) acc 31.2500 (36.2500) lr 1.9511e-03 eta 0:06:53
epoch [7/50] batch [10/25] time 0.153 (0.266) data 0.000 (0.112) loss 5.2148 (4.5109) acc 18.7500 (36.5625) lr 1.9511e-03 eta 0:04:49
epoch [7/50] batch [15/25] time 0.151 (0.228) data 0.000 (0.075) loss 4.5391 (4.5857) acc 31.2500 (35.0000) lr 1.9511e-03 eta 0:04:07
epoch [7/50] batch [20/25] time 0.152 (0.209) data 0.000 (0.056) loss 4.8203 (4.6365) acc 37.5000 (34.6875) lr 1.9511e-03 eta 0:03:45
epoch [7/50] batch [25/25] time 0.151 (0.197) data 0.000 (0.045) loss 3.9844 (4.6652) acc 53.1250 (34.5000) lr 1.9298e-03 eta 0:03:32
epoch [8/50] batch [5/25] time 0.153 (0.346) data 0.000 (0.192) loss 4.7969 (4.6867) acc 40.6250 (38.7500) lr 1.9298e-03 eta 0:06:09
epoch [8/50] batch [10/25] time 0.153 (0.249) data 0.000 (0.096) loss 4.7031 (4.6852) acc 40.6250 (36.8750) lr 1.9298e-03 eta 0:04:25
epoch [8/50] batch [15/25] time 0.152 (0.217) data 0.000 (0.064) loss 4.0703 (4.6323) acc 43.7500 (34.7917) lr 1.9298e-03 eta 0:03:49
epoch [8/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 5.1211 (4.6420) acc 28.1250 (35.0000) lr 1.9298e-03 eta 0:03:31
epoch [8/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.039) loss 4.2812 (4.6353) acc 37.5000 (35.0000) lr 1.9048e-03 eta 0:03:20
epoch [9/50] batch [5/25] time 0.153 (0.359) data 0.000 (0.204) loss 4.3828 (4.4664) acc 37.5000 (34.3750) lr 1.9048e-03 eta 0:06:14
epoch [9/50] batch [10/25] time 0.157 (0.257) data 0.000 (0.102) loss 4.3594 (4.4617) acc 40.6250 (36.8750) lr 1.9048e-03 eta 0:04:27
epoch [9/50] batch [15/25] time 0.156 (0.223) data 0.000 (0.068) loss 4.6953 (4.4082) acc 31.2500 (37.5000) lr 1.9048e-03 eta 0:03:50
epoch [9/50] batch [20/25] time 0.153 (0.205) data 0.001 (0.051) loss 4.5430 (4.5063) acc 40.6250 (36.8750) lr 1.9048e-03 eta 0:03:31
epoch [9/50] batch [25/25] time 0.151 (0.195) data 0.000 (0.041) loss 4.7539 (4.5579) acc 34.3750 (35.8750) lr 1.8763e-03 eta 0:03:19
epoch [10/50] batch [5/25] time 0.154 (0.334) data 0.000 (0.180) loss 4.4648 (4.6797) acc 31.2500 (28.7500) lr 1.8763e-03 eta 0:05:40
epoch [10/50] batch [10/25] time 0.156 (0.244) data 0.000 (0.090) loss 4.4922 (4.5027) acc 34.3750 (35.0000) lr 1.8763e-03 eta 0:04:07
epoch [10/50] batch [15/25] time 0.152 (0.213) data 0.000 (0.060) loss 4.7695 (4.5195) acc 28.1250 (34.3750) lr 1.8763e-03 eta 0:03:35
epoch [10/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.045) loss 4.9531 (4.5438) acc 37.5000 (35.4688) lr 1.8763e-03 eta 0:03:19
epoch [10/50] batch [25/25] time 0.153 (0.189) data 0.000 (0.036) loss 5.2188 (4.5391) acc 31.2500 (36.8750) lr 1.8443e-03 eta 0:03:09
epoch [11/50] batch [5/25] time 0.154 (0.359) data 0.000 (0.204) loss 3.8164 (4.3234) acc 53.1250 (43.7500) lr 1.8443e-03 eta 0:05:56
epoch [11/50] batch [10/25] time 0.154 (0.256) data 0.000 (0.102) loss 3.9336 (4.2465) acc 40.6250 (43.7500) lr 1.8443e-03 eta 0:04:13
epoch [11/50] batch [15/25] time 0.152 (0.222) data 0.000 (0.068) loss 4.2695 (4.3362) acc 46.8750 (42.0833) lr 1.8443e-03 eta 0:03:38
epoch [11/50] batch [20/25] time 0.152 (0.204) data 0.000 (0.051) loss 4.8477 (4.3719) acc 40.6250 (40.7812) lr 1.8443e-03 eta 0:03:20
epoch [11/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.041) loss 5.0234 (4.4418) acc 18.7500 (38.5000) lr 1.8090e-03 eta 0:03:08
epoch [12/50] batch [5/25] time 0.154 (0.360) data 0.000 (0.206) loss 4.2891 (4.3531) acc 46.8750 (39.3750) lr 1.8090e-03 eta 0:05:49
epoch [12/50] batch [10/25] time 0.153 (0.257) data 0.000 (0.103) loss 4.1914 (4.4672) acc 43.7500 (40.6250) lr 1.8090e-03 eta 0:04:08
epoch [12/50] batch [15/25] time 0.153 (0.222) data 0.000 (0.069) loss 4.8281 (4.5064) acc 21.8750 (37.7083) lr 1.8090e-03 eta 0:03:33
epoch [12/50] batch [20/25] time 0.153 (0.205) data 0.000 (0.052) loss 3.8809 (4.4666) acc 40.6250 (38.4375) lr 1.8090e-03 eta 0:03:15
epoch [12/50] batch [25/25] time 0.151 (0.194) data 0.000 (0.041) loss 4.4766 (4.4553) acc 34.3750 (38.0000) lr 1.7705e-03 eta 0:03:04
epoch [13/50] batch [5/25] time 0.155 (0.341) data 0.000 (0.187) loss 4.3047 (4.4680) acc 31.2500 (40.6250) lr 1.7705e-03 eta 0:05:22
epoch [13/50] batch [10/25] time 0.154 (0.248) data 0.000 (0.094) loss 4.9062 (4.5352) acc 37.5000 (41.5625) lr 1.7705e-03 eta 0:03:52
epoch [13/50] batch [15/25] time 0.152 (0.217) data 0.000 (0.063) loss 4.2773 (4.4880) acc 34.3750 (40.2083) lr 1.7705e-03 eta 0:03:22
epoch [13/50] batch [20/25] time 0.153 (0.201) data 0.000 (0.047) loss 4.0234 (4.4807) acc 50.0000 (38.9062) lr 1.7705e-03 eta 0:03:06
epoch [13/50] batch [25/25] time 0.153 (0.191) data 0.000 (0.038) loss 4.1289 (4.4847) acc 46.8750 (39.2500) lr 1.7290e-03 eta 0:02:56
epoch [14/50] batch [5/25] time 0.155 (0.355) data 0.000 (0.201) loss 4.2969 (4.3984) acc 34.3750 (38.1250) lr 1.7290e-03 eta 0:05:27
epoch [14/50] batch [10/25] time 0.154 (0.255) data 0.000 (0.101) loss 4.4531 (4.4215) acc 31.2500 (37.1875) lr 1.7290e-03 eta 0:03:53
epoch [14/50] batch [15/25] time 0.153 (0.221) data 0.000 (0.067) loss 3.9453 (4.4029) acc 40.6250 (36.8750) lr 1.7290e-03 eta 0:03:21
epoch [14/50] batch [20/25] time 0.152 (0.204) data 0.000 (0.050) loss 3.9375 (4.3621) acc 46.8750 (37.5000) lr 1.7290e-03 eta 0:03:04
epoch [14/50] batch [25/25] time 0.152 (0.194) data 0.000 (0.040) loss 4.7344 (4.3925) acc 31.2500 (37.3750) lr 1.6845e-03 eta 0:02:54
epoch [15/50] batch [5/25] time 0.153 (0.346) data 0.000 (0.192) loss 4.5312 (4.3320) acc 37.5000 (41.2500) lr 1.6845e-03 eta 0:05:09
epoch [15/50] batch [10/25] time 0.154 (0.250) data 0.000 (0.096) loss 4.1992 (4.3309) acc 43.7500 (44.6875) lr 1.6845e-03 eta 0:03:42
epoch [15/50] batch [15/25] time 0.153 (0.218) data 0.000 (0.064) loss 4.1484 (4.3786) acc 40.6250 (41.6667) lr 1.6845e-03 eta 0:03:12
epoch [15/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 4.4727 (4.3738) acc 43.7500 (41.4062) lr 1.6845e-03 eta 0:02:57
epoch [15/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.039) loss 4.1289 (4.3703) acc 50.0000 (42.5000) lr 1.6374e-03 eta 0:02:47
epoch [16/50] batch [5/25] time 0.154 (0.346) data 0.000 (0.193) loss 4.6875 (4.2578) acc 34.3750 (36.8750) lr 1.6374e-03 eta 0:05:01
epoch [16/50] batch [10/25] time 0.154 (0.250) data 0.000 (0.097) loss 4.3828 (4.2707) acc 37.5000 (37.8125) lr 1.6374e-03 eta 0:03:36
epoch [16/50] batch [15/25] time 0.153 (0.217) data 0.000 (0.064) loss 4.3906 (4.3607) acc 43.7500 (38.7500) lr 1.6374e-03 eta 0:03:07
epoch [16/50] batch [20/25] time 0.153 (0.201) data 0.000 (0.048) loss 4.5117 (4.3693) acc 40.6250 (39.2188) lr 1.6374e-03 eta 0:02:52
epoch [16/50] batch [25/25] time 0.152 (0.191) data 0.000 (0.039) loss 4.6953 (4.3965) acc 25.0000 (38.3750) lr 1.5878e-03 eta 0:02:42
epoch [17/50] batch [5/25] time 0.154 (0.377) data 0.000 (0.223) loss 4.4062 (4.4398) acc 34.3750 (41.2500) lr 1.5878e-03 eta 0:05:18
epoch [17/50] batch [10/25] time 0.154 (0.266) data 0.000 (0.111) loss 4.2305 (4.2359) acc 56.2500 (45.6250) lr 1.5878e-03 eta 0:03:43
epoch [17/50] batch [15/25] time 0.153 (0.228) data 0.000 (0.074) loss 4.9453 (4.2026) acc 31.2500 (46.4583) lr 1.5878e-03 eta 0:03:10
epoch [17/50] batch [20/25] time 0.152 (0.210) data 0.000 (0.056) loss 4.5625 (4.2971) acc 21.8750 (42.9688) lr 1.5878e-03 eta 0:02:53
epoch [17/50] batch [25/25] time 0.154 (0.198) data 0.000 (0.045) loss 4.5352 (4.2839) acc 34.3750 (43.1250) lr 1.5358e-03 eta 0:02:43
epoch [18/50] batch [5/25] time 0.153 (0.337) data 0.000 (0.184) loss 4.6953 (4.2367) acc 37.5000 (46.2500) lr 1.5358e-03 eta 0:04:36
epoch [18/50] batch [10/25] time 0.154 (0.246) data 0.000 (0.092) loss 4.1680 (4.2441) acc 53.1250 (46.5625) lr 1.5358e-03 eta 0:03:20
epoch [18/50] batch [15/25] time 0.152 (0.215) data 0.000 (0.061) loss 4.1797 (4.2013) acc 43.7500 (45.8333) lr 1.5358e-03 eta 0:02:53
epoch [18/50] batch [20/25] time 0.152 (0.199) data 0.000 (0.046) loss 4.0391 (4.1703) acc 53.1250 (45.4688) lr 1.5358e-03 eta 0:02:40
epoch [18/50] batch [25/25] time 0.152 (0.190) data 0.000 (0.037) loss 4.6953 (4.2145) acc 31.2500 (44.8750) lr 1.4818e-03 eta 0:02:31
epoch [19/50] batch [5/25] time 0.154 (0.336) data 0.000 (0.181) loss 4.2656 (4.4477) acc 40.6250 (40.6250) lr 1.4818e-03 eta 0:04:27
epoch [19/50] batch [10/25] time 0.154 (0.246) data 0.000 (0.091) loss 4.0742 (4.3824) acc 40.6250 (40.6250) lr 1.4818e-03 eta 0:03:14
epoch [19/50] batch [15/25] time 0.153 (0.215) data 0.000 (0.061) loss 4.2930 (4.3145) acc 56.2500 (43.3333) lr 1.4818e-03 eta 0:02:48
epoch [19/50] batch [20/25] time 0.153 (0.200) data 0.000 (0.046) loss 4.5078 (4.2657) acc 43.7500 (42.9688) lr 1.4818e-03 eta 0:02:35
epoch [19/50] batch [25/25] time 0.154 (0.191) data 0.000 (0.036) loss 4.2422 (4.2998) acc 40.6250 (41.3750) lr 1.4258e-03 eta 0:02:27
epoch [20/50] batch [5/25] time 0.154 (0.358) data 0.000 (0.204) loss 4.5156 (4.5945) acc 28.1250 (34.3750) lr 1.4258e-03 eta 0:04:35
epoch [20/50] batch [10/25] time 0.154 (0.257) data 0.000 (0.102) loss 4.4648 (4.4141) acc 40.6250 (38.7500) lr 1.4258e-03 eta 0:03:16
epoch [20/50] batch [15/25] time 0.154 (0.223) data 0.000 (0.068) loss 4.1562 (4.3753) acc 50.0000 (41.4583) lr 1.4258e-03 eta 0:02:49
epoch [20/50] batch [20/25] time 0.153 (0.205) data 0.000 (0.051) loss 4.2109 (4.3385) acc 53.1250 (42.3438) lr 1.4258e-03 eta 0:02:35
epoch [20/50] batch [25/25] time 0.153 (0.195) data 0.000 (0.041) loss 4.5859 (4.3544) acc 37.5000 (42.0000) lr 1.3681e-03 eta 0:02:26
epoch [21/50] batch [5/25] time 0.156 (0.380) data 0.000 (0.225) loss 3.8457 (4.0879) acc 43.7500 (45.6250) lr 1.3681e-03 eta 0:04:43
epoch [21/50] batch [10/25] time 0.154 (0.268) data 0.000 (0.113) loss 3.8809 (4.0824) acc 56.2500 (46.5625) lr 1.3681e-03 eta 0:03:18
epoch [21/50] batch [15/25] time 0.153 (0.230) data 0.000 (0.075) loss 4.2891 (4.1451) acc 50.0000 (47.2917) lr 1.3681e-03 eta 0:02:49
epoch [21/50] batch [20/25] time 0.153 (0.211) data 0.000 (0.057) loss 4.0547 (4.1649) acc 37.5000 (45.3125) lr 1.3681e-03 eta 0:02:33
epoch [21/50] batch [25/25] time 0.153 (0.199) data 0.000 (0.045) loss 3.7598 (4.1612) acc 43.7500 (44.8750) lr 1.3090e-03 eta 0:02:24
epoch [22/50] batch [5/25] time 0.156 (0.378) data 0.000 (0.222) loss 4.0156 (3.9984) acc 59.3750 (47.5000) lr 1.3090e-03 eta 0:04:31
epoch [22/50] batch [10/25] time 0.154 (0.266) data 0.000 (0.111) loss 4.4492 (4.1723) acc 34.3750 (43.4375) lr 1.3090e-03 eta 0:03:10
epoch [22/50] batch [15/25] time 0.152 (0.229) data 0.000 (0.074) loss 3.8203 (4.1803) acc 53.1250 (46.0417) lr 1.3090e-03 eta 0:02:42
epoch [22/50] batch [20/25] time 0.153 (0.210) data 0.000 (0.056) loss 4.3125 (4.2113) acc 56.2500 (45.6250) lr 1.3090e-03 eta 0:02:27
epoch [22/50] batch [25/25] time 0.154 (0.198) data 0.000 (0.045) loss 4.2500 (4.2213) acc 40.6250 (44.8750) lr 1.2487e-03 eta 0:02:18
epoch [23/50] batch [5/25] time 0.155 (0.383) data 0.000 (0.228) loss 4.5547 (4.3242) acc 43.7500 (45.0000) lr 1.2487e-03 eta 0:04:26
epoch [23/50] batch [10/25] time 0.154 (0.269) data 0.000 (0.114) loss 4.5391 (4.1975) acc 31.2500 (44.6875) lr 1.2487e-03 eta 0:03:05
epoch [23/50] batch [15/25] time 0.154 (0.231) data 0.000 (0.076) loss 4.1953 (4.2065) acc 37.5000 (43.3333) lr 1.2487e-03 eta 0:02:38
epoch [23/50] batch [20/25] time 0.153 (0.211) data 0.000 (0.057) loss 4.0742 (4.1929) acc 50.0000 (43.5938) lr 1.2487e-03 eta 0:02:23
epoch [23/50] batch [25/25] time 0.153 (0.200) data 0.000 (0.046) loss 3.7578 (4.1846) acc 56.2500 (44.7500) lr 1.1874e-03 eta 0:02:14
epoch [24/50] batch [5/25] time 0.154 (0.355) data 0.000 (0.200) loss 4.0391 (4.3066) acc 43.7500 (40.0000) lr 1.1874e-03 eta 0:03:57
epoch [24/50] batch [10/25] time 0.153 (0.254) data 0.000 (0.100) loss 3.6348 (4.0531) acc 59.3750 (47.1875) lr 1.1874e-03 eta 0:02:49
epoch [24/50] batch [15/25] time 0.152 (0.221) data 0.000 (0.067) loss 3.9336 (4.1180) acc 46.8750 (44.1667) lr 1.1874e-03 eta 0:02:25
epoch [24/50] batch [20/25] time 0.153 (0.203) data 0.000 (0.050) loss 4.8125 (4.1512) acc 34.3750 (43.5938) lr 1.1874e-03 eta 0:02:13
epoch [24/50] batch [25/25] time 0.154 (0.193) data 0.000 (0.040) loss 4.5352 (4.1062) acc 46.8750 (45.2500) lr 1.1253e-03 eta 0:02:05
epoch [25/50] batch [5/25] time 0.156 (0.387) data 0.000 (0.232) loss 3.7031 (3.8805) acc 53.1250 (45.6250) lr 1.1253e-03 eta 0:04:09
epoch [25/50] batch [10/25] time 0.154 (0.271) data 0.000 (0.116) loss 4.1562 (4.0043) acc 50.0000 (46.2500) lr 1.1253e-03 eta 0:02:53
epoch [25/50] batch [15/25] time 0.154 (0.232) data 0.000 (0.078) loss 3.7383 (3.9810) acc 68.7500 (48.1250) lr 1.1253e-03 eta 0:02:27
epoch [25/50] batch [20/25] time 0.152 (0.212) data 0.000 (0.058) loss 3.8242 (3.9832) acc 50.0000 (47.6562) lr 1.1253e-03 eta 0:02:13
epoch [25/50] batch [25/25] time 0.154 (0.200) data 0.000 (0.047) loss 4.2031 (4.0213) acc 43.7500 (47.0000) lr 1.0628e-03 eta 0:02:05
epoch [26/50] batch [5/25] time 0.154 (0.372) data 0.000 (0.217) loss 4.2266 (3.6941) acc 40.6250 (55.0000) lr 1.0628e-03 eta 0:03:50
epoch [26/50] batch [10/25] time 0.154 (0.263) data 0.000 (0.109) loss 3.6484 (3.7639) acc 46.8750 (53.1250) lr 1.0628e-03 eta 0:02:41
epoch [26/50] batch [15/25] time 0.152 (0.226) data 0.000 (0.073) loss 4.9766 (3.8855) acc 18.7500 (51.6667) lr 1.0628e-03 eta 0:02:18
epoch [26/50] batch [20/25] time 0.153 (0.208) data 0.000 (0.054) loss 3.9531 (3.9360) acc 56.2500 (50.9375) lr 1.0628e-03 eta 0:02:05
epoch [26/50] batch [25/25] time 0.153 (0.197) data 0.000 (0.044) loss 4.2734 (4.0320) acc 40.6250 (48.7500) lr 1.0000e-03 eta 0:01:58
epoch [27/50] batch [5/25] time 0.154 (0.335) data 0.000 (0.179) loss 4.3047 (4.0266) acc 50.0000 (45.6250) lr 1.0000e-03 eta 0:03:19
epoch [27/50] batch [10/25] time 0.155 (0.245) data 0.000 (0.090) loss 4.4922 (4.0857) acc 40.6250 (45.6250) lr 1.0000e-03 eta 0:02:24
epoch [27/50] batch [15/25] time 0.152 (0.214) data 0.000 (0.060) loss 4.4727 (4.1361) acc 34.3750 (42.9167) lr 1.0000e-03 eta 0:02:05
epoch [27/50] batch [20/25] time 0.154 (0.199) data 0.000 (0.045) loss 3.5664 (4.1545) acc 62.5000 (43.1250) lr 1.0000e-03 eta 0:01:55
epoch [27/50] batch [25/25] time 0.154 (0.190) data 0.000 (0.036) loss 3.9531 (4.1569) acc 62.5000 (44.1250) lr 9.3721e-04 eta 0:01:48
epoch [28/50] batch [5/25] time 0.155 (0.334) data 0.000 (0.179) loss 3.9531 (4.2781) acc 56.2500 (49.3750) lr 9.3721e-04 eta 0:03:10
epoch [28/50] batch [10/25] time 0.154 (0.244) data 0.000 (0.090) loss 4.3047 (4.2203) acc 43.7500 (46.5625) lr 9.3721e-04 eta 0:02:17
epoch [28/50] batch [15/25] time 0.154 (0.214) data 0.000 (0.060) loss 4.1055 (4.1775) acc 59.3750 (46.6667) lr 9.3721e-04 eta 0:01:59
epoch [28/50] batch [20/25] time 0.153 (0.199) data 0.000 (0.045) loss 3.6465 (4.1570) acc 59.3750 (46.7188) lr 9.3721e-04 eta 0:01:50
epoch [28/50] batch [25/25] time 0.153 (0.189) data 0.000 (0.036) loss 3.7559 (4.0687) acc 65.6250 (49.1250) lr 8.7467e-04 eta 0:01:44
epoch [29/50] batch [5/25] time 0.155 (0.340) data 0.000 (0.183) loss 3.6055 (3.7801) acc 40.6250 (51.8750) lr 8.7467e-04 eta 0:03:05
epoch [29/50] batch [10/25] time 0.154 (0.247) data 0.000 (0.092) loss 3.9355 (3.8230) acc 40.6250 (51.2500) lr 8.7467e-04 eta 0:02:13
epoch [29/50] batch [15/25] time 0.152 (0.216) data 0.000 (0.061) loss 4.1172 (3.9111) acc 46.8750 (50.6250) lr 8.7467e-04 eta 0:01:55
epoch [29/50] batch [20/25] time 0.152 (0.200) data 0.000 (0.046) loss 3.9922 (3.9737) acc 53.1250 (49.3750) lr 8.7467e-04 eta 0:01:45
epoch [29/50] batch [25/25] time 0.152 (0.190) data 0.000 (0.037) loss 3.8789 (4.0030) acc 40.6250 (48.8750) lr 8.1262e-04 eta 0:01:39
epoch [30/50] batch [5/25] time 0.154 (0.333) data 0.000 (0.177) loss 3.9824 (3.8668) acc 46.8750 (53.7500) lr 8.1262e-04 eta 0:02:53
epoch [30/50] batch [10/25] time 0.155 (0.244) data 0.000 (0.089) loss 4.0625 (3.9932) acc 56.2500 (50.9375) lr 8.1262e-04 eta 0:02:05
epoch [30/50] batch [15/25] time 0.152 (0.213) data 0.000 (0.059) loss 4.3281 (4.0668) acc 40.6250 (47.7083) lr 8.1262e-04 eta 0:01:48
epoch [30/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.044) loss 3.9492 (4.0422) acc 53.1250 (49.2188) lr 8.1262e-04 eta 0:01:40
epoch [30/50] batch [25/25] time 0.153 (0.189) data 0.000 (0.036) loss 3.5820 (4.0298) acc 50.0000 (48.8750) lr 7.5131e-04 eta 0:01:34
epoch [31/50] batch [5/25] time 0.154 (0.348) data 0.000 (0.192) loss 4.0352 (4.0133) acc 59.3750 (46.8750) lr 7.5131e-04 eta 0:02:52
epoch [31/50] batch [10/25] time 0.154 (0.251) data 0.000 (0.096) loss 4.1562 (4.0467) acc 46.8750 (49.3750) lr 7.5131e-04 eta 0:02:03
epoch [31/50] batch [15/25] time 0.153 (0.219) data 0.000 (0.064) loss 3.7109 (4.0659) acc 65.6250 (49.3750) lr 7.5131e-04 eta 0:01:46
epoch [31/50] batch [20/25] time 0.152 (0.202) data 0.000 (0.048) loss 4.1484 (4.0212) acc 53.1250 (49.3750) lr 7.5131e-04 eta 0:01:37
epoch [31/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.039) loss 4.4414 (4.0449) acc 37.5000 (48.8750) lr 6.9098e-04 eta 0:01:31
epoch [32/50] batch [5/25] time 0.154 (0.336) data 0.000 (0.181) loss 3.7832 (4.0277) acc 46.8750 (42.5000) lr 6.9098e-04 eta 0:02:37
epoch [32/50] batch [10/25] time 0.154 (0.245) data 0.000 (0.091) loss 4.1680 (4.0168) acc 50.0000 (45.3125) lr 6.9098e-04 eta 0:01:53
epoch [32/50] batch [15/25] time 0.155 (0.215) data 0.000 (0.061) loss 4.2461 (4.0591) acc 53.1250 (48.1250) lr 6.9098e-04 eta 0:01:38
epoch [32/50] batch [20/25] time 0.153 (0.199) data 0.000 (0.046) loss 4.0469 (4.0832) acc 46.8750 (47.1875) lr 6.9098e-04 eta 0:01:30
epoch [32/50] batch [25/25] time 0.152 (0.190) data 0.000 (0.036) loss 4.0312 (4.0672) acc 37.5000 (47.0000) lr 6.3188e-04 eta 0:01:25
epoch [33/50] batch [5/25] time 0.155 (0.346) data 0.000 (0.191) loss 3.9648 (4.1586) acc 40.6250 (42.5000) lr 6.3188e-04 eta 0:02:34
epoch [33/50] batch [10/25] time 0.153 (0.250) data 0.000 (0.096) loss 3.6738 (3.9855) acc 65.6250 (48.7500) lr 6.3188e-04 eta 0:01:50
epoch [33/50] batch [15/25] time 0.152 (0.217) data 0.000 (0.064) loss 3.8945 (3.9023) acc 53.1250 (50.0000) lr 6.3188e-04 eta 0:01:34
epoch [33/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 4.5234 (3.9199) acc 56.2500 (50.7812) lr 6.3188e-04 eta 0:01:26
epoch [33/50] batch [25/25] time 0.154 (0.191) data 0.000 (0.038) loss 3.7656 (3.9580) acc 56.2500 (49.6250) lr 5.7422e-04 eta 0:01:21
epoch [34/50] batch [5/25] time 0.154 (0.332) data 0.000 (0.177) loss 3.3008 (3.7555) acc 62.5000 (54.3750) lr 5.7422e-04 eta 0:02:19
epoch [34/50] batch [10/25] time 0.154 (0.243) data 0.000 (0.089) loss 4.0000 (3.9508) acc 56.2500 (53.1250) lr 5.7422e-04 eta 0:01:40
epoch [34/50] batch [15/25] time 0.152 (0.213) data 0.000 (0.059) loss 4.1367 (3.9477) acc 46.8750 (52.0833) lr 5.7422e-04 eta 0:01:27
epoch [34/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.044) loss 4.0391 (3.9308) acc 46.8750 (52.3438) lr 5.7422e-04 eta 0:01:20
epoch [34/50] batch [25/25] time 0.152 (0.188) data 0.000 (0.036) loss 3.9355 (3.8821) acc 56.2500 (53.3750) lr 5.1825e-04 eta 0:01:15
epoch [35/50] batch [5/25] time 0.154 (0.326) data 0.000 (0.171) loss 4.1875 (4.1289) acc 53.1250 (45.6250) lr 5.1825e-04 eta 0:02:08
epoch [35/50] batch [10/25] time 0.158 (0.241) data 0.000 (0.086) loss 4.2656 (4.0738) acc 46.8750 (47.8125) lr 5.1825e-04 eta 0:01:34
epoch [35/50] batch [15/25] time 0.169 (0.213) data 0.000 (0.057) loss 3.4648 (3.9781) acc 62.5000 (50.8333) lr 5.1825e-04 eta 0:01:21
epoch [35/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.043) loss 4.4453 (4.0689) acc 50.0000 (49.5312) lr 5.1825e-04 eta 0:01:15
epoch [35/50] batch [25/25] time 0.152 (0.189) data 0.000 (0.034) loss 3.8223 (4.0026) acc 46.8750 (49.6250) lr 4.6417e-04 eta 0:01:10
epoch [36/50] batch [5/25] time 0.155 (0.347) data 0.000 (0.191) loss 3.8633 (3.9383) acc 50.0000 (48.7500) lr 4.6417e-04 eta 0:02:08
epoch [36/50] batch [10/25] time 0.153 (0.251) data 0.000 (0.096) loss 4.0742 (4.0449) acc 50.0000 (46.8750) lr 4.6417e-04 eta 0:01:31
epoch [36/50] batch [15/25] time 0.152 (0.218) data 0.000 (0.064) loss 3.5820 (3.9837) acc 53.1250 (48.9583) lr 4.6417e-04 eta 0:01:18
epoch [36/50] batch [20/25] time 0.152 (0.201) data 0.000 (0.048) loss 4.2656 (3.9624) acc 37.5000 (49.6875) lr 4.6417e-04 eta 0:01:11
epoch [36/50] batch [25/25] time 0.152 (0.192) data 0.000 (0.038) loss 4.2227 (3.9921) acc 56.2500 (49.8750) lr 4.1221e-04 eta 0:01:07
epoch [37/50] batch [5/25] time 0.155 (0.320) data 0.000 (0.165) loss 3.6367 (4.0750) acc 50.0000 (51.2500) lr 4.1221e-04 eta 0:01:50
epoch [37/50] batch [10/25] time 0.155 (0.237) data 0.000 (0.083) loss 3.7852 (3.9617) acc 53.1250 (51.2500) lr 4.1221e-04 eta 0:01:20
epoch [37/50] batch [15/25] time 0.153 (0.209) data 0.000 (0.055) loss 4.2500 (3.9721) acc 56.2500 (51.4583) lr 4.1221e-04 eta 0:01:10
epoch [37/50] batch [20/25] time 0.153 (0.195) data 0.000 (0.041) loss 3.7500 (3.9183) acc 59.3750 (52.6562) lr 4.1221e-04 eta 0:01:04
epoch [37/50] batch [25/25] time 0.153 (0.187) data 0.000 (0.033) loss 4.0625 (3.9532) acc 53.1250 (51.1250) lr 3.6258e-04 eta 0:01:00
epoch [38/50] batch [5/25] time 0.155 (0.319) data 0.000 (0.164) loss 4.1953 (3.7324) acc 56.2500 (56.8750) lr 3.6258e-04 eta 0:01:42
epoch [38/50] batch [10/25] time 0.153 (0.237) data 0.000 (0.082) loss 3.8320 (3.7928) acc 53.1250 (54.3750) lr 3.6258e-04 eta 0:01:14
epoch [38/50] batch [15/25] time 0.152 (0.209) data 0.000 (0.055) loss 4.1055 (3.9056) acc 40.6250 (50.4167) lr 3.6258e-04 eta 0:01:04
epoch [38/50] batch [20/25] time 0.152 (0.195) data 0.000 (0.041) loss 4.0117 (3.9458) acc 53.1250 (50.1562) lr 3.6258e-04 eta 0:00:59
epoch [38/50] batch [25/25] time 0.152 (0.186) data 0.000 (0.033) loss 3.6914 (3.9283) acc 62.5000 (51.1250) lr 3.1545e-04 eta 0:00:55
epoch [39/50] batch [5/25] time 0.154 (0.319) data 0.000 (0.163) loss 4.2656 (4.0191) acc 50.0000 (52.5000) lr 3.1545e-04 eta 0:01:34
epoch [39/50] batch [10/25] time 0.154 (0.236) data 0.000 (0.081) loss 4.0391 (3.9631) acc 50.0000 (53.7500) lr 3.1545e-04 eta 0:01:08
epoch [39/50] batch [15/25] time 0.154 (0.209) data 0.000 (0.054) loss 3.3867 (3.8970) acc 62.5000 (53.7500) lr 3.1545e-04 eta 0:00:59
epoch [39/50] batch [20/25] time 0.155 (0.195) data 0.000 (0.041) loss 3.9922 (3.9161) acc 53.1250 (54.0625) lr 3.1545e-04 eta 0:00:54
epoch [39/50] batch [25/25] time 0.153 (0.187) data 0.000 (0.033) loss 4.2344 (3.9423) acc 53.1250 (53.1250) lr 2.7103e-04 eta 0:00:51
epoch [40/50] batch [5/25] time 0.156 (0.339) data 0.000 (0.182) loss 3.8477 (4.1469) acc 56.2500 (51.2500) lr 2.7103e-04 eta 0:01:31
epoch [40/50] batch [10/25] time 0.154 (0.246) data 0.000 (0.091) loss 3.7012 (4.0742) acc 46.8750 (48.7500) lr 2.7103e-04 eta 0:01:05
epoch [40/50] batch [15/25] time 0.153 (0.215) data 0.000 (0.061) loss 4.2891 (4.0743) acc 43.7500 (50.4167) lr 2.7103e-04 eta 0:00:56
epoch [40/50] batch [20/25] time 0.153 (0.200) data 0.000 (0.046) loss 3.9922 (3.9771) acc 53.1250 (52.3438) lr 2.7103e-04 eta 0:00:50
epoch [40/50] batch [25/25] time 0.156 (0.191) data 0.000 (0.037) loss 4.4883 (4.0411) acc 50.0000 (50.8750) lr 2.2949e-04 eta 0:00:47
epoch [41/50] batch [5/25] time 0.152 (0.329) data 0.000 (0.174) loss 3.4648 (3.7133) acc 56.2500 (57.5000) lr 2.2949e-04 eta 0:01:20
epoch [41/50] batch [10/25] time 0.155 (0.242) data 0.000 (0.087) loss 3.7734 (3.8445) acc 46.8750 (53.7500) lr 2.2949e-04 eta 0:00:58
epoch [41/50] batch [15/25] time 0.153 (0.212) data 0.000 (0.058) loss 4.0742 (3.8408) acc 40.6250 (54.3750) lr 2.2949e-04 eta 0:00:49
epoch [41/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.044) loss 3.9863 (3.8379) acc 59.3750 (55.6250) lr 2.2949e-04 eta 0:00:45
epoch [41/50] batch [25/25] time 0.154 (0.189) data 0.000 (0.035) loss 4.5508 (3.8870) acc 34.3750 (53.6250) lr 1.9098e-04 eta 0:00:42
epoch [42/50] batch [5/25] time 0.154 (0.338) data 0.000 (0.183) loss 3.4688 (3.8344) acc 62.5000 (57.5000) lr 1.9098e-04 eta 0:01:14
epoch [42/50] batch [10/25] time 0.155 (0.247) data 0.000 (0.092) loss 3.7773 (3.9230) acc 62.5000 (54.0625) lr 1.9098e-04 eta 0:00:53
epoch [42/50] batch [15/25] time 0.153 (0.215) data 0.000 (0.061) loss 3.4746 (3.8591) acc 59.3750 (55.2083) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [20/25] time 0.153 (0.200) data 0.000 (0.046) loss 4.0977 (3.8904) acc 50.0000 (53.7500) lr 1.9098e-04 eta 0:00:40
epoch [42/50] batch [25/25] time 0.153 (0.190) data 0.000 (0.037) loss 4.0391 (3.9252) acc 43.7500 (52.8750) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [5/25] time 0.158 (0.355) data 0.000 (0.198) loss 4.2227 (3.9172) acc 43.7500 (51.8750) lr 1.5567e-04 eta 0:01:09
epoch [43/50] batch [10/25] time 0.155 (0.255) data 0.000 (0.099) loss 4.2852 (3.9041) acc 43.7500 (51.5625) lr 1.5567e-04 eta 0:00:48
epoch [43/50] batch [15/25] time 0.153 (0.221) data 0.000 (0.066) loss 3.1816 (3.8098) acc 65.6250 (55.0000) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [20/25] time 0.153 (0.204) data 0.000 (0.050) loss 4.4883 (3.8763) acc 53.1250 (54.2188) lr 1.5567e-04 eta 0:00:36
epoch [43/50] batch [25/25] time 0.153 (0.194) data 0.000 (0.040) loss 3.3516 (3.8610) acc 68.7500 (54.5000) lr 1.2369e-04 eta 0:00:33
epoch [44/50] batch [5/25] time 0.154 (0.328) data 0.000 (0.174) loss 3.7188 (3.7121) acc 46.8750 (59.3750) lr 1.2369e-04 eta 0:00:55
epoch [44/50] batch [10/25] time 0.158 (0.242) data 0.000 (0.087) loss 3.8438 (3.8937) acc 56.2500 (56.2500) lr 1.2369e-04 eta 0:00:39
epoch [44/50] batch [15/25] time 0.152 (0.212) data 0.000 (0.058) loss 3.7969 (3.8898) acc 53.1250 (55.0000) lr 1.2369e-04 eta 0:00:33
epoch [44/50] batch [20/25] time 0.152 (0.197) data 0.000 (0.044) loss 4.0391 (3.8900) acc 50.0000 (53.7500) lr 1.2369e-04 eta 0:00:30
epoch [44/50] batch [25/25] time 0.152 (0.188) data 0.000 (0.035) loss 3.8945 (3.8907) acc 59.3750 (53.5000) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [5/25] time 0.155 (0.322) data 0.000 (0.166) loss 3.7148 (3.8203) acc 59.3750 (50.0000) lr 9.5173e-05 eta 0:00:46
epoch [45/50] batch [10/25] time 0.154 (0.239) data 0.000 (0.083) loss 3.6211 (3.8377) acc 53.1250 (53.1250) lr 9.5173e-05 eta 0:00:33
epoch [45/50] batch [15/25] time 0.155 (0.211) data 0.000 (0.056) loss 3.9336 (3.8337) acc 53.1250 (54.3750) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [20/25] time 0.155 (0.197) data 0.000 (0.042) loss 3.7617 (3.8934) acc 53.1250 (52.6562) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/25] time 0.155 (0.188) data 0.000 (0.033) loss 3.5430 (3.8730) acc 62.5000 (52.8750) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [5/25] time 0.155 (0.342) data 0.000 (0.187) loss 4.3516 (3.9348) acc 50.0000 (50.6250) lr 7.0224e-05 eta 0:00:41
epoch [46/50] batch [10/25] time 0.155 (0.249) data 0.000 (0.094) loss 4.6211 (4.0096) acc 46.8750 (52.5000) lr 7.0224e-05 eta 0:00:28
epoch [46/50] batch [15/25] time 0.153 (0.217) data 0.000 (0.062) loss 3.6758 (3.9527) acc 56.2500 (52.5000) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [20/25] time 0.153 (0.201) data 0.000 (0.047) loss 3.8008 (3.9349) acc 56.2500 (53.9062) lr 7.0224e-05 eta 0:00:21
epoch [46/50] batch [25/25] time 0.153 (0.191) data 0.000 (0.038) loss 4.1836 (3.9135) acc 46.8750 (54.3750) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [5/25] time 0.153 (0.327) data 0.000 (0.172) loss 3.8105 (3.9289) acc 56.2500 (52.5000) lr 4.8943e-05 eta 0:00:31
epoch [47/50] batch [10/25] time 0.156 (0.241) data 0.000 (0.086) loss 4.2578 (3.9766) acc 37.5000 (51.5625) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [15/25] time 0.153 (0.212) data 0.000 (0.058) loss 3.9277 (4.0178) acc 53.1250 (51.0417) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/25] time 0.156 (0.198) data 0.000 (0.043) loss 3.8594 (3.9716) acc 46.8750 (52.0312) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/25] time 0.155 (0.189) data 0.000 (0.035) loss 3.9922 (3.9495) acc 40.6250 (52.2500) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [5/25] time 0.154 (0.352) data 0.000 (0.195) loss 4.1445 (3.8719) acc 56.2500 (60.6250) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [10/25] time 0.156 (0.254) data 0.000 (0.098) loss 3.7461 (3.7535) acc 62.5000 (60.0000) lr 3.1417e-05 eta 0:00:16
epoch [48/50] batch [15/25] time 0.153 (0.221) data 0.000 (0.065) loss 4.0820 (3.8273) acc 43.7500 (57.7083) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/25] time 0.152 (0.204) data 0.000 (0.049) loss 3.6094 (3.8219) acc 59.3750 (57.8125) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [25/25] time 0.152 (0.193) data 0.000 (0.039) loss 3.4609 (3.8255) acc 65.6250 (58.3750) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [5/25] time 0.154 (0.332) data 0.000 (0.177) loss 4.2344 (4.0262) acc 56.2500 (53.1250) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [10/25] time 0.154 (0.243) data 0.000 (0.089) loss 3.5391 (3.9314) acc 62.5000 (55.9375) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/25] time 0.152 (0.213) data 0.000 (0.059) loss 3.4492 (3.9188) acc 68.7500 (54.3750) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/25] time 0.152 (0.198) data 0.000 (0.044) loss 3.6055 (3.8906) acc 59.3750 (55.0000) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/25] time 0.152 (0.188) data 0.000 (0.036) loss 3.1816 (3.8543) acc 62.5000 (55.6250) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [5/25] time 0.154 (0.322) data 0.000 (0.168) loss 3.4844 (3.7305) acc 53.1250 (57.5000) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [10/25] time 0.153 (0.238) data 0.000 (0.084) loss 3.6895 (3.8170) acc 59.3750 (54.6875) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/25] time 0.152 (0.209) data 0.000 (0.056) loss 4.1367 (3.8758) acc 53.1250 (53.1250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/25] time 0.152 (0.195) data 0.000 (0.042) loss 4.2617 (3.8726) acc 65.6250 (53.5938) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.152 (0.186) data 0.000 (0.034) loss 3.7344 (3.8494) acc 46.8750 (54.5000) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:30, 10.09s/it] 50%|█████     | 2/4 [00:11<00:09,  4.82s/it] 75%|███████▌  | 3/4 [00:12<00:03,  3.14s/it]100%|██████████| 4/4 [00:12<00:00,  2.07s/it]100%|██████████| 4/4 [00:12<00:00,  3.22s/it]
=> result
* total: 1,666
* correct: 727
* accuracy: 43.6%
* error: 56.4%
* macro_f1: 41.5%
Elapsed: 0:04:18
Run this job and save the output to output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,667
---------  ------------
['Cessna 560', 'Challenger 600', 'DC-10', 'DC-3', 'DC-6', 'DC-8', 'DC-9-30', 'DH-82', 'DHC-1', 'DHC-6', 'DHC-8-100', 'DHC-8-300', 'DR-400', 'Dornier 328', 'E-170', 'E-190', 'E-195', 'EMB-120', 'ERJ 135', 'ERJ 145', 'Embraer Legacy 600', 'Eurofighter Typhoon', 'F-16A/B', 'F/A-18', 'Falcon 2000', 'Falcon 900', 'Fokker 100', 'Fokker 50', 'Fokker 70', 'Global Express', 'Gulfstream IV', 'Gulfstream V', 'Hawk T1', 'Il-76', 'L-1011', 'MD-11', 'MD-80', 'MD-87', 'MD-90', 'Metroliner', 'Model B200', 'PA-28', 'SR-20', 'Saab 2000', 'Saab 340', 'Spitfire', 'Tornado', 'Tu-134', 'Tu-154', 'Yak-42']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Cessna 560, a type of aircraft.', 'X X X X Challenger 600, a type of aircraft.', 'X X X X DC-10, a type of aircraft.', 'X X X X DC-3, a type of aircraft.', 'X X X X DC-6, a type of aircraft.', 'X X X X DC-8, a type of aircraft.', 'X X X X DC-9-30, a type of aircraft.', 'X X X X DH-82, a type of aircraft.', 'X X X X DHC-1, a type of aircraft.', 'X X X X DHC-6, a type of aircraft.', 'X X X X DHC-8-100, a type of aircraft.', 'X X X X DHC-8-300, a type of aircraft.', 'X X X X DR-400, a type of aircraft.', 'X X X X Dornier 328, a type of aircraft.', 'X X X X E-170, a type of aircraft.', 'X X X X E-190, a type of aircraft.', 'X X X X E-195, a type of aircraft.', 'X X X X EMB-120, a type of aircraft.', 'X X X X ERJ 135, a type of aircraft.', 'X X X X ERJ 145, a type of aircraft.', 'X X X X Embraer Legacy 600, a type of aircraft.', 'X X X X Eurofighter Typhoon, a type of aircraft.', 'X X X X F-16A/B, a type of aircraft.', 'X X X X F/A-18, a type of aircraft.', 'X X X X Falcon 2000, a type of aircraft.', 'X X X X Falcon 900, a type of aircraft.', 'X X X X Fokker 100, a type of aircraft.', 'X X X X Fokker 50, a type of aircraft.', 'X X X X Fokker 70, a type of aircraft.', 'X X X X Global Express, a type of aircraft.', 'X X X X Gulfstream IV, a type of aircraft.', 'X X X X Gulfstream V, a type of aircraft.', 'X X X X Hawk T1, a type of aircraft.', 'X X X X Il-76, a type of aircraft.', 'X X X X L-1011, a type of aircraft.', 'X X X X MD-11, a type of aircraft.', 'X X X X MD-80, a type of aircraft.', 'X X X X MD-87, a type of aircraft.', 'X X X X MD-90, a type of aircraft.', 'X X X X Metroliner, a type of aircraft.', 'X X X X Model B200, a type of aircraft.', 'X X X X PA-28, a type of aircraft.', 'X X X X SR-20, a type of aircraft.', 'X X X X Saab 2000, a type of aircraft.', 'X X X X Saab 340, a type of aircraft.', 'X X X X Spitfire, a type of aircraft.', 'X X X X Tornado, a type of aircraft.', 'X X X X Tu-134, a type of aircraft.', 'X X X X Tu-154, a type of aircraft.', 'X X X X Yak-42, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:30, 10.26s/it] 50%|█████     | 2/4 [00:11<00:09,  4.88s/it] 75%|███████▌  | 3/4 [00:12<00:03,  3.16s/it]100%|██████████| 4/4 [00:12<00:00,  2.08s/it]100%|██████████| 4/4 [00:13<00:00,  3.26s/it]
=> result
* total: 1,667
* correct: 589
* accuracy: 35.3%
* error: 64.7%
* macro_f1: 32.8%
Run this job and save the output to output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,667
---------  ------------
['Cessna 560', 'Challenger 600', 'DC-10', 'DC-3', 'DC-6', 'DC-8', 'DC-9-30', 'DH-82', 'DHC-1', 'DHC-6', 'DHC-8-100', 'DHC-8-300', 'DR-400', 'Dornier 328', 'E-170', 'E-190', 'E-195', 'EMB-120', 'ERJ 135', 'ERJ 145', 'Embraer Legacy 600', 'Eurofighter Typhoon', 'F-16A/B', 'F/A-18', 'Falcon 2000', 'Falcon 900', 'Fokker 100', 'Fokker 50', 'Fokker 70', 'Global Express', 'Gulfstream IV', 'Gulfstream V', 'Hawk T1', 'Il-76', 'L-1011', 'MD-11', 'MD-80', 'MD-87', 'MD-90', 'Metroliner', 'Model B200', 'PA-28', 'SR-20', 'Saab 2000', 'Saab 340', 'Spitfire', 'Tornado', 'Tu-134', 'Tu-154', 'Yak-42']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Cessna 560, a type of aircraft.', 'X X X X Challenger 600, a type of aircraft.', 'X X X X DC-10, a type of aircraft.', 'X X X X DC-3, a type of aircraft.', 'X X X X DC-6, a type of aircraft.', 'X X X X DC-8, a type of aircraft.', 'X X X X DC-9-30, a type of aircraft.', 'X X X X DH-82, a type of aircraft.', 'X X X X DHC-1, a type of aircraft.', 'X X X X DHC-6, a type of aircraft.', 'X X X X DHC-8-100, a type of aircraft.', 'X X X X DHC-8-300, a type of aircraft.', 'X X X X DR-400, a type of aircraft.', 'X X X X Dornier 328, a type of aircraft.', 'X X X X E-170, a type of aircraft.', 'X X X X E-190, a type of aircraft.', 'X X X X E-195, a type of aircraft.', 'X X X X EMB-120, a type of aircraft.', 'X X X X ERJ 135, a type of aircraft.', 'X X X X ERJ 145, a type of aircraft.', 'X X X X Embraer Legacy 600, a type of aircraft.', 'X X X X Eurofighter Typhoon, a type of aircraft.', 'X X X X F-16A/B, a type of aircraft.', 'X X X X F/A-18, a type of aircraft.', 'X X X X Falcon 2000, a type of aircraft.', 'X X X X Falcon 900, a type of aircraft.', 'X X X X Fokker 100, a type of aircraft.', 'X X X X Fokker 50, a type of aircraft.', 'X X X X Fokker 70, a type of aircraft.', 'X X X X Global Express, a type of aircraft.', 'X X X X Gulfstream IV, a type of aircraft.', 'X X X X Gulfstream V, a type of aircraft.', 'X X X X Hawk T1, a type of aircraft.', 'X X X X Il-76, a type of aircraft.', 'X X X X L-1011, a type of aircraft.', 'X X X X MD-11, a type of aircraft.', 'X X X X MD-80, a type of aircraft.', 'X X X X MD-87, a type of aircraft.', 'X X X X MD-90, a type of aircraft.', 'X X X X Metroliner, a type of aircraft.', 'X X X X Model B200, a type of aircraft.', 'X X X X PA-28, a type of aircraft.', 'X X X X SR-20, a type of aircraft.', 'X X X X Saab 2000, a type of aircraft.', 'X X X X Saab 340, a type of aircraft.', 'X X X X Spitfire, a type of aircraft.', 'X X X X Tornado, a type of aircraft.', 'X X X X Tu-134, a type of aircraft.', 'X X X X Tu-154, a type of aircraft.', 'X X X X Yak-42, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:30, 10.26s/it] 50%|█████     | 2/4 [00:11<00:09,  4.89s/it] 75%|███████▌  | 3/4 [00:12<00:03,  3.17s/it]100%|██████████| 4/4 [00:12<00:00,  2.09s/it]100%|██████████| 4/4 [00:13<00:00,  3.26s/it]
=> result
* total: 1,667
* correct: 580
* accuracy: 34.8%
* error: 65.2%
* macro_f1: 31.9%
Run this job and save the output to output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/fgvc_aircraft.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: FGVCAircraft
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: FGVCAircraft
Loading preprocessed few-shot data from /data/yht/data/cl/data/fgvc_aircraft/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    FGVCAircraft
# classes  50
# train_x  800
# val      200
# test     1,667
---------  ------------
['Cessna 560', 'Challenger 600', 'DC-10', 'DC-3', 'DC-6', 'DC-8', 'DC-9-30', 'DH-82', 'DHC-1', 'DHC-6', 'DHC-8-100', 'DHC-8-300', 'DR-400', 'Dornier 328', 'E-170', 'E-190', 'E-195', 'EMB-120', 'ERJ 135', 'ERJ 145', 'Embraer Legacy 600', 'Eurofighter Typhoon', 'F-16A/B', 'F/A-18', 'Falcon 2000', 'Falcon 900', 'Fokker 100', 'Fokker 50', 'Fokker 70', 'Global Express', 'Gulfstream IV', 'Gulfstream V', 'Hawk T1', 'Il-76', 'L-1011', 'MD-11', 'MD-80', 'MD-87', 'MD-90', 'Metroliner', 'Model B200', 'PA-28', 'SR-20', 'Saab 2000', 'Saab 340', 'Spitfire', 'Tornado', 'Tu-134', 'Tu-154', 'Yak-42']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X Cessna 560, a type of aircraft.', 'X X X X Challenger 600, a type of aircraft.', 'X X X X DC-10, a type of aircraft.', 'X X X X DC-3, a type of aircraft.', 'X X X X DC-6, a type of aircraft.', 'X X X X DC-8, a type of aircraft.', 'X X X X DC-9-30, a type of aircraft.', 'X X X X DH-82, a type of aircraft.', 'X X X X DHC-1, a type of aircraft.', 'X X X X DHC-6, a type of aircraft.', 'X X X X DHC-8-100, a type of aircraft.', 'X X X X DHC-8-300, a type of aircraft.', 'X X X X DR-400, a type of aircraft.', 'X X X X Dornier 328, a type of aircraft.', 'X X X X E-170, a type of aircraft.', 'X X X X E-190, a type of aircraft.', 'X X X X E-195, a type of aircraft.', 'X X X X EMB-120, a type of aircraft.', 'X X X X ERJ 135, a type of aircraft.', 'X X X X ERJ 145, a type of aircraft.', 'X X X X Embraer Legacy 600, a type of aircraft.', 'X X X X Eurofighter Typhoon, a type of aircraft.', 'X X X X F-16A/B, a type of aircraft.', 'X X X X F/A-18, a type of aircraft.', 'X X X X Falcon 2000, a type of aircraft.', 'X X X X Falcon 900, a type of aircraft.', 'X X X X Fokker 100, a type of aircraft.', 'X X X X Fokker 50, a type of aircraft.', 'X X X X Fokker 70, a type of aircraft.', 'X X X X Global Express, a type of aircraft.', 'X X X X Gulfstream IV, a type of aircraft.', 'X X X X Gulfstream V, a type of aircraft.', 'X X X X Hawk T1, a type of aircraft.', 'X X X X Il-76, a type of aircraft.', 'X X X X L-1011, a type of aircraft.', 'X X X X MD-11, a type of aircraft.', 'X X X X MD-80, a type of aircraft.', 'X X X X MD-87, a type of aircraft.', 'X X X X MD-90, a type of aircraft.', 'X X X X Metroliner, a type of aircraft.', 'X X X X Model B200, a type of aircraft.', 'X X X X PA-28, a type of aircraft.', 'X X X X SR-20, a type of aircraft.', 'X X X X Saab 2000, a type of aircraft.', 'X X X X Saab 340, a type of aircraft.', 'X X X X Spitfire, a type of aircraft.', 'X X X X Tornado, a type of aircraft.', 'X X X X Tu-134, a type of aircraft.', 'X X X X Tu-154, a type of aircraft.', 'X X X X Yak-42, a type of aircraft.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/fgvc_aircraft/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:10<00:31, 10.37s/it] 50%|█████     | 2/4 [00:11<00:09,  4.93s/it] 75%|███████▌  | 3/4 [00:12<00:03,  3.19s/it]100%|██████████| 4/4 [00:13<00:00,  2.10s/it]100%|██████████| 4/4 [00:13<00:00,  3.29s/it]
=> result
* total: 1,667
* correct: 627
* accuracy: 37.6%
* error: 62.4%
* macro_f1: 33.9%
Run this job and save the output to output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  51
# train_x  816
# val      204
# test     15,300
---------  -------
['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X apple pie, a type of food.', 'X X X X baby back ribs, a type of food.', 'X X X X baklava, a type of food.', 'X X X X beef carpaccio, a type of food.', 'X X X X beef tartare, a type of food.', 'X X X X beet salad, a type of food.', 'X X X X beignets, a type of food.', 'X X X X bibimbap, a type of food.', 'X X X X bread pudding, a type of food.', 'X X X X breakfast burrito, a type of food.', 'X X X X bruschetta, a type of food.', 'X X X X caesar salad, a type of food.', 'X X X X cannoli, a type of food.', 'X X X X caprese salad, a type of food.', 'X X X X carrot cake, a type of food.', 'X X X X ceviche, a type of food.', 'X X X X cheese plate, a type of food.', 'X X X X cheesecake, a type of food.', 'X X X X chicken curry, a type of food.', 'X X X X chicken quesadilla, a type of food.', 'X X X X chicken wings, a type of food.', 'X X X X chocolate cake, a type of food.', 'X X X X chocolate mousse, a type of food.', 'X X X X churros, a type of food.', 'X X X X clam chowder, a type of food.', 'X X X X club sandwich, a type of food.', 'X X X X crab cakes, a type of food.', 'X X X X creme brulee, a type of food.', 'X X X X croque madame, a type of food.', 'X X X X cup cakes, a type of food.', 'X X X X deviled eggs, a type of food.', 'X X X X donuts, a type of food.', 'X X X X dumplings, a type of food.', 'X X X X edamame, a type of food.', 'X X X X eggs benedict, a type of food.', 'X X X X escargots, a type of food.', 'X X X X falafel, a type of food.', 'X X X X filet mignon, a type of food.', 'X X X X fish and chips, a type of food.', 'X X X X foie gras, a type of food.', 'X X X X french fries, a type of food.', 'X X X X french onion soup, a type of food.', 'X X X X french toast, a type of food.', 'X X X X fried calamari, a type of food.', 'X X X X fried rice, a type of food.', 'X X X X frozen yogurt, a type of food.', 'X X X X garlic bread, a type of food.', 'X X X X gnocchi, a type of food.', 'X X X X greek salad, a type of food.', 'X X X X grilled cheese sandwich, a type of food.', 'X X X X grilled salmon, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/26] time 0.154 (0.319) data 0.000 (0.153) loss 5.6133 (5.3070) acc 65.6250 (75.0000) lr 1.0000e-05 eta 0:06:53
epoch [1/50] batch [10/26] time 0.153 (0.237) data 0.001 (0.077) loss 4.7852 (5.1934) acc 81.2500 (75.6250) lr 1.0000e-05 eta 0:05:05
epoch [1/50] batch [15/26] time 0.153 (0.209) data 0.000 (0.051) loss 5.1836 (5.1526) acc 78.1250 (75.4167) lr 1.0000e-05 eta 0:04:28
epoch [1/50] batch [20/26] time 0.154 (0.195) data 0.000 (0.038) loss 5.4922 (5.1471) acc 62.5000 (75.1562) lr 1.0000e-05 eta 0:04:09
epoch [1/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.031) loss 4.8047 (5.0984) acc 75.0000 (76.2500) lr 1.0000e-05 eta 0:03:57
epoch [2/50] batch [5/26] time 0.154 (0.305) data 0.000 (0.148) loss 3.6328 (3.9145) acc 84.3750 (87.5000) lr 2.0000e-03 eta 0:06:26
epoch [2/50] batch [10/26] time 0.155 (0.230) data 0.000 (0.074) loss 3.5410 (3.7252) acc 71.8750 (84.6875) lr 2.0000e-03 eta 0:04:50
epoch [2/50] batch [15/26] time 0.154 (0.205) data 0.000 (0.050) loss 3.1055 (3.6000) acc 90.6250 (83.5417) lr 2.0000e-03 eta 0:04:17
epoch [2/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.037) loss 3.8438 (3.4836) acc 68.7500 (83.9062) lr 2.0000e-03 eta 0:04:00
epoch [2/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.030) loss 3.0957 (3.4127) acc 84.3750 (83.3750) lr 2.0000e-03 eta 0:03:49
epoch [3/50] batch [5/26] time 0.155 (0.311) data 0.000 (0.155) loss 2.2246 (2.8715) acc 87.5000 (81.8750) lr 1.9980e-03 eta 0:06:25
epoch [3/50] batch [10/26] time 0.154 (0.232) data 0.000 (0.078) loss 2.6035 (2.8664) acc 87.5000 (82.5000) lr 1.9980e-03 eta 0:04:47
epoch [3/50] batch [15/26] time 0.152 (0.206) data 0.000 (0.052) loss 2.9180 (2.8560) acc 78.1250 (82.0833) lr 1.9980e-03 eta 0:04:13
epoch [3/50] batch [20/26] time 0.152 (0.192) data 0.000 (0.039) loss 2.9219 (2.8546) acc 78.1250 (81.5625) lr 1.9980e-03 eta 0:03:56
epoch [3/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.031) loss 2.5957 (2.8254) acc 90.6250 (82.5000) lr 1.9980e-03 eta 0:03:45
epoch [4/50] batch [5/26] time 0.154 (0.307) data 0.000 (0.152) loss 2.2812 (2.6195) acc 81.2500 (79.3750) lr 1.9921e-03 eta 0:06:13
epoch [4/50] batch [10/26] time 0.154 (0.231) data 0.000 (0.076) loss 2.0137 (2.5494) acc 93.7500 (82.8125) lr 1.9921e-03 eta 0:04:39
epoch [4/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.051) loss 2.8027 (2.5571) acc 78.1250 (83.3333) lr 1.9921e-03 eta 0:04:07
epoch [4/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.038) loss 2.3711 (2.5044) acc 84.3750 (84.3750) lr 1.9921e-03 eta 0:03:51
epoch [4/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.031) loss 3.2910 (2.5059) acc 71.8750 (83.6250) lr 1.9921e-03 eta 0:03:40
epoch [5/50] batch [5/26] time 0.156 (0.298) data 0.000 (0.142) loss 2.2148 (2.1896) acc 90.6250 (85.6250) lr 1.9823e-03 eta 0:05:55
epoch [5/50] batch [10/26] time 0.155 (0.226) data 0.000 (0.071) loss 2.5195 (2.2899) acc 81.2500 (85.9375) lr 1.9823e-03 eta 0:04:28
epoch [5/50] batch [15/26] time 0.152 (0.202) data 0.000 (0.048) loss 2.9707 (2.3313) acc 71.8750 (83.9583) lr 1.9823e-03 eta 0:03:58
epoch [5/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.036) loss 2.8164 (2.3879) acc 81.2500 (84.0625) lr 1.9823e-03 eta 0:03:42
epoch [5/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.029) loss 2.2852 (2.3989) acc 87.5000 (83.3750) lr 1.9823e-03 eta 0:03:33
epoch [6/50] batch [5/26] time 0.157 (0.297) data 0.000 (0.141) loss 2.5742 (2.3490) acc 81.2500 (83.1250) lr 1.9686e-03 eta 0:05:45
epoch [6/50] batch [10/26] time 0.153 (0.225) data 0.000 (0.071) loss 2.4648 (2.2931) acc 84.3750 (84.0625) lr 1.9686e-03 eta 0:04:20
epoch [6/50] batch [15/26] time 0.152 (0.201) data 0.000 (0.047) loss 2.7480 (2.3025) acc 78.1250 (84.5833) lr 1.9686e-03 eta 0:03:51
epoch [6/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.035) loss 2.3105 (2.3156) acc 87.5000 (84.2188) lr 1.9686e-03 eta 0:03:36
epoch [6/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.028) loss 2.2930 (2.3069) acc 87.5000 (84.5000) lr 1.9686e-03 eta 0:03:27
epoch [7/50] batch [5/26] time 0.153 (0.296) data 0.000 (0.140) loss 2.8945 (2.4750) acc 71.8750 (83.1250) lr 1.9511e-03 eta 0:05:37
epoch [7/50] batch [10/26] time 0.153 (0.225) data 0.000 (0.070) loss 2.1836 (2.3559) acc 84.3750 (83.4375) lr 1.9511e-03 eta 0:04:15
epoch [7/50] batch [15/26] time 0.152 (0.201) data 0.000 (0.047) loss 2.3633 (2.2876) acc 81.2500 (83.5417) lr 1.9511e-03 eta 0:03:46
epoch [7/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.035) loss 2.2012 (2.2727) acc 90.6250 (84.0625) lr 1.9511e-03 eta 0:03:32
epoch [7/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.028) loss 2.6641 (2.3107) acc 71.8750 (83.3750) lr 1.9511e-03 eta 0:03:23
epoch [8/50] batch [5/26] time 0.152 (0.314) data 0.000 (0.160) loss 1.8516 (2.3762) acc 87.5000 (78.7500) lr 1.9298e-03 eta 0:05:48
epoch [8/50] batch [10/26] time 0.153 (0.234) data 0.000 (0.080) loss 2.2988 (2.2539) acc 78.1250 (81.8750) lr 1.9298e-03 eta 0:04:19
epoch [8/50] batch [15/26] time 0.152 (0.207) data 0.000 (0.053) loss 2.1641 (2.2240) acc 84.3750 (83.5417) lr 1.9298e-03 eta 0:03:47
epoch [8/50] batch [20/26] time 0.151 (0.193) data 0.000 (0.040) loss 2.0059 (2.2669) acc 87.5000 (82.5000) lr 1.9298e-03 eta 0:03:31
epoch [8/50] batch [25/26] time 0.154 (0.185) data 0.000 (0.032) loss 2.0430 (2.2687) acc 78.1250 (82.5000) lr 1.9298e-03 eta 0:03:22
epoch [9/50] batch [5/26] time 0.152 (0.303) data 0.000 (0.150) loss 1.9785 (1.9752) acc 87.5000 (85.0000) lr 1.9048e-03 eta 0:05:29
epoch [9/50] batch [10/26] time 0.155 (0.229) data 0.000 (0.075) loss 1.9785 (2.0507) acc 90.6250 (86.2500) lr 1.9048e-03 eta 0:04:07
epoch [9/50] batch [15/26] time 0.152 (0.204) data 0.000 (0.050) loss 2.3867 (2.0880) acc 75.0000 (84.7917) lr 1.9048e-03 eta 0:03:39
epoch [9/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.038) loss 2.2461 (2.1521) acc 90.6250 (84.8438) lr 1.9048e-03 eta 0:03:24
epoch [9/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.2246 (2.1473) acc 84.3750 (85.3750) lr 1.9048e-03 eta 0:03:15
epoch [10/50] batch [5/26] time 0.157 (0.302) data 0.000 (0.147) loss 1.7930 (2.4434) acc 87.5000 (80.6250) lr 1.8763e-03 eta 0:05:19
epoch [10/50] batch [10/26] time 0.156 (0.229) data 0.000 (0.074) loss 1.9951 (2.2027) acc 87.5000 (85.0000) lr 1.8763e-03 eta 0:04:01
epoch [10/50] batch [15/26] time 0.152 (0.203) data 0.000 (0.049) loss 2.0312 (2.1898) acc 81.2500 (84.3750) lr 1.8763e-03 eta 0:03:33
epoch [10/50] batch [20/26] time 0.152 (0.191) data 0.000 (0.037) loss 1.9160 (2.1560) acc 93.7500 (85.4688) lr 1.8763e-03 eta 0:03:19
epoch [10/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.0234 (2.1637) acc 90.6250 (85.5000) lr 1.8763e-03 eta 0:03:10
epoch [11/50] batch [5/26] time 0.156 (0.306) data 0.004 (0.152) loss 2.6035 (2.3773) acc 87.5000 (84.3750) lr 1.8443e-03 eta 0:05:16
epoch [11/50] batch [10/26] time 0.153 (0.230) data 0.000 (0.076) loss 2.4941 (2.2721) acc 78.1250 (84.3750) lr 1.8443e-03 eta 0:03:56
epoch [11/50] batch [15/26] time 0.152 (0.204) data 0.000 (0.051) loss 2.2031 (2.2123) acc 84.3750 (84.3750) lr 1.8443e-03 eta 0:03:29
epoch [11/50] batch [20/26] time 0.152 (0.191) data 0.000 (0.038) loss 2.4629 (2.1929) acc 65.6250 (83.1250) lr 1.8443e-03 eta 0:03:14
epoch [11/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.2715 (2.1661) acc 84.3750 (83.8750) lr 1.8443e-03 eta 0:03:06
epoch [12/50] batch [5/26] time 0.154 (0.304) data 0.000 (0.150) loss 2.4180 (1.9939) acc 75.0000 (83.7500) lr 1.8090e-03 eta 0:05:06
epoch [12/50] batch [10/26] time 0.155 (0.228) data 0.000 (0.075) loss 1.5234 (2.0145) acc 93.7500 (84.3750) lr 1.8090e-03 eta 0:03:49
epoch [12/50] batch [15/26] time 0.152 (0.203) data 0.000 (0.050) loss 2.8535 (2.1043) acc 75.0000 (84.3750) lr 1.8090e-03 eta 0:03:22
epoch [12/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.038) loss 2.8125 (2.1034) acc 78.1250 (84.6875) lr 1.8090e-03 eta 0:03:09
epoch [12/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.4912 (2.0887) acc 90.6250 (85.5000) lr 1.8090e-03 eta 0:03:00
epoch [13/50] batch [5/26] time 0.153 (0.310) data 0.000 (0.154) loss 1.9375 (2.0754) acc 87.5000 (86.2500) lr 1.7705e-03 eta 0:05:04
epoch [13/50] batch [10/26] time 0.153 (0.232) data 0.000 (0.077) loss 1.6465 (1.9961) acc 93.7500 (86.2500) lr 1.7705e-03 eta 0:03:46
epoch [13/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.052) loss 2.1367 (2.0367) acc 87.5000 (86.0417) lr 1.7705e-03 eta 0:03:19
epoch [13/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.039) loss 2.4023 (2.1041) acc 87.5000 (85.0000) lr 1.7705e-03 eta 0:03:05
epoch [13/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.031) loss 2.6367 (2.1352) acc 75.0000 (84.5000) lr 1.7705e-03 eta 0:02:57
epoch [14/50] batch [5/26] time 0.154 (0.330) data 0.000 (0.175) loss 2.0508 (1.9941) acc 87.5000 (85.0000) lr 1.7290e-03 eta 0:05:15
epoch [14/50] batch [10/26] time 0.153 (0.242) data 0.000 (0.088) loss 1.9434 (2.0267) acc 87.5000 (85.6250) lr 1.7290e-03 eta 0:03:50
epoch [14/50] batch [15/26] time 0.154 (0.213) data 0.000 (0.058) loss 2.2520 (2.0570) acc 81.2500 (85.6250) lr 1.7290e-03 eta 0:03:21
epoch [14/50] batch [20/26] time 0.153 (0.198) data 0.000 (0.044) loss 2.4961 (2.0465) acc 75.0000 (85.7812) lr 1.7290e-03 eta 0:03:06
epoch [14/50] batch [25/26] time 0.152 (0.189) data 0.000 (0.035) loss 2.1797 (2.0274) acc 84.3750 (86.2500) lr 1.7290e-03 eta 0:02:56
epoch [15/50] batch [5/26] time 0.156 (0.296) data 0.000 (0.141) loss 2.2090 (2.0135) acc 78.1250 (86.2500) lr 1.6845e-03 eta 0:04:36
epoch [15/50] batch [10/26] time 0.154 (0.226) data 0.001 (0.071) loss 3.0312 (2.0680) acc 68.7500 (85.3125) lr 1.6845e-03 eta 0:03:29
epoch [15/50] batch [15/26] time 0.152 (0.201) data 0.000 (0.047) loss 2.7422 (2.1324) acc 78.1250 (85.2083) lr 1.6845e-03 eta 0:03:05
epoch [15/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.035) loss 1.7256 (2.1155) acc 90.6250 (85.9375) lr 1.6845e-03 eta 0:02:53
epoch [15/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.028) loss 2.1172 (2.1229) acc 81.2500 (86.0000) lr 1.6845e-03 eta 0:02:45
epoch [16/50] batch [5/26] time 0.153 (0.306) data 0.000 (0.152) loss 1.4248 (1.8803) acc 93.7500 (86.8750) lr 1.6374e-03 eta 0:04:37
epoch [16/50] batch [10/26] time 0.157 (0.231) data 0.000 (0.077) loss 1.7373 (1.9016) acc 93.7500 (87.5000) lr 1.6374e-03 eta 0:03:27
epoch [16/50] batch [15/26] time 0.155 (0.206) data 0.000 (0.051) loss 1.9121 (1.8878) acc 93.7500 (88.1250) lr 1.6374e-03 eta 0:03:04
epoch [16/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.038) loss 2.6016 (1.9494) acc 84.3750 (87.6562) lr 1.6374e-03 eta 0:02:51
epoch [16/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.031) loss 2.3613 (2.0121) acc 84.3750 (86.6250) lr 1.6374e-03 eta 0:02:43
epoch [17/50] batch [5/26] time 0.152 (0.324) data 0.000 (0.171) loss 1.7852 (1.7998) acc 93.7500 (89.3750) lr 1.5878e-03 eta 0:04:44
epoch [17/50] batch [10/26] time 0.153 (0.239) data 0.000 (0.085) loss 1.9629 (1.8468) acc 87.5000 (88.4375) lr 1.5878e-03 eta 0:03:29
epoch [17/50] batch [15/26] time 0.153 (0.211) data 0.000 (0.057) loss 1.9355 (1.9635) acc 90.6250 (86.6667) lr 1.5878e-03 eta 0:03:03
epoch [17/50] batch [20/26] time 0.152 (0.196) data 0.000 (0.043) loss 1.7051 (2.0264) acc 93.7500 (85.1562) lr 1.5878e-03 eta 0:02:49
epoch [17/50] batch [25/26] time 0.152 (0.187) data 0.000 (0.034) loss 2.1250 (2.0058) acc 81.2500 (85.7500) lr 1.5878e-03 eta 0:02:40
epoch [18/50] batch [5/26] time 0.157 (0.293) data 0.000 (0.139) loss 1.6533 (1.8236) acc 96.8750 (91.2500) lr 1.5358e-03 eta 0:04:10
epoch [18/50] batch [10/26] time 0.154 (0.224) data 0.000 (0.069) loss 2.1289 (1.9498) acc 87.5000 (89.6875) lr 1.5358e-03 eta 0:03:09
epoch [18/50] batch [15/26] time 0.154 (0.200) data 0.000 (0.046) loss 1.9590 (1.9665) acc 87.5000 (88.9583) lr 1.5358e-03 eta 0:02:48
epoch [18/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.035) loss 1.9277 (2.0295) acc 87.5000 (87.3438) lr 1.5358e-03 eta 0:02:37
epoch [18/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.028) loss 2.2852 (2.0276) acc 84.3750 (87.5000) lr 1.5358e-03 eta 0:02:30
epoch [19/50] batch [5/26] time 0.153 (0.303) data 0.000 (0.148) loss 2.1738 (2.0551) acc 84.3750 (86.8750) lr 1.4818e-03 eta 0:04:10
epoch [19/50] batch [10/26] time 0.156 (0.229) data 0.000 (0.074) loss 1.9658 (2.0839) acc 87.5000 (85.9375) lr 1.4818e-03 eta 0:03:08
epoch [19/50] batch [15/26] time 0.152 (0.203) data 0.000 (0.049) loss 1.8516 (2.0465) acc 84.3750 (86.8750) lr 1.4818e-03 eta 0:02:46
epoch [19/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 2.0605 (2.0585) acc 90.6250 (86.4062) lr 1.4818e-03 eta 0:02:34
epoch [19/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.030) loss 2.3164 (2.0407) acc 84.3750 (86.2500) lr 1.4818e-03 eta 0:02:27
epoch [20/50] batch [5/26] time 0.153 (0.302) data 0.000 (0.147) loss 1.7490 (2.0693) acc 93.7500 (86.2500) lr 1.4258e-03 eta 0:04:01
epoch [20/50] batch [10/26] time 0.152 (0.228) data 0.000 (0.074) loss 1.8965 (2.1368) acc 87.5000 (84.3750) lr 1.4258e-03 eta 0:03:01
epoch [20/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 1.8711 (2.0201) acc 87.5000 (85.6250) lr 1.4258e-03 eta 0:02:40
epoch [20/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 1.9814 (2.0036) acc 87.5000 (86.2500) lr 1.4258e-03 eta 0:02:29
epoch [20/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.6816 (2.0106) acc 87.5000 (85.8750) lr 1.4258e-03 eta 0:02:22
epoch [21/50] batch [5/26] time 0.157 (0.305) data 0.000 (0.150) loss 1.9053 (1.9605) acc 81.2500 (85.6250) lr 1.3681e-03 eta 0:03:56
epoch [21/50] batch [10/26] time 0.154 (0.229) data 0.000 (0.075) loss 1.7891 (2.0082) acc 93.7500 (85.6250) lr 1.3681e-03 eta 0:02:56
epoch [21/50] batch [15/26] time 0.153 (0.204) data 0.000 (0.050) loss 1.7832 (1.9782) acc 90.6250 (87.0833) lr 1.3681e-03 eta 0:02:35
epoch [21/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.038) loss 2.0625 (2.0303) acc 84.3750 (86.2500) lr 1.3681e-03 eta 0:02:25
epoch [21/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.6611 (2.0195) acc 90.6250 (85.8750) lr 1.3681e-03 eta 0:02:18
epoch [22/50] batch [5/26] time 0.154 (0.302) data 0.000 (0.147) loss 2.6797 (2.0238) acc 75.0000 (85.6250) lr 1.3090e-03 eta 0:03:46
epoch [22/50] batch [10/26] time 0.153 (0.228) data 0.000 (0.074) loss 2.2148 (1.9884) acc 84.3750 (86.5625) lr 1.3090e-03 eta 0:02:49
epoch [22/50] batch [15/26] time 0.152 (0.203) data 0.000 (0.049) loss 1.9277 (1.9958) acc 87.5000 (86.2500) lr 1.3090e-03 eta 0:02:29
epoch [22/50] batch [20/26] time 0.152 (0.190) data 0.000 (0.037) loss 1.4922 (1.9544) acc 93.7500 (86.8750) lr 1.3090e-03 eta 0:02:19
epoch [22/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.1777 (1.9678) acc 81.2500 (86.7500) lr 1.3090e-03 eta 0:02:13
epoch [23/50] batch [5/26] time 0.152 (0.311) data 0.000 (0.157) loss 1.6318 (1.9402) acc 93.7500 (86.2500) lr 1.2487e-03 eta 0:03:44
epoch [23/50] batch [10/26] time 0.156 (0.234) data 0.000 (0.079) loss 2.1543 (2.0564) acc 87.5000 (85.6250) lr 1.2487e-03 eta 0:02:47
epoch [23/50] batch [15/26] time 0.152 (0.207) data 0.000 (0.053) loss 1.7588 (2.0143) acc 90.6250 (86.0417) lr 1.2487e-03 eta 0:02:27
epoch [23/50] batch [20/26] time 0.152 (0.193) data 0.000 (0.039) loss 1.6465 (1.9962) acc 96.8750 (86.4062) lr 1.2487e-03 eta 0:02:16
epoch [23/50] batch [25/26] time 0.152 (0.185) data 0.000 (0.032) loss 1.7236 (2.0128) acc 84.3750 (85.8750) lr 1.2487e-03 eta 0:02:10
epoch [24/50] batch [5/26] time 0.154 (0.307) data 0.000 (0.152) loss 2.2402 (2.0693) acc 84.3750 (85.0000) lr 1.1874e-03 eta 0:03:34
epoch [24/50] batch [10/26] time 0.155 (0.231) data 0.000 (0.076) loss 1.6270 (1.8721) acc 93.7500 (88.7500) lr 1.1874e-03 eta 0:02:39
epoch [24/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.051) loss 2.0293 (1.9165) acc 87.5000 (88.1250) lr 1.1874e-03 eta 0:02:20
epoch [24/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.038) loss 1.4609 (1.8726) acc 96.8750 (88.7500) lr 1.1874e-03 eta 0:02:10
epoch [24/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.030) loss 1.7959 (1.9505) acc 87.5000 (87.6250) lr 1.1874e-03 eta 0:02:04
epoch [25/50] batch [5/26] time 0.154 (0.301) data 0.000 (0.146) loss 2.0039 (1.9646) acc 87.5000 (88.1250) lr 1.1253e-03 eta 0:03:22
epoch [25/50] batch [10/26] time 0.154 (0.228) data 0.000 (0.073) loss 1.8486 (1.9867) acc 81.2500 (86.2500) lr 1.1253e-03 eta 0:02:31
epoch [25/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 2.2090 (1.9965) acc 87.5000 (86.4583) lr 1.1253e-03 eta 0:02:13
epoch [25/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.037) loss 2.6680 (2.0174) acc 71.8750 (85.7812) lr 1.1253e-03 eta 0:02:04
epoch [25/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.029) loss 1.7568 (1.9935) acc 87.5000 (85.8750) lr 1.1253e-03 eta 0:01:58
epoch [26/50] batch [5/26] time 0.159 (0.304) data 0.004 (0.150) loss 1.7539 (1.8920) acc 93.7500 (88.1250) lr 1.0628e-03 eta 0:03:16
epoch [26/50] batch [10/26] time 0.155 (0.229) data 0.001 (0.075) loss 2.3906 (1.9674) acc 81.2500 (87.1875) lr 1.0628e-03 eta 0:02:26
epoch [26/50] batch [15/26] time 0.153 (0.204) data 0.000 (0.050) loss 2.0332 (1.9809) acc 81.2500 (87.2917) lr 1.0628e-03 eta 0:02:09
epoch [26/50] batch [20/26] time 0.154 (0.191) data 0.000 (0.038) loss 2.0742 (2.0041) acc 81.2500 (86.4062) lr 1.0628e-03 eta 0:02:00
epoch [26/50] batch [25/26] time 0.154 (0.184) data 0.000 (0.030) loss 2.2148 (2.0289) acc 81.2500 (85.3750) lr 1.0628e-03 eta 0:01:54
epoch [27/50] batch [5/26] time 0.159 (0.301) data 0.000 (0.145) loss 1.8408 (1.9281) acc 90.6250 (86.8750) lr 1.0000e-03 eta 0:03:06
epoch [27/50] batch [10/26] time 0.154 (0.228) data 0.000 (0.072) loss 1.6084 (1.8415) acc 90.6250 (88.1250) lr 1.0000e-03 eta 0:02:20
epoch [27/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.048) loss 1.5527 (1.8562) acc 96.8750 (88.7500) lr 1.0000e-03 eta 0:02:03
epoch [27/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.036) loss 2.0781 (1.9659) acc 78.1250 (86.2500) lr 1.0000e-03 eta 0:01:55
epoch [27/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.029) loss 1.9395 (1.9846) acc 87.5000 (86.2500) lr 1.0000e-03 eta 0:01:49
epoch [28/50] batch [5/26] time 0.154 (0.293) data 0.000 (0.139) loss 1.6885 (2.0330) acc 87.5000 (85.6250) lr 9.3721e-04 eta 0:02:53
epoch [28/50] batch [10/26] time 0.153 (0.224) data 0.000 (0.070) loss 2.2422 (1.9762) acc 84.3750 (85.9375) lr 9.3721e-04 eta 0:02:11
epoch [28/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 2.2930 (1.9656) acc 78.1250 (87.0833) lr 9.3721e-04 eta 0:01:56
epoch [28/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.035) loss 1.6523 (1.9706) acc 93.7500 (87.8125) lr 9.3721e-04 eta 0:01:48
epoch [28/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.028) loss 2.3965 (1.9870) acc 84.3750 (87.3750) lr 9.3721e-04 eta 0:01:43
epoch [29/50] batch [5/26] time 0.154 (0.305) data 0.000 (0.149) loss 1.4531 (1.8117) acc 96.8750 (88.7500) lr 8.7467e-04 eta 0:02:53
epoch [29/50] batch [10/26] time 0.153 (0.230) data 0.000 (0.075) loss 1.6973 (1.8133) acc 81.2500 (88.1250) lr 8.7467e-04 eta 0:02:09
epoch [29/50] batch [15/26] time 0.152 (0.204) data 0.000 (0.050) loss 2.1445 (1.8906) acc 84.3750 (86.8750) lr 8.7467e-04 eta 0:01:53
epoch [29/50] batch [20/26] time 0.152 (0.191) data 0.000 (0.037) loss 2.3086 (1.9145) acc 84.3750 (86.7188) lr 8.7467e-04 eta 0:01:45
epoch [29/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.030) loss 1.8203 (1.9279) acc 84.3750 (86.5000) lr 8.7467e-04 eta 0:01:40
epoch [30/50] batch [5/26] time 0.170 (0.286) data 0.000 (0.128) loss 1.7949 (2.0137) acc 90.6250 (87.5000) lr 8.1262e-04 eta 0:02:34
epoch [30/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.064) loss 2.0781 (2.0735) acc 84.3750 (85.6250) lr 8.1262e-04 eta 0:01:58
epoch [30/50] batch [15/26] time 0.153 (0.202) data 0.000 (0.043) loss 1.9258 (2.0410) acc 87.5000 (86.0417) lr 8.1262e-04 eta 0:01:47
epoch [30/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.032) loss 1.6387 (2.0239) acc 93.7500 (86.2500) lr 8.1262e-04 eta 0:01:39
epoch [30/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.026) loss 1.5557 (1.9796) acc 96.8750 (86.8750) lr 8.1262e-04 eta 0:01:35
epoch [31/50] batch [5/26] time 0.156 (0.279) data 0.000 (0.125) loss 1.7168 (1.7318) acc 96.8750 (90.6250) lr 7.5131e-04 eta 0:02:23
epoch [31/50] batch [10/26] time 0.154 (0.217) data 0.000 (0.063) loss 1.8379 (1.8106) acc 81.2500 (88.4375) lr 7.5131e-04 eta 0:01:50
epoch [31/50] batch [15/26] time 0.153 (0.196) data 0.000 (0.042) loss 2.7129 (1.9001) acc 78.1250 (87.5000) lr 7.5131e-04 eta 0:01:38
epoch [31/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.031) loss 2.3594 (1.9120) acc 84.3750 (87.3438) lr 7.5131e-04 eta 0:01:32
epoch [31/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 1.7803 (1.9171) acc 87.5000 (87.2500) lr 7.5131e-04 eta 0:01:28
epoch [32/50] batch [5/26] time 0.153 (0.289) data 0.000 (0.134) loss 2.0488 (1.9715) acc 87.5000 (88.1250) lr 6.9098e-04 eta 0:02:21
epoch [32/50] batch [10/26] time 0.154 (0.222) data 0.000 (0.067) loss 1.4033 (1.8282) acc 100.0000 (89.3750) lr 6.9098e-04 eta 0:01:47
epoch [32/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.045) loss 2.1777 (1.9567) acc 84.3750 (87.2917) lr 6.9098e-04 eta 0:01:35
epoch [32/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.034) loss 1.4043 (1.9169) acc 93.7500 (87.8125) lr 6.9098e-04 eta 0:01:28
epoch [32/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 2.1055 (1.9198) acc 84.3750 (87.3750) lr 6.9098e-04 eta 0:01:24
epoch [33/50] batch [5/26] time 0.154 (0.290) data 0.000 (0.137) loss 2.5840 (2.0285) acc 81.2500 (86.2500) lr 6.3188e-04 eta 0:02:14
epoch [33/50] batch [10/26] time 0.313 (0.239) data 0.001 (0.068) loss 1.5684 (1.9218) acc 87.5000 (86.5625) lr 6.3188e-04 eta 0:01:49
epoch [33/50] batch [15/26] time 0.153 (0.210) data 0.000 (0.046) loss 2.0566 (1.8753) acc 84.3750 (87.2917) lr 6.3188e-04 eta 0:01:35
epoch [33/50] batch [20/26] time 0.154 (0.199) data 0.000 (0.034) loss 1.8291 (1.8997) acc 90.6250 (88.2812) lr 6.3188e-04 eta 0:01:29
epoch [33/50] batch [25/26] time 0.155 (0.190) data 0.000 (0.027) loss 1.7988 (1.8826) acc 90.6250 (88.5000) lr 6.3188e-04 eta 0:01:24
epoch [34/50] batch [5/26] time 0.158 (0.281) data 0.000 (0.125) loss 1.9121 (1.8602) acc 81.2500 (86.8750) lr 5.7422e-04 eta 0:02:02
epoch [34/50] batch [10/26] time 0.153 (0.218) data 0.000 (0.063) loss 1.5400 (1.8783) acc 90.6250 (85.9375) lr 5.7422e-04 eta 0:01:34
epoch [34/50] batch [15/26] time 0.153 (0.196) data 0.000 (0.042) loss 2.0938 (1.8866) acc 84.3750 (86.8750) lr 5.7422e-04 eta 0:01:23
epoch [34/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.031) loss 1.7002 (1.8588) acc 93.7500 (87.0312) lr 5.7422e-04 eta 0:01:18
epoch [34/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 2.0195 (1.8646) acc 87.5000 (87.5000) lr 5.7422e-04 eta 0:01:14
epoch [35/50] batch [5/26] time 0.155 (0.288) data 0.000 (0.132) loss 2.0996 (1.7566) acc 84.3750 (88.7500) lr 5.1825e-04 eta 0:01:58
epoch [35/50] batch [10/26] time 0.154 (0.222) data 0.001 (0.066) loss 1.6426 (1.8442) acc 96.8750 (89.0625) lr 5.1825e-04 eta 0:01:30
epoch [35/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.044) loss 2.1211 (1.8605) acc 84.3750 (89.3750) lr 5.1825e-04 eta 0:01:19
epoch [35/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.033) loss 1.6426 (1.8353) acc 93.7500 (89.2188) lr 5.1825e-04 eta 0:01:14
epoch [35/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 2.3125 (1.9464) acc 81.2500 (87.6250) lr 5.1825e-04 eta 0:01:10
epoch [36/50] batch [5/26] time 0.154 (0.283) data 0.000 (0.126) loss 2.1973 (2.1203) acc 81.2500 (86.8750) lr 4.6417e-04 eta 0:01:48
epoch [36/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.063) loss 1.5020 (1.9719) acc 93.7500 (89.0625) lr 4.6417e-04 eta 0:01:23
epoch [36/50] batch [15/26] time 0.154 (0.197) data 0.000 (0.042) loss 1.8301 (1.9585) acc 84.3750 (87.5000) lr 4.6417e-04 eta 0:01:13
epoch [36/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.9609 (1.9352) acc 84.3750 (88.1250) lr 4.6417e-04 eta 0:01:08
epoch [36/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 1.7773 (1.9248) acc 93.7500 (88.5000) lr 4.6417e-04 eta 0:01:05
epoch [37/50] batch [5/26] time 0.153 (0.281) data 0.000 (0.126) loss 2.5859 (2.2020) acc 78.1250 (85.6250) lr 4.1221e-04 eta 0:01:40
epoch [37/50] batch [10/26] time 0.158 (0.218) data 0.000 (0.063) loss 1.5469 (2.1264) acc 90.6250 (84.6875) lr 4.1221e-04 eta 0:01:17
epoch [37/50] batch [15/26] time 0.152 (0.197) data 0.000 (0.042) loss 1.9102 (1.9807) acc 90.6250 (87.7083) lr 4.1221e-04 eta 0:01:08
epoch [37/50] batch [20/26] time 0.152 (0.186) data 0.000 (0.032) loss 2.3125 (1.9675) acc 71.8750 (87.1875) lr 4.1221e-04 eta 0:01:03
epoch [37/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 1.4355 (1.9075) acc 100.0000 (88.3750) lr 4.1221e-04 eta 0:01:00
epoch [38/50] batch [5/26] time 0.155 (0.292) data 0.000 (0.136) loss 2.0547 (1.7850) acc 87.5000 (88.7500) lr 3.6258e-04 eta 0:01:37
epoch [38/50] batch [10/26] time 0.156 (0.224) data 0.000 (0.068) loss 1.6562 (1.7489) acc 87.5000 (89.3750) lr 3.6258e-04 eta 0:01:13
epoch [38/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.046) loss 1.9717 (1.7888) acc 90.6250 (89.1667) lr 3.6258e-04 eta 0:01:04
epoch [38/50] batch [20/26] time 0.154 (0.189) data 0.000 (0.034) loss 1.9238 (1.7850) acc 87.5000 (88.5938) lr 3.6258e-04 eta 0:01:00
epoch [38/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.027) loss 1.7432 (1.7939) acc 93.7500 (89.1250) lr 3.6258e-04 eta 0:00:56
epoch [39/50] batch [5/26] time 0.153 (0.302) data 0.000 (0.147) loss 1.8789 (2.0221) acc 87.5000 (85.6250) lr 3.1545e-04 eta 0:01:32
epoch [39/50] batch [10/26] time 0.154 (0.228) data 0.000 (0.074) loss 2.0195 (1.9019) acc 84.3750 (88.1250) lr 3.1545e-04 eta 0:01:08
epoch [39/50] batch [15/26] time 0.155 (0.203) data 0.000 (0.049) loss 2.0977 (1.8868) acc 87.5000 (88.5417) lr 3.1545e-04 eta 0:01:00
epoch [39/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 2.0273 (1.9114) acc 81.2500 (87.3438) lr 3.1545e-04 eta 0:00:55
epoch [39/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.030) loss 1.6914 (1.9071) acc 93.7500 (87.6250) lr 3.1545e-04 eta 0:00:52
epoch [40/50] batch [5/26] time 0.153 (0.301) data 0.000 (0.146) loss 2.2266 (2.0695) acc 84.3750 (86.2500) lr 2.7103e-04 eta 0:01:24
epoch [40/50] batch [10/26] time 0.154 (0.228) data 0.001 (0.073) loss 2.4473 (1.9869) acc 81.2500 (87.1875) lr 2.7103e-04 eta 0:01:03
epoch [40/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 1.8604 (1.9101) acc 87.5000 (88.9583) lr 2.7103e-04 eta 0:00:55
epoch [40/50] batch [20/26] time 0.154 (0.191) data 0.000 (0.037) loss 1.5254 (1.8756) acc 93.7500 (88.9062) lr 2.7103e-04 eta 0:00:50
epoch [40/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.9414 (1.8329) acc 87.5000 (89.3750) lr 2.7103e-04 eta 0:00:47
epoch [41/50] batch [5/26] time 0.155 (0.306) data 0.000 (0.149) loss 1.8691 (1.8180) acc 87.5000 (88.7500) lr 2.2949e-04 eta 0:01:17
epoch [41/50] batch [10/26] time 0.157 (0.231) data 0.000 (0.075) loss 2.0000 (1.8503) acc 87.5000 (89.0625) lr 2.2949e-04 eta 0:00:57
epoch [41/50] batch [15/26] time 0.155 (0.206) data 0.000 (0.050) loss 2.0371 (1.8596) acc 84.3750 (89.5833) lr 2.2949e-04 eta 0:00:50
epoch [41/50] batch [20/26] time 0.155 (0.193) data 0.001 (0.038) loss 1.9346 (1.8297) acc 90.6250 (89.6875) lr 2.2949e-04 eta 0:00:46
epoch [41/50] batch [25/26] time 0.152 (0.185) data 0.000 (0.030) loss 2.5918 (1.8638) acc 81.2500 (89.2500) lr 2.2949e-04 eta 0:00:43
epoch [42/50] batch [5/26] time 0.156 (0.283) data 0.000 (0.128) loss 1.8213 (1.8061) acc 90.6250 (91.2500) lr 1.9098e-04 eta 0:01:04
epoch [42/50] batch [10/26] time 0.155 (0.219) data 0.000 (0.064) loss 1.7070 (1.8500) acc 96.8750 (89.3750) lr 1.9098e-04 eta 0:00:49
epoch [42/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.8438 (1.8875) acc 93.7500 (89.3750) lr 1.9098e-04 eta 0:00:43
epoch [42/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.7109 (1.8523) acc 90.6250 (89.5312) lr 1.9098e-04 eta 0:00:39
epoch [42/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.026) loss 1.6221 (1.8441) acc 87.5000 (88.8750) lr 1.9098e-04 eta 0:00:37
epoch [43/50] batch [5/26] time 0.154 (0.293) data 0.000 (0.137) loss 1.6348 (2.1326) acc 93.7500 (87.5000) lr 1.5567e-04 eta 0:00:59
epoch [43/50] batch [10/26] time 0.156 (0.225) data 0.000 (0.069) loss 1.5967 (1.9104) acc 93.7500 (89.6875) lr 1.5567e-04 eta 0:00:44
epoch [43/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.046) loss 2.0391 (1.9003) acc 87.5000 (90.2083) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.034) loss 1.6240 (1.9016) acc 93.7500 (90.0000) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.7080 (1.8611) acc 90.6250 (90.2500) lr 1.5567e-04 eta 0:00:33
epoch [44/50] batch [5/26] time 0.153 (0.311) data 0.000 (0.157) loss 1.9316 (1.8627) acc 81.2500 (88.1250) lr 1.2369e-04 eta 0:00:55
epoch [44/50] batch [10/26] time 0.154 (0.233) data 0.000 (0.078) loss 1.5195 (1.7437) acc 93.7500 (89.3750) lr 1.2369e-04 eta 0:00:40
epoch [44/50] batch [15/26] time 0.154 (0.206) data 0.000 (0.052) loss 2.2168 (1.8087) acc 87.5000 (89.3750) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.039) loss 2.3633 (1.8294) acc 78.1250 (88.9062) lr 1.2369e-04 eta 0:00:31
epoch [44/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.031) loss 2.0332 (1.8183) acc 81.2500 (89.0000) lr 1.2369e-04 eta 0:00:29
epoch [45/50] batch [5/26] time 0.158 (0.283) data 0.000 (0.127) loss 1.7969 (1.9109) acc 84.3750 (88.7500) lr 9.5173e-05 eta 0:00:42
epoch [45/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.064) loss 1.9883 (1.9143) acc 90.6250 (89.3750) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.6016 (1.8958) acc 93.7500 (89.1667) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.9590 (1.8683) acc 87.5000 (89.2188) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.5498 (1.8291) acc 90.6250 (89.2500) lr 9.5173e-05 eta 0:00:23
epoch [46/50] batch [5/26] time 0.156 (0.287) data 0.000 (0.130) loss 1.4697 (1.7771) acc 96.8750 (91.2500) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [10/26] time 0.154 (0.222) data 0.000 (0.065) loss 1.9893 (1.9417) acc 81.2500 (88.1250) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.044) loss 1.5684 (1.8580) acc 93.7500 (90.0000) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.155 (0.188) data 0.000 (0.033) loss 1.3750 (1.8119) acc 96.8750 (90.7812) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.156 (0.182) data 0.000 (0.026) loss 1.9111 (1.8273) acc 81.2500 (90.6250) lr 7.0224e-05 eta 0:00:19
epoch [47/50] batch [5/26] time 0.159 (0.294) data 0.000 (0.137) loss 2.2773 (2.0230) acc 84.3750 (86.8750) lr 4.8943e-05 eta 0:00:29
epoch [47/50] batch [10/26] time 0.158 (0.226) data 0.004 (0.069) loss 1.6855 (1.9147) acc 90.6250 (87.1875) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [15/26] time 0.154 (0.202) data 0.000 (0.046) loss 1.9883 (1.8659) acc 90.6250 (88.7500) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [20/26] time 0.152 (0.190) data 0.000 (0.035) loss 2.0742 (1.8633) acc 87.5000 (88.9062) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.028) loss 1.6680 (1.8529) acc 90.6250 (88.8750) lr 4.8943e-05 eta 0:00:14
epoch [48/50] batch [5/26] time 0.157 (0.288) data 0.000 (0.133) loss 1.5137 (1.6881) acc 100.0000 (92.5000) lr 3.1417e-05 eta 0:00:21
epoch [48/50] batch [10/26] time 0.154 (0.222) data 0.000 (0.066) loss 1.8008 (1.7576) acc 87.5000 (91.5625) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [15/26] time 0.152 (0.199) data 0.000 (0.044) loss 1.8203 (1.8248) acc 90.6250 (90.2083) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.033) loss 1.9229 (1.8208) acc 87.5000 (89.3750) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.7441 (1.8548) acc 87.5000 (89.6250) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.155 (0.295) data 0.000 (0.140) loss 1.9756 (1.8949) acc 93.7500 (90.6250) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [10/26] time 0.154 (0.225) data 0.000 (0.070) loss 1.7656 (1.8296) acc 84.3750 (90.6250) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.047) loss 1.4961 (1.8470) acc 90.6250 (89.5833) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.035) loss 2.2852 (1.8435) acc 81.2500 (89.3750) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.028) loss 1.8721 (1.8282) acc 90.6250 (89.8750) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.153 (0.281) data 0.000 (0.125) loss 1.6768 (1.8449) acc 93.7500 (90.6250) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/26] time 0.155 (0.218) data 0.000 (0.063) loss 1.6260 (1.7411) acc 87.5000 (90.9375) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.042) loss 1.7461 (1.7862) acc 87.5000 (90.0000) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.155 (0.186) data 0.000 (0.031) loss 1.8301 (1.8534) acc 90.6250 (89.2188) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.154 (0.180) data 0.000 (0.025) loss 1.8193 (1.8234) acc 87.5000 (89.8750) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [00:05<02:41,  5.38s/it]  6%|▋         | 2/31 [00:06<01:24,  2.91s/it] 10%|▉         | 3/31 [00:07<00:58,  2.10s/it] 13%|█▎        | 4/31 [00:08<00:46,  1.72s/it] 16%|█▌        | 5/31 [00:09<00:39,  1.51s/it] 19%|█▉        | 6/31 [00:11<00:34,  1.38s/it] 23%|██▎       | 7/31 [00:12<00:31,  1.30s/it] 26%|██▌       | 8/31 [00:13<00:28,  1.25s/it] 29%|██▉       | 9/31 [00:14<00:26,  1.22s/it] 32%|███▏      | 10/31 [00:15<00:25,  1.19s/it] 35%|███▌      | 11/31 [00:16<00:23,  1.18s/it] 39%|███▊      | 12/31 [00:17<00:22,  1.16s/it] 42%|████▏     | 13/31 [00:19<00:20,  1.16s/it] 45%|████▌     | 14/31 [00:20<00:19,  1.15s/it] 48%|████▊     | 15/31 [00:21<00:18,  1.15s/it] 52%|█████▏    | 16/31 [00:22<00:17,  1.15s/it] 55%|█████▍    | 17/31 [00:23<00:16,  1.14s/it] 58%|█████▊    | 18/31 [00:24<00:14,  1.14s/it] 61%|██████▏   | 19/31 [00:25<00:13,  1.14s/it] 65%|██████▍   | 20/31 [00:27<00:12,  1.14s/it] 68%|██████▊   | 21/31 [00:28<00:11,  1.14s/it] 71%|███████   | 22/31 [00:29<00:10,  1.14s/it] 74%|███████▍  | 23/31 [00:30<00:09,  1.14s/it] 77%|███████▋  | 24/31 [00:31<00:07,  1.14s/it] 81%|████████  | 25/31 [00:32<00:06,  1.14s/it] 84%|████████▍ | 26/31 [00:33<00:05,  1.14s/it] 87%|████████▋ | 27/31 [00:35<00:04,  1.14s/it] 90%|█████████ | 28/31 [00:36<00:03,  1.14s/it] 94%|█████████▎| 29/31 [00:37<00:02,  1.14s/it] 97%|█████████▋| 30/31 [00:38<00:01,  1.14s/it]100%|██████████| 31/31 [00:39<00:00,  1.02s/it]100%|██████████| 31/31 [00:39<00:00,  1.27s/it]
=> result
* total: 15,300
* correct: 13,882
* accuracy: 90.7%
* error: 9.3%
* macro_f1: 90.7%
Elapsed: 0:04:38
Run this job and save the output to output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  51
# train_x  816
# val      204
# test     15,300
---------  -------
['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X apple pie, a type of food.', 'X X X X baby back ribs, a type of food.', 'X X X X baklava, a type of food.', 'X X X X beef carpaccio, a type of food.', 'X X X X beef tartare, a type of food.', 'X X X X beet salad, a type of food.', 'X X X X beignets, a type of food.', 'X X X X bibimbap, a type of food.', 'X X X X bread pudding, a type of food.', 'X X X X breakfast burrito, a type of food.', 'X X X X bruschetta, a type of food.', 'X X X X caesar salad, a type of food.', 'X X X X cannoli, a type of food.', 'X X X X caprese salad, a type of food.', 'X X X X carrot cake, a type of food.', 'X X X X ceviche, a type of food.', 'X X X X cheese plate, a type of food.', 'X X X X cheesecake, a type of food.', 'X X X X chicken curry, a type of food.', 'X X X X chicken quesadilla, a type of food.', 'X X X X chicken wings, a type of food.', 'X X X X chocolate cake, a type of food.', 'X X X X chocolate mousse, a type of food.', 'X X X X churros, a type of food.', 'X X X X clam chowder, a type of food.', 'X X X X club sandwich, a type of food.', 'X X X X crab cakes, a type of food.', 'X X X X creme brulee, a type of food.', 'X X X X croque madame, a type of food.', 'X X X X cup cakes, a type of food.', 'X X X X deviled eggs, a type of food.', 'X X X X donuts, a type of food.', 'X X X X dumplings, a type of food.', 'X X X X edamame, a type of food.', 'X X X X eggs benedict, a type of food.', 'X X X X escargots, a type of food.', 'X X X X falafel, a type of food.', 'X X X X filet mignon, a type of food.', 'X X X X fish and chips, a type of food.', 'X X X X foie gras, a type of food.', 'X X X X french fries, a type of food.', 'X X X X french onion soup, a type of food.', 'X X X X french toast, a type of food.', 'X X X X fried calamari, a type of food.', 'X X X X fried rice, a type of food.', 'X X X X frozen yogurt, a type of food.', 'X X X X garlic bread, a type of food.', 'X X X X gnocchi, a type of food.', 'X X X X greek salad, a type of food.', 'X X X X grilled cheese sandwich, a type of food.', 'X X X X grilled salmon, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/26] time 0.152 (0.290) data 0.000 (0.127) loss 4.5508 (4.5828) acc 84.3750 (86.8750) lr 1.0000e-05 eta 0:06:15
epoch [1/50] batch [10/26] time 0.153 (0.221) data 0.000 (0.064) loss 4.9062 (4.6625) acc 71.8750 (84.3750) lr 1.0000e-05 eta 0:04:45
epoch [1/50] batch [15/26] time 0.151 (0.198) data 0.000 (0.043) loss 5.4062 (4.7398) acc 62.5000 (81.0417) lr 1.0000e-05 eta 0:04:14
epoch [1/50] batch [20/26] time 0.151 (0.186) data 0.000 (0.032) loss 4.3906 (4.7420) acc 84.3750 (80.7812) lr 1.0000e-05 eta 0:03:58
epoch [1/50] batch [25/26] time 0.151 (0.179) data 0.000 (0.026) loss 4.9297 (4.7431) acc 71.8750 (80.6250) lr 1.0000e-05 eta 0:03:48
epoch [2/50] batch [5/26] time 0.154 (0.289) data 0.000 (0.134) loss 3.8086 (3.9527) acc 87.5000 (85.0000) lr 2.0000e-03 eta 0:06:06
epoch [2/50] batch [10/26] time 0.152 (0.221) data 0.000 (0.067) loss 3.2852 (3.8107) acc 84.3750 (83.1250) lr 2.0000e-03 eta 0:04:39
epoch [2/50] batch [15/26] time 0.151 (0.198) data 0.000 (0.045) loss 3.8516 (3.7062) acc 68.7500 (81.6667) lr 2.0000e-03 eta 0:04:08
epoch [2/50] batch [20/26] time 0.151 (0.186) data 0.000 (0.034) loss 2.7598 (3.5639) acc 87.5000 (82.1875) lr 2.0000e-03 eta 0:03:53
epoch [2/50] batch [25/26] time 0.151 (0.179) data 0.000 (0.027) loss 2.4688 (3.4625) acc 93.7500 (82.3750) lr 2.0000e-03 eta 0:03:43
epoch [3/50] batch [5/26] time 0.151 (0.287) data 0.000 (0.133) loss 2.8281 (3.0715) acc 81.2500 (80.0000) lr 1.9980e-03 eta 0:05:57
epoch [3/50] batch [10/26] time 0.151 (0.219) data 0.000 (0.066) loss 2.2910 (2.8787) acc 93.7500 (83.7500) lr 1.9980e-03 eta 0:04:31
epoch [3/50] batch [15/26] time 0.151 (0.197) data 0.000 (0.044) loss 2.9824 (2.8736) acc 75.0000 (83.1250) lr 1.9980e-03 eta 0:04:02
epoch [3/50] batch [20/26] time 0.152 (0.185) data 0.000 (0.033) loss 2.5859 (2.7656) acc 90.6250 (84.5312) lr 1.9980e-03 eta 0:03:47
epoch [3/50] batch [25/26] time 0.151 (0.179) data 0.000 (0.027) loss 2.5508 (2.7402) acc 84.3750 (84.5000) lr 1.9980e-03 eta 0:03:38
epoch [4/50] batch [5/26] time 0.156 (0.337) data 0.000 (0.183) loss 2.0859 (2.3871) acc 90.6250 (87.5000) lr 1.9921e-03 eta 0:06:49
epoch [4/50] batch [10/26] time 0.153 (0.245) data 0.000 (0.092) loss 2.6133 (2.4459) acc 84.3750 (85.3125) lr 1.9921e-03 eta 0:04:57
epoch [4/50] batch [15/26] time 0.152 (0.217) data 0.000 (0.061) loss 2.4922 (2.5238) acc 90.6250 (84.5833) lr 1.9921e-03 eta 0:04:22
epoch [4/50] batch [20/26] time 0.152 (0.205) data 0.000 (0.046) loss 2.6602 (2.5836) acc 81.2500 (83.7500) lr 1.9921e-03 eta 0:04:05
epoch [4/50] batch [25/26] time 0.152 (0.194) data 0.000 (0.037) loss 2.4961 (2.5698) acc 87.5000 (84.0000) lr 1.9921e-03 eta 0:03:52
epoch [5/50] batch [5/26] time 0.154 (0.305) data 0.000 (0.134) loss 1.8789 (2.4539) acc 90.6250 (85.6250) lr 1.9823e-03 eta 0:06:03
epoch [5/50] batch [10/26] time 0.154 (0.235) data 0.000 (0.067) loss 1.9121 (2.3608) acc 90.6250 (85.6250) lr 1.9823e-03 eta 0:04:38
epoch [5/50] batch [15/26] time 0.151 (0.207) data 0.000 (0.045) loss 2.9473 (2.4641) acc 75.0000 (83.7500) lr 1.9823e-03 eta 0:04:04
epoch [5/50] batch [20/26] time 0.152 (0.193) data 0.000 (0.034) loss 2.6719 (2.4756) acc 81.2500 (83.7500) lr 1.9823e-03 eta 0:03:47
epoch [5/50] batch [25/26] time 0.152 (0.185) data 0.000 (0.027) loss 2.1055 (2.4650) acc 87.5000 (83.2500) lr 1.9823e-03 eta 0:03:36
epoch [6/50] batch [5/26] time 0.154 (0.302) data 0.000 (0.148) loss 2.0508 (2.3008) acc 84.3750 (84.3750) lr 1.9686e-03 eta 0:05:51
epoch [6/50] batch [10/26] time 0.153 (0.229) data 0.000 (0.074) loss 2.6758 (2.2824) acc 68.7500 (85.3125) lr 1.9686e-03 eta 0:04:25
epoch [6/50] batch [15/26] time 0.152 (0.203) data 0.000 (0.049) loss 2.3789 (2.2893) acc 84.3750 (85.6250) lr 1.9686e-03 eta 0:03:54
epoch [6/50] batch [20/26] time 0.152 (0.190) data 0.000 (0.037) loss 2.8359 (2.3498) acc 71.8750 (84.0625) lr 1.9686e-03 eta 0:03:39
epoch [6/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.1094 (2.3293) acc 81.2500 (84.3750) lr 1.9686e-03 eta 0:03:29
epoch [7/50] batch [5/26] time 0.154 (0.307) data 0.000 (0.152) loss 1.8994 (1.9584) acc 87.5000 (90.6250) lr 1.9511e-03 eta 0:05:49
epoch [7/50] batch [10/26] time 0.153 (0.231) data 0.000 (0.077) loss 2.8164 (2.1479) acc 81.2500 (87.5000) lr 1.9511e-03 eta 0:04:21
epoch [7/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.051) loss 2.7227 (2.2548) acc 75.0000 (84.3750) lr 1.9511e-03 eta 0:03:51
epoch [7/50] batch [20/26] time 0.155 (0.193) data 0.001 (0.038) loss 2.0918 (2.2597) acc 87.5000 (85.0000) lr 1.9511e-03 eta 0:03:36
epoch [7/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.031) loss 2.1797 (2.2651) acc 84.3750 (85.2500) lr 1.9511e-03 eta 0:03:26
epoch [8/50] batch [5/26] time 0.153 (0.282) data 0.000 (0.128) loss 2.0078 (2.1129) acc 87.5000 (85.6250) lr 1.9298e-03 eta 0:05:13
epoch [8/50] batch [10/26] time 0.153 (0.218) data 0.000 (0.064) loss 2.2832 (2.1580) acc 84.3750 (85.0000) lr 1.9298e-03 eta 0:04:01
epoch [8/50] batch [15/26] time 0.152 (0.196) data 0.000 (0.043) loss 2.1758 (2.1547) acc 81.2500 (84.5833) lr 1.9298e-03 eta 0:03:35
epoch [8/50] batch [20/26] time 0.152 (0.185) data 0.000 (0.032) loss 2.1250 (2.1851) acc 87.5000 (84.5312) lr 1.9298e-03 eta 0:03:23
epoch [8/50] batch [25/26] time 0.151 (0.178) data 0.000 (0.026) loss 2.4883 (2.2104) acc 75.0000 (84.3750) lr 1.9298e-03 eta 0:03:14
epoch [9/50] batch [5/26] time 0.153 (0.282) data 0.000 (0.128) loss 1.8516 (2.0721) acc 96.8750 (86.8750) lr 1.9048e-03 eta 0:05:06
epoch [9/50] batch [10/26] time 0.154 (0.218) data 0.000 (0.064) loss 3.1660 (2.3062) acc 65.6250 (82.8125) lr 1.9048e-03 eta 0:03:55
epoch [9/50] batch [15/26] time 0.152 (0.196) data 0.000 (0.043) loss 2.0312 (2.2497) acc 87.5000 (84.3750) lr 1.9048e-03 eta 0:03:31
epoch [9/50] batch [20/26] time 0.152 (0.185) data 0.000 (0.032) loss 1.7754 (2.1885) acc 93.7500 (85.0000) lr 1.9048e-03 eta 0:03:18
epoch [9/50] batch [25/26] time 0.152 (0.179) data 0.000 (0.026) loss 2.5879 (2.1716) acc 81.2500 (85.5000) lr 1.9048e-03 eta 0:03:10
epoch [10/50] batch [5/26] time 0.155 (0.283) data 0.000 (0.127) loss 2.3906 (2.2215) acc 87.5000 (82.5000) lr 1.8763e-03 eta 0:04:59
epoch [10/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.064) loss 1.8164 (2.0929) acc 93.7500 (86.8750) lr 1.8763e-03 eta 0:03:50
epoch [10/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.042) loss 1.8242 (2.0862) acc 90.6250 (86.8750) lr 1.8763e-03 eta 0:03:26
epoch [10/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.8789 (2.0574) acc 84.3750 (86.8750) lr 1.8763e-03 eta 0:03:14
epoch [10/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.7207 (2.0780) acc 87.5000 (86.3750) lr 1.8763e-03 eta 0:03:06
epoch [11/50] batch [5/26] time 0.154 (0.282) data 0.000 (0.128) loss 2.0449 (1.9531) acc 87.5000 (90.0000) lr 1.8443e-03 eta 0:04:51
epoch [11/50] batch [10/26] time 0.155 (0.219) data 0.000 (0.064) loss 1.6904 (1.9701) acc 93.7500 (89.3750) lr 1.8443e-03 eta 0:03:45
epoch [11/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 2.4199 (2.1124) acc 84.3750 (86.2500) lr 1.8443e-03 eta 0:03:21
epoch [11/50] batch [20/26] time 0.157 (0.186) data 0.000 (0.032) loss 2.6543 (2.1513) acc 81.2500 (85.7812) lr 1.8443e-03 eta 0:03:09
epoch [11/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 2.5332 (2.1955) acc 78.1250 (84.8750) lr 1.8443e-03 eta 0:03:02
epoch [12/50] batch [5/26] time 0.160 (0.291) data 0.004 (0.136) loss 2.0703 (2.1068) acc 87.5000 (87.5000) lr 1.8090e-03 eta 0:04:54
epoch [12/50] batch [10/26] time 0.152 (0.223) data 0.000 (0.068) loss 1.8789 (2.0256) acc 90.6250 (87.8125) lr 1.8090e-03 eta 0:03:43
epoch [12/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.046) loss 3.2891 (2.1659) acc 68.7500 (85.8333) lr 1.8090e-03 eta 0:03:19
epoch [12/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.034) loss 2.1328 (2.1556) acc 90.6250 (85.6250) lr 1.8090e-03 eta 0:03:06
epoch [12/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.027) loss 1.9658 (2.1476) acc 93.7500 (85.6250) lr 1.8090e-03 eta 0:02:58
epoch [13/50] batch [5/26] time 0.153 (0.284) data 0.000 (0.129) loss 2.3027 (1.8926) acc 81.2500 (87.5000) lr 1.7705e-03 eta 0:04:39
epoch [13/50] batch [10/26] time 0.154 (0.220) data 0.000 (0.065) loss 2.6094 (2.0389) acc 84.3750 (85.9375) lr 1.7705e-03 eta 0:03:35
epoch [13/50] batch [15/26] time 0.152 (0.198) data 0.000 (0.044) loss 2.4160 (2.0710) acc 81.2500 (85.2083) lr 1.7705e-03 eta 0:03:12
epoch [13/50] batch [20/26] time 0.152 (0.186) data 0.000 (0.033) loss 2.6035 (2.1564) acc 78.1250 (84.3750) lr 1.7705e-03 eta 0:03:00
epoch [13/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.026) loss 2.1211 (2.1871) acc 87.5000 (84.3750) lr 1.7705e-03 eta 0:02:52
epoch [14/50] batch [5/26] time 0.153 (0.287) data 0.000 (0.132) loss 1.5098 (2.0807) acc 96.8750 (86.2500) lr 1.7290e-03 eta 0:04:34
epoch [14/50] batch [10/26] time 0.154 (0.220) data 0.000 (0.066) loss 1.9766 (2.0333) acc 87.5000 (86.5625) lr 1.7290e-03 eta 0:03:29
epoch [14/50] batch [15/26] time 0.154 (0.198) data 0.000 (0.044) loss 2.0352 (2.0146) acc 87.5000 (86.6667) lr 1.7290e-03 eta 0:03:07
epoch [14/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.033) loss 1.6270 (2.0022) acc 90.6250 (87.1875) lr 1.7290e-03 eta 0:02:56
epoch [14/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.027) loss 2.8926 (2.0793) acc 78.1250 (85.7500) lr 1.7290e-03 eta 0:02:48
epoch [15/50] batch [5/26] time 0.152 (0.295) data 0.000 (0.141) loss 1.8232 (2.0990) acc 93.7500 (85.6250) lr 1.6845e-03 eta 0:04:34
epoch [15/50] batch [10/26] time 0.154 (0.225) data 0.000 (0.070) loss 1.6738 (1.9575) acc 90.6250 (87.1875) lr 1.6845e-03 eta 0:03:28
epoch [15/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.047) loss 2.1680 (1.9955) acc 90.6250 (86.6667) lr 1.6845e-03 eta 0:03:05
epoch [15/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.035) loss 2.3047 (1.9829) acc 78.1250 (86.5625) lr 1.6845e-03 eta 0:02:52
epoch [15/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.028) loss 1.8076 (2.0251) acc 87.5000 (85.7500) lr 1.6845e-03 eta 0:02:45
epoch [16/50] batch [5/26] time 0.155 (0.286) data 0.000 (0.131) loss 1.6650 (1.9211) acc 84.3750 (83.7500) lr 1.6374e-03 eta 0:04:18
epoch [16/50] batch [10/26] time 0.154 (0.220) data 0.001 (0.066) loss 1.8955 (1.9639) acc 87.5000 (85.6250) lr 1.6374e-03 eta 0:03:17
epoch [16/50] batch [15/26] time 0.152 (0.197) data 0.000 (0.044) loss 2.0820 (1.9591) acc 84.3750 (85.2083) lr 1.6374e-03 eta 0:02:56
epoch [16/50] batch [20/26] time 0.152 (0.186) data 0.000 (0.033) loss 2.0996 (1.9827) acc 87.5000 (85.4688) lr 1.6374e-03 eta 0:02:45
epoch [16/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.9551 (2.0334) acc 87.5000 (85.2500) lr 1.6374e-03 eta 0:02:38
epoch [17/50] batch [5/26] time 0.158 (0.280) data 0.000 (0.125) loss 1.7949 (2.1758) acc 78.1250 (81.8750) lr 1.5878e-03 eta 0:04:06
epoch [17/50] batch [10/26] time 0.153 (0.217) data 0.000 (0.063) loss 2.0000 (2.1786) acc 78.1250 (81.8750) lr 1.5878e-03 eta 0:03:09
epoch [17/50] batch [15/26] time 0.152 (0.196) data 0.000 (0.042) loss 2.2402 (2.0893) acc 84.3750 (84.3750) lr 1.5878e-03 eta 0:02:50
epoch [17/50] batch [20/26] time 0.152 (0.185) data 0.000 (0.031) loss 1.8125 (2.0926) acc 93.7500 (84.5312) lr 1.5878e-03 eta 0:02:39
epoch [17/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.025) loss 1.8789 (2.0876) acc 90.6250 (85.1250) lr 1.5878e-03 eta 0:02:33
epoch [18/50] batch [5/26] time 0.157 (0.291) data 0.000 (0.136) loss 2.5000 (2.1879) acc 75.0000 (83.1250) lr 1.5358e-03 eta 0:04:07
epoch [18/50] batch [10/26] time 0.153 (0.222) data 0.000 (0.068) loss 1.9590 (2.0532) acc 90.6250 (86.5625) lr 1.5358e-03 eta 0:03:08
epoch [18/50] batch [15/26] time 0.152 (0.199) data 0.000 (0.046) loss 1.8633 (2.0531) acc 87.5000 (86.4583) lr 1.5358e-03 eta 0:02:47
epoch [18/50] batch [20/26] time 0.152 (0.187) data 0.000 (0.034) loss 1.6221 (2.0728) acc 100.0000 (86.5625) lr 1.5358e-03 eta 0:02:37
epoch [18/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.5898 (2.0676) acc 93.7500 (86.5000) lr 1.5358e-03 eta 0:02:30
epoch [19/50] batch [5/26] time 0.152 (0.288) data 0.000 (0.135) loss 2.3652 (2.1357) acc 84.3750 (84.3750) lr 1.4818e-03 eta 0:03:58
epoch [19/50] batch [10/26] time 0.155 (0.220) data 0.000 (0.068) loss 2.2539 (2.1258) acc 84.3750 (85.3125) lr 1.4818e-03 eta 0:03:01
epoch [19/50] batch [15/26] time 0.152 (0.198) data 0.000 (0.045) loss 2.5742 (2.0350) acc 68.7500 (86.4583) lr 1.4818e-03 eta 0:02:41
epoch [19/50] batch [20/26] time 0.152 (0.186) data 0.000 (0.034) loss 1.8398 (1.9991) acc 90.6250 (87.3438) lr 1.4818e-03 eta 0:02:31
epoch [19/50] batch [25/26] time 0.152 (0.179) data 0.000 (0.027) loss 1.9141 (2.0208) acc 87.5000 (86.7500) lr 1.4818e-03 eta 0:02:24
epoch [20/50] batch [5/26] time 0.160 (0.281) data 0.000 (0.125) loss 1.6865 (1.7324) acc 87.5000 (90.0000) lr 1.4258e-03 eta 0:03:45
epoch [20/50] batch [10/26] time 0.153 (0.217) data 0.000 (0.063) loss 1.7598 (1.8028) acc 87.5000 (89.3750) lr 1.4258e-03 eta 0:02:52
epoch [20/50] batch [15/26] time 0.152 (0.195) data 0.000 (0.042) loss 1.5703 (1.8871) acc 96.8750 (88.3333) lr 1.4258e-03 eta 0:02:34
epoch [20/50] batch [20/26] time 0.152 (0.184) data 0.000 (0.031) loss 1.9590 (1.9969) acc 84.3750 (87.0312) lr 1.4258e-03 eta 0:02:24
epoch [20/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.025) loss 1.8105 (1.9912) acc 84.3750 (86.8750) lr 1.4258e-03 eta 0:02:18
epoch [21/50] batch [5/26] time 0.153 (0.298) data 0.000 (0.142) loss 2.0547 (2.0432) acc 87.5000 (83.7500) lr 1.3681e-03 eta 0:03:50
epoch [21/50] batch [10/26] time 0.152 (0.225) data 0.000 (0.071) loss 1.7852 (2.0374) acc 87.5000 (85.3125) lr 1.3681e-03 eta 0:02:53
epoch [21/50] batch [15/26] time 0.152 (0.201) data 0.000 (0.048) loss 2.2188 (1.9787) acc 84.3750 (85.6250) lr 1.3681e-03 eta 0:02:33
epoch [21/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.036) loss 1.7100 (1.9868) acc 84.3750 (85.9375) lr 1.3681e-03 eta 0:02:23
epoch [21/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.029) loss 2.0215 (2.0029) acc 84.3750 (85.8750) lr 1.3681e-03 eta 0:02:16
epoch [22/50] batch [5/26] time 0.152 (0.292) data 0.000 (0.139) loss 2.0703 (2.1895) acc 90.6250 (85.6250) lr 1.3090e-03 eta 0:03:38
epoch [22/50] batch [10/26] time 0.155 (0.223) data 0.000 (0.069) loss 1.7686 (1.9845) acc 93.7500 (89.0625) lr 1.3090e-03 eta 0:02:45
epoch [22/50] batch [15/26] time 0.152 (0.199) data 0.000 (0.046) loss 1.9570 (2.0247) acc 90.6250 (88.1250) lr 1.3090e-03 eta 0:02:27
epoch [22/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.035) loss 2.0820 (1.9892) acc 90.6250 (88.2812) lr 1.3090e-03 eta 0:02:17
epoch [22/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.028) loss 1.9824 (2.0126) acc 81.2500 (87.7500) lr 1.3090e-03 eta 0:02:11
epoch [23/50] batch [5/26] time 0.154 (0.286) data 0.000 (0.132) loss 2.1523 (2.1232) acc 81.2500 (85.6250) lr 1.2487e-03 eta 0:03:27
epoch [23/50] batch [10/26] time 0.153 (0.220) data 0.000 (0.066) loss 1.6396 (2.0436) acc 90.6250 (85.3125) lr 1.2487e-03 eta 0:02:38
epoch [23/50] batch [15/26] time 0.152 (0.197) data 0.000 (0.044) loss 2.1133 (2.0600) acc 87.5000 (85.0000) lr 1.2487e-03 eta 0:02:20
epoch [23/50] batch [20/26] time 0.152 (0.186) data 0.000 (0.033) loss 1.6406 (2.0362) acc 93.7500 (85.4688) lr 1.2487e-03 eta 0:02:11
epoch [23/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.027) loss 1.9434 (2.0383) acc 78.1250 (85.1250) lr 1.2487e-03 eta 0:02:06
epoch [24/50] batch [5/26] time 0.156 (0.290) data 0.000 (0.135) loss 1.9775 (2.2895) acc 87.5000 (84.3750) lr 1.1874e-03 eta 0:03:22
epoch [24/50] batch [10/26] time 0.154 (0.221) data 0.000 (0.068) loss 1.9883 (2.1187) acc 84.3750 (86.2500) lr 1.1874e-03 eta 0:02:33
epoch [24/50] batch [15/26] time 0.152 (0.198) data 0.000 (0.045) loss 1.8672 (2.0915) acc 96.8750 (87.2917) lr 1.1874e-03 eta 0:02:16
epoch [24/50] batch [20/26] time 0.152 (0.187) data 0.000 (0.034) loss 1.9912 (2.0857) acc 90.6250 (86.7188) lr 1.1874e-03 eta 0:02:07
epoch [24/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 2.2031 (2.0682) acc 87.5000 (87.2500) lr 1.1874e-03 eta 0:02:01
epoch [25/50] batch [5/26] time 0.155 (0.287) data 0.000 (0.131) loss 1.7627 (1.8895) acc 93.7500 (91.2500) lr 1.1253e-03 eta 0:03:12
epoch [25/50] batch [10/26] time 0.153 (0.220) data 0.000 (0.066) loss 1.5664 (1.8781) acc 93.7500 (89.3750) lr 1.1253e-03 eta 0:02:26
epoch [25/50] batch [15/26] time 0.152 (0.198) data 0.000 (0.044) loss 1.9346 (1.9316) acc 90.6250 (87.9167) lr 1.1253e-03 eta 0:02:10
epoch [25/50] batch [20/26] time 0.153 (0.187) data 0.001 (0.033) loss 2.1387 (1.9135) acc 84.3750 (88.5938) lr 1.1253e-03 eta 0:02:02
epoch [25/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 2.5488 (1.9643) acc 78.1250 (87.7500) lr 1.1253e-03 eta 0:01:57
epoch [26/50] batch [5/26] time 0.154 (0.329) data 0.000 (0.175) loss 2.3223 (1.9348) acc 84.3750 (86.2500) lr 1.0628e-03 eta 0:03:32
epoch [26/50] batch [10/26] time 0.155 (0.242) data 0.000 (0.088) loss 2.3008 (1.9457) acc 81.2500 (85.9375) lr 1.0628e-03 eta 0:02:34
epoch [26/50] batch [15/26] time 0.152 (0.212) data 0.000 (0.058) loss 1.9277 (1.9800) acc 90.6250 (87.2917) lr 1.0628e-03 eta 0:02:14
epoch [26/50] batch [20/26] time 0.153 (0.197) data 0.000 (0.044) loss 2.1348 (1.9743) acc 81.2500 (87.0312) lr 1.0628e-03 eta 0:02:04
epoch [26/50] batch [25/26] time 0.152 (0.188) data 0.000 (0.035) loss 1.9717 (1.9745) acc 84.3750 (87.5000) lr 1.0628e-03 eta 0:01:57
epoch [27/50] batch [5/26] time 0.154 (0.280) data 0.000 (0.126) loss 1.8457 (2.1689) acc 87.5000 (82.5000) lr 1.0000e-03 eta 0:02:53
epoch [27/50] batch [10/26] time 0.153 (0.217) data 0.000 (0.063) loss 1.7754 (2.0523) acc 87.5000 (84.3750) lr 1.0000e-03 eta 0:02:13
epoch [27/50] batch [15/26] time 0.152 (0.195) data 0.000 (0.042) loss 1.8066 (2.0191) acc 87.5000 (85.2083) lr 1.0000e-03 eta 0:01:59
epoch [27/50] batch [20/26] time 0.152 (0.185) data 0.000 (0.032) loss 1.6777 (2.0021) acc 93.7500 (86.2500) lr 1.0000e-03 eta 0:01:51
epoch [27/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.025) loss 2.5469 (1.9730) acc 78.1250 (86.7500) lr 1.0000e-03 eta 0:01:46
epoch [28/50] batch [5/26] time 0.152 (0.275) data 0.000 (0.121) loss 2.1230 (2.0041) acc 84.3750 (85.6250) lr 9.3721e-04 eta 0:02:43
epoch [28/50] batch [10/26] time 0.153 (0.214) data 0.000 (0.061) loss 2.4785 (2.0822) acc 78.1250 (83.7500) lr 9.3721e-04 eta 0:02:06
epoch [28/50] batch [15/26] time 0.152 (0.194) data 0.000 (0.040) loss 1.6992 (2.0041) acc 87.5000 (85.2083) lr 9.3721e-04 eta 0:01:52
epoch [28/50] batch [20/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.6514 (2.0179) acc 93.7500 (85.9375) lr 9.3721e-04 eta 0:01:45
epoch [28/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.024) loss 2.0117 (2.0309) acc 81.2500 (85.6250) lr 9.3721e-04 eta 0:01:41
epoch [29/50] batch [5/26] time 0.154 (0.271) data 0.000 (0.116) loss 1.8154 (2.1666) acc 90.6250 (82.5000) lr 8.7467e-04 eta 0:02:33
epoch [29/50] batch [10/26] time 0.155 (0.213) data 0.000 (0.058) loss 1.9980 (2.1634) acc 81.2500 (83.1250) lr 8.7467e-04 eta 0:01:59
epoch [29/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.039) loss 1.6230 (2.0201) acc 93.7500 (86.4583) lr 8.7467e-04 eta 0:01:47
epoch [29/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.3301 (2.0031) acc 71.8750 (86.4062) lr 8.7467e-04 eta 0:01:41
epoch [29/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.023) loss 1.6885 (1.9644) acc 90.6250 (87.1250) lr 8.7467e-04 eta 0:01:36
epoch [30/50] batch [5/26] time 0.156 (0.268) data 0.000 (0.113) loss 1.4268 (1.9311) acc 96.8750 (90.0000) lr 8.1262e-04 eta 0:02:25
epoch [30/50] batch [10/26] time 0.154 (0.211) data 0.000 (0.056) loss 1.4385 (1.7254) acc 96.8750 (92.1875) lr 8.1262e-04 eta 0:01:53
epoch [30/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.038) loss 2.4980 (1.8405) acc 81.2500 (90.0000) lr 8.1262e-04 eta 0:01:41
epoch [30/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.028) loss 1.6934 (1.8927) acc 90.6250 (88.9062) lr 8.1262e-04 eta 0:01:35
epoch [30/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.023) loss 2.3379 (1.9060) acc 78.1250 (88.7500) lr 8.1262e-04 eta 0:01:31
epoch [31/50] batch [5/26] time 0.156 (0.272) data 0.000 (0.118) loss 2.0176 (1.7773) acc 90.6250 (91.8750) lr 7.5131e-04 eta 0:02:20
epoch [31/50] batch [10/26] time 0.156 (0.214) data 0.000 (0.059) loss 2.0488 (1.7981) acc 84.3750 (91.5625) lr 7.5131e-04 eta 0:01:49
epoch [31/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.8262 (1.8433) acc 87.5000 (90.2083) lr 7.5131e-04 eta 0:01:37
epoch [31/50] batch [20/26] time 0.155 (0.186) data 0.001 (0.030) loss 1.7549 (1.8643) acc 93.7500 (90.0000) lr 7.5131e-04 eta 0:01:33
epoch [31/50] batch [25/26] time 0.154 (0.187) data 0.000 (0.028) loss 1.7207 (1.8539) acc 93.7500 (90.3750) lr 7.5131e-04 eta 0:01:32
epoch [32/50] batch [5/26] time 0.153 (0.274) data 0.000 (0.119) loss 1.8965 (1.8836) acc 90.6250 (90.0000) lr 6.9098e-04 eta 0:02:13
epoch [32/50] batch [10/26] time 0.153 (0.214) data 0.000 (0.060) loss 1.6484 (1.9130) acc 90.6250 (87.8125) lr 6.9098e-04 eta 0:01:43
epoch [32/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.040) loss 2.1406 (1.9098) acc 84.3750 (88.3333) lr 6.9098e-04 eta 0:01:32
epoch [32/50] batch [20/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.8906 (1.8808) acc 90.6250 (89.3750) lr 6.9098e-04 eta 0:01:26
epoch [32/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.024) loss 1.9541 (1.8909) acc 93.7500 (88.7500) lr 6.9098e-04 eta 0:01:22
epoch [33/50] batch [5/26] time 0.154 (0.279) data 0.000 (0.123) loss 2.9004 (2.0266) acc 75.0000 (88.7500) lr 6.3188e-04 eta 0:02:09
epoch [33/50] batch [10/26] time 0.154 (0.217) data 0.000 (0.062) loss 1.6650 (1.9762) acc 93.7500 (88.7500) lr 6.3188e-04 eta 0:01:39
epoch [33/50] batch [15/26] time 0.153 (0.196) data 0.000 (0.041) loss 1.9551 (1.9852) acc 84.3750 (87.5000) lr 6.3188e-04 eta 0:01:28
epoch [33/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.6279 (1.9508) acc 93.7500 (87.9688) lr 6.3188e-04 eta 0:01:22
epoch [33/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.025) loss 1.7754 (1.9166) acc 90.6250 (88.2500) lr 6.3188e-04 eta 0:01:19
epoch [34/50] batch [5/26] time 0.154 (0.269) data 0.000 (0.114) loss 1.4990 (1.8045) acc 93.7500 (90.6250) lr 5.7422e-04 eta 0:01:57
epoch [34/50] batch [10/26] time 0.156 (0.213) data 0.000 (0.057) loss 1.8047 (1.8500) acc 87.5000 (89.6875) lr 5.7422e-04 eta 0:01:31
epoch [34/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.038) loss 1.6035 (1.8406) acc 87.5000 (89.5833) lr 5.7422e-04 eta 0:01:22
epoch [34/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.6641 (1.8866) acc 75.0000 (88.2812) lr 5.7422e-04 eta 0:01:17
epoch [34/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.023) loss 1.4609 (1.8591) acc 100.0000 (89.0000) lr 5.7422e-04 eta 0:01:13
epoch [35/50] batch [5/26] time 0.158 (0.277) data 0.004 (0.123) loss 2.4863 (2.1055) acc 78.1250 (84.3750) lr 5.1825e-04 eta 0:01:53
epoch [35/50] batch [10/26] time 0.154 (0.215) data 0.000 (0.061) loss 2.1953 (2.0334) acc 90.6250 (85.6250) lr 5.1825e-04 eta 0:01:27
epoch [35/50] batch [15/26] time 0.152 (0.194) data 0.000 (0.041) loss 1.7490 (1.9247) acc 90.6250 (87.7083) lr 5.1825e-04 eta 0:01:17
epoch [35/50] batch [20/26] time 0.151 (0.183) data 0.000 (0.031) loss 2.0195 (1.9354) acc 84.3750 (88.2812) lr 5.1825e-04 eta 0:01:12
epoch [35/50] batch [25/26] time 0.151 (0.177) data 0.000 (0.025) loss 1.7285 (1.8932) acc 90.6250 (88.8750) lr 5.1825e-04 eta 0:01:09
epoch [36/50] batch [5/26] time 0.157 (0.265) data 0.004 (0.111) loss 1.8672 (2.1187) acc 87.5000 (86.8750) lr 4.6417e-04 eta 0:01:42
epoch [36/50] batch [10/26] time 0.156 (0.210) data 0.000 (0.056) loss 2.2734 (2.0063) acc 84.3750 (87.1875) lr 4.6417e-04 eta 0:01:19
epoch [36/50] batch [15/26] time 0.153 (0.191) data 0.000 (0.037) loss 1.8838 (1.9342) acc 90.6250 (88.1250) lr 4.6417e-04 eta 0:01:11
epoch [36/50] batch [20/26] time 0.152 (0.181) data 0.000 (0.028) loss 1.9678 (1.9342) acc 93.7500 (88.7500) lr 4.6417e-04 eta 0:01:07
epoch [36/50] batch [25/26] time 0.152 (0.176) data 0.000 (0.022) loss 1.6475 (1.9929) acc 90.6250 (88.2500) lr 4.6417e-04 eta 0:01:04
epoch [37/50] batch [5/26] time 0.154 (0.272) data 0.000 (0.115) loss 1.4629 (1.8990) acc 96.8750 (88.7500) lr 4.1221e-04 eta 0:01:37
epoch [37/50] batch [10/26] time 0.154 (0.214) data 0.000 (0.058) loss 1.6191 (1.8669) acc 93.7500 (89.6875) lr 4.1221e-04 eta 0:01:15
epoch [37/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.039) loss 2.6348 (1.8984) acc 81.2500 (88.7500) lr 4.1221e-04 eta 0:01:07
epoch [37/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.029) loss 2.0801 (1.8847) acc 81.2500 (88.7500) lr 4.1221e-04 eta 0:01:03
epoch [37/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.023) loss 2.0391 (1.9158) acc 87.5000 (88.1250) lr 4.1221e-04 eta 0:01:00
epoch [38/50] batch [5/26] time 0.154 (0.284) data 0.000 (0.129) loss 2.1074 (1.8479) acc 93.7500 (90.0000) lr 3.6258e-04 eta 0:01:34
epoch [38/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.065) loss 2.0938 (1.9470) acc 84.3750 (89.3750) lr 3.6258e-04 eta 0:01:11
epoch [38/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.7041 (1.9133) acc 93.7500 (90.0000) lr 3.6258e-04 eta 0:01:03
epoch [38/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.8945 (1.8391) acc 93.7500 (91.2500) lr 3.6258e-04 eta 0:00:59
epoch [38/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.7324 (1.8539) acc 93.7500 (90.8750) lr 3.6258e-04 eta 0:00:56
epoch [39/50] batch [5/26] time 0.153 (0.284) data 0.000 (0.128) loss 1.9023 (2.1262) acc 87.5000 (86.8750) lr 3.1545e-04 eta 0:01:27
epoch [39/50] batch [10/26] time 0.155 (0.219) data 0.000 (0.064) loss 1.5293 (2.0119) acc 90.6250 (87.5000) lr 3.1545e-04 eta 0:01:06
epoch [39/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 2.0469 (1.9115) acc 81.2500 (88.1250) lr 3.1545e-04 eta 0:00:58
epoch [39/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.5645 (1.8717) acc 96.8750 (89.2188) lr 3.1545e-04 eta 0:00:54
epoch [39/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.8076 (1.8489) acc 87.5000 (89.6250) lr 3.1545e-04 eta 0:00:51
epoch [40/50] batch [5/26] time 0.158 (0.271) data 0.000 (0.114) loss 1.5137 (1.7609) acc 93.7500 (89.3750) lr 2.7103e-04 eta 0:01:16
epoch [40/50] batch [10/26] time 0.157 (0.213) data 0.000 (0.057) loss 2.1328 (1.7473) acc 81.2500 (90.6250) lr 2.7103e-04 eta 0:00:58
epoch [40/50] batch [15/26] time 0.154 (0.194) data 0.000 (0.038) loss 1.9551 (1.8820) acc 87.5000 (88.5417) lr 2.7103e-04 eta 0:00:52
epoch [40/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 1.4688 (1.8587) acc 100.0000 (89.5312) lr 2.7103e-04 eta 0:00:48
epoch [40/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.023) loss 1.6260 (1.8093) acc 93.7500 (90.0000) lr 2.7103e-04 eta 0:00:46
epoch [41/50] batch [5/26] time 0.154 (0.270) data 0.000 (0.115) loss 1.3828 (1.7289) acc 96.8750 (90.6250) lr 2.2949e-04 eta 0:01:08
epoch [41/50] batch [10/26] time 0.155 (0.213) data 0.000 (0.058) loss 1.8838 (1.8520) acc 87.5000 (88.7500) lr 2.2949e-04 eta 0:00:53
epoch [41/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.038) loss 1.6201 (1.8758) acc 90.6250 (88.7500) lr 2.2949e-04 eta 0:00:47
epoch [41/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 1.6309 (1.9102) acc 90.6250 (88.5938) lr 2.2949e-04 eta 0:00:43
epoch [41/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.023) loss 2.0469 (1.8857) acc 87.5000 (89.1250) lr 2.2949e-04 eta 0:00:41
epoch [42/50] batch [5/26] time 0.155 (0.265) data 0.000 (0.110) loss 1.8242 (1.8762) acc 90.6250 (91.8750) lr 1.9098e-04 eta 0:01:00
epoch [42/50] batch [10/26] time 0.153 (0.209) data 0.000 (0.055) loss 1.5508 (1.7678) acc 100.0000 (92.1875) lr 1.9098e-04 eta 0:00:46
epoch [42/50] batch [15/26] time 0.153 (0.191) data 0.000 (0.037) loss 1.9043 (1.7919) acc 90.6250 (91.2500) lr 1.9098e-04 eta 0:00:41
epoch [42/50] batch [20/26] time 0.153 (0.181) data 0.000 (0.028) loss 1.6602 (1.7994) acc 96.8750 (91.0938) lr 1.9098e-04 eta 0:00:38
epoch [42/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.022) loss 1.6416 (1.8591) acc 87.5000 (90.1250) lr 1.9098e-04 eta 0:00:36
epoch [43/50] batch [5/26] time 0.154 (0.274) data 0.000 (0.118) loss 1.5195 (1.6969) acc 93.7500 (91.8750) lr 1.5567e-04 eta 0:00:55
epoch [43/50] batch [10/26] time 0.156 (0.214) data 0.000 (0.059) loss 1.7627 (1.9089) acc 93.7500 (88.7500) lr 1.5567e-04 eta 0:00:42
epoch [43/50] batch [15/26] time 0.154 (0.194) data 0.000 (0.040) loss 1.7715 (1.8710) acc 96.8750 (89.7917) lr 1.5567e-04 eta 0:00:37
epoch [43/50] batch [20/26] time 0.155 (0.184) data 0.000 (0.030) loss 2.6172 (1.9037) acc 78.1250 (88.9062) lr 1.5567e-04 eta 0:00:34
epoch [43/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 1.6172 (1.9157) acc 93.7500 (88.8750) lr 1.5567e-04 eta 0:00:32
epoch [44/50] batch [5/26] time 0.157 (0.272) data 0.000 (0.117) loss 1.5312 (1.7469) acc 96.8750 (93.1250) lr 1.2369e-04 eta 0:00:48
epoch [44/50] batch [10/26] time 0.154 (0.214) data 0.000 (0.059) loss 1.7539 (1.7541) acc 87.5000 (91.8750) lr 1.2369e-04 eta 0:00:36
epoch [44/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.039) loss 2.0117 (1.7834) acc 87.5000 (91.4583) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.030) loss 1.7773 (1.7255) acc 93.7500 (92.6562) lr 1.2369e-04 eta 0:00:29
epoch [44/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.024) loss 1.7734 (1.7421) acc 93.7500 (92.2500) lr 1.2369e-04 eta 0:00:27
epoch [45/50] batch [5/26] time 0.155 (0.267) data 0.000 (0.112) loss 2.3418 (2.0912) acc 84.3750 (88.1250) lr 9.5173e-05 eta 0:00:40
epoch [45/50] batch [10/26] time 0.154 (0.212) data 0.000 (0.056) loss 2.1191 (1.9938) acc 84.3750 (88.7500) lr 9.5173e-05 eta 0:00:30
epoch [45/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.038) loss 1.9316 (1.8826) acc 87.5000 (89.5833) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [20/26] time 0.155 (0.183) data 0.000 (0.028) loss 1.6777 (1.8468) acc 96.8750 (90.1562) lr 9.5173e-05 eta 0:00:24
epoch [45/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.023) loss 2.0195 (1.8255) acc 78.1250 (90.5000) lr 9.5173e-05 eta 0:00:23
epoch [46/50] batch [5/26] time 0.157 (0.277) data 0.000 (0.120) loss 1.7529 (1.7463) acc 87.5000 (91.2500) lr 7.0224e-05 eta 0:00:34
epoch [46/50] batch [10/26] time 0.153 (0.217) data 0.000 (0.060) loss 1.9551 (1.7197) acc 90.6250 (91.5625) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [15/26] time 0.153 (0.196) data 0.000 (0.040) loss 2.2598 (1.7648) acc 87.5000 (91.2500) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.030) loss 1.9941 (1.7781) acc 87.5000 (90.7812) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.152 (0.179) data 0.000 (0.024) loss 1.6650 (1.7833) acc 93.7500 (90.8750) lr 7.0224e-05 eta 0:00:18
epoch [47/50] batch [5/26] time 0.154 (0.274) data 0.000 (0.118) loss 1.9854 (2.0662) acc 84.3750 (85.0000) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [10/26] time 0.153 (0.215) data 0.000 (0.059) loss 1.7676 (1.9491) acc 90.6250 (88.1250) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.5391 (1.9084) acc 90.6250 (88.5417) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.030) loss 1.6377 (1.8279) acc 93.7500 (89.8438) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 2.0879 (1.8247) acc 84.3750 (89.8750) lr 4.8943e-05 eta 0:00:14
epoch [48/50] batch [5/26] time 0.157 (0.283) data 0.000 (0.128) loss 1.8115 (1.9414) acc 87.5000 (86.8750) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.064) loss 2.1855 (1.8912) acc 78.1250 (86.5625) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 2.2402 (1.9010) acc 87.5000 (88.3333) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/26] time 0.154 (0.186) data 0.000 (0.032) loss 1.4121 (1.8941) acc 96.8750 (88.4375) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/26] time 0.152 (0.179) data 0.000 (0.026) loss 1.8477 (1.8666) acc 93.7500 (89.2500) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.154 (0.267) data 0.000 (0.111) loss 2.1992 (1.8643) acc 90.6250 (91.2500) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [10/26] time 0.156 (0.211) data 0.000 (0.056) loss 1.8311 (1.8451) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.037) loss 1.7158 (1.8277) acc 93.7500 (90.6250) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.8809 (1.8116) acc 90.6250 (90.9375) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.022) loss 1.6270 (1.8210) acc 96.8750 (90.8750) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.158 (0.266) data 0.004 (0.111) loss 1.7578 (1.8732) acc 90.6250 (91.8750) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/26] time 0.155 (0.211) data 0.000 (0.055) loss 1.8467 (1.9186) acc 90.6250 (88.7500) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.037) loss 1.5762 (1.8818) acc 90.6250 (89.1667) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.028) loss 2.5039 (1.9067) acc 81.2500 (89.3750) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.022) loss 1.5723 (1.8596) acc 93.7500 (89.7500) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [00:05<02:41,  5.39s/it]  6%|▋         | 2/31 [00:06<01:23,  2.89s/it] 10%|▉         | 3/31 [00:07<00:58,  2.09s/it] 13%|█▎        | 4/31 [00:08<00:46,  1.71s/it] 16%|█▌        | 5/31 [00:09<00:39,  1.51s/it] 19%|█▉        | 6/31 [00:11<00:34,  1.38s/it] 23%|██▎       | 7/31 [00:12<00:31,  1.30s/it] 26%|██▌       | 8/31 [00:13<00:28,  1.25s/it] 29%|██▉       | 9/31 [00:14<00:26,  1.22s/it] 32%|███▏      | 10/31 [00:15<00:25,  1.19s/it] 35%|███▌      | 11/31 [00:16<00:23,  1.18s/it] 39%|███▊      | 12/31 [00:17<00:22,  1.16s/it] 42%|████▏     | 13/31 [00:19<00:20,  1.16s/it] 45%|████▌     | 14/31 [00:20<00:19,  1.15s/it] 48%|████▊     | 15/31 [00:21<00:18,  1.15s/it] 52%|█████▏    | 16/31 [00:23<00:20,  1.38s/it] 55%|█████▍    | 17/31 [00:24<00:18,  1.30s/it] 58%|█████▊    | 18/31 [00:25<00:16,  1.25s/it] 61%|██████▏   | 19/31 [00:26<00:14,  1.22s/it] 65%|██████▍   | 20/31 [00:27<00:13,  1.19s/it] 68%|██████▊   | 21/31 [00:28<00:11,  1.18s/it] 71%|███████   | 22/31 [00:30<00:10,  1.17s/it] 74%|███████▍  | 23/31 [00:31<00:09,  1.16s/it] 77%|███████▋  | 24/31 [00:32<00:08,  1.15s/it] 81%|████████  | 25/31 [00:33<00:06,  1.15s/it] 84%|████████▍ | 26/31 [00:34<00:05,  1.14s/it] 87%|████████▋ | 27/31 [00:35<00:04,  1.14s/it] 90%|█████████ | 28/31 [00:36<00:03,  1.14s/it] 94%|█████████▎| 29/31 [00:38<00:02,  1.14s/it] 97%|█████████▋| 30/31 [00:39<00:01,  1.14s/it]100%|██████████| 31/31 [00:39<00:00,  1.02s/it]100%|██████████| 31/31 [00:40<00:00,  1.29s/it]
=> result
* total: 15,300
* correct: 13,888
* accuracy: 90.8%
* error: 9.2%
* macro_f1: 90.8%
Elapsed: 0:04:35
Run this job and save the output to output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  51
# train_x  816
# val      204
# test     15,300
---------  -------
['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X apple pie, a type of food.', 'X X X X baby back ribs, a type of food.', 'X X X X baklava, a type of food.', 'X X X X beef carpaccio, a type of food.', 'X X X X beef tartare, a type of food.', 'X X X X beet salad, a type of food.', 'X X X X beignets, a type of food.', 'X X X X bibimbap, a type of food.', 'X X X X bread pudding, a type of food.', 'X X X X breakfast burrito, a type of food.', 'X X X X bruschetta, a type of food.', 'X X X X caesar salad, a type of food.', 'X X X X cannoli, a type of food.', 'X X X X caprese salad, a type of food.', 'X X X X carrot cake, a type of food.', 'X X X X ceviche, a type of food.', 'X X X X cheese plate, a type of food.', 'X X X X cheesecake, a type of food.', 'X X X X chicken curry, a type of food.', 'X X X X chicken quesadilla, a type of food.', 'X X X X chicken wings, a type of food.', 'X X X X chocolate cake, a type of food.', 'X X X X chocolate mousse, a type of food.', 'X X X X churros, a type of food.', 'X X X X clam chowder, a type of food.', 'X X X X club sandwich, a type of food.', 'X X X X crab cakes, a type of food.', 'X X X X creme brulee, a type of food.', 'X X X X croque madame, a type of food.', 'X X X X cup cakes, a type of food.', 'X X X X deviled eggs, a type of food.', 'X X X X donuts, a type of food.', 'X X X X dumplings, a type of food.', 'X X X X edamame, a type of food.', 'X X X X eggs benedict, a type of food.', 'X X X X escargots, a type of food.', 'X X X X falafel, a type of food.', 'X X X X filet mignon, a type of food.', 'X X X X fish and chips, a type of food.', 'X X X X foie gras, a type of food.', 'X X X X french fries, a type of food.', 'X X X X french onion soup, a type of food.', 'X X X X french toast, a type of food.', 'X X X X fried calamari, a type of food.', 'X X X X fried rice, a type of food.', 'X X X X frozen yogurt, a type of food.', 'X X X X garlic bread, a type of food.', 'X X X X gnocchi, a type of food.', 'X X X X greek salad, a type of food.', 'X X X X grilled cheese sandwich, a type of food.', 'X X X X grilled salmon, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/26] time 0.153 (0.300) data 0.000 (0.137) loss 4.9648 (5.1164) acc 84.3750 (83.1250) lr 1.0000e-05 eta 0:06:28
epoch [1/50] batch [10/26] time 0.156 (0.227) data 0.001 (0.069) loss 5.2148 (5.1719) acc 71.8750 (79.0625) lr 1.0000e-05 eta 0:04:53
epoch [1/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.046) loss 5.0391 (5.0698) acc 78.1250 (80.0000) lr 1.0000e-05 eta 0:04:20
epoch [1/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.035) loss 4.9180 (5.0311) acc 84.3750 (79.5312) lr 1.0000e-05 eta 0:04:03
epoch [1/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.028) loss 5.0273 (5.0194) acc 81.2500 (79.7500) lr 1.0000e-05 eta 0:03:52
epoch [2/50] batch [5/26] time 0.152 (0.297) data 0.000 (0.141) loss 3.6719 (4.0551) acc 78.1250 (83.1250) lr 2.0000e-03 eta 0:06:16
epoch [2/50] batch [10/26] time 0.154 (0.226) data 0.000 (0.071) loss 3.5020 (3.8260) acc 87.5000 (84.6875) lr 2.0000e-03 eta 0:04:45
epoch [2/50] batch [15/26] time 0.153 (0.202) data 0.000 (0.047) loss 3.2891 (3.7142) acc 81.2500 (83.5417) lr 2.0000e-03 eta 0:04:14
epoch [2/50] batch [20/26] time 0.154 (0.190) data 0.001 (0.036) loss 3.8730 (3.5896) acc 71.8750 (83.2812) lr 2.0000e-03 eta 0:03:57
epoch [2/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.029) loss 3.1660 (3.5263) acc 78.1250 (82.5000) lr 2.0000e-03 eta 0:03:47
epoch [3/50] batch [5/26] time 0.156 (0.302) data 0.004 (0.149) loss 2.9727 (2.8992) acc 84.3750 (84.3750) lr 1.9980e-03 eta 0:06:15
epoch [3/50] batch [10/26] time 0.153 (0.228) data 0.000 (0.075) loss 2.3770 (2.7963) acc 93.7500 (85.3125) lr 1.9980e-03 eta 0:04:42
epoch [3/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.050) loss 3.1367 (2.8609) acc 78.1250 (83.3333) lr 1.9980e-03 eta 0:04:10
epoch [3/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.037) loss 2.6953 (2.7848) acc 90.6250 (85.0000) lr 1.9980e-03 eta 0:03:53
epoch [3/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 3.5625 (2.8139) acc 68.7500 (84.0000) lr 1.9980e-03 eta 0:03:43
epoch [4/50] batch [5/26] time 0.151 (0.294) data 0.000 (0.141) loss 2.6699 (2.6617) acc 78.1250 (83.1250) lr 1.9921e-03 eta 0:05:57
epoch [4/50] batch [10/26] time 0.152 (0.224) data 0.000 (0.071) loss 2.5391 (2.6359) acc 87.5000 (83.7500) lr 1.9921e-03 eta 0:04:31
epoch [4/50] batch [15/26] time 0.156 (0.201) data 0.000 (0.047) loss 2.7227 (2.6129) acc 78.1250 (83.7500) lr 1.9921e-03 eta 0:04:03
epoch [4/50] batch [20/26] time 0.157 (0.190) data 0.000 (0.035) loss 2.4531 (2.5631) acc 87.5000 (84.5312) lr 1.9921e-03 eta 0:03:48
epoch [4/50] batch [25/26] time 0.157 (0.183) data 0.000 (0.028) loss 2.7051 (2.5870) acc 84.3750 (83.5000) lr 1.9921e-03 eta 0:03:39
epoch [5/50] batch [5/26] time 0.151 (0.288) data 0.000 (0.136) loss 2.3184 (2.4816) acc 81.2500 (81.2500) lr 1.9823e-03 eta 0:05:42
epoch [5/50] batch [10/26] time 0.155 (0.220) data 0.000 (0.068) loss 1.9648 (2.3854) acc 84.3750 (83.4375) lr 1.9823e-03 eta 0:04:20
epoch [5/50] batch [15/26] time 0.159 (0.199) data 0.000 (0.045) loss 2.3398 (2.4053) acc 81.2500 (83.7500) lr 1.9823e-03 eta 0:03:55
epoch [5/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.034) loss 2.1367 (2.4083) acc 87.5000 (83.9062) lr 1.9823e-03 eta 0:03:40
epoch [5/50] batch [25/26] time 0.158 (0.181) data 0.000 (0.027) loss 2.5684 (2.4385) acc 84.3750 (83.8750) lr 1.9823e-03 eta 0:03:31
epoch [6/50] batch [5/26] time 0.155 (0.308) data 0.000 (0.149) loss 2.5566 (2.5469) acc 90.6250 (83.1250) lr 1.9686e-03 eta 0:05:58
epoch [6/50] batch [10/26] time 0.154 (0.234) data 0.000 (0.075) loss 2.0996 (2.3771) acc 87.5000 (82.8125) lr 1.9686e-03 eta 0:04:30
epoch [6/50] batch [15/26] time 0.153 (0.207) data 0.001 (0.050) loss 2.2598 (2.3150) acc 84.3750 (84.5833) lr 1.9686e-03 eta 0:03:58
epoch [6/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.038) loss 2.5430 (2.3109) acc 84.3750 (85.3125) lr 1.9686e-03 eta 0:03:42
epoch [6/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.030) loss 2.8711 (2.3247) acc 75.0000 (85.1250) lr 1.9686e-03 eta 0:03:32
epoch [7/50] batch [5/26] time 0.154 (0.319) data 0.000 (0.165) loss 2.0781 (2.2930) acc 90.6250 (84.3750) lr 1.9511e-03 eta 0:06:03
epoch [7/50] batch [10/26] time 0.154 (0.237) data 0.000 (0.083) loss 1.9404 (2.2495) acc 90.6250 (84.0625) lr 1.9511e-03 eta 0:04:28
epoch [7/50] batch [15/26] time 0.154 (0.211) data 0.001 (0.055) loss 2.5430 (2.1878) acc 84.3750 (84.3750) lr 1.9511e-03 eta 0:03:58
epoch [7/50] batch [20/26] time 0.154 (0.198) data 0.000 (0.042) loss 1.7900 (2.1729) acc 93.7500 (85.4688) lr 1.9511e-03 eta 0:03:42
epoch [7/50] batch [25/26] time 0.153 (0.190) data 0.000 (0.034) loss 2.4199 (2.2274) acc 81.2500 (84.8750) lr 1.9511e-03 eta 0:03:32
epoch [8/50] batch [5/26] time 0.154 (0.300) data 0.000 (0.144) loss 2.2480 (2.2873) acc 81.2500 (81.8750) lr 1.9298e-03 eta 0:05:33
epoch [8/50] batch [10/26] time 0.153 (0.227) data 0.000 (0.072) loss 1.6748 (2.1649) acc 96.8750 (85.6250) lr 1.9298e-03 eta 0:04:11
epoch [8/50] batch [15/26] time 0.156 (0.204) data 0.000 (0.048) loss 1.7178 (2.1866) acc 90.6250 (86.0417) lr 1.9298e-03 eta 0:03:44
epoch [8/50] batch [20/26] time 0.155 (0.192) data 0.000 (0.036) loss 2.0625 (2.2341) acc 87.5000 (85.3125) lr 1.9298e-03 eta 0:03:30
epoch [8/50] batch [25/26] time 0.157 (0.185) data 0.000 (0.029) loss 2.6914 (2.2143) acc 78.1250 (85.8750) lr 1.9298e-03 eta 0:03:21
epoch [9/50] batch [5/26] time 0.153 (0.300) data 0.000 (0.147) loss 1.8789 (2.1631) acc 93.7500 (88.7500) lr 1.9048e-03 eta 0:05:26
epoch [9/50] batch [10/26] time 0.155 (0.227) data 0.000 (0.073) loss 1.6387 (2.0837) acc 93.7500 (88.4375) lr 1.9048e-03 eta 0:04:05
epoch [9/50] batch [15/26] time 0.156 (0.204) data 0.000 (0.049) loss 2.1777 (2.1020) acc 84.3750 (87.7083) lr 1.9048e-03 eta 0:03:39
epoch [9/50] batch [20/26] time 0.157 (0.192) data 0.000 (0.037) loss 2.2383 (2.1742) acc 87.5000 (86.7188) lr 1.9048e-03 eta 0:03:25
epoch [9/50] batch [25/26] time 0.156 (0.185) data 0.000 (0.029) loss 1.8867 (2.1885) acc 84.3750 (86.0000) lr 1.9048e-03 eta 0:03:17
epoch [10/50] batch [5/26] time 0.154 (0.294) data 0.000 (0.140) loss 2.4609 (2.2633) acc 78.1250 (84.3750) lr 1.8763e-03 eta 0:05:11
epoch [10/50] batch [10/26] time 0.154 (0.225) data 0.000 (0.071) loss 2.0879 (2.2038) acc 78.1250 (84.3750) lr 1.8763e-03 eta 0:03:57
epoch [10/50] batch [15/26] time 0.154 (0.201) data 0.000 (0.047) loss 2.0352 (2.2456) acc 84.3750 (83.5417) lr 1.8763e-03 eta 0:03:31
epoch [10/50] batch [20/26] time 0.154 (0.189) data 0.000 (0.035) loss 2.0312 (2.2268) acc 87.5000 (84.0625) lr 1.8763e-03 eta 0:03:17
epoch [10/50] batch [25/26] time 0.154 (0.182) data 0.001 (0.028) loss 1.6416 (2.1920) acc 93.7500 (84.6250) lr 1.8763e-03 eta 0:03:09
epoch [11/50] batch [5/26] time 0.154 (0.299) data 0.000 (0.144) loss 2.3438 (2.2076) acc 81.2500 (85.6250) lr 1.8443e-03 eta 0:05:09
epoch [11/50] batch [10/26] time 0.155 (0.227) data 0.001 (0.072) loss 1.8291 (2.0055) acc 90.6250 (87.8125) lr 1.8443e-03 eta 0:03:53
epoch [11/50] batch [15/26] time 0.153 (0.204) data 0.000 (0.048) loss 2.7871 (2.0536) acc 81.2500 (87.5000) lr 1.8443e-03 eta 0:03:28
epoch [11/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.036) loss 2.3027 (2.0762) acc 75.0000 (87.0312) lr 1.8443e-03 eta 0:03:14
epoch [11/50] batch [25/26] time 0.155 (0.184) data 0.000 (0.029) loss 1.7637 (2.1131) acc 93.7500 (86.6250) lr 1.8443e-03 eta 0:03:06
epoch [12/50] batch [5/26] time 0.158 (0.289) data 0.000 (0.133) loss 2.1582 (2.0014) acc 87.5000 (88.7500) lr 1.8090e-03 eta 0:04:51
epoch [12/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.067) loss 1.5957 (2.0087) acc 93.7500 (88.1250) lr 1.8090e-03 eta 0:03:42
epoch [12/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.045) loss 2.6582 (2.1128) acc 75.0000 (85.4167) lr 1.8090e-03 eta 0:03:18
epoch [12/50] batch [20/26] time 0.157 (0.188) data 0.001 (0.033) loss 1.6816 (2.0949) acc 93.7500 (86.7188) lr 1.8090e-03 eta 0:03:06
epoch [12/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 2.0586 (2.0871) acc 84.3750 (86.6250) lr 1.8090e-03 eta 0:02:59
epoch [13/50] batch [5/26] time 0.154 (0.300) data 0.000 (0.145) loss 1.9131 (1.9035) acc 87.5000 (88.1250) lr 1.7705e-03 eta 0:04:54
epoch [13/50] batch [10/26] time 0.154 (0.227) data 0.000 (0.073) loss 1.8359 (2.1131) acc 90.6250 (85.0000) lr 1.7705e-03 eta 0:03:42
epoch [13/50] batch [15/26] time 0.153 (0.202) data 0.000 (0.048) loss 1.9395 (2.0753) acc 87.5000 (85.4167) lr 1.7705e-03 eta 0:03:17
epoch [13/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.036) loss 2.0938 (2.0573) acc 81.2500 (85.6250) lr 1.7705e-03 eta 0:03:04
epoch [13/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.0547 (2.0777) acc 81.2500 (85.0000) lr 1.7705e-03 eta 0:02:56
epoch [14/50] batch [5/26] time 0.155 (0.285) data 0.001 (0.130) loss 1.9648 (2.0355) acc 84.3750 (86.2500) lr 1.7290e-03 eta 0:04:33
epoch [14/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.066) loss 1.7178 (2.0211) acc 93.7500 (86.2500) lr 1.7290e-03 eta 0:03:30
epoch [14/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.044) loss 1.6875 (2.0200) acc 93.7500 (86.2500) lr 1.7290e-03 eta 0:03:07
epoch [14/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.033) loss 2.6250 (2.0950) acc 75.0000 (85.3125) lr 1.7290e-03 eta 0:02:56
epoch [14/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.026) loss 1.9561 (2.0950) acc 93.7500 (85.2500) lr 1.7290e-03 eta 0:02:49
epoch [15/50] batch [5/26] time 0.155 (0.303) data 0.000 (0.148) loss 1.4131 (1.9426) acc 96.8750 (88.7500) lr 1.6845e-03 eta 0:04:41
epoch [15/50] batch [10/26] time 0.154 (0.229) data 0.000 (0.074) loss 2.3438 (2.0576) acc 87.5000 (87.1875) lr 1.6845e-03 eta 0:03:31
epoch [15/50] batch [15/26] time 0.154 (0.204) data 0.000 (0.050) loss 2.2227 (2.0579) acc 84.3750 (87.5000) lr 1.6845e-03 eta 0:03:07
epoch [15/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 2.3242 (2.0472) acc 87.5000 (87.8125) lr 1.6845e-03 eta 0:02:55
epoch [15/50] batch [25/26] time 0.154 (0.184) data 0.000 (0.030) loss 1.7188 (2.0305) acc 84.3750 (87.2500) lr 1.6845e-03 eta 0:02:47
epoch [16/50] batch [5/26] time 0.155 (0.305) data 0.000 (0.150) loss 1.6367 (1.8691) acc 93.7500 (88.7500) lr 1.6374e-03 eta 0:04:35
epoch [16/50] batch [10/26] time 0.154 (0.230) data 0.000 (0.075) loss 2.0527 (1.9333) acc 84.3750 (88.1250) lr 1.6374e-03 eta 0:03:26
epoch [16/50] batch [15/26] time 0.152 (0.204) data 0.000 (0.050) loss 2.1406 (1.9810) acc 90.6250 (87.5000) lr 1.6374e-03 eta 0:03:02
epoch [16/50] batch [20/26] time 0.152 (0.191) data 0.000 (0.038) loss 2.0918 (1.9699) acc 81.2500 (87.6562) lr 1.6374e-03 eta 0:02:50
epoch [16/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.030) loss 2.4180 (1.9925) acc 78.1250 (87.5000) lr 1.6374e-03 eta 0:02:42
epoch [17/50] batch [5/26] time 0.154 (0.293) data 0.001 (0.138) loss 2.0508 (2.0256) acc 84.3750 (83.1250) lr 1.5878e-03 eta 0:04:17
epoch [17/50] batch [10/26] time 0.158 (0.224) data 0.000 (0.069) loss 1.8955 (2.0408) acc 84.3750 (84.0625) lr 1.5878e-03 eta 0:03:15
epoch [17/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 1.4844 (1.9911) acc 93.7500 (85.8333) lr 1.5878e-03 eta 0:02:54
epoch [17/50] batch [20/26] time 0.157 (0.189) data 0.000 (0.035) loss 2.0684 (1.9645) acc 84.3750 (86.4062) lr 1.5878e-03 eta 0:02:43
epoch [17/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.028) loss 2.4980 (2.0288) acc 87.5000 (85.6250) lr 1.5878e-03 eta 0:02:36
epoch [18/50] batch [5/26] time 0.154 (0.291) data 0.000 (0.135) loss 1.6387 (1.6555) acc 96.8750 (93.1250) lr 1.5358e-03 eta 0:04:08
epoch [18/50] batch [10/26] time 0.156 (0.224) data 0.000 (0.068) loss 1.9512 (1.7535) acc 87.5000 (89.6875) lr 1.5358e-03 eta 0:03:09
epoch [18/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.045) loss 1.7812 (1.8824) acc 90.6250 (87.7083) lr 1.5358e-03 eta 0:02:48
epoch [18/50] batch [20/26] time 0.154 (0.189) data 0.000 (0.034) loss 1.8760 (1.9719) acc 90.6250 (87.3438) lr 1.5358e-03 eta 0:02:38
epoch [18/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.027) loss 1.8525 (1.9945) acc 96.8750 (87.5000) lr 1.5358e-03 eta 0:02:31
epoch [19/50] batch [5/26] time 0.157 (0.304) data 0.000 (0.148) loss 1.6709 (1.9916) acc 93.7500 (89.3750) lr 1.4818e-03 eta 0:04:11
epoch [19/50] batch [10/26] time 0.156 (0.230) data 0.000 (0.074) loss 1.6270 (1.9301) acc 96.8750 (89.3750) lr 1.4818e-03 eta 0:03:09
epoch [19/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.049) loss 1.8613 (1.9439) acc 90.6250 (88.9583) lr 1.4818e-03 eta 0:02:47
epoch [19/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.037) loss 1.7949 (1.9506) acc 84.3750 (87.6562) lr 1.4818e-03 eta 0:02:35
epoch [19/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.030) loss 1.9609 (1.9838) acc 90.6250 (87.7500) lr 1.4818e-03 eta 0:02:28
epoch [20/50] batch [5/26] time 0.159 (0.294) data 0.000 (0.137) loss 1.7002 (1.6633) acc 90.6250 (93.1250) lr 1.4258e-03 eta 0:03:55
epoch [20/50] batch [10/26] time 0.155 (0.225) data 0.000 (0.069) loss 1.7461 (1.7808) acc 93.7500 (90.9375) lr 1.4258e-03 eta 0:02:58
epoch [20/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.046) loss 1.9668 (1.8130) acc 81.2500 (89.7917) lr 1.4258e-03 eta 0:02:38
epoch [20/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.034) loss 2.1953 (1.8408) acc 84.3750 (89.3750) lr 1.4258e-03 eta 0:02:28
epoch [20/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.028) loss 2.2480 (1.8969) acc 90.6250 (88.8750) lr 1.4258e-03 eta 0:02:22
epoch [21/50] batch [5/26] time 0.154 (0.290) data 0.000 (0.136) loss 2.8066 (2.1105) acc 71.8750 (83.1250) lr 1.3681e-03 eta 0:03:45
epoch [21/50] batch [10/26] time 0.154 (0.223) data 0.000 (0.068) loss 1.9678 (1.9929) acc 87.5000 (85.0000) lr 1.3681e-03 eta 0:02:51
epoch [21/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 1.7021 (1.9199) acc 90.6250 (86.4583) lr 1.3681e-03 eta 0:02:32
epoch [21/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.034) loss 1.6602 (1.9789) acc 96.8750 (86.4062) lr 1.3681e-03 eta 0:02:23
epoch [21/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 2.0078 (1.9525) acc 87.5000 (87.1250) lr 1.3681e-03 eta 0:02:16
epoch [22/50] batch [5/26] time 0.158 (0.303) data 0.000 (0.147) loss 2.0059 (1.7393) acc 84.3750 (90.0000) lr 1.3090e-03 eta 0:03:46
epoch [22/50] batch [10/26] time 0.156 (0.229) data 0.000 (0.074) loss 2.1367 (1.7682) acc 78.1250 (89.3750) lr 1.3090e-03 eta 0:02:50
epoch [22/50] batch [15/26] time 0.154 (0.204) data 0.000 (0.049) loss 1.6230 (1.8116) acc 96.8750 (89.5833) lr 1.3090e-03 eta 0:02:30
epoch [22/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 1.5078 (1.8402) acc 90.6250 (88.4375) lr 1.3090e-03 eta 0:02:20
epoch [22/50] batch [25/26] time 0.154 (0.184) data 0.000 (0.030) loss 2.5547 (1.8696) acc 75.0000 (88.0000) lr 1.3090e-03 eta 0:02:13
epoch [23/50] batch [5/26] time 0.157 (0.299) data 0.000 (0.143) loss 1.9766 (2.0824) acc 87.5000 (85.6250) lr 1.2487e-03 eta 0:03:36
epoch [23/50] batch [10/26] time 0.156 (0.227) data 0.000 (0.071) loss 1.5176 (2.0433) acc 96.8750 (85.3125) lr 1.2487e-03 eta 0:02:43
epoch [23/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.048) loss 1.7500 (1.9367) acc 90.6250 (87.0833) lr 1.2487e-03 eta 0:02:24
epoch [23/50] batch [20/26] time 0.154 (0.191) data 0.000 (0.036) loss 1.9160 (1.9183) acc 84.3750 (87.9688) lr 1.2487e-03 eta 0:02:14
epoch [23/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.029) loss 1.8594 (1.9407) acc 87.5000 (87.5000) lr 1.2487e-03 eta 0:02:08
epoch [24/50] batch [5/26] time 0.154 (0.309) data 0.000 (0.152) loss 2.1016 (2.0187) acc 84.3750 (86.2500) lr 1.1874e-03 eta 0:03:35
epoch [24/50] batch [10/26] time 0.157 (0.232) data 0.000 (0.076) loss 2.0703 (1.9338) acc 93.7500 (88.4375) lr 1.1874e-03 eta 0:02:40
epoch [24/50] batch [15/26] time 0.153 (0.206) data 0.000 (0.051) loss 2.2812 (1.9202) acc 84.3750 (88.1250) lr 1.1874e-03 eta 0:02:21
epoch [24/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.038) loss 1.6748 (1.9150) acc 90.6250 (88.4375) lr 1.1874e-03 eta 0:02:11
epoch [24/50] batch [25/26] time 0.154 (0.185) data 0.000 (0.031) loss 1.5615 (1.9045) acc 87.5000 (88.1250) lr 1.1874e-03 eta 0:02:05
epoch [25/50] batch [5/26] time 0.154 (0.316) data 0.000 (0.160) loss 2.4570 (1.9082) acc 81.2500 (88.1250) lr 1.1253e-03 eta 0:03:31
epoch [25/50] batch [10/26] time 0.155 (0.235) data 0.000 (0.080) loss 1.6299 (1.8549) acc 93.7500 (89.6875) lr 1.1253e-03 eta 0:02:36
epoch [25/50] batch [15/26] time 0.153 (0.208) data 0.000 (0.053) loss 1.8340 (1.8434) acc 87.5000 (89.5833) lr 1.1253e-03 eta 0:02:17
epoch [25/50] batch [20/26] time 0.155 (0.194) data 0.000 (0.040) loss 1.8008 (1.8819) acc 87.5000 (88.5938) lr 1.1253e-03 eta 0:02:07
epoch [25/50] batch [25/26] time 0.154 (0.186) data 0.000 (0.032) loss 1.8398 (1.8457) acc 90.6250 (89.0000) lr 1.1253e-03 eta 0:02:01
epoch [26/50] batch [5/26] time 0.157 (0.304) data 0.000 (0.149) loss 1.9814 (1.9795) acc 87.5000 (88.7500) lr 1.0628e-03 eta 0:03:16
epoch [26/50] batch [10/26] time 0.155 (0.230) data 0.000 (0.074) loss 2.1504 (1.8964) acc 84.3750 (89.3750) lr 1.0628e-03 eta 0:02:27
epoch [26/50] batch [15/26] time 0.154 (0.205) data 0.000 (0.050) loss 2.0430 (1.8906) acc 90.6250 (89.3750) lr 1.0628e-03 eta 0:02:10
epoch [26/50] batch [20/26] time 0.154 (0.192) data 0.000 (0.037) loss 1.3496 (1.9338) acc 96.8750 (87.5000) lr 1.0628e-03 eta 0:02:01
epoch [26/50] batch [25/26] time 0.157 (0.184) data 0.000 (0.030) loss 2.1895 (1.9290) acc 78.1250 (87.2500) lr 1.0628e-03 eta 0:01:55
epoch [27/50] batch [5/26] time 0.156 (0.303) data 0.000 (0.147) loss 1.9082 (1.7465) acc 87.5000 (90.0000) lr 1.0000e-03 eta 0:03:07
epoch [27/50] batch [10/26] time 0.156 (0.229) data 0.000 (0.074) loss 1.9141 (1.9346) acc 93.7500 (87.1875) lr 1.0000e-03 eta 0:02:20
epoch [27/50] batch [15/26] time 0.155 (0.204) data 0.000 (0.049) loss 2.0508 (1.9833) acc 87.5000 (86.8750) lr 1.0000e-03 eta 0:02:04
epoch [27/50] batch [20/26] time 0.154 (0.192) data 0.000 (0.037) loss 1.9258 (1.9602) acc 81.2500 (86.8750) lr 1.0000e-03 eta 0:01:55
epoch [27/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.030) loss 1.6191 (1.9366) acc 93.7500 (87.8750) lr 1.0000e-03 eta 0:01:50
epoch [28/50] batch [5/26] time 0.153 (0.290) data 0.000 (0.135) loss 1.5723 (1.9436) acc 96.8750 (86.8750) lr 9.3721e-04 eta 0:02:51
epoch [28/50] batch [10/26] time 0.158 (0.222) data 0.001 (0.067) loss 1.6631 (1.8566) acc 93.7500 (89.0625) lr 9.3721e-04 eta 0:02:10
epoch [28/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.045) loss 1.7207 (1.8813) acc 87.5000 (88.5417) lr 9.3721e-04 eta 0:01:56
epoch [28/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.034) loss 2.3906 (1.8840) acc 81.2500 (88.2812) lr 9.3721e-04 eta 0:01:48
epoch [28/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.027) loss 1.7539 (1.9098) acc 93.7500 (87.3750) lr 9.3721e-04 eta 0:01:43
epoch [29/50] batch [5/26] time 0.154 (0.291) data 0.000 (0.136) loss 1.7803 (2.0811) acc 96.8750 (86.2500) lr 8.7467e-04 eta 0:02:44
epoch [29/50] batch [10/26] time 0.155 (0.223) data 0.001 (0.068) loss 1.6426 (1.8971) acc 93.7500 (88.7500) lr 8.7467e-04 eta 0:02:05
epoch [29/50] batch [15/26] time 0.154 (0.200) data 0.000 (0.046) loss 1.4688 (1.8373) acc 93.7500 (90.0000) lr 8.7467e-04 eta 0:01:51
epoch [29/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.034) loss 1.9141 (1.8452) acc 87.5000 (89.8438) lr 8.7467e-04 eta 0:01:43
epoch [29/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.027) loss 1.9570 (1.8836) acc 90.6250 (89.3750) lr 8.7467e-04 eta 0:01:38
epoch [30/50] batch [5/26] time 0.155 (0.287) data 0.000 (0.131) loss 1.7129 (1.9930) acc 93.7500 (86.2500) lr 8.1262e-04 eta 0:02:35
epoch [30/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.066) loss 2.1992 (1.9630) acc 81.2500 (85.6250) lr 8.1262e-04 eta 0:01:58
epoch [30/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.044) loss 2.0977 (1.9312) acc 84.3750 (86.4583) lr 8.1262e-04 eta 0:01:45
epoch [30/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.033) loss 1.9277 (1.9077) acc 93.7500 (87.3438) lr 8.1262e-04 eta 0:01:39
epoch [30/50] batch [25/26] time 0.174 (0.186) data 0.000 (0.027) loss 1.7881 (1.9412) acc 90.6250 (87.2500) lr 8.1262e-04 eta 0:01:36
epoch [31/50] batch [5/26] time 0.156 (0.285) data 0.000 (0.129) loss 1.9209 (1.8256) acc 87.5000 (89.3750) lr 7.5131e-04 eta 0:02:26
epoch [31/50] batch [10/26] time 0.157 (0.220) data 0.000 (0.065) loss 2.1895 (1.8807) acc 87.5000 (89.6875) lr 7.5131e-04 eta 0:01:52
epoch [31/50] batch [15/26] time 0.235 (0.208) data 0.000 (0.043) loss 1.6914 (1.9184) acc 87.5000 (88.7500) lr 7.5131e-04 eta 0:01:45
epoch [31/50] batch [20/26] time 0.155 (0.201) data 0.000 (0.032) loss 1.8008 (1.8734) acc 84.3750 (89.3750) lr 7.5131e-04 eta 0:01:40
epoch [31/50] batch [25/26] time 0.154 (0.191) data 0.000 (0.026) loss 1.6387 (1.8662) acc 90.6250 (89.5000) lr 7.5131e-04 eta 0:01:34
epoch [32/50] batch [5/26] time 0.153 (0.310) data 0.000 (0.154) loss 1.8359 (1.6840) acc 90.6250 (90.6250) lr 6.9098e-04 eta 0:02:31
epoch [32/50] batch [10/26] time 0.156 (0.232) data 0.000 (0.077) loss 2.0312 (1.7969) acc 84.3750 (89.0625) lr 6.9098e-04 eta 0:01:52
epoch [32/50] batch [15/26] time 0.154 (0.216) data 0.000 (0.052) loss 1.7627 (1.7682) acc 84.3750 (89.7917) lr 6.9098e-04 eta 0:01:43
epoch [32/50] batch [20/26] time 0.187 (0.210) data 0.000 (0.039) loss 1.7539 (1.8010) acc 93.7500 (89.5312) lr 6.9098e-04 eta 0:01:39
epoch [32/50] batch [25/26] time 0.154 (0.200) data 0.000 (0.031) loss 1.7188 (1.8282) acc 93.7500 (89.6250) lr 6.9098e-04 eta 0:01:33
epoch [33/50] batch [5/26] time 0.154 (0.285) data 0.000 (0.129) loss 1.9150 (1.8652) acc 84.3750 (87.5000) lr 6.3188e-04 eta 0:02:12
epoch [33/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.065) loss 1.6621 (1.8434) acc 93.7500 (89.3750) lr 6.3188e-04 eta 0:01:41
epoch [33/50] batch [15/26] time 0.154 (0.204) data 0.001 (0.043) loss 2.2637 (1.9237) acc 78.1250 (87.9167) lr 6.3188e-04 eta 0:01:32
epoch [33/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.032) loss 1.9395 (1.9128) acc 87.5000 (88.1250) lr 6.3188e-04 eta 0:01:25
epoch [33/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.026) loss 1.3867 (1.8808) acc 100.0000 (89.0000) lr 6.3188e-04 eta 0:01:21
epoch [34/50] batch [5/26] time 0.154 (0.280) data 0.001 (0.125) loss 1.4316 (1.7611) acc 93.7500 (89.3750) lr 5.7422e-04 eta 0:02:02
epoch [34/50] batch [10/26] time 0.156 (0.218) data 0.000 (0.063) loss 1.7363 (1.8551) acc 87.5000 (89.0625) lr 5.7422e-04 eta 0:01:34
epoch [34/50] batch [15/26] time 0.155 (0.197) data 0.000 (0.042) loss 1.6836 (1.8399) acc 90.6250 (89.7917) lr 5.7422e-04 eta 0:01:24
epoch [34/50] batch [20/26] time 0.154 (0.186) data 0.000 (0.032) loss 2.1016 (1.8702) acc 84.3750 (88.4375) lr 5.7422e-04 eta 0:01:18
epoch [34/50] batch [25/26] time 0.154 (0.180) data 0.000 (0.025) loss 2.1523 (1.9044) acc 87.5000 (87.7500) lr 5.7422e-04 eta 0:01:15
epoch [35/50] batch [5/26] time 0.157 (0.302) data 0.000 (0.145) loss 1.5166 (1.8463) acc 93.7500 (90.0000) lr 5.1825e-04 eta 0:02:03
epoch [35/50] batch [10/26] time 0.154 (0.228) data 0.000 (0.073) loss 1.8779 (1.9200) acc 93.7500 (89.0625) lr 5.1825e-04 eta 0:01:32
epoch [35/50] batch [15/26] time 0.155 (0.204) data 0.000 (0.049) loss 1.5947 (1.8377) acc 93.7500 (89.7917) lr 5.1825e-04 eta 0:01:21
epoch [35/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 1.9648 (1.8433) acc 90.6250 (89.6875) lr 5.1825e-04 eta 0:01:15
epoch [35/50] batch [25/26] time 0.154 (0.184) data 0.000 (0.029) loss 2.0977 (1.8490) acc 81.2500 (89.2500) lr 5.1825e-04 eta 0:01:11
epoch [36/50] batch [5/26] time 0.154 (0.304) data 0.000 (0.148) loss 2.1641 (1.9768) acc 78.1250 (85.6250) lr 4.6417e-04 eta 0:01:57
epoch [36/50] batch [10/26] time 0.155 (0.230) data 0.000 (0.074) loss 1.5850 (1.9462) acc 96.8750 (86.8750) lr 4.6417e-04 eta 0:01:27
epoch [36/50] batch [15/26] time 0.155 (0.205) data 0.000 (0.049) loss 2.0977 (1.9088) acc 90.6250 (88.3333) lr 4.6417e-04 eta 0:01:16
epoch [36/50] batch [20/26] time 0.155 (0.193) data 0.000 (0.037) loss 2.4316 (1.9141) acc 78.1250 (87.9688) lr 4.6417e-04 eta 0:01:11
epoch [36/50] batch [25/26] time 0.155 (0.185) data 0.000 (0.030) loss 1.4473 (1.8895) acc 93.7500 (88.5000) lr 4.6417e-04 eta 0:01:07
epoch [37/50] batch [5/26] time 0.157 (0.315) data 0.000 (0.160) loss 2.0020 (1.8344) acc 90.6250 (91.2500) lr 4.1221e-04 eta 0:01:53
epoch [37/50] batch [10/26] time 0.155 (0.237) data 0.000 (0.081) loss 1.7441 (1.8983) acc 87.5000 (89.3750) lr 4.1221e-04 eta 0:01:23
epoch [37/50] batch [15/26] time 0.155 (0.209) data 0.000 (0.054) loss 1.9258 (1.9014) acc 90.6250 (89.3750) lr 4.1221e-04 eta 0:01:13
epoch [37/50] batch [20/26] time 0.155 (0.196) data 0.000 (0.041) loss 1.9707 (1.9290) acc 81.2500 (87.8125) lr 4.1221e-04 eta 0:01:07
epoch [37/50] batch [25/26] time 0.154 (0.188) data 0.000 (0.032) loss 2.1719 (1.9532) acc 87.5000 (87.6250) lr 4.1221e-04 eta 0:01:03
epoch [38/50] batch [5/26] time 0.154 (0.281) data 0.000 (0.125) loss 1.7148 (2.0375) acc 90.6250 (85.6250) lr 3.6258e-04 eta 0:01:33
epoch [38/50] batch [10/26] time 0.156 (0.218) data 0.000 (0.063) loss 1.9414 (1.8837) acc 90.6250 (89.3750) lr 3.6258e-04 eta 0:01:11
epoch [38/50] batch [15/26] time 0.154 (0.197) data 0.000 (0.042) loss 1.9238 (1.8898) acc 90.6250 (89.5833) lr 3.6258e-04 eta 0:01:03
epoch [38/50] batch [20/26] time 0.154 (0.186) data 0.001 (0.031) loss 2.2480 (1.9375) acc 81.2500 (89.5312) lr 3.6258e-04 eta 0:00:59
epoch [38/50] batch [25/26] time 0.154 (0.180) data 0.000 (0.025) loss 1.8760 (1.9356) acc 87.5000 (89.0000) lr 3.6258e-04 eta 0:00:56
epoch [39/50] batch [5/26] time 0.157 (0.315) data 0.000 (0.159) loss 2.1445 (2.0502) acc 87.5000 (84.3750) lr 3.1545e-04 eta 0:01:36
epoch [39/50] batch [10/26] time 0.156 (0.235) data 0.000 (0.080) loss 1.3320 (1.8704) acc 100.0000 (88.4375) lr 3.1545e-04 eta 0:01:10
epoch [39/50] batch [15/26] time 0.154 (0.208) data 0.000 (0.053) loss 2.2227 (1.8443) acc 87.5000 (89.3750) lr 3.1545e-04 eta 0:01:01
epoch [39/50] batch [20/26] time 0.154 (0.195) data 0.000 (0.040) loss 1.9023 (1.8445) acc 87.5000 (89.2188) lr 3.1545e-04 eta 0:00:56
epoch [39/50] batch [25/26] time 0.155 (0.187) data 0.000 (0.032) loss 1.7266 (1.8480) acc 96.8750 (89.3750) lr 3.1545e-04 eta 0:00:53
epoch [40/50] batch [5/26] time 0.157 (0.302) data 0.000 (0.145) loss 1.5146 (1.8215) acc 93.7500 (87.5000) lr 2.7103e-04 eta 0:01:24
epoch [40/50] batch [10/26] time 0.154 (0.229) data 0.000 (0.073) loss 1.5469 (1.7563) acc 93.7500 (90.0000) lr 2.7103e-04 eta 0:01:03
epoch [40/50] batch [15/26] time 0.152 (0.206) data 0.000 (0.048) loss 2.0566 (1.8234) acc 81.2500 (88.7500) lr 2.7103e-04 eta 0:00:55
epoch [40/50] batch [20/26] time 0.153 (0.194) data 0.000 (0.036) loss 1.8340 (1.8790) acc 90.6250 (88.1250) lr 2.7103e-04 eta 0:00:51
epoch [40/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.029) loss 1.8213 (1.8952) acc 87.5000 (87.8750) lr 2.7103e-04 eta 0:00:48
epoch [41/50] batch [5/26] time 0.157 (0.283) data 0.004 (0.127) loss 2.1738 (1.7848) acc 84.3750 (89.3750) lr 2.2949e-04 eta 0:01:12
epoch [41/50] batch [10/26] time 0.155 (0.219) data 0.000 (0.064) loss 1.9404 (1.8218) acc 87.5000 (89.6875) lr 2.2949e-04 eta 0:00:54
epoch [41/50] batch [15/26] time 0.154 (0.197) data 0.000 (0.042) loss 2.0684 (1.8180) acc 87.5000 (89.3750) lr 2.2949e-04 eta 0:00:48
epoch [41/50] batch [20/26] time 0.156 (0.187) data 0.000 (0.032) loss 2.0059 (1.8410) acc 81.2500 (88.5938) lr 2.2949e-04 eta 0:00:44
epoch [41/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.9902 (1.8304) acc 90.6250 (89.1250) lr 2.2949e-04 eta 0:00:42
epoch [42/50] batch [5/26] time 0.154 (0.295) data 0.000 (0.140) loss 1.5410 (1.7314) acc 93.7500 (93.7500) lr 1.9098e-04 eta 0:01:07
epoch [42/50] batch [10/26] time 0.156 (0.225) data 0.000 (0.070) loss 1.7324 (1.7541) acc 90.6250 (91.8750) lr 1.9098e-04 eta 0:00:50
epoch [42/50] batch [15/26] time 0.154 (0.201) data 0.000 (0.047) loss 1.7500 (1.7560) acc 90.6250 (91.4583) lr 1.9098e-04 eta 0:00:44
epoch [42/50] batch [20/26] time 0.154 (0.189) data 0.000 (0.035) loss 1.6025 (1.7156) acc 93.7500 (91.4062) lr 1.9098e-04 eta 0:00:40
epoch [42/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.028) loss 2.0039 (1.7168) acc 84.3750 (91.5000) lr 1.9098e-04 eta 0:00:38
epoch [43/50] batch [5/26] time 0.154 (0.283) data 0.000 (0.128) loss 1.6895 (1.6713) acc 96.8750 (95.0000) lr 1.5567e-04 eta 0:00:57
epoch [43/50] batch [10/26] time 0.155 (0.219) data 0.001 (0.064) loss 1.5547 (1.7283) acc 93.7500 (93.1250) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 2.2598 (1.7721) acc 75.0000 (91.8750) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [20/26] time 0.154 (0.186) data 0.000 (0.032) loss 1.8203 (1.8372) acc 90.6250 (90.6250) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/26] time 0.154 (0.180) data 0.000 (0.026) loss 2.1133 (1.8159) acc 87.5000 (90.7500) lr 1.5567e-04 eta 0:00:32
epoch [44/50] batch [5/26] time 0.157 (0.326) data 0.000 (0.170) loss 1.4893 (1.5211) acc 96.8750 (96.2500) lr 1.2369e-04 eta 0:00:57
epoch [44/50] batch [10/26] time 0.156 (0.241) data 0.000 (0.085) loss 1.8936 (1.7145) acc 90.6250 (93.1250) lr 1.2369e-04 eta 0:00:41
epoch [44/50] batch [15/26] time 0.153 (0.212) data 0.000 (0.057) loss 1.7598 (1.7363) acc 90.6250 (91.8750) lr 1.2369e-04 eta 0:00:35
epoch [44/50] batch [20/26] time 0.154 (0.198) data 0.000 (0.043) loss 2.0234 (1.7482) acc 87.5000 (91.4062) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [25/26] time 0.154 (0.189) data 0.000 (0.034) loss 1.7314 (1.8061) acc 90.6250 (90.5000) lr 1.2369e-04 eta 0:00:29
epoch [45/50] batch [5/26] time 0.154 (0.287) data 0.000 (0.133) loss 1.5410 (1.8396) acc 93.7500 (88.1250) lr 9.5173e-05 eta 0:00:43
epoch [45/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.067) loss 2.0352 (1.8296) acc 84.3750 (88.4375) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [15/26] time 0.155 (0.199) data 0.000 (0.044) loss 1.7441 (1.7835) acc 84.3750 (88.7500) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [20/26] time 0.158 (0.189) data 0.000 (0.033) loss 2.1133 (1.8965) acc 87.5000 (87.6562) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/26] time 0.156 (0.182) data 0.000 (0.027) loss 1.7891 (1.8775) acc 87.5000 (88.1250) lr 9.5173e-05 eta 0:00:23
epoch [46/50] batch [5/26] time 0.157 (0.281) data 0.000 (0.123) loss 1.3926 (1.5689) acc 93.7500 (92.5000) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [10/26] time 0.155 (0.218) data 0.000 (0.062) loss 1.9287 (1.6927) acc 84.3750 (91.5625) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [15/26] time 0.154 (0.197) data 0.000 (0.041) loss 1.6445 (1.7549) acc 87.5000 (90.2083) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.031) loss 1.7676 (1.7619) acc 87.5000 (90.0000) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.025) loss 1.6816 (1.7504) acc 90.6250 (90.0000) lr 7.0224e-05 eta 0:00:18
epoch [47/50] batch [5/26] time 0.159 (0.291) data 0.000 (0.132) loss 1.9268 (2.0354) acc 96.8750 (90.0000) lr 4.8943e-05 eta 0:00:28
epoch [47/50] batch [10/26] time 0.157 (0.225) data 0.003 (0.066) loss 2.4336 (2.0098) acc 84.3750 (90.3125) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [15/26] time 0.154 (0.201) data 0.000 (0.044) loss 2.3516 (1.9079) acc 90.6250 (91.4583) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.033) loss 2.5293 (1.9199) acc 81.2500 (90.3125) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.027) loss 1.9902 (1.9222) acc 90.6250 (90.2500) lr 4.8943e-05 eta 0:00:14
epoch [48/50] batch [5/26] time 0.155 (0.286) data 0.000 (0.131) loss 1.9805 (1.8105) acc 90.6250 (90.0000) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [10/26] time 0.157 (0.221) data 0.000 (0.066) loss 1.6816 (1.8491) acc 90.6250 (90.0000) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.044) loss 1.5029 (1.7700) acc 93.7500 (90.6250) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.033) loss 1.4111 (1.7563) acc 96.8750 (91.0938) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.026) loss 1.6523 (1.7517) acc 90.6250 (90.8750) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.153 (0.289) data 0.000 (0.134) loss 1.9355 (1.9375) acc 87.5000 (88.1250) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [10/26] time 0.158 (0.222) data 0.000 (0.067) loss 1.5186 (1.8124) acc 93.7500 (90.3125) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/26] time 0.154 (0.200) data 0.000 (0.045) loss 1.9590 (1.8611) acc 84.3750 (89.5833) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.034) loss 1.5000 (1.8525) acc 90.6250 (89.3750) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 2.0703 (1.8297) acc 84.3750 (89.2500) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.156 (0.289) data 0.001 (0.134) loss 1.5361 (1.8797) acc 96.8750 (87.5000) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.067) loss 1.7783 (1.8148) acc 90.6250 (89.3750) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.045) loss 1.9922 (1.7913) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.034) loss 1.8877 (1.8208) acc 87.5000 (90.3125) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.6533 (1.8375) acc 90.6250 (89.8750) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/31 [00:00<?, ?it/s]  3%|▎         | 1/31 [00:05<02:45,  5.51s/it]  6%|▋         | 2/31 [00:06<01:26,  2.97s/it] 10%|▉         | 3/31 [00:07<00:59,  2.13s/it] 13%|█▎        | 4/31 [00:09<00:47,  1.77s/it] 16%|█▌        | 5/31 [00:10<00:40,  1.54s/it] 19%|█▉        | 6/31 [00:11<00:35,  1.40s/it] 23%|██▎       | 7/31 [00:12<00:31,  1.32s/it] 26%|██▌       | 8/31 [00:13<00:28,  1.26s/it] 29%|██▉       | 9/31 [00:14<00:26,  1.22s/it] 32%|███▏      | 10/31 [00:15<00:25,  1.20s/it] 35%|███▌      | 11/31 [00:17<00:23,  1.18s/it] 39%|███▊      | 12/31 [00:18<00:22,  1.17s/it] 42%|████▏     | 13/31 [00:19<00:20,  1.16s/it] 45%|████▌     | 14/31 [00:20<00:19,  1.15s/it] 48%|████▊     | 15/31 [00:21<00:18,  1.15s/it] 52%|█████▏    | 16/31 [00:22<00:17,  1.15s/it] 55%|█████▍    | 17/31 [00:23<00:16,  1.14s/it] 58%|█████▊    | 18/31 [00:24<00:14,  1.14s/it] 61%|██████▏   | 19/31 [00:26<00:13,  1.14s/it] 65%|██████▍   | 20/31 [00:27<00:12,  1.14s/it] 68%|██████▊   | 21/31 [00:28<00:11,  1.14s/it] 71%|███████   | 22/31 [00:29<00:10,  1.14s/it] 74%|███████▍  | 23/31 [00:30<00:09,  1.14s/it] 77%|███████▋  | 24/31 [00:31<00:07,  1.14s/it] 81%|████████  | 25/31 [00:32<00:06,  1.14s/it] 84%|████████▍ | 26/31 [00:34<00:05,  1.14s/it] 87%|████████▋ | 27/31 [00:35<00:04,  1.14s/it] 90%|█████████ | 28/31 [00:36<00:03,  1.14s/it] 94%|█████████▎| 29/31 [00:37<00:02,  1.14s/it] 97%|█████████▋| 30/31 [00:38<00:01,  1.14s/it]100%|██████████| 31/31 [00:39<00:00,  1.02s/it]100%|██████████| 31/31 [00:39<00:00,  1.27s/it]
=> result
* total: 15,300
* correct: 13,860
* accuracy: 90.6%
* error: 9.4%
* macro_f1: 90.6%
Elapsed: 0:04:40
Run this job and save the output to output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  50
# train_x  800
# val      200
# test     15,000
---------  -------
['guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X guacamole, a type of food.', 'X X X X gyoza, a type of food.', 'X X X X hamburger, a type of food.', 'X X X X hot and sour soup, a type of food.', 'X X X X hot dog, a type of food.', 'X X X X huevos rancheros, a type of food.', 'X X X X hummus, a type of food.', 'X X X X ice cream, a type of food.', 'X X X X lasagna, a type of food.', 'X X X X lobster bisque, a type of food.', 'X X X X lobster roll sandwich, a type of food.', 'X X X X macaroni and cheese, a type of food.', 'X X X X macarons, a type of food.', 'X X X X miso soup, a type of food.', 'X X X X mussels, a type of food.', 'X X X X nachos, a type of food.', 'X X X X omelette, a type of food.', 'X X X X onion rings, a type of food.', 'X X X X oysters, a type of food.', 'X X X X pad thai, a type of food.', 'X X X X paella, a type of food.', 'X X X X pancakes, a type of food.', 'X X X X panna cotta, a type of food.', 'X X X X peking duck, a type of food.', 'X X X X pho, a type of food.', 'X X X X pizza, a type of food.', 'X X X X pork chop, a type of food.', 'X X X X poutine, a type of food.', 'X X X X prime rib, a type of food.', 'X X X X pulled pork sandwich, a type of food.', 'X X X X ramen, a type of food.', 'X X X X ravioli, a type of food.', 'X X X X red velvet cake, a type of food.', 'X X X X risotto, a type of food.', 'X X X X samosa, a type of food.', 'X X X X sashimi, a type of food.', 'X X X X scallops, a type of food.', 'X X X X seaweed salad, a type of food.', 'X X X X shrimp and grits, a type of food.', 'X X X X spaghetti bolognese, a type of food.', 'X X X X spaghetti carbonara, a type of food.', 'X X X X spring rolls, a type of food.', 'X X X X steak, a type of food.', 'X X X X strawberry shortcake, a type of food.', 'X X X X sushi, a type of food.', 'X X X X tacos, a type of food.', 'X X X X takoyaki, a type of food.', 'X X X X tiramisu, a type of food.', 'X X X X tuna tartare, a type of food.', 'X X X X waffles, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:05<02:52,  5.94s/it]  7%|▋         | 2/30 [00:07<01:27,  3.12s/it] 10%|█         | 3/30 [00:08<00:59,  2.21s/it] 13%|█▎        | 4/30 [00:09<00:46,  1.78s/it] 17%|█▋        | 5/30 [00:10<00:38,  1.54s/it] 20%|██        | 6/30 [00:11<00:33,  1.40s/it] 23%|██▎       | 7/30 [00:12<00:30,  1.31s/it] 27%|██▋       | 8/30 [00:13<00:27,  1.25s/it] 30%|███       | 9/30 [00:14<00:25,  1.21s/it] 33%|███▎      | 10/30 [00:16<00:23,  1.19s/it] 37%|███▋      | 11/30 [00:17<00:22,  1.17s/it] 40%|████      | 12/30 [00:18<00:20,  1.16s/it] 43%|████▎     | 13/30 [00:19<00:19,  1.15s/it] 47%|████▋     | 14/30 [00:20<00:18,  1.15s/it] 50%|█████     | 15/30 [00:21<00:17,  1.14s/it] 53%|█████▎    | 16/30 [00:22<00:15,  1.14s/it] 57%|█████▋    | 17/30 [00:24<00:14,  1.14s/it] 60%|██████    | 18/30 [00:25<00:13,  1.14s/it] 63%|██████▎   | 19/30 [00:26<00:12,  1.14s/it] 67%|██████▋   | 20/30 [00:27<00:11,  1.13s/it] 70%|███████   | 21/30 [00:28<00:10,  1.13s/it] 73%|███████▎  | 22/30 [00:29<00:09,  1.13s/it] 77%|███████▋  | 23/30 [00:30<00:07,  1.13s/it] 80%|████████  | 24/30 [00:31<00:06,  1.13s/it] 83%|████████▎ | 25/30 [00:33<00:05,  1.13s/it] 87%|████████▋ | 26/30 [00:34<00:04,  1.13s/it] 90%|█████████ | 27/30 [00:35<00:03,  1.13s/it] 93%|█████████▎| 28/30 [00:36<00:02,  1.13s/it] 97%|█████████▋| 29/30 [00:37<00:01,  1.13s/it]100%|██████████| 30/30 [00:38<00:00,  1.13s/it]100%|██████████| 30/30 [00:38<00:00,  1.30s/it]
=> result
* total: 15,000
* correct: 13,692
* accuracy: 91.3%
* error: 8.7%
* macro_f1: 91.3%
Run this job and save the output to output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  50
# train_x  800
# val      200
# test     15,000
---------  -------
['guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X guacamole, a type of food.', 'X X X X gyoza, a type of food.', 'X X X X hamburger, a type of food.', 'X X X X hot and sour soup, a type of food.', 'X X X X hot dog, a type of food.', 'X X X X huevos rancheros, a type of food.', 'X X X X hummus, a type of food.', 'X X X X ice cream, a type of food.', 'X X X X lasagna, a type of food.', 'X X X X lobster bisque, a type of food.', 'X X X X lobster roll sandwich, a type of food.', 'X X X X macaroni and cheese, a type of food.', 'X X X X macarons, a type of food.', 'X X X X miso soup, a type of food.', 'X X X X mussels, a type of food.', 'X X X X nachos, a type of food.', 'X X X X omelette, a type of food.', 'X X X X onion rings, a type of food.', 'X X X X oysters, a type of food.', 'X X X X pad thai, a type of food.', 'X X X X paella, a type of food.', 'X X X X pancakes, a type of food.', 'X X X X panna cotta, a type of food.', 'X X X X peking duck, a type of food.', 'X X X X pho, a type of food.', 'X X X X pizza, a type of food.', 'X X X X pork chop, a type of food.', 'X X X X poutine, a type of food.', 'X X X X prime rib, a type of food.', 'X X X X pulled pork sandwich, a type of food.', 'X X X X ramen, a type of food.', 'X X X X ravioli, a type of food.', 'X X X X red velvet cake, a type of food.', 'X X X X risotto, a type of food.', 'X X X X samosa, a type of food.', 'X X X X sashimi, a type of food.', 'X X X X scallops, a type of food.', 'X X X X seaweed salad, a type of food.', 'X X X X shrimp and grits, a type of food.', 'X X X X spaghetti bolognese, a type of food.', 'X X X X spaghetti carbonara, a type of food.', 'X X X X spring rolls, a type of food.', 'X X X X steak, a type of food.', 'X X X X strawberry shortcake, a type of food.', 'X X X X sushi, a type of food.', 'X X X X tacos, a type of food.', 'X X X X takoyaki, a type of food.', 'X X X X tiramisu, a type of food.', 'X X X X tuna tartare, a type of food.', 'X X X X waffles, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:06<03:02,  6.30s/it]  7%|▋         | 2/30 [00:07<01:31,  3.26s/it] 10%|█         | 3/30 [00:08<01:01,  2.28s/it] 13%|█▎        | 4/30 [00:09<00:47,  1.82s/it] 17%|█▋        | 5/30 [00:10<00:39,  1.57s/it] 20%|██        | 6/30 [00:11<00:34,  1.42s/it] 23%|██▎       | 7/30 [00:13<00:30,  1.32s/it] 27%|██▋       | 8/30 [00:14<00:27,  1.26s/it] 30%|███       | 9/30 [00:15<00:25,  1.22s/it] 33%|███▎      | 10/30 [00:16<00:23,  1.19s/it] 37%|███▋      | 11/30 [00:17<00:22,  1.17s/it] 40%|████      | 12/30 [00:18<00:20,  1.16s/it] 43%|████▎     | 13/30 [00:19<00:19,  1.15s/it] 47%|████▋     | 14/30 [00:20<00:18,  1.14s/it] 50%|█████     | 15/30 [00:22<00:17,  1.14s/it] 53%|█████▎    | 16/30 [00:23<00:15,  1.14s/it] 57%|█████▋    | 17/30 [00:24<00:14,  1.13s/it] 60%|██████    | 18/30 [00:25<00:13,  1.13s/it] 63%|██████▎   | 19/30 [00:26<00:12,  1.13s/it] 67%|██████▋   | 20/30 [00:27<00:11,  1.13s/it] 70%|███████   | 21/30 [00:28<00:10,  1.13s/it] 73%|███████▎  | 22/30 [00:29<00:09,  1.13s/it] 77%|███████▋  | 23/30 [00:31<00:07,  1.13s/it] 80%|████████  | 24/30 [00:32<00:06,  1.13s/it] 83%|████████▎ | 25/30 [00:33<00:05,  1.13s/it] 87%|████████▋ | 26/30 [00:34<00:04,  1.13s/it] 90%|█████████ | 27/30 [00:35<00:03,  1.13s/it] 93%|█████████▎| 28/30 [00:36<00:02,  1.13s/it] 97%|█████████▋| 29/30 [00:37<00:01,  1.13s/it]100%|██████████| 30/30 [00:39<00:00,  1.13s/it]100%|██████████| 30/30 [00:39<00:00,  1.30s/it]
=> result
* total: 15,000
* correct: 13,721
* accuracy: 91.5%
* error: 8.5%
* macro_f1: 91.4%
Run this job and save the output to output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/food101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Food101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Food101
Reading split from /data/yht/data/cl/data/food-101/split_zhou_Food101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/food-101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------
Dataset    Food101
# classes  50
# train_x  800
# val      200
# test     15,000
---------  -------
['guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X guacamole, a type of food.', 'X X X X gyoza, a type of food.', 'X X X X hamburger, a type of food.', 'X X X X hot and sour soup, a type of food.', 'X X X X hot dog, a type of food.', 'X X X X huevos rancheros, a type of food.', 'X X X X hummus, a type of food.', 'X X X X ice cream, a type of food.', 'X X X X lasagna, a type of food.', 'X X X X lobster bisque, a type of food.', 'X X X X lobster roll sandwich, a type of food.', 'X X X X macaroni and cheese, a type of food.', 'X X X X macarons, a type of food.', 'X X X X miso soup, a type of food.', 'X X X X mussels, a type of food.', 'X X X X nachos, a type of food.', 'X X X X omelette, a type of food.', 'X X X X onion rings, a type of food.', 'X X X X oysters, a type of food.', 'X X X X pad thai, a type of food.', 'X X X X paella, a type of food.', 'X X X X pancakes, a type of food.', 'X X X X panna cotta, a type of food.', 'X X X X peking duck, a type of food.', 'X X X X pho, a type of food.', 'X X X X pizza, a type of food.', 'X X X X pork chop, a type of food.', 'X X X X poutine, a type of food.', 'X X X X prime rib, a type of food.', 'X X X X pulled pork sandwich, a type of food.', 'X X X X ramen, a type of food.', 'X X X X ravioli, a type of food.', 'X X X X red velvet cake, a type of food.', 'X X X X risotto, a type of food.', 'X X X X samosa, a type of food.', 'X X X X sashimi, a type of food.', 'X X X X scallops, a type of food.', 'X X X X seaweed salad, a type of food.', 'X X X X shrimp and grits, a type of food.', 'X X X X spaghetti bolognese, a type of food.', 'X X X X spaghetti carbonara, a type of food.', 'X X X X spring rolls, a type of food.', 'X X X X steak, a type of food.', 'X X X X strawberry shortcake, a type of food.', 'X X X X sushi, a type of food.', 'X X X X tacos, a type of food.', 'X X X X takoyaki, a type of food.', 'X X X X tiramisu, a type of food.', 'X X X X tuna tartare, a type of food.', 'X X X X waffles, a type of food.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/food101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/30 [00:00<?, ?it/s]  3%|▎         | 1/30 [00:06<03:01,  6.27s/it]  7%|▋         | 2/30 [00:07<01:34,  3.36s/it] 10%|█         | 3/30 [00:08<01:03,  2.34s/it] 13%|█▎        | 4/30 [00:09<00:48,  1.87s/it] 17%|█▋        | 5/30 [00:10<00:40,  1.60s/it] 20%|██        | 6/30 [00:12<00:34,  1.44s/it] 23%|██▎       | 7/30 [00:13<00:30,  1.34s/it] 27%|██▋       | 8/30 [00:14<00:27,  1.27s/it] 30%|███       | 9/30 [00:15<00:25,  1.23s/it] 33%|███▎      | 10/30 [00:16<00:23,  1.19s/it] 37%|███▋      | 11/30 [00:17<00:22,  1.17s/it] 40%|████      | 12/30 [00:18<00:20,  1.16s/it] 43%|████▎     | 13/30 [00:20<00:19,  1.15s/it] 47%|████▋     | 14/30 [00:21<00:18,  1.14s/it] 50%|█████     | 15/30 [00:22<00:17,  1.14s/it] 53%|█████▎    | 16/30 [00:23<00:15,  1.14s/it] 57%|█████▋    | 17/30 [00:24<00:14,  1.13s/it] 60%|██████    | 18/30 [00:25<00:13,  1.13s/it] 63%|██████▎   | 19/30 [00:26<00:12,  1.13s/it] 67%|██████▋   | 20/30 [00:27<00:11,  1.13s/it] 70%|███████   | 21/30 [00:29<00:10,  1.13s/it] 73%|███████▎  | 22/30 [00:30<00:09,  1.13s/it] 77%|███████▋  | 23/30 [00:31<00:07,  1.13s/it] 80%|████████  | 24/30 [00:32<00:06,  1.13s/it] 83%|████████▎ | 25/30 [00:33<00:05,  1.13s/it] 87%|████████▋ | 26/30 [00:34<00:04,  1.13s/it] 90%|█████████ | 27/30 [00:35<00:03,  1.13s/it] 93%|█████████▎| 28/30 [00:36<00:02,  1.13s/it] 97%|█████████▋| 29/30 [00:38<00:01,  1.13s/it]100%|██████████| 30/30 [00:39<00:00,  1.13s/it]100%|██████████| 30/30 [00:39<00:00,  1.31s/it]
=> result
* total: 15,000
* correct: 13,759
* accuracy: 91.7%
* error: 8.3%
* macro_f1: 91.7%
Run this job and save the output to output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', "colt's foot", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X pink primrose, a type of flower.', 'X X X X hard-leaved pocket orchid, a type of flower.', 'X X X X canterbury bells, a type of flower.', 'X X X X sweet pea, a type of flower.', 'X X X X english marigold, a type of flower.', 'X X X X tiger lily, a type of flower.', 'X X X X moon orchid, a type of flower.', 'X X X X bird of paradise, a type of flower.', 'X X X X monkshood, a type of flower.', 'X X X X globe thistle, a type of flower.', 'X X X X snapdragon, a type of flower.', "X X X X colt's foot, a type of flower.", 'X X X X king protea, a type of flower.', 'X X X X spear thistle, a type of flower.', 'X X X X yellow iris, a type of flower.', 'X X X X globe-flower, a type of flower.', 'X X X X purple coneflower, a type of flower.', 'X X X X peruvian lily, a type of flower.', 'X X X X balloon flower, a type of flower.', 'X X X X giant white arum lily, a type of flower.', 'X X X X fire lily, a type of flower.', 'X X X X pincushion flower, a type of flower.', 'X X X X fritillary, a type of flower.', 'X X X X red ginger, a type of flower.', 'X X X X grape hyacinth, a type of flower.', 'X X X X corn poppy, a type of flower.', 'X X X X prince of wales feathers, a type of flower.', 'X X X X stemless gentian, a type of flower.', 'X X X X artichoke, a type of flower.', 'X X X X sweet william, a type of flower.', 'X X X X carnation, a type of flower.', 'X X X X garden phlox, a type of flower.', 'X X X X love in the mist, a type of flower.', 'X X X X mexican aster, a type of flower.', 'X X X X alpine sea holly, a type of flower.', 'X X X X ruby-lipped cattleya, a type of flower.', 'X X X X cape flower, a type of flower.', 'X X X X great masterwort, a type of flower.', 'X X X X siam tulip, a type of flower.', 'X X X X lenten rose, a type of flower.', 'X X X X barbeton daisy, a type of flower.', 'X X X X daffodil, a type of flower.', 'X X X X sword lily, a type of flower.', 'X X X X poinsettia, a type of flower.', 'X X X X bolero deep blue, a type of flower.', 'X X X X wallflower, a type of flower.', 'X X X X marigold, a type of flower.', 'X X X X buttercup, a type of flower.', 'X X X X oxeye daisy, a type of flower.', 'X X X X common dandelion, a type of flower.', 'X X X X petunia, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/26] time 0.152 (2.215) data 0.000 (0.132) loss 6.5703 (6.3469) acc 50.0000 (51.2500) lr 1.0000e-05 eta 0:47:48
epoch [1/50] batch [10/26] time 0.152 (1.184) data 0.000 (0.066) loss 6.1992 (6.2656) acc 65.6250 (53.4375) lr 1.0000e-05 eta 0:25:26
epoch [1/50] batch [15/26] time 0.152 (0.840) data 0.000 (0.044) loss 6.3711 (6.1826) acc 59.3750 (57.7083) lr 1.0000e-05 eta 0:17:59
epoch [1/50] batch [20/26] time 0.152 (0.668) data 0.000 (0.033) loss 6.0547 (6.1480) acc 59.3750 (57.3438) lr 1.0000e-05 eta 0:14:14
epoch [1/50] batch [25/26] time 0.152 (0.565) data 0.000 (0.026) loss 6.0000 (6.0986) acc 50.0000 (57.7500) lr 1.0000e-05 eta 0:11:59
epoch [2/50] batch [5/26] time 0.153 (0.297) data 0.000 (0.143) loss 4.3320 (5.0367) acc 71.8750 (63.1250) lr 2.0000e-03 eta 0:06:16
epoch [2/50] batch [10/26] time 0.154 (0.226) data 0.000 (0.072) loss 4.5391 (4.7734) acc 56.2500 (62.8125) lr 2.0000e-03 eta 0:04:45
epoch [2/50] batch [15/26] time 0.151 (0.201) data 0.000 (0.048) loss 4.1719 (4.5930) acc 56.2500 (61.8750) lr 2.0000e-03 eta 0:04:12
epoch [2/50] batch [20/26] time 0.151 (0.188) data 0.000 (0.036) loss 3.8867 (4.3682) acc 50.0000 (62.5000) lr 2.0000e-03 eta 0:03:56
epoch [2/50] batch [25/26] time 0.151 (0.181) data 0.000 (0.029) loss 3.6094 (4.2481) acc 65.6250 (63.7500) lr 2.0000e-03 eta 0:03:45
epoch [3/50] batch [5/26] time 0.153 (0.299) data 0.000 (0.145) loss 3.3613 (3.4996) acc 71.8750 (67.5000) lr 1.9980e-03 eta 0:06:12
epoch [3/50] batch [10/26] time 0.156 (0.226) data 0.000 (0.073) loss 3.0625 (3.4254) acc 71.8750 (69.3750) lr 1.9980e-03 eta 0:04:40
epoch [3/50] batch [15/26] time 0.151 (0.201) data 0.000 (0.048) loss 3.0000 (3.3483) acc 84.3750 (72.0833) lr 1.9980e-03 eta 0:04:08
epoch [3/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.036) loss 2.9160 (3.2704) acc 81.2500 (72.6562) lr 1.9980e-03 eta 0:03:51
epoch [3/50] batch [25/26] time 0.151 (0.181) data 0.000 (0.029) loss 2.8652 (3.2055) acc 75.0000 (72.7500) lr 1.9980e-03 eta 0:03:41
epoch [4/50] batch [5/26] time 0.153 (0.289) data 0.000 (0.136) loss 2.5938 (2.6539) acc 84.3750 (81.8750) lr 1.9921e-03 eta 0:05:52
epoch [4/50] batch [10/26] time 0.153 (0.221) data 0.000 (0.068) loss 2.7812 (2.7125) acc 71.8750 (79.0625) lr 1.9921e-03 eta 0:04:28
epoch [4/50] batch [15/26] time 0.152 (0.198) data 0.000 (0.045) loss 2.8457 (2.7479) acc 81.2500 (79.3750) lr 1.9921e-03 eta 0:03:59
epoch [4/50] batch [20/26] time 0.151 (0.187) data 0.000 (0.034) loss 2.5020 (2.6999) acc 75.0000 (80.3125) lr 1.9921e-03 eta 0:03:44
epoch [4/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.027) loss 2.7109 (2.7042) acc 81.2500 (80.1250) lr 1.9921e-03 eta 0:03:35
epoch [5/50] batch [5/26] time 0.152 (0.286) data 0.000 (0.133) loss 2.3379 (2.6098) acc 87.5000 (84.3750) lr 1.9823e-03 eta 0:05:40
epoch [5/50] batch [10/26] time 0.152 (0.219) data 0.000 (0.067) loss 2.9707 (2.5967) acc 78.1250 (83.4375) lr 1.9823e-03 eta 0:04:19
epoch [5/50] batch [15/26] time 0.152 (0.197) data 0.000 (0.045) loss 2.7539 (2.5966) acc 81.2500 (83.5417) lr 1.9823e-03 eta 0:03:52
epoch [5/50] batch [20/26] time 0.152 (0.185) data 0.000 (0.033) loss 2.3672 (2.5764) acc 78.1250 (83.2812) lr 1.9823e-03 eta 0:03:38
epoch [5/50] batch [25/26] time 0.151 (0.179) data 0.000 (0.027) loss 2.5371 (2.5932) acc 84.3750 (82.7500) lr 1.9823e-03 eta 0:03:29
epoch [6/50] batch [5/26] time 0.154 (0.308) data 0.000 (0.153) loss 2.4941 (2.2785) acc 90.6250 (86.8750) lr 1.9686e-03 eta 0:05:58
epoch [6/50] batch [10/26] time 0.153 (0.231) data 0.000 (0.077) loss 1.9336 (2.2521) acc 93.7500 (88.1250) lr 1.9686e-03 eta 0:04:27
epoch [6/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.051) loss 2.5117 (2.3138) acc 81.2500 (87.9167) lr 1.9686e-03 eta 0:03:56
epoch [6/50] batch [20/26] time 0.152 (0.192) data 0.000 (0.039) loss 2.0645 (2.3884) acc 96.8750 (86.8750) lr 1.9686e-03 eta 0:03:40
epoch [6/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.031) loss 1.9248 (2.3690) acc 93.7500 (87.5000) lr 1.9686e-03 eta 0:03:30
epoch [7/50] batch [5/26] time 0.152 (0.296) data 0.000 (0.143) loss 2.0430 (2.1773) acc 93.7500 (91.8750) lr 1.9511e-03 eta 0:05:37
epoch [7/50] batch [10/26] time 0.153 (0.225) data 0.000 (0.072) loss 1.8613 (2.2360) acc 96.8750 (89.0625) lr 1.9511e-03 eta 0:04:14
epoch [7/50] batch [15/26] time 0.151 (0.200) data 0.000 (0.048) loss 2.2031 (2.2191) acc 93.7500 (89.7917) lr 1.9511e-03 eta 0:03:46
epoch [7/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.036) loss 2.2383 (2.2183) acc 84.3750 (89.3750) lr 1.9511e-03 eta 0:03:31
epoch [7/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.029) loss 2.0059 (2.2129) acc 93.7500 (89.1250) lr 1.9511e-03 eta 0:03:22
epoch [8/50] batch [5/26] time 0.152 (0.300) data 0.000 (0.148) loss 2.3203 (2.2252) acc 81.2500 (88.1250) lr 1.9298e-03 eta 0:05:34
epoch [8/50] batch [10/26] time 0.154 (0.227) data 0.000 (0.074) loss 2.0293 (2.1019) acc 93.7500 (89.6875) lr 1.9298e-03 eta 0:04:11
epoch [8/50] batch [15/26] time 0.152 (0.202) data 0.000 (0.049) loss 2.2285 (2.1383) acc 93.7500 (88.9583) lr 1.9298e-03 eta 0:03:42
epoch [8/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.037) loss 1.9766 (2.1474) acc 93.7500 (89.0625) lr 1.9298e-03 eta 0:03:28
epoch [8/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.030) loss 2.0020 (2.1218) acc 90.6250 (89.5000) lr 1.9298e-03 eta 0:03:18
epoch [9/50] batch [5/26] time 0.152 (0.297) data 0.000 (0.144) loss 2.4883 (2.1588) acc 81.2500 (88.1250) lr 1.9048e-03 eta 0:05:22
epoch [9/50] batch [10/26] time 0.153 (0.225) data 0.000 (0.072) loss 2.2422 (2.1258) acc 81.2500 (88.4375) lr 1.9048e-03 eta 0:04:03
epoch [9/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.048) loss 2.2500 (2.1077) acc 87.5000 (89.3750) lr 1.9048e-03 eta 0:03:36
epoch [9/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.036) loss 1.8535 (2.0957) acc 90.6250 (90.0000) lr 1.9048e-03 eta 0:03:22
epoch [9/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.029) loss 2.4688 (2.0956) acc 84.3750 (90.2500) lr 1.9048e-03 eta 0:03:13
epoch [10/50] batch [5/26] time 0.153 (0.290) data 0.000 (0.136) loss 1.9414 (2.0553) acc 90.6250 (90.0000) lr 1.8763e-03 eta 0:05:07
epoch [10/50] batch [10/26] time 0.153 (0.221) data 0.000 (0.068) loss 2.1406 (2.1587) acc 93.7500 (88.7500) lr 1.8763e-03 eta 0:03:53
epoch [10/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.045) loss 2.2656 (2.1075) acc 87.5000 (90.0000) lr 1.8763e-03 eta 0:03:28
epoch [10/50] batch [20/26] time 0.152 (0.187) data 0.000 (0.034) loss 1.9639 (2.0614) acc 93.7500 (90.9375) lr 1.8763e-03 eta 0:03:15
epoch [10/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.8564 (2.0361) acc 93.7500 (91.2500) lr 1.8763e-03 eta 0:03:07
epoch [11/50] batch [5/26] time 0.153 (0.293) data 0.000 (0.140) loss 1.7588 (1.9793) acc 100.0000 (92.5000) lr 1.8443e-03 eta 0:05:03
epoch [11/50] batch [10/26] time 0.155 (0.223) data 0.000 (0.070) loss 1.7676 (1.9553) acc 96.8750 (93.4375) lr 1.8443e-03 eta 0:03:49
epoch [11/50] batch [15/26] time 0.152 (0.200) data 0.000 (0.047) loss 1.9678 (1.9824) acc 90.6250 (92.2917) lr 1.8443e-03 eta 0:03:24
epoch [11/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.035) loss 1.8848 (1.9876) acc 96.8750 (92.1875) lr 1.8443e-03 eta 0:03:11
epoch [11/50] batch [25/26] time 0.156 (0.181) data 0.000 (0.028) loss 2.2344 (2.0126) acc 90.6250 (92.2500) lr 1.8443e-03 eta 0:03:04
epoch [12/50] batch [5/26] time 0.152 (0.295) data 0.000 (0.142) loss 1.7891 (1.9355) acc 100.0000 (95.0000) lr 1.8090e-03 eta 0:04:57
epoch [12/50] batch [10/26] time 0.153 (0.224) data 0.000 (0.071) loss 1.7266 (1.9431) acc 96.8750 (93.7500) lr 1.8090e-03 eta 0:03:45
epoch [12/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.047) loss 2.3125 (1.9502) acc 87.5000 (92.9167) lr 1.8090e-03 eta 0:03:20
epoch [12/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.036) loss 2.1406 (1.9875) acc 87.5000 (92.3438) lr 1.8090e-03 eta 0:03:07
epoch [12/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.029) loss 1.6797 (1.9946) acc 96.8750 (92.0000) lr 1.8090e-03 eta 0:02:59
epoch [13/50] batch [5/26] time 0.154 (0.301) data 0.000 (0.147) loss 1.8838 (1.8633) acc 87.5000 (90.0000) lr 1.7705e-03 eta 0:04:56
epoch [13/50] batch [10/26] time 0.153 (0.227) data 0.000 (0.073) loss 1.7764 (1.8392) acc 96.8750 (91.5625) lr 1.7705e-03 eta 0:03:42
epoch [13/50] batch [15/26] time 0.153 (0.202) data 0.000 (0.049) loss 2.1211 (1.8930) acc 93.7500 (91.4583) lr 1.7705e-03 eta 0:03:16
epoch [13/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.037) loss 1.7832 (1.9129) acc 96.8750 (91.4062) lr 1.7705e-03 eta 0:03:03
epoch [13/50] batch [25/26] time 0.155 (0.183) data 0.000 (0.029) loss 1.7021 (1.9411) acc 96.8750 (91.1250) lr 1.7705e-03 eta 0:02:55
epoch [14/50] batch [5/26] time 0.152 (0.293) data 0.000 (0.140) loss 2.0781 (1.9115) acc 93.7500 (93.1250) lr 1.7290e-03 eta 0:04:40
epoch [14/50] batch [10/26] time 0.153 (0.223) data 0.000 (0.070) loss 1.8340 (1.8973) acc 93.7500 (91.5625) lr 1.7290e-03 eta 0:03:32
epoch [14/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.047) loss 1.8301 (1.8872) acc 96.8750 (92.5000) lr 1.7290e-03 eta 0:03:08
epoch [14/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.035) loss 1.9688 (1.8651) acc 87.5000 (93.2812) lr 1.7290e-03 eta 0:02:56
epoch [14/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.028) loss 2.1777 (1.9207) acc 90.6250 (92.2500) lr 1.7290e-03 eta 0:02:49
epoch [15/50] batch [5/26] time 0.154 (0.296) data 0.000 (0.141) loss 1.7539 (1.7557) acc 93.7500 (96.8750) lr 1.6845e-03 eta 0:04:35
epoch [15/50] batch [10/26] time 0.154 (0.225) data 0.000 (0.071) loss 2.0703 (1.7751) acc 90.6250 (95.6250) lr 1.6845e-03 eta 0:03:28
epoch [15/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.047) loss 1.9229 (1.8475) acc 90.6250 (93.3333) lr 1.6845e-03 eta 0:03:05
epoch [15/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.035) loss 1.8096 (1.8571) acc 84.3750 (92.6562) lr 1.6845e-03 eta 0:02:53
epoch [15/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.028) loss 1.9014 (1.8667) acc 96.8750 (92.7500) lr 1.6845e-03 eta 0:02:45
epoch [16/50] batch [5/26] time 0.153 (0.306) data 0.000 (0.151) loss 1.9238 (1.9535) acc 93.7500 (91.8750) lr 1.6374e-03 eta 0:04:36
epoch [16/50] batch [10/26] time 0.155 (0.230) data 0.000 (0.076) loss 1.5430 (1.8694) acc 96.8750 (92.5000) lr 1.6374e-03 eta 0:03:26
epoch [16/50] batch [15/26] time 0.153 (0.204) data 0.000 (0.051) loss 2.2207 (1.9008) acc 87.5000 (92.0833) lr 1.6374e-03 eta 0:03:02
epoch [16/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.038) loss 1.9766 (1.8969) acc 96.8750 (92.9688) lr 1.6374e-03 eta 0:02:50
epoch [16/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.030) loss 1.6172 (1.8962) acc 96.8750 (92.7500) lr 1.6374e-03 eta 0:02:42
epoch [17/50] batch [5/26] time 0.153 (0.287) data 0.000 (0.134) loss 1.5615 (1.7365) acc 96.8750 (93.1250) lr 1.5878e-03 eta 0:04:12
epoch [17/50] batch [10/26] time 0.153 (0.220) data 0.000 (0.067) loss 1.9248 (1.7376) acc 84.3750 (94.0625) lr 1.5878e-03 eta 0:03:12
epoch [17/50] batch [15/26] time 0.152 (0.198) data 0.000 (0.045) loss 2.0117 (1.8002) acc 87.5000 (91.8750) lr 1.5878e-03 eta 0:02:51
epoch [17/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.034) loss 2.3125 (1.8493) acc 84.3750 (91.4062) lr 1.5878e-03 eta 0:02:41
epoch [17/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.5283 (1.8327) acc 100.0000 (92.0000) lr 1.5878e-03 eta 0:02:34
epoch [18/50] batch [5/26] time 0.153 (0.288) data 0.000 (0.134) loss 1.8857 (1.7439) acc 90.6250 (93.7500) lr 1.5358e-03 eta 0:04:05
epoch [18/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.067) loss 1.7344 (1.8045) acc 93.7500 (92.1875) lr 1.5358e-03 eta 0:03:07
epoch [18/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.045) loss 1.7627 (1.8273) acc 96.8750 (92.5000) lr 1.5358e-03 eta 0:02:47
epoch [18/50] batch [20/26] time 0.154 (0.187) data 0.000 (0.034) loss 1.6689 (1.8426) acc 96.8750 (92.6562) lr 1.5358e-03 eta 0:02:36
epoch [18/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.8486 (1.8517) acc 93.7500 (93.0000) lr 1.5358e-03 eta 0:02:30
epoch [19/50] batch [5/26] time 0.156 (0.322) data 0.000 (0.167) loss 1.6211 (1.7869) acc 100.0000 (95.6250) lr 1.4818e-03 eta 0:04:26
epoch [19/50] batch [10/26] time 0.157 (0.239) data 0.000 (0.084) loss 1.7588 (1.7705) acc 93.7500 (94.3750) lr 1.4818e-03 eta 0:03:16
epoch [19/50] batch [15/26] time 0.154 (0.211) data 0.000 (0.056) loss 1.8242 (1.7830) acc 96.8750 (93.9583) lr 1.4818e-03 eta 0:02:52
epoch [19/50] batch [20/26] time 0.154 (0.196) data 0.000 (0.042) loss 1.9033 (1.7780) acc 93.7500 (94.2188) lr 1.4818e-03 eta 0:02:39
epoch [19/50] batch [25/26] time 0.154 (0.188) data 0.000 (0.034) loss 1.9492 (1.8039) acc 93.7500 (94.0000) lr 1.4818e-03 eta 0:02:31
epoch [20/50] batch [5/26] time 0.154 (0.293) data 0.000 (0.139) loss 1.4473 (1.6928) acc 100.0000 (95.0000) lr 1.4258e-03 eta 0:03:54
epoch [20/50] batch [10/26] time 0.155 (0.224) data 0.000 (0.070) loss 1.5352 (1.6694) acc 100.0000 (95.3125) lr 1.4258e-03 eta 0:02:57
epoch [20/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 1.8506 (1.7439) acc 96.8750 (95.0000) lr 1.4258e-03 eta 0:02:38
epoch [20/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.035) loss 1.6875 (1.7644) acc 96.8750 (94.0625) lr 1.4258e-03 eta 0:02:28
epoch [20/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.028) loss 2.1191 (1.7622) acc 87.5000 (94.1250) lr 1.4258e-03 eta 0:02:22
epoch [21/50] batch [5/26] time 0.155 (0.302) data 0.000 (0.147) loss 1.5469 (1.6990) acc 96.8750 (95.6250) lr 1.3681e-03 eta 0:03:53
epoch [21/50] batch [10/26] time 0.154 (0.228) data 0.000 (0.074) loss 1.9922 (1.7219) acc 93.7500 (95.3125) lr 1.3681e-03 eta 0:02:55
epoch [21/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 1.6689 (1.7277) acc 90.6250 (94.3750) lr 1.3681e-03 eta 0:02:35
epoch [21/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 1.9814 (1.7510) acc 90.6250 (94.3750) lr 1.3681e-03 eta 0:02:24
epoch [21/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.030) loss 2.0117 (1.7435) acc 93.7500 (94.3750) lr 1.3681e-03 eta 0:02:18
epoch [22/50] batch [5/26] time 0.155 (0.292) data 0.000 (0.138) loss 1.7441 (1.7316) acc 96.8750 (97.5000) lr 1.3090e-03 eta 0:03:38
epoch [22/50] batch [10/26] time 0.156 (0.224) data 0.000 (0.069) loss 1.5498 (1.7005) acc 96.8750 (96.5625) lr 1.3090e-03 eta 0:02:46
epoch [22/50] batch [15/26] time 0.267 (0.208) data 0.000 (0.046) loss 1.7158 (1.7160) acc 93.7500 (95.8333) lr 1.3090e-03 eta 0:02:33
epoch [22/50] batch [20/26] time 0.156 (0.196) data 0.000 (0.035) loss 1.7246 (1.7516) acc 93.7500 (94.8438) lr 1.3090e-03 eta 0:02:23
epoch [22/50] batch [25/26] time 0.152 (0.187) data 0.000 (0.028) loss 1.5811 (1.7277) acc 96.8750 (95.1250) lr 1.3090e-03 eta 0:02:16
epoch [23/50] batch [5/26] time 0.155 (0.297) data 0.000 (0.141) loss 1.8242 (1.7324) acc 87.5000 (91.8750) lr 1.2487e-03 eta 0:03:34
epoch [23/50] batch [10/26] time 0.156 (0.226) data 0.000 (0.071) loss 1.8945 (1.7520) acc 93.7500 (92.8125) lr 1.2487e-03 eta 0:02:42
epoch [23/50] batch [15/26] time 0.152 (0.202) data 0.000 (0.047) loss 1.8965 (1.7816) acc 96.8750 (93.3333) lr 1.2487e-03 eta 0:02:23
epoch [23/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.035) loss 1.9736 (1.7971) acc 96.8750 (93.5938) lr 1.2487e-03 eta 0:02:14
epoch [23/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.028) loss 1.6201 (1.7862) acc 96.8750 (93.5000) lr 1.2487e-03 eta 0:02:08
epoch [24/50] batch [5/26] time 0.153 (0.301) data 0.000 (0.147) loss 1.8730 (1.8820) acc 96.8750 (93.7500) lr 1.1874e-03 eta 0:03:29
epoch [24/50] batch [10/26] time 0.153 (0.227) data 0.000 (0.073) loss 1.5771 (1.7929) acc 100.0000 (95.0000) lr 1.1874e-03 eta 0:02:37
epoch [24/50] batch [15/26] time 0.152 (0.202) data 0.000 (0.049) loss 1.6465 (1.7604) acc 96.8750 (95.4167) lr 1.1874e-03 eta 0:02:18
epoch [24/50] batch [20/26] time 0.152 (0.190) data 0.000 (0.037) loss 1.5029 (1.7170) acc 96.8750 (95.7812) lr 1.1874e-03 eta 0:02:09
epoch [24/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.029) loss 1.6582 (1.7114) acc 93.7500 (95.3750) lr 1.1874e-03 eta 0:02:03
epoch [25/50] batch [5/26] time 0.154 (0.302) data 0.000 (0.147) loss 1.8447 (1.7400) acc 93.7500 (95.6250) lr 1.1253e-03 eta 0:03:22
epoch [25/50] batch [10/26] time 0.153 (0.232) data 0.000 (0.074) loss 1.6738 (1.6971) acc 96.8750 (96.5625) lr 1.1253e-03 eta 0:02:34
epoch [25/50] batch [15/26] time 0.153 (0.206) data 0.000 (0.049) loss 1.9805 (1.7747) acc 87.5000 (94.1667) lr 1.1253e-03 eta 0:02:15
epoch [25/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.037) loss 1.7451 (1.7580) acc 96.8750 (94.8438) lr 1.1253e-03 eta 0:02:06
epoch [25/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.030) loss 1.8613 (1.7675) acc 90.6250 (94.2500) lr 1.1253e-03 eta 0:02:00
epoch [26/50] batch [5/26] time 0.152 (0.298) data 0.000 (0.145) loss 1.9609 (1.7777) acc 90.6250 (92.5000) lr 1.0628e-03 eta 0:03:12
epoch [26/50] batch [10/26] time 0.153 (0.225) data 0.000 (0.072) loss 1.7939 (1.7517) acc 93.7500 (93.1250) lr 1.0628e-03 eta 0:02:24
epoch [26/50] batch [15/26] time 0.152 (0.201) data 0.000 (0.048) loss 1.7559 (1.7919) acc 90.6250 (92.2917) lr 1.0628e-03 eta 0:02:07
epoch [26/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.036) loss 1.7617 (1.7821) acc 93.7500 (92.9688) lr 1.0628e-03 eta 0:01:58
epoch [26/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.029) loss 2.2559 (1.7866) acc 90.6250 (93.1250) lr 1.0628e-03 eta 0:01:53
epoch [27/50] batch [5/26] time 0.154 (0.314) data 0.000 (0.159) loss 1.9824 (1.5996) acc 90.6250 (97.5000) lr 1.0000e-03 eta 0:03:14
epoch [27/50] batch [10/26] time 0.155 (0.234) data 0.000 (0.080) loss 2.0000 (1.6880) acc 87.5000 (95.0000) lr 1.0000e-03 eta 0:02:23
epoch [27/50] batch [15/26] time 0.154 (0.207) data 0.000 (0.053) loss 1.6836 (1.6983) acc 93.7500 (94.5833) lr 1.0000e-03 eta 0:02:06
epoch [27/50] batch [20/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.5664 (1.7015) acc 96.8750 (94.5312) lr 1.0000e-03 eta 0:01:56
epoch [27/50] batch [25/26] time 0.152 (0.185) data 0.000 (0.032) loss 2.0625 (1.7168) acc 93.7500 (94.6250) lr 1.0000e-03 eta 0:01:51
epoch [28/50] batch [5/26] time 0.154 (0.272) data 0.000 (0.118) loss 1.5391 (1.6092) acc 96.8750 (94.3750) lr 9.3721e-04 eta 0:02:41
epoch [28/50] batch [10/26] time 0.161 (0.214) data 0.000 (0.059) loss 1.7129 (1.6314) acc 93.7500 (95.3125) lr 9.3721e-04 eta 0:02:05
epoch [28/50] batch [15/26] time 0.154 (0.195) data 0.000 (0.039) loss 1.9297 (1.6809) acc 90.6250 (94.5833) lr 9.3721e-04 eta 0:01:53
epoch [28/50] batch [20/26] time 0.152 (0.184) data 0.000 (0.030) loss 1.6943 (1.7006) acc 93.7500 (94.3750) lr 9.3721e-04 eta 0:01:46
epoch [28/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.024) loss 1.7646 (1.7349) acc 93.7500 (93.6250) lr 9.3721e-04 eta 0:01:42
epoch [29/50] batch [5/26] time 0.154 (0.301) data 0.000 (0.147) loss 1.6328 (1.6814) acc 100.0000 (95.6250) lr 8.7467e-04 eta 0:02:50
epoch [29/50] batch [10/26] time 0.155 (0.227) data 0.000 (0.073) loss 1.8662 (1.6917) acc 93.7500 (95.6250) lr 8.7467e-04 eta 0:02:07
epoch [29/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 1.7227 (1.7383) acc 100.0000 (95.2083) lr 8.7467e-04 eta 0:01:52
epoch [29/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.037) loss 1.7275 (1.7440) acc 93.7500 (94.8438) lr 8.7467e-04 eta 0:01:45
epoch [29/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.030) loss 1.6055 (1.7308) acc 96.8750 (94.8750) lr 8.7467e-04 eta 0:01:40
epoch [30/50] batch [5/26] time 0.154 (0.289) data 0.000 (0.133) loss 1.9727 (1.7463) acc 87.5000 (94.3750) lr 8.1262e-04 eta 0:02:36
epoch [30/50] batch [10/26] time 0.154 (0.221) data 0.000 (0.067) loss 1.8408 (1.7380) acc 96.8750 (94.0625) lr 8.1262e-04 eta 0:01:58
epoch [30/50] batch [15/26] time 0.152 (0.199) data 0.000 (0.045) loss 1.8242 (1.7363) acc 90.6250 (93.7500) lr 8.1262e-04 eta 0:01:45
epoch [30/50] batch [20/26] time 0.152 (0.187) data 0.000 (0.034) loss 1.7461 (1.6957) acc 100.0000 (94.5312) lr 8.1262e-04 eta 0:01:38
epoch [30/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.5400 (1.7014) acc 96.8750 (94.1250) lr 8.1262e-04 eta 0:01:33
epoch [31/50] batch [5/26] time 0.154 (0.285) data 0.000 (0.130) loss 1.7959 (1.6773) acc 93.7500 (95.6250) lr 7.5131e-04 eta 0:02:26
epoch [31/50] batch [10/26] time 0.155 (0.220) data 0.000 (0.065) loss 1.4033 (1.7430) acc 100.0000 (94.6875) lr 7.5131e-04 eta 0:01:52
epoch [31/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.043) loss 1.7637 (1.7287) acc 93.7500 (93.5417) lr 7.5131e-04 eta 0:01:39
epoch [31/50] batch [20/26] time 0.152 (0.187) data 0.000 (0.033) loss 1.3975 (1.6811) acc 100.0000 (94.6875) lr 7.5131e-04 eta 0:01:33
epoch [31/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.026) loss 1.8066 (1.6972) acc 100.0000 (95.0000) lr 7.5131e-04 eta 0:01:28
epoch [32/50] batch [5/26] time 0.156 (0.315) data 0.000 (0.159) loss 1.6855 (1.6627) acc 93.7500 (96.2500) lr 6.9098e-04 eta 0:02:33
epoch [32/50] batch [10/26] time 0.153 (0.234) data 0.000 (0.080) loss 1.5371 (1.6906) acc 93.7500 (95.0000) lr 6.9098e-04 eta 0:01:53
epoch [32/50] batch [15/26] time 0.152 (0.207) data 0.000 (0.053) loss 1.5605 (1.6988) acc 93.7500 (94.7917) lr 6.9098e-04 eta 0:01:39
epoch [32/50] batch [20/26] time 0.153 (0.194) data 0.000 (0.040) loss 2.0898 (1.6800) acc 93.7500 (95.3125) lr 6.9098e-04 eta 0:01:31
epoch [32/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.032) loss 1.7656 (1.6952) acc 90.6250 (94.8750) lr 6.9098e-04 eta 0:01:26
epoch [33/50] batch [5/26] time 0.154 (0.282) data 0.000 (0.127) loss 1.6270 (1.6506) acc 100.0000 (96.8750) lr 6.3188e-04 eta 0:02:10
epoch [33/50] batch [10/26] time 0.156 (0.219) data 0.000 (0.064) loss 1.6582 (1.7159) acc 96.8750 (95.3125) lr 6.3188e-04 eta 0:01:40
epoch [33/50] batch [15/26] time 0.154 (0.203) data 0.000 (0.043) loss 1.3379 (1.6919) acc 100.0000 (96.0417) lr 6.3188e-04 eta 0:01:31
epoch [33/50] batch [20/26] time 0.154 (0.191) data 0.000 (0.032) loss 1.5137 (1.6744) acc 100.0000 (96.0938) lr 6.3188e-04 eta 0:01:25
epoch [33/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.026) loss 1.4717 (1.6606) acc 100.0000 (96.0000) lr 6.3188e-04 eta 0:01:22
epoch [34/50] batch [5/26] time 0.154 (0.285) data 0.000 (0.131) loss 1.9922 (1.6695) acc 90.6250 (95.0000) lr 5.7422e-04 eta 0:02:04
epoch [34/50] batch [10/26] time 0.154 (0.220) data 0.000 (0.066) loss 1.7295 (1.6323) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:01:34
epoch [34/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.044) loss 1.6494 (1.6239) acc 96.8750 (96.6667) lr 5.7422e-04 eta 0:01:24
epoch [34/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.033) loss 1.4043 (1.5985) acc 96.8750 (96.5625) lr 5.7422e-04 eta 0:01:18
epoch [34/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.7871 (1.6293) acc 93.7500 (95.6250) lr 5.7422e-04 eta 0:01:14
epoch [35/50] batch [5/26] time 0.153 (0.279) data 0.000 (0.124) loss 1.3906 (1.6717) acc 96.8750 (94.3750) lr 5.1825e-04 eta 0:01:54
epoch [35/50] batch [10/26] time 0.155 (0.217) data 0.000 (0.062) loss 1.6865 (1.6434) acc 96.8750 (95.3125) lr 5.1825e-04 eta 0:01:27
epoch [35/50] batch [15/26] time 0.175 (0.199) data 0.000 (0.042) loss 2.0664 (1.6562) acc 93.7500 (95.0000) lr 5.1825e-04 eta 0:01:19
epoch [35/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.031) loss 1.8770 (1.6794) acc 90.6250 (94.5312) lr 5.1825e-04 eta 0:01:14
epoch [35/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.025) loss 1.4512 (1.7011) acc 96.8750 (94.5000) lr 5.1825e-04 eta 0:01:10
epoch [36/50] batch [5/26] time 0.153 (0.307) data 0.000 (0.153) loss 1.8398 (1.6857) acc 93.7500 (96.2500) lr 4.6417e-04 eta 0:01:58
epoch [36/50] batch [10/26] time 0.153 (0.230) data 0.000 (0.077) loss 1.3965 (1.6017) acc 96.8750 (96.8750) lr 4.6417e-04 eta 0:01:27
epoch [36/50] batch [15/26] time 0.153 (0.204) data 0.000 (0.051) loss 1.7568 (1.6424) acc 93.7500 (96.0417) lr 4.6417e-04 eta 0:01:16
epoch [36/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.038) loss 1.6670 (1.6412) acc 96.8750 (96.0938) lr 4.6417e-04 eta 0:01:10
epoch [36/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.031) loss 1.6660 (1.6787) acc 93.7500 (95.2500) lr 4.6417e-04 eta 0:01:07
epoch [37/50] batch [5/26] time 0.154 (0.311) data 0.000 (0.156) loss 2.2871 (1.6266) acc 87.5000 (95.6250) lr 4.1221e-04 eta 0:01:51
epoch [37/50] batch [10/26] time 0.155 (0.233) data 0.000 (0.078) loss 1.6289 (1.6785) acc 96.8750 (94.6875) lr 4.1221e-04 eta 0:01:22
epoch [37/50] batch [15/26] time 0.153 (0.206) data 0.000 (0.052) loss 1.7305 (1.7149) acc 96.8750 (94.1667) lr 4.1221e-04 eta 0:01:11
epoch [37/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.039) loss 2.3047 (1.7606) acc 84.3750 (93.4375) lr 4.1221e-04 eta 0:01:06
epoch [37/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.031) loss 1.5859 (1.7355) acc 93.7500 (93.6250) lr 4.1221e-04 eta 0:01:02
epoch [38/50] batch [5/26] time 0.155 (0.296) data 0.000 (0.140) loss 1.8096 (1.6682) acc 96.8750 (98.1250) lr 3.6258e-04 eta 0:01:38
epoch [38/50] batch [10/26] time 0.155 (0.225) data 0.000 (0.070) loss 1.8008 (1.6368) acc 96.8750 (96.8750) lr 3.6258e-04 eta 0:01:13
epoch [38/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.047) loss 1.4512 (1.6366) acc 100.0000 (96.8750) lr 3.6258e-04 eta 0:01:05
epoch [38/50] batch [20/26] time 0.152 (0.190) data 0.000 (0.035) loss 1.8652 (1.6770) acc 90.6250 (96.2500) lr 3.6258e-04 eta 0:01:00
epoch [38/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.028) loss 1.7910 (1.6648) acc 93.7500 (96.0000) lr 3.6258e-04 eta 0:00:57
epoch [39/50] batch [5/26] time 0.156 (0.286) data 0.000 (0.130) loss 1.8770 (1.7590) acc 90.6250 (92.5000) lr 3.1545e-04 eta 0:01:27
epoch [39/50] batch [10/26] time 0.154 (0.220) data 0.000 (0.065) loss 1.7363 (1.7636) acc 93.7500 (95.0000) lr 3.1545e-04 eta 0:01:06
epoch [39/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.044) loss 1.8936 (1.7227) acc 87.5000 (95.4167) lr 3.1545e-04 eta 0:00:58
epoch [39/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.033) loss 1.9316 (1.7192) acc 90.6250 (95.1562) lr 3.1545e-04 eta 0:00:54
epoch [39/50] batch [25/26] time 0.154 (0.180) data 0.000 (0.026) loss 1.4863 (1.6963) acc 100.0000 (95.3750) lr 3.1545e-04 eta 0:00:51
epoch [40/50] batch [5/26] time 0.155 (0.277) data 0.000 (0.122) loss 1.7656 (1.6922) acc 96.8750 (96.8750) lr 2.7103e-04 eta 0:01:17
epoch [40/50] batch [10/26] time 0.154 (0.216) data 0.000 (0.061) loss 1.6055 (1.7092) acc 90.6250 (95.6250) lr 2.7103e-04 eta 0:00:59
epoch [40/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.041) loss 1.8525 (1.6856) acc 87.5000 (95.6250) lr 2.7103e-04 eta 0:00:52
epoch [40/50] batch [20/26] time 0.155 (0.185) data 0.000 (0.031) loss 1.4297 (1.6563) acc 100.0000 (96.2500) lr 2.7103e-04 eta 0:00:49
epoch [40/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 1.7930 (1.6669) acc 96.8750 (96.1250) lr 2.7103e-04 eta 0:00:46
epoch [41/50] batch [5/26] time 0.154 (0.286) data 0.000 (0.131) loss 1.4482 (1.5629) acc 96.8750 (95.0000) lr 2.2949e-04 eta 0:01:12
epoch [41/50] batch [10/26] time 0.155 (0.220) data 0.000 (0.066) loss 1.4160 (1.5903) acc 96.8750 (96.2500) lr 2.2949e-04 eta 0:00:55
epoch [41/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.044) loss 1.4287 (1.6083) acc 96.8750 (95.8333) lr 2.2949e-04 eta 0:00:48
epoch [41/50] batch [20/26] time 0.154 (0.187) data 0.000 (0.033) loss 1.7891 (1.6281) acc 93.7500 (95.9375) lr 2.2949e-04 eta 0:00:44
epoch [41/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.8555 (1.6566) acc 96.8750 (96.0000) lr 2.2949e-04 eta 0:00:42
epoch [42/50] batch [5/26] time 0.154 (0.284) data 0.000 (0.129) loss 1.7041 (1.6271) acc 100.0000 (97.5000) lr 1.9098e-04 eta 0:01:05
epoch [42/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.065) loss 1.9053 (1.6542) acc 87.5000 (95.3125) lr 1.9098e-04 eta 0:00:49
epoch [42/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.7344 (1.6663) acc 96.8750 (94.7917) lr 1.9098e-04 eta 0:00:43
epoch [42/50] batch [20/26] time 0.152 (0.186) data 0.000 (0.033) loss 1.5332 (1.6613) acc 96.8750 (95.0000) lr 1.9098e-04 eta 0:00:39
epoch [42/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.026) loss 1.4688 (1.6497) acc 100.0000 (95.1250) lr 1.9098e-04 eta 0:00:37
epoch [43/50] batch [5/26] time 0.153 (0.295) data 0.000 (0.141) loss 1.5713 (1.5717) acc 96.8750 (96.8750) lr 1.5567e-04 eta 0:00:59
epoch [43/50] batch [10/26] time 0.154 (0.225) data 0.000 (0.071) loss 1.4707 (1.6057) acc 100.0000 (96.5625) lr 1.5567e-04 eta 0:00:44
epoch [43/50] batch [15/26] time 0.153 (0.201) data 0.000 (0.047) loss 1.6172 (1.6227) acc 100.0000 (96.8750) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.035) loss 1.8457 (1.6393) acc 96.8750 (96.5625) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.028) loss 1.7881 (1.6448) acc 93.7500 (96.1250) lr 1.5567e-04 eta 0:00:33
epoch [44/50] batch [5/26] time 0.154 (0.282) data 0.000 (0.126) loss 1.5254 (1.6590) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:49
epoch [44/50] batch [10/26] time 0.155 (0.219) data 0.000 (0.063) loss 1.8535 (1.6426) acc 93.7500 (95.0000) lr 1.2369e-04 eta 0:00:37
epoch [44/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.042) loss 1.6055 (1.6513) acc 93.7500 (95.0000) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.5400 (1.6583) acc 100.0000 (95.1562) lr 1.2369e-04 eta 0:00:30
epoch [44/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.025) loss 1.6826 (1.6400) acc 96.8750 (95.6250) lr 1.2369e-04 eta 0:00:28
epoch [45/50] batch [5/26] time 0.153 (0.290) data 0.000 (0.136) loss 1.3594 (1.6236) acc 100.0000 (94.3750) lr 9.5173e-05 eta 0:00:43
epoch [45/50] batch [10/26] time 0.154 (0.221) data 0.000 (0.068) loss 1.4648 (1.6405) acc 100.0000 (95.3125) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.045) loss 1.6738 (1.6614) acc 96.8750 (95.6250) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.034) loss 1.7568 (1.6617) acc 93.7500 (95.7812) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 1.6045 (1.6472) acc 100.0000 (96.2500) lr 9.5173e-05 eta 0:00:23
epoch [46/50] batch [5/26] time 0.153 (0.274) data 0.000 (0.120) loss 1.7578 (1.6738) acc 93.7500 (96.2500) lr 7.0224e-05 eta 0:00:34
epoch [46/50] batch [10/26] time 0.154 (0.214) data 0.000 (0.060) loss 1.5625 (1.6114) acc 96.8750 (96.5625) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [15/26] time 0.153 (0.193) data 0.000 (0.040) loss 1.5098 (1.5949) acc 100.0000 (97.0833) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.030) loss 1.5508 (1.6074) acc 96.8750 (96.8750) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.024) loss 1.6689 (1.6223) acc 87.5000 (96.1250) lr 7.0224e-05 eta 0:00:18
epoch [47/50] batch [5/26] time 0.153 (0.283) data 0.000 (0.129) loss 1.5879 (1.7658) acc 96.8750 (96.2500) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [10/26] time 0.154 (0.218) data 0.000 (0.065) loss 1.5977 (1.7273) acc 93.7500 (95.0000) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [15/26] time 0.155 (0.197) data 0.000 (0.043) loss 1.8623 (1.7221) acc 93.7500 (94.3750) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.033) loss 1.8457 (1.7020) acc 93.7500 (94.3750) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.4824 (1.6997) acc 96.8750 (94.2500) lr 4.8943e-05 eta 0:00:14
epoch [48/50] batch [5/26] time 0.154 (0.313) data 0.000 (0.158) loss 1.6602 (1.5695) acc 96.8750 (95.0000) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [10/26] time 0.154 (0.234) data 0.000 (0.079) loss 1.9551 (1.5804) acc 93.7500 (96.8750) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [15/26] time 0.154 (0.207) data 0.000 (0.053) loss 1.8799 (1.6250) acc 93.7500 (96.4583) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.6367 (1.6130) acc 96.8750 (96.0938) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.5518 (1.6178) acc 96.8750 (95.8750) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.153 (0.289) data 0.000 (0.135) loss 1.6543 (1.6174) acc 93.7500 (94.3750) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [10/26] time 0.153 (0.221) data 0.000 (0.068) loss 1.8096 (1.7175) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.045) loss 1.6455 (1.7393) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.034) loss 1.5811 (1.7081) acc 96.8750 (94.5312) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.8691 (1.7165) acc 90.6250 (94.3750) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.153 (0.284) data 0.000 (0.130) loss 1.4941 (1.5756) acc 96.8750 (96.8750) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/26] time 0.159 (0.220) data 0.000 (0.065) loss 1.6582 (1.5931) acc 93.7500 (96.2500) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.044) loss 1.5010 (1.5712) acc 100.0000 (97.0833) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.155 (0.186) data 0.000 (0.033) loss 1.6084 (1.5833) acc 100.0000 (97.1875) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.026) loss 1.6006 (1.6048) acc 93.7500 (96.3750) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:11,  5.74s/it] 67%|██████▋   | 2/3 [00:06<00:03,  3.03s/it]100%|██████████| 3/3 [00:07<00:00,  1.73s/it]100%|██████████| 3/3 [00:07<00:00,  2.39s/it]
=> result
* total: 1,053
* correct: 1,031
* accuracy: 97.9%
* error: 2.1%
* macro_f1: 97.9%
Elapsed: 0:04:15
Run this job and save the output to output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', "colt's foot", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X pink primrose, a type of flower.', 'X X X X hard-leaved pocket orchid, a type of flower.', 'X X X X canterbury bells, a type of flower.', 'X X X X sweet pea, a type of flower.', 'X X X X english marigold, a type of flower.', 'X X X X tiger lily, a type of flower.', 'X X X X moon orchid, a type of flower.', 'X X X X bird of paradise, a type of flower.', 'X X X X monkshood, a type of flower.', 'X X X X globe thistle, a type of flower.', 'X X X X snapdragon, a type of flower.', "X X X X colt's foot, a type of flower.", 'X X X X king protea, a type of flower.', 'X X X X spear thistle, a type of flower.', 'X X X X yellow iris, a type of flower.', 'X X X X globe-flower, a type of flower.', 'X X X X purple coneflower, a type of flower.', 'X X X X peruvian lily, a type of flower.', 'X X X X balloon flower, a type of flower.', 'X X X X giant white arum lily, a type of flower.', 'X X X X fire lily, a type of flower.', 'X X X X pincushion flower, a type of flower.', 'X X X X fritillary, a type of flower.', 'X X X X red ginger, a type of flower.', 'X X X X grape hyacinth, a type of flower.', 'X X X X corn poppy, a type of flower.', 'X X X X prince of wales feathers, a type of flower.', 'X X X X stemless gentian, a type of flower.', 'X X X X artichoke, a type of flower.', 'X X X X sweet william, a type of flower.', 'X X X X carnation, a type of flower.', 'X X X X garden phlox, a type of flower.', 'X X X X love in the mist, a type of flower.', 'X X X X mexican aster, a type of flower.', 'X X X X alpine sea holly, a type of flower.', 'X X X X ruby-lipped cattleya, a type of flower.', 'X X X X cape flower, a type of flower.', 'X X X X great masterwort, a type of flower.', 'X X X X siam tulip, a type of flower.', 'X X X X lenten rose, a type of flower.', 'X X X X barbeton daisy, a type of flower.', 'X X X X daffodil, a type of flower.', 'X X X X sword lily, a type of flower.', 'X X X X poinsettia, a type of flower.', 'X X X X bolero deep blue, a type of flower.', 'X X X X wallflower, a type of flower.', 'X X X X marigold, a type of flower.', 'X X X X buttercup, a type of flower.', 'X X X X oxeye daisy, a type of flower.', 'X X X X common dandelion, a type of flower.', 'X X X X petunia, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/26] time 0.156 (0.537) data 0.000 (0.126) loss 5.7305 (5.9875) acc 59.3750 (55.6250) lr 1.0000e-05 eta 0:11:35
epoch [1/50] batch [10/26] time 0.156 (0.346) data 0.001 (0.063) loss 6.1758 (6.0695) acc 50.0000 (54.6875) lr 1.0000e-05 eta 0:07:26
epoch [1/50] batch [15/26] time 0.154 (0.282) data 0.000 (0.042) loss 6.4297 (6.0523) acc 40.6250 (54.5833) lr 1.0000e-05 eta 0:06:02
epoch [1/50] batch [20/26] time 0.153 (0.250) data 0.000 (0.032) loss 5.9102 (6.0480) acc 56.2500 (54.3750) lr 1.0000e-05 eta 0:05:20
epoch [1/50] batch [25/26] time 0.154 (0.231) data 0.000 (0.025) loss 6.5469 (6.0738) acc 46.8750 (52.8750) lr 1.0000e-05 eta 0:04:54
epoch [2/50] batch [5/26] time 0.152 (0.277) data 0.000 (0.123) loss 4.8359 (5.0133) acc 56.2500 (57.5000) lr 2.0000e-03 eta 0:05:51
epoch [2/50] batch [10/26] time 0.157 (0.216) data 0.000 (0.062) loss 4.5039 (4.6877) acc 56.2500 (60.0000) lr 2.0000e-03 eta 0:04:32
epoch [2/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.041) loss 4.0859 (4.4697) acc 65.6250 (62.2917) lr 2.0000e-03 eta 0:04:05
epoch [2/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.031) loss 3.2988 (4.2755) acc 90.6250 (64.2188) lr 2.0000e-03 eta 0:03:51
epoch [2/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.025) loss 3.5664 (4.1625) acc 78.1250 (65.2500) lr 2.0000e-03 eta 0:03:42
epoch [3/50] batch [5/26] time 0.153 (0.269) data 0.000 (0.115) loss 3.1523 (3.3488) acc 75.0000 (70.6250) lr 1.9980e-03 eta 0:05:34
epoch [3/50] batch [10/26] time 0.156 (0.211) data 0.000 (0.058) loss 3.6367 (3.3322) acc 71.8750 (73.1250) lr 1.9980e-03 eta 0:04:21
epoch [3/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.038) loss 3.3027 (3.2755) acc 78.1250 (74.7917) lr 1.9980e-03 eta 0:03:56
epoch [3/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.029) loss 2.9922 (3.1482) acc 78.1250 (77.0312) lr 1.9980e-03 eta 0:03:43
epoch [3/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.023) loss 3.1230 (3.0970) acc 71.8750 (77.1250) lr 1.9980e-03 eta 0:03:35
epoch [4/50] batch [5/26] time 0.154 (0.274) data 0.000 (0.119) loss 2.7812 (2.7914) acc 78.1250 (79.3750) lr 1.9921e-03 eta 0:05:33
epoch [4/50] batch [10/26] time 0.155 (0.214) data 0.000 (0.060) loss 2.8320 (2.9109) acc 81.2500 (76.5625) lr 1.9921e-03 eta 0:04:19
epoch [4/50] batch [15/26] time 0.152 (0.194) data 0.000 (0.040) loss 2.7812 (2.8167) acc 84.3750 (78.9583) lr 1.9921e-03 eta 0:03:53
epoch [4/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.030) loss 2.8672 (2.7950) acc 75.0000 (79.5312) lr 1.9921e-03 eta 0:03:40
epoch [4/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.024) loss 2.9883 (2.7825) acc 81.2500 (80.7500) lr 1.9921e-03 eta 0:03:32
epoch [5/50] batch [5/26] time 0.154 (0.270) data 0.000 (0.115) loss 2.5020 (2.5633) acc 78.1250 (85.0000) lr 1.9823e-03 eta 0:05:21
epoch [5/50] batch [10/26] time 0.155 (0.212) data 0.000 (0.058) loss 2.3398 (2.4941) acc 93.7500 (87.5000) lr 1.9823e-03 eta 0:04:11
epoch [5/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.039) loss 2.7461 (2.4796) acc 68.7500 (86.0417) lr 1.9823e-03 eta 0:03:47
epoch [5/50] batch [20/26] time 0.154 (0.183) data 0.000 (0.029) loss 2.7109 (2.4868) acc 93.7500 (86.2500) lr 1.9823e-03 eta 0:03:35
epoch [5/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.023) loss 3.1289 (2.4741) acc 71.8750 (86.1250) lr 1.9823e-03 eta 0:03:27
epoch [6/50] batch [5/26] time 0.154 (0.269) data 0.000 (0.115) loss 2.0977 (2.2391) acc 100.0000 (91.8750) lr 1.9686e-03 eta 0:05:13
epoch [6/50] batch [10/26] time 0.155 (0.212) data 0.000 (0.058) loss 2.2734 (2.2469) acc 90.6250 (89.6875) lr 1.9686e-03 eta 0:04:05
epoch [6/50] batch [15/26] time 0.151 (0.193) data 0.000 (0.038) loss 2.4785 (2.2527) acc 87.5000 (89.3750) lr 1.9686e-03 eta 0:03:42
epoch [6/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.0938 (2.2840) acc 90.6250 (88.7500) lr 1.9686e-03 eta 0:03:30
epoch [6/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.023) loss 2.1016 (2.2773) acc 87.5000 (89.0000) lr 1.9686e-03 eta 0:03:22
epoch [7/50] batch [5/26] time 0.155 (0.274) data 0.000 (0.118) loss 2.4375 (2.0736) acc 81.2500 (91.8750) lr 1.9511e-03 eta 0:05:12
epoch [7/50] batch [10/26] time 0.158 (0.215) data 0.000 (0.059) loss 2.7949 (2.1361) acc 87.5000 (90.6250) lr 1.9511e-03 eta 0:04:03
epoch [7/50] batch [15/26] time 0.155 (0.195) data 0.000 (0.039) loss 2.2246 (2.1823) acc 84.3750 (90.0000) lr 1.9511e-03 eta 0:03:39
epoch [7/50] batch [20/26] time 0.156 (0.185) data 0.000 (0.030) loss 2.5117 (2.1928) acc 78.1250 (89.8438) lr 1.9511e-03 eta 0:03:27
epoch [7/50] batch [25/26] time 0.155 (0.179) data 0.000 (0.024) loss 2.5898 (2.1909) acc 81.2500 (89.2500) lr 1.9511e-03 eta 0:03:20
epoch [8/50] batch [5/26] time 0.154 (0.270) data 0.000 (0.114) loss 2.1191 (2.1508) acc 87.5000 (88.1250) lr 1.9298e-03 eta 0:05:00
epoch [8/50] batch [10/26] time 0.154 (0.212) data 0.000 (0.057) loss 2.3965 (2.1345) acc 87.5000 (89.6875) lr 1.9298e-03 eta 0:03:55
epoch [8/50] batch [15/26] time 0.154 (0.193) data 0.000 (0.038) loss 2.2617 (2.2092) acc 90.6250 (87.9167) lr 1.9298e-03 eta 0:03:32
epoch [8/50] batch [20/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.8594 (2.1623) acc 93.7500 (89.3750) lr 1.9298e-03 eta 0:03:20
epoch [8/50] batch [25/26] time 0.155 (0.177) data 0.000 (0.023) loss 2.1758 (2.1602) acc 87.5000 (89.2500) lr 1.9298e-03 eta 0:03:13
epoch [9/50] batch [5/26] time 0.154 (0.274) data 0.000 (0.118) loss 1.6934 (1.9471) acc 100.0000 (92.5000) lr 1.9048e-03 eta 0:04:57
epoch [9/50] batch [10/26] time 0.155 (0.214) data 0.000 (0.059) loss 2.0391 (1.9782) acc 90.6250 (92.5000) lr 1.9048e-03 eta 0:03:51
epoch [9/50] batch [15/26] time 0.154 (0.194) data 0.000 (0.039) loss 2.1387 (2.0148) acc 90.6250 (92.0833) lr 1.9048e-03 eta 0:03:29
epoch [9/50] batch [20/26] time 0.155 (0.184) data 0.000 (0.030) loss 2.3203 (2.0078) acc 87.5000 (92.5000) lr 1.9048e-03 eta 0:03:17
epoch [9/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 1.8633 (2.0030) acc 93.7500 (92.2500) lr 1.9048e-03 eta 0:03:10
epoch [10/50] batch [5/26] time 0.155 (0.284) data 0.000 (0.128) loss 2.0957 (1.8971) acc 84.3750 (92.5000) lr 1.8763e-03 eta 0:05:01
epoch [10/50] batch [10/26] time 0.155 (0.220) data 0.000 (0.064) loss 2.0195 (1.9599) acc 90.6250 (90.9375) lr 1.8763e-03 eta 0:03:52
epoch [10/50] batch [15/26] time 0.155 (0.198) data 0.000 (0.043) loss 2.0957 (1.9313) acc 84.3750 (91.6667) lr 1.8763e-03 eta 0:03:28
epoch [10/50] batch [20/26] time 0.154 (0.187) data 0.000 (0.032) loss 2.2480 (1.9453) acc 84.3750 (90.7812) lr 1.8763e-03 eta 0:03:15
epoch [10/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.026) loss 1.7578 (1.9330) acc 100.0000 (91.8750) lr 1.8763e-03 eta 0:03:08
epoch [11/50] batch [5/26] time 0.154 (0.278) data 0.000 (0.122) loss 1.8291 (1.7615) acc 96.8750 (95.0000) lr 1.8443e-03 eta 0:04:47
epoch [11/50] batch [10/26] time 0.155 (0.216) data 0.000 (0.061) loss 1.8711 (1.7556) acc 93.7500 (95.3125) lr 1.8443e-03 eta 0:03:42
epoch [11/50] batch [15/26] time 0.154 (0.196) data 0.000 (0.041) loss 1.9180 (1.8004) acc 96.8750 (94.7917) lr 1.8443e-03 eta 0:03:20
epoch [11/50] batch [20/26] time 0.154 (0.186) data 0.000 (0.031) loss 1.9141 (1.8627) acc 90.6250 (93.7500) lr 1.8443e-03 eta 0:03:09
epoch [11/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.025) loss 2.2695 (1.9008) acc 87.5000 (92.8750) lr 1.8443e-03 eta 0:03:02
epoch [12/50] batch [5/26] time 0.154 (0.272) data 0.000 (0.116) loss 1.9453 (1.8025) acc 90.6250 (94.3750) lr 1.8090e-03 eta 0:04:34
epoch [12/50] batch [10/26] time 0.155 (0.213) data 0.000 (0.058) loss 1.8848 (1.8862) acc 96.8750 (93.7500) lr 1.8090e-03 eta 0:03:33
epoch [12/50] batch [15/26] time 0.154 (0.193) data 0.000 (0.039) loss 1.9375 (1.8885) acc 87.5000 (92.9167) lr 1.8090e-03 eta 0:03:13
epoch [12/50] batch [20/26] time 0.154 (0.184) data 0.000 (0.029) loss 1.6836 (1.8729) acc 96.8750 (93.1250) lr 1.8090e-03 eta 0:03:02
epoch [12/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.023) loss 1.8906 (1.8931) acc 93.7500 (92.5000) lr 1.8090e-03 eta 0:02:55
epoch [13/50] batch [5/26] time 0.154 (0.262) data 0.000 (0.106) loss 1.7754 (1.9133) acc 96.8750 (93.1250) lr 1.7705e-03 eta 0:04:17
epoch [13/50] batch [10/26] time 0.154 (0.208) data 0.000 (0.053) loss 2.0703 (1.8885) acc 90.6250 (94.0625) lr 1.7705e-03 eta 0:03:23
epoch [13/50] batch [15/26] time 0.154 (0.190) data 0.000 (0.036) loss 1.7080 (1.8428) acc 96.8750 (94.1667) lr 1.7705e-03 eta 0:03:04
epoch [13/50] batch [20/26] time 0.154 (0.181) data 0.000 (0.027) loss 2.0156 (1.8806) acc 93.7500 (93.1250) lr 1.7705e-03 eta 0:02:55
epoch [13/50] batch [25/26] time 0.154 (0.175) data 0.000 (0.021) loss 1.7266 (1.8543) acc 100.0000 (93.8750) lr 1.7705e-03 eta 0:02:48
epoch [14/50] batch [5/26] time 0.154 (0.280) data 0.000 (0.124) loss 2.1641 (2.0061) acc 93.7500 (93.7500) lr 1.7290e-03 eta 0:04:28
epoch [14/50] batch [10/26] time 0.155 (0.217) data 0.000 (0.062) loss 1.7686 (1.8993) acc 93.7500 (93.4375) lr 1.7290e-03 eta 0:03:26
epoch [14/50] batch [15/26] time 0.154 (0.196) data 0.000 (0.042) loss 2.1348 (1.9036) acc 90.6250 (93.5417) lr 1.7290e-03 eta 0:03:05
epoch [14/50] batch [20/26] time 0.154 (0.186) data 0.000 (0.031) loss 1.8984 (1.8771) acc 96.8750 (93.5938) lr 1.7290e-03 eta 0:02:54
epoch [14/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.025) loss 1.9941 (1.8584) acc 90.6250 (93.7500) lr 1.7290e-03 eta 0:02:47
epoch [15/50] batch [5/26] time 0.155 (0.269) data 0.000 (0.114) loss 1.7637 (1.8607) acc 93.7500 (93.1250) lr 1.6845e-03 eta 0:04:10
epoch [15/50] batch [10/26] time 0.157 (0.212) data 0.000 (0.057) loss 2.0156 (1.8490) acc 84.3750 (92.5000) lr 1.6845e-03 eta 0:03:16
epoch [15/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.038) loss 1.9111 (1.8593) acc 93.7500 (92.9167) lr 1.6845e-03 eta 0:02:57
epoch [15/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.0527 (1.8513) acc 84.3750 (92.3438) lr 1.6845e-03 eta 0:02:47
epoch [15/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.023) loss 1.7480 (1.8564) acc 100.0000 (92.2500) lr 1.6845e-03 eta 0:02:41
epoch [16/50] batch [5/26] time 0.154 (0.275) data 0.000 (0.120) loss 1.9414 (1.8029) acc 93.7500 (95.6250) lr 1.6374e-03 eta 0:04:08
epoch [16/50] batch [10/26] time 0.155 (0.215) data 0.000 (0.060) loss 1.7891 (1.8347) acc 90.6250 (93.4375) lr 1.6374e-03 eta 0:03:13
epoch [16/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.9141 (1.8475) acc 93.7500 (92.9167) lr 1.6374e-03 eta 0:02:53
epoch [16/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.030) loss 1.5742 (1.8342) acc 96.8750 (93.4375) lr 1.6374e-03 eta 0:02:43
epoch [16/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 1.8379 (1.8331) acc 90.6250 (93.6250) lr 1.6374e-03 eta 0:02:37
epoch [17/50] batch [5/26] time 0.155 (0.274) data 0.000 (0.118) loss 1.4121 (1.7951) acc 100.0000 (93.1250) lr 1.5878e-03 eta 0:04:01
epoch [17/50] batch [10/26] time 0.156 (0.215) data 0.000 (0.059) loss 1.7461 (1.8034) acc 93.7500 (92.5000) lr 1.5878e-03 eta 0:03:07
epoch [17/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.7656 (1.7980) acc 90.6250 (92.2917) lr 1.5878e-03 eta 0:02:48
epoch [17/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.030) loss 2.1445 (1.8009) acc 93.7500 (92.6562) lr 1.5878e-03 eta 0:02:39
epoch [17/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 1.9355 (1.7816) acc 90.6250 (92.8750) lr 1.5878e-03 eta 0:02:33
epoch [18/50] batch [5/26] time 0.155 (0.279) data 0.000 (0.123) loss 1.7598 (1.7285) acc 90.6250 (91.8750) lr 1.5358e-03 eta 0:03:58
epoch [18/50] batch [10/26] time 0.154 (0.217) data 0.000 (0.062) loss 1.8223 (1.7840) acc 90.6250 (91.5625) lr 1.5358e-03 eta 0:03:03
epoch [18/50] batch [15/26] time 0.154 (0.196) data 0.000 (0.041) loss 1.6465 (1.8163) acc 96.8750 (91.8750) lr 1.5358e-03 eta 0:02:44
epoch [18/50] batch [20/26] time 0.154 (0.185) data 0.000 (0.031) loss 1.5684 (1.8057) acc 96.8750 (92.5000) lr 1.5358e-03 eta 0:02:35
epoch [18/50] batch [25/26] time 0.155 (0.179) data 0.000 (0.025) loss 1.7930 (1.8040) acc 93.7500 (92.8750) lr 1.5358e-03 eta 0:02:29
epoch [19/50] batch [5/26] time 0.154 (0.262) data 0.000 (0.108) loss 1.6357 (1.7619) acc 100.0000 (93.1250) lr 1.4818e-03 eta 0:03:36
epoch [19/50] batch [10/26] time 0.156 (0.208) data 0.000 (0.054) loss 1.9648 (1.7771) acc 90.6250 (93.7500) lr 1.4818e-03 eta 0:02:50
epoch [19/50] batch [15/26] time 0.155 (0.190) data 0.000 (0.036) loss 1.6260 (1.7665) acc 100.0000 (94.1667) lr 1.4818e-03 eta 0:02:35
epoch [19/50] batch [20/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.6680 (1.7943) acc 96.8750 (93.5938) lr 1.4818e-03 eta 0:02:26
epoch [19/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.022) loss 1.4883 (1.7896) acc 96.8750 (93.8750) lr 1.4818e-03 eta 0:02:21
epoch [20/50] batch [5/26] time 0.155 (0.269) data 0.000 (0.114) loss 1.9541 (1.8035) acc 90.6250 (92.5000) lr 1.4258e-03 eta 0:03:35
epoch [20/50] batch [10/26] time 0.155 (0.212) data 0.000 (0.057) loss 1.4766 (1.7256) acc 96.8750 (94.3750) lr 1.4258e-03 eta 0:02:48
epoch [20/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.038) loss 1.6025 (1.7643) acc 96.8750 (94.3750) lr 1.4258e-03 eta 0:02:32
epoch [20/50] batch [20/26] time 0.155 (0.183) data 0.000 (0.029) loss 1.8291 (1.7411) acc 93.7500 (95.0000) lr 1.4258e-03 eta 0:02:23
epoch [20/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.023) loss 2.1250 (1.7676) acc 90.6250 (94.8750) lr 1.4258e-03 eta 0:02:18
epoch [21/50] batch [5/26] time 0.154 (0.275) data 0.000 (0.119) loss 1.7891 (1.7145) acc 96.8750 (95.0000) lr 1.3681e-03 eta 0:03:32
epoch [21/50] batch [10/26] time 0.156 (0.215) data 0.000 (0.060) loss 1.7959 (1.7898) acc 100.0000 (95.0000) lr 1.3681e-03 eta 0:02:45
epoch [21/50] batch [15/26] time 0.154 (0.195) data 0.000 (0.040) loss 1.7803 (1.7700) acc 93.7500 (95.2083) lr 1.3681e-03 eta 0:02:28
epoch [21/50] batch [20/26] time 0.154 (0.185) data 0.000 (0.030) loss 1.8408 (1.7979) acc 90.6250 (94.3750) lr 1.3681e-03 eta 0:02:20
epoch [21/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.024) loss 1.5977 (1.8031) acc 96.8750 (94.1250) lr 1.3681e-03 eta 0:02:14
epoch [22/50] batch [5/26] time 0.155 (0.277) data 0.000 (0.121) loss 1.8633 (1.7320) acc 93.7500 (95.6250) lr 1.3090e-03 eta 0:03:27
epoch [22/50] batch [10/26] time 0.155 (0.216) data 0.000 (0.061) loss 1.6777 (1.7146) acc 96.8750 (96.2500) lr 1.3090e-03 eta 0:02:40
epoch [22/50] batch [15/26] time 0.154 (0.195) data 0.000 (0.040) loss 1.8330 (1.7478) acc 90.6250 (94.7917) lr 1.3090e-03 eta 0:02:24
epoch [22/50] batch [20/26] time 0.154 (0.185) data 0.000 (0.030) loss 1.7881 (1.7654) acc 87.5000 (93.5938) lr 1.3090e-03 eta 0:02:15
epoch [22/50] batch [25/26] time 0.155 (0.179) data 0.000 (0.024) loss 1.9844 (1.7766) acc 90.6250 (93.3750) lr 1.3090e-03 eta 0:02:10
epoch [23/50] batch [5/26] time 0.154 (0.275) data 0.000 (0.120) loss 1.7109 (1.5947) acc 100.0000 (97.5000) lr 1.2487e-03 eta 0:03:18
epoch [23/50] batch [10/26] time 0.156 (0.215) data 0.000 (0.060) loss 1.5430 (1.6083) acc 100.0000 (97.8125) lr 1.2487e-03 eta 0:02:34
epoch [23/50] batch [15/26] time 0.157 (0.195) data 0.000 (0.040) loss 1.9062 (1.6427) acc 87.5000 (96.2500) lr 1.2487e-03 eta 0:02:19
epoch [23/50] batch [20/26] time 0.155 (0.186) data 0.000 (0.030) loss 1.5576 (1.7006) acc 96.8750 (95.4688) lr 1.2487e-03 eta 0:02:11
epoch [23/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.024) loss 1.5205 (1.7142) acc 96.8750 (95.5000) lr 1.2487e-03 eta 0:02:06
epoch [24/50] batch [5/26] time 0.154 (0.271) data 0.000 (0.117) loss 1.6357 (1.6566) acc 100.0000 (96.2500) lr 1.1874e-03 eta 0:03:09
epoch [24/50] batch [10/26] time 0.155 (0.213) data 0.000 (0.058) loss 1.7646 (1.6472) acc 96.8750 (95.9375) lr 1.1874e-03 eta 0:02:27
epoch [24/50] batch [15/26] time 0.156 (0.194) data 0.000 (0.039) loss 1.7568 (1.7036) acc 96.8750 (95.4167) lr 1.1874e-03 eta 0:02:12
epoch [24/50] batch [20/26] time 0.155 (0.184) data 0.000 (0.029) loss 1.8252 (1.7143) acc 90.6250 (94.8438) lr 1.1874e-03 eta 0:02:05
epoch [24/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.023) loss 1.8467 (1.6971) acc 93.7500 (95.0000) lr 1.1874e-03 eta 0:02:00
epoch [25/50] batch [5/26] time 0.154 (0.275) data 0.000 (0.120) loss 1.5645 (1.5990) acc 96.8750 (96.2500) lr 1.1253e-03 eta 0:03:04
epoch [25/50] batch [10/26] time 0.158 (0.216) data 0.000 (0.060) loss 1.6562 (1.6648) acc 93.7500 (94.3750) lr 1.1253e-03 eta 0:02:23
epoch [25/50] batch [15/26] time 0.155 (0.195) data 0.000 (0.040) loss 1.7266 (1.7004) acc 87.5000 (92.7083) lr 1.1253e-03 eta 0:02:09
epoch [25/50] batch [20/26] time 0.154 (0.185) data 0.000 (0.030) loss 1.6680 (1.6709) acc 90.6250 (94.0625) lr 1.1253e-03 eta 0:02:01
epoch [25/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.024) loss 1.8799 (1.6644) acc 93.7500 (94.6250) lr 1.1253e-03 eta 0:01:56
epoch [26/50] batch [5/26] time 0.154 (0.271) data 0.000 (0.116) loss 1.7891 (1.6971) acc 93.7500 (96.2500) lr 1.0628e-03 eta 0:02:54
epoch [26/50] batch [10/26] time 0.155 (0.213) data 0.000 (0.058) loss 1.7832 (1.7480) acc 90.6250 (94.3750) lr 1.0628e-03 eta 0:02:16
epoch [26/50] batch [15/26] time 0.154 (0.193) data 0.000 (0.039) loss 1.6299 (1.8133) acc 100.0000 (94.1667) lr 1.0628e-03 eta 0:02:02
epoch [26/50] batch [20/26] time 0.155 (0.183) data 0.000 (0.029) loss 1.8340 (1.7829) acc 90.6250 (94.2188) lr 1.0628e-03 eta 0:01:55
epoch [26/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.023) loss 1.8584 (1.7588) acc 87.5000 (94.3750) lr 1.0628e-03 eta 0:01:51
epoch [27/50] batch [5/26] time 0.155 (0.259) data 0.000 (0.104) loss 1.6367 (1.6525) acc 96.8750 (93.7500) lr 1.0000e-03 eta 0:02:40
epoch [27/50] batch [10/26] time 0.155 (0.207) data 0.000 (0.052) loss 1.7754 (1.7246) acc 96.8750 (94.3750) lr 1.0000e-03 eta 0:02:07
epoch [27/50] batch [15/26] time 0.154 (0.189) data 0.000 (0.035) loss 1.8057 (1.7548) acc 96.8750 (94.3750) lr 1.0000e-03 eta 0:01:55
epoch [27/50] batch [20/26] time 0.155 (0.181) data 0.000 (0.026) loss 1.7227 (1.7394) acc 96.8750 (94.5312) lr 1.0000e-03 eta 0:01:49
epoch [27/50] batch [25/26] time 0.153 (0.175) data 0.000 (0.021) loss 1.7207 (1.7209) acc 96.8750 (94.8750) lr 1.0000e-03 eta 0:01:45
epoch [28/50] batch [5/26] time 0.156 (0.261) data 0.000 (0.105) loss 1.7539 (1.5736) acc 93.7500 (97.5000) lr 9.3721e-04 eta 0:02:34
epoch [28/50] batch [10/26] time 0.156 (0.208) data 0.000 (0.053) loss 2.0078 (1.6796) acc 87.5000 (96.5625) lr 9.3721e-04 eta 0:02:02
epoch [28/50] batch [15/26] time 0.155 (0.190) data 0.000 (0.035) loss 1.7012 (1.7449) acc 96.8750 (95.4167) lr 9.3721e-04 eta 0:01:50
epoch [28/50] batch [20/26] time 0.154 (0.181) data 0.000 (0.026) loss 1.5410 (1.7257) acc 100.0000 (95.0000) lr 9.3721e-04 eta 0:01:44
epoch [28/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.021) loss 1.7432 (1.7426) acc 96.8750 (94.8750) lr 9.3721e-04 eta 0:01:40
epoch [29/50] batch [5/26] time 0.154 (0.269) data 0.000 (0.115) loss 1.7188 (1.7256) acc 96.8750 (93.7500) lr 8.7467e-04 eta 0:02:32
epoch [29/50] batch [10/26] time 0.155 (0.212) data 0.000 (0.058) loss 1.4590 (1.7123) acc 93.7500 (94.3750) lr 8.7467e-04 eta 0:01:59
epoch [29/50] batch [15/26] time 0.153 (0.193) data 0.000 (0.038) loss 1.7324 (1.7451) acc 93.7500 (92.5000) lr 8.7467e-04 eta 0:01:47
epoch [29/50] batch [20/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.6074 (1.7348) acc 93.7500 (92.9688) lr 8.7467e-04 eta 0:01:40
epoch [29/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.023) loss 1.4863 (1.7098) acc 96.8750 (93.7500) lr 8.7467e-04 eta 0:01:36
epoch [30/50] batch [5/26] time 0.154 (0.264) data 0.000 (0.108) loss 1.4219 (1.4646) acc 100.0000 (97.5000) lr 8.1262e-04 eta 0:02:22
epoch [30/50] batch [10/26] time 0.155 (0.209) data 0.000 (0.054) loss 1.8525 (1.5787) acc 90.6250 (95.3125) lr 8.1262e-04 eta 0:01:51
epoch [30/50] batch [15/26] time 0.154 (0.191) data 0.000 (0.036) loss 1.5527 (1.5566) acc 93.7500 (96.2500) lr 8.1262e-04 eta 0:01:41
epoch [30/50] batch [20/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.5020 (1.5941) acc 100.0000 (95.4688) lr 8.1262e-04 eta 0:01:35
epoch [30/50] batch [25/26] time 0.156 (0.176) data 0.000 (0.022) loss 1.9814 (1.6004) acc 90.6250 (95.7500) lr 8.1262e-04 eta 0:01:31
epoch [31/50] batch [5/26] time 0.154 (0.262) data 0.000 (0.108) loss 1.5137 (1.5152) acc 100.0000 (99.3750) lr 7.5131e-04 eta 0:02:15
epoch [31/50] batch [10/26] time 0.155 (0.208) data 0.000 (0.054) loss 1.3965 (1.5079) acc 93.7500 (97.8125) lr 7.5131e-04 eta 0:01:46
epoch [31/50] batch [15/26] time 0.155 (0.190) data 0.000 (0.036) loss 1.6377 (1.5491) acc 96.8750 (97.2917) lr 7.5131e-04 eta 0:01:36
epoch [31/50] batch [20/26] time 0.156 (0.182) data 0.000 (0.027) loss 1.6592 (1.5965) acc 96.8750 (96.2500) lr 7.5131e-04 eta 0:01:30
epoch [31/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.022) loss 2.1152 (1.6340) acc 87.5000 (95.3750) lr 7.5131e-04 eta 0:01:27
epoch [32/50] batch [5/26] time 0.155 (0.256) data 0.000 (0.101) loss 1.8379 (1.7781) acc 96.8750 (95.0000) lr 6.9098e-04 eta 0:02:05
epoch [32/50] batch [10/26] time 0.158 (0.206) data 0.000 (0.051) loss 1.5977 (1.6833) acc 93.7500 (95.3125) lr 6.9098e-04 eta 0:01:39
epoch [32/50] batch [15/26] time 0.155 (0.189) data 0.000 (0.034) loss 1.5840 (1.6568) acc 96.8750 (95.8333) lr 6.9098e-04 eta 0:01:30
epoch [32/50] batch [20/26] time 0.154 (0.180) data 0.000 (0.025) loss 1.6641 (1.6439) acc 100.0000 (96.5625) lr 6.9098e-04 eta 0:01:25
epoch [32/50] batch [25/26] time 0.154 (0.175) data 0.000 (0.020) loss 1.4561 (1.6287) acc 96.8750 (96.5000) lr 6.9098e-04 eta 0:01:22
epoch [33/50] batch [5/26] time 0.155 (0.258) data 0.000 (0.102) loss 1.6836 (1.7971) acc 93.7500 (92.5000) lr 6.3188e-04 eta 0:01:59
epoch [33/50] batch [10/26] time 0.156 (0.207) data 0.000 (0.051) loss 1.8965 (1.7104) acc 87.5000 (94.0625) lr 6.3188e-04 eta 0:01:34
epoch [33/50] batch [15/26] time 0.154 (0.189) data 0.000 (0.034) loss 1.6152 (1.7117) acc 96.8750 (94.5833) lr 6.3188e-04 eta 0:01:25
epoch [33/50] batch [20/26] time 0.154 (0.180) data 0.000 (0.026) loss 1.6396 (1.6985) acc 100.0000 (94.8438) lr 6.3188e-04 eta 0:01:20
epoch [33/50] batch [25/26] time 0.154 (0.175) data 0.000 (0.021) loss 1.7617 (1.6871) acc 90.6250 (94.6250) lr 6.3188e-04 eta 0:01:17
epoch [34/50] batch [5/26] time 0.155 (0.263) data 0.000 (0.108) loss 1.7109 (1.5762) acc 93.7500 (96.8750) lr 5.7422e-04 eta 0:01:55
epoch [34/50] batch [10/26] time 0.156 (0.209) data 0.000 (0.054) loss 1.4365 (1.5687) acc 100.0000 (96.8750) lr 5.7422e-04 eta 0:01:30
epoch [34/50] batch [15/26] time 0.154 (0.191) data 0.000 (0.036) loss 1.5078 (1.5761) acc 96.8750 (96.8750) lr 5.7422e-04 eta 0:01:21
epoch [34/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.027) loss 1.6133 (1.5861) acc 96.8750 (96.5625) lr 5.7422e-04 eta 0:01:16
epoch [34/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.022) loss 1.7373 (1.5852) acc 90.6250 (96.2500) lr 5.7422e-04 eta 0:01:13
epoch [35/50] batch [5/26] time 0.154 (0.263) data 0.000 (0.108) loss 1.6211 (1.6580) acc 96.8750 (96.8750) lr 5.1825e-04 eta 0:01:48
epoch [35/50] batch [10/26] time 0.157 (0.209) data 0.000 (0.054) loss 1.3555 (1.6122) acc 100.0000 (97.5000) lr 5.1825e-04 eta 0:01:24
epoch [35/50] batch [15/26] time 0.154 (0.191) data 0.000 (0.036) loss 1.6895 (1.6087) acc 93.7500 (96.4583) lr 5.1825e-04 eta 0:01:16
epoch [35/50] batch [20/26] time 0.155 (0.182) data 0.000 (0.027) loss 1.8486 (1.6369) acc 90.6250 (95.3125) lr 5.1825e-04 eta 0:01:11
epoch [35/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.022) loss 1.8359 (1.6277) acc 96.8750 (95.6250) lr 5.1825e-04 eta 0:01:08
epoch [36/50] batch [5/26] time 0.154 (0.263) data 0.000 (0.107) loss 1.3828 (1.6072) acc 100.0000 (98.1250) lr 4.6417e-04 eta 0:01:41
epoch [36/50] batch [10/26] time 0.155 (0.209) data 0.000 (0.054) loss 1.5898 (1.6005) acc 90.6250 (97.1875) lr 4.6417e-04 eta 0:01:19
epoch [36/50] batch [15/26] time 0.157 (0.191) data 0.000 (0.036) loss 1.6348 (1.6042) acc 93.7500 (96.8750) lr 4.6417e-04 eta 0:01:11
epoch [36/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.027) loss 1.8066 (1.6132) acc 93.7500 (96.4062) lr 4.6417e-04 eta 0:01:07
epoch [36/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.022) loss 1.9004 (1.6449) acc 87.5000 (95.6250) lr 4.6417e-04 eta 0:01:04
epoch [37/50] batch [5/26] time 0.155 (0.251) data 0.000 (0.096) loss 1.5908 (1.6004) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:01:30
epoch [37/50] batch [10/26] time 0.156 (0.203) data 0.000 (0.048) loss 1.9072 (1.6241) acc 93.7500 (96.5625) lr 4.1221e-04 eta 0:01:11
epoch [37/50] batch [15/26] time 0.154 (0.187) data 0.000 (0.032) loss 1.5439 (1.6283) acc 93.7500 (96.0417) lr 4.1221e-04 eta 0:01:05
epoch [37/50] batch [20/26] time 0.155 (0.179) data 0.000 (0.024) loss 1.4941 (1.6193) acc 100.0000 (96.0938) lr 4.1221e-04 eta 0:01:01
epoch [37/50] batch [25/26] time 0.154 (0.174) data 0.000 (0.019) loss 1.6494 (1.6282) acc 96.8750 (96.3750) lr 4.1221e-04 eta 0:00:58
epoch [38/50] batch [5/26] time 0.155 (0.259) data 0.000 (0.103) loss 1.4375 (1.5588) acc 93.7500 (95.6250) lr 3.6258e-04 eta 0:01:26
epoch [38/50] batch [10/26] time 0.155 (0.207) data 0.000 (0.052) loss 1.8828 (1.6332) acc 90.6250 (95.9375) lr 3.6258e-04 eta 0:01:07
epoch [38/50] batch [15/26] time 0.154 (0.189) data 0.000 (0.035) loss 2.0176 (1.6727) acc 93.7500 (95.8333) lr 3.6258e-04 eta 0:01:01
epoch [38/50] batch [20/26] time 0.154 (0.180) data 0.000 (0.026) loss 1.4668 (1.6585) acc 100.0000 (95.7812) lr 3.6258e-04 eta 0:00:57
epoch [38/50] batch [25/26] time 0.155 (0.175) data 0.000 (0.021) loss 1.7510 (1.6873) acc 93.7500 (94.8750) lr 3.6258e-04 eta 0:00:54
epoch [39/50] batch [5/26] time 0.155 (0.270) data 0.000 (0.115) loss 1.4395 (1.6299) acc 100.0000 (96.2500) lr 3.1545e-04 eta 0:01:22
epoch [39/50] batch [10/26] time 0.155 (0.212) data 0.000 (0.057) loss 1.6475 (1.6438) acc 93.7500 (95.3125) lr 3.1545e-04 eta 0:01:04
epoch [39/50] batch [15/26] time 0.154 (0.193) data 0.000 (0.038) loss 1.7441 (1.6255) acc 93.7500 (95.6250) lr 3.1545e-04 eta 0:00:57
epoch [39/50] batch [20/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.5684 (1.6391) acc 93.7500 (95.0000) lr 3.1545e-04 eta 0:00:53
epoch [39/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.023) loss 1.5430 (1.6223) acc 93.7500 (95.0000) lr 3.1545e-04 eta 0:00:50
epoch [40/50] batch [5/26] time 0.155 (0.268) data 0.000 (0.112) loss 1.4648 (1.5697) acc 96.8750 (95.6250) lr 2.7103e-04 eta 0:01:15
epoch [40/50] batch [10/26] time 0.157 (0.212) data 0.000 (0.056) loss 1.5605 (1.5228) acc 100.0000 (97.5000) lr 2.7103e-04 eta 0:00:58
epoch [40/50] batch [15/26] time 0.154 (0.193) data 0.000 (0.038) loss 1.7500 (1.6410) acc 96.8750 (95.6250) lr 2.7103e-04 eta 0:00:52
epoch [40/50] batch [20/26] time 0.155 (0.183) data 0.000 (0.028) loss 1.7510 (1.6004) acc 93.7500 (96.4062) lr 2.7103e-04 eta 0:00:48
epoch [40/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.023) loss 1.5000 (1.6154) acc 96.8750 (96.1250) lr 2.7103e-04 eta 0:00:46
epoch [41/50] batch [5/26] time 0.155 (0.258) data 0.000 (0.102) loss 1.6748 (1.6410) acc 100.0000 (96.2500) lr 2.2949e-04 eta 0:01:05
epoch [41/50] batch [10/26] time 0.156 (0.207) data 0.000 (0.051) loss 1.6816 (1.6627) acc 96.8750 (95.6250) lr 2.2949e-04 eta 0:00:51
epoch [41/50] batch [15/26] time 0.154 (0.189) data 0.000 (0.034) loss 1.9287 (1.6438) acc 87.5000 (95.6250) lr 2.2949e-04 eta 0:00:46
epoch [41/50] batch [20/26] time 0.154 (0.180) data 0.000 (0.026) loss 1.4521 (1.6401) acc 100.0000 (95.9375) lr 2.2949e-04 eta 0:00:43
epoch [41/50] batch [25/26] time 0.154 (0.175) data 0.000 (0.021) loss 1.7041 (1.6436) acc 93.7500 (95.6250) lr 2.2949e-04 eta 0:00:41
epoch [42/50] batch [5/26] time 0.154 (0.266) data 0.000 (0.111) loss 1.4336 (1.5262) acc 100.0000 (96.2500) lr 1.9098e-04 eta 0:01:00
epoch [42/50] batch [10/26] time 0.155 (0.211) data 0.000 (0.056) loss 1.5176 (1.5642) acc 96.8750 (95.3125) lr 1.9098e-04 eta 0:00:47
epoch [42/50] batch [15/26] time 0.154 (0.192) data 0.000 (0.037) loss 1.4854 (1.5868) acc 100.0000 (95.4167) lr 1.9098e-04 eta 0:00:42
epoch [42/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.6270 (1.5983) acc 93.7500 (95.7812) lr 1.9098e-04 eta 0:00:39
epoch [42/50] batch [25/26] time 0.155 (0.177) data 0.000 (0.022) loss 1.5439 (1.5861) acc 93.7500 (96.0000) lr 1.9098e-04 eta 0:00:36
epoch [43/50] batch [5/26] time 0.154 (0.278) data 0.000 (0.123) loss 1.2617 (1.5574) acc 100.0000 (95.0000) lr 1.5567e-04 eta 0:00:56
epoch [43/50] batch [10/26] time 0.156 (0.217) data 0.000 (0.062) loss 1.7842 (1.6141) acc 96.8750 (95.6250) lr 1.5567e-04 eta 0:00:42
epoch [43/50] batch [15/26] time 0.154 (0.196) data 0.000 (0.041) loss 1.4434 (1.5955) acc 96.8750 (95.6250) lr 1.5567e-04 eta 0:00:37
epoch [43/50] batch [20/26] time 0.155 (0.186) data 0.000 (0.031) loss 1.4795 (1.6046) acc 100.0000 (96.0938) lr 1.5567e-04 eta 0:00:34
epoch [43/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.025) loss 1.5410 (1.6205) acc 100.0000 (96.1250) lr 1.5567e-04 eta 0:00:32
epoch [44/50] batch [5/26] time 0.155 (0.270) data 0.000 (0.114) loss 1.6328 (1.5697) acc 90.6250 (96.2500) lr 1.2369e-04 eta 0:00:47
epoch [44/50] batch [10/26] time 0.155 (0.212) data 0.000 (0.057) loss 1.4893 (1.5905) acc 100.0000 (95.9375) lr 1.2369e-04 eta 0:00:36
epoch [44/50] batch [15/26] time 0.156 (0.193) data 0.000 (0.038) loss 1.8047 (1.6044) acc 90.6250 (95.8333) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [20/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.4209 (1.5807) acc 100.0000 (96.4062) lr 1.2369e-04 eta 0:00:29
epoch [44/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.023) loss 1.4180 (1.5797) acc 100.0000 (96.6250) lr 1.2369e-04 eta 0:00:27
epoch [45/50] batch [5/26] time 0.154 (0.267) data 0.000 (0.110) loss 1.6445 (1.7031) acc 96.8750 (94.3750) lr 9.5173e-05 eta 0:00:40
epoch [45/50] batch [10/26] time 0.156 (0.211) data 0.000 (0.055) loss 2.1094 (1.6961) acc 90.6250 (94.6875) lr 9.5173e-05 eta 0:00:30
epoch [45/50] batch [15/26] time 0.154 (0.192) data 0.000 (0.037) loss 1.3672 (1.6430) acc 100.0000 (95.8333) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [20/26] time 0.155 (0.183) data 0.000 (0.028) loss 1.4834 (1.6235) acc 96.8750 (96.0938) lr 9.5173e-05 eta 0:00:24
epoch [45/50] batch [25/26] time 0.155 (0.177) data 0.000 (0.022) loss 1.7402 (1.6499) acc 100.0000 (96.2500) lr 9.5173e-05 eta 0:00:23
epoch [46/50] batch [5/26] time 0.155 (0.270) data 0.000 (0.113) loss 1.5977 (1.5535) acc 96.8750 (97.5000) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [10/26] time 0.155 (0.212) data 0.000 (0.057) loss 1.6465 (1.6109) acc 90.6250 (95.9375) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.038) loss 1.7793 (1.5917) acc 93.7500 (96.0417) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.155 (0.184) data 0.000 (0.029) loss 1.7207 (1.6076) acc 96.8750 (96.4062) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.156 (0.178) data 0.000 (0.023) loss 1.8311 (1.5920) acc 93.7500 (96.6250) lr 7.0224e-05 eta 0:00:18
epoch [47/50] batch [5/26] time 0.155 (0.267) data 0.000 (0.112) loss 1.7998 (1.6902) acc 93.7500 (95.0000) lr 4.8943e-05 eta 0:00:26
epoch [47/50] batch [10/26] time 0.155 (0.211) data 0.000 (0.056) loss 1.6719 (1.6477) acc 96.8750 (95.6250) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [15/26] time 0.154 (0.192) data 0.000 (0.037) loss 1.4287 (1.6163) acc 100.0000 (95.8333) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [20/26] time 0.155 (0.183) data 0.000 (0.028) loss 1.8535 (1.6244) acc 93.7500 (95.9375) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.022) loss 1.4160 (1.6112) acc 100.0000 (95.8750) lr 4.8943e-05 eta 0:00:13
epoch [48/50] batch [5/26] time 0.155 (0.263) data 0.000 (0.108) loss 1.4453 (1.6404) acc 96.8750 (96.2500) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [10/26] time 0.155 (0.209) data 0.000 (0.054) loss 1.3740 (1.6036) acc 100.0000 (96.8750) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [15/26] time 0.155 (0.191) data 0.000 (0.036) loss 1.6758 (1.6764) acc 96.8750 (95.6250) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.027) loss 1.3115 (1.6167) acc 100.0000 (96.4062) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.022) loss 2.0273 (1.6221) acc 87.5000 (95.7500) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.154 (0.261) data 0.000 (0.106) loss 1.6973 (1.7982) acc 93.7500 (93.7500) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [10/26] time 0.156 (0.209) data 0.000 (0.053) loss 1.5645 (1.6521) acc 100.0000 (95.6250) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [15/26] time 0.154 (0.190) data 0.000 (0.035) loss 1.4766 (1.6413) acc 96.8750 (95.4167) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.4697 (1.6778) acc 100.0000 (95.1562) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.021) loss 1.7773 (1.6981) acc 93.7500 (95.0000) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.154 (0.266) data 0.000 (0.110) loss 1.7764 (1.7787) acc 96.8750 (94.3750) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/26] time 0.156 (0.210) data 0.000 (0.055) loss 1.4082 (1.6893) acc 100.0000 (95.3125) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.155 (0.192) data 0.000 (0.037) loss 1.7051 (1.6789) acc 93.7500 (95.6250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.6201 (1.7029) acc 96.8750 (95.0000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.155 (0.177) data 0.000 (0.022) loss 1.5615 (1.7126) acc 96.8750 (94.5000) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:10,  5.49s/it] 67%|██████▋   | 2/3 [00:06<00:02,  2.93s/it]100%|██████████| 3/3 [00:06<00:00,  1.67s/it]100%|██████████| 3/3 [00:06<00:00,  2.29s/it]
=> result
* total: 1,053
* correct: 1,026
* accuracy: 97.4%
* error: 2.6%
* macro_f1: 97.4%
Elapsed: 0:03:59
Run this job and save the output to output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,053
---------  -------------
['pink primrose', 'hard-leaved pocket orchid', 'canterbury bells', 'sweet pea', 'english marigold', 'tiger lily', 'moon orchid', 'bird of paradise', 'monkshood', 'globe thistle', 'snapdragon', "colt's foot", 'king protea', 'spear thistle', 'yellow iris', 'globe-flower', 'purple coneflower', 'peruvian lily', 'balloon flower', 'giant white arum lily', 'fire lily', 'pincushion flower', 'fritillary', 'red ginger', 'grape hyacinth', 'corn poppy', 'prince of wales feathers', 'stemless gentian', 'artichoke', 'sweet william', 'carnation', 'garden phlox', 'love in the mist', 'mexican aster', 'alpine sea holly', 'ruby-lipped cattleya', 'cape flower', 'great masterwort', 'siam tulip', 'lenten rose', 'barbeton daisy', 'daffodil', 'sword lily', 'poinsettia', 'bolero deep blue', 'wallflower', 'marigold', 'buttercup', 'oxeye daisy', 'common dandelion', 'petunia']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X pink primrose, a type of flower.', 'X X X X hard-leaved pocket orchid, a type of flower.', 'X X X X canterbury bells, a type of flower.', 'X X X X sweet pea, a type of flower.', 'X X X X english marigold, a type of flower.', 'X X X X tiger lily, a type of flower.', 'X X X X moon orchid, a type of flower.', 'X X X X bird of paradise, a type of flower.', 'X X X X monkshood, a type of flower.', 'X X X X globe thistle, a type of flower.', 'X X X X snapdragon, a type of flower.', "X X X X colt's foot, a type of flower.", 'X X X X king protea, a type of flower.', 'X X X X spear thistle, a type of flower.', 'X X X X yellow iris, a type of flower.', 'X X X X globe-flower, a type of flower.', 'X X X X purple coneflower, a type of flower.', 'X X X X peruvian lily, a type of flower.', 'X X X X balloon flower, a type of flower.', 'X X X X giant white arum lily, a type of flower.', 'X X X X fire lily, a type of flower.', 'X X X X pincushion flower, a type of flower.', 'X X X X fritillary, a type of flower.', 'X X X X red ginger, a type of flower.', 'X X X X grape hyacinth, a type of flower.', 'X X X X corn poppy, a type of flower.', 'X X X X prince of wales feathers, a type of flower.', 'X X X X stemless gentian, a type of flower.', 'X X X X artichoke, a type of flower.', 'X X X X sweet william, a type of flower.', 'X X X X carnation, a type of flower.', 'X X X X garden phlox, a type of flower.', 'X X X X love in the mist, a type of flower.', 'X X X X mexican aster, a type of flower.', 'X X X X alpine sea holly, a type of flower.', 'X X X X ruby-lipped cattleya, a type of flower.', 'X X X X cape flower, a type of flower.', 'X X X X great masterwort, a type of flower.', 'X X X X siam tulip, a type of flower.', 'X X X X lenten rose, a type of flower.', 'X X X X barbeton daisy, a type of flower.', 'X X X X daffodil, a type of flower.', 'X X X X sword lily, a type of flower.', 'X X X X poinsettia, a type of flower.', 'X X X X bolero deep blue, a type of flower.', 'X X X X wallflower, a type of flower.', 'X X X X marigold, a type of flower.', 'X X X X buttercup, a type of flower.', 'X X X X oxeye daisy, a type of flower.', 'X X X X common dandelion, a type of flower.', 'X X X X petunia, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/26] time 0.154 (0.329) data 0.000 (0.168) loss 6.4023 (6.2953) acc 53.1250 (58.1250) lr 1.0000e-05 eta 0:07:05
epoch [1/50] batch [10/26] time 0.157 (0.242) data 0.000 (0.084) loss 5.9336 (6.1875) acc 71.8750 (57.5000) lr 1.0000e-05 eta 0:05:11
epoch [1/50] batch [15/26] time 0.152 (0.212) data 0.000 (0.056) loss 5.7031 (6.1219) acc 68.7500 (58.9583) lr 1.0000e-05 eta 0:04:32
epoch [1/50] batch [20/26] time 0.152 (0.197) data 0.000 (0.042) loss 6.3047 (6.1576) acc 50.0000 (57.5000) lr 1.0000e-05 eta 0:04:12
epoch [1/50] batch [25/26] time 0.152 (0.188) data 0.000 (0.034) loss 6.3125 (6.1498) acc 53.1250 (57.1250) lr 1.0000e-05 eta 0:04:00
epoch [2/50] batch [5/26] time 0.154 (0.310) data 0.000 (0.154) loss 4.4609 (4.9281) acc 65.6250 (64.3750) lr 2.0000e-03 eta 0:06:33
epoch [2/50] batch [10/26] time 0.152 (0.232) data 0.000 (0.077) loss 4.1406 (4.5832) acc 65.6250 (66.2500) lr 2.0000e-03 eta 0:04:53
epoch [2/50] batch [15/26] time 0.151 (0.205) data 0.000 (0.051) loss 4.0664 (4.4456) acc 62.5000 (65.0000) lr 2.0000e-03 eta 0:04:18
epoch [2/50] batch [20/26] time 0.151 (0.192) data 0.000 (0.039) loss 3.3320 (4.3074) acc 78.1250 (65.4688) lr 2.0000e-03 eta 0:04:00
epoch [2/50] batch [25/26] time 0.152 (0.184) data 0.000 (0.031) loss 3.7969 (4.1449) acc 50.0000 (65.1250) lr 2.0000e-03 eta 0:03:49
epoch [3/50] batch [5/26] time 0.153 (0.321) data 0.000 (0.167) loss 3.3340 (3.3414) acc 68.7500 (73.1250) lr 1.9980e-03 eta 0:06:38
epoch [3/50] batch [10/26] time 0.154 (0.237) data 0.000 (0.084) loss 3.6797 (3.3441) acc 62.5000 (72.1875) lr 1.9980e-03 eta 0:04:53
epoch [3/50] batch [15/26] time 0.151 (0.209) data 0.000 (0.056) loss 3.5195 (3.2892) acc 65.6250 (71.8750) lr 1.9980e-03 eta 0:04:17
epoch [3/50] batch [20/26] time 0.152 (0.195) data 0.000 (0.042) loss 3.6875 (3.2990) acc 62.5000 (72.1875) lr 1.9980e-03 eta 0:03:58
epoch [3/50] batch [25/26] time 0.152 (0.186) data 0.000 (0.034) loss 3.2773 (3.2291) acc 62.5000 (74.0000) lr 1.9980e-03 eta 0:03:47
epoch [4/50] batch [5/26] time 0.154 (0.317) data 0.000 (0.162) loss 3.0059 (2.8801) acc 81.2500 (80.6250) lr 1.9921e-03 eta 0:06:25
epoch [4/50] batch [10/26] time 0.153 (0.235) data 0.000 (0.081) loss 2.5625 (2.8693) acc 87.5000 (79.6875) lr 1.9921e-03 eta 0:04:45
epoch [4/50] batch [15/26] time 0.153 (0.208) data 0.000 (0.054) loss 2.8633 (2.8703) acc 75.0000 (79.7917) lr 1.9921e-03 eta 0:04:11
epoch [4/50] batch [20/26] time 0.152 (0.194) data 0.000 (0.041) loss 2.5000 (2.7949) acc 87.5000 (81.2500) lr 1.9921e-03 eta 0:03:53
epoch [4/50] batch [25/26] time 0.152 (0.186) data 0.000 (0.033) loss 2.7344 (2.7594) acc 81.2500 (82.0000) lr 1.9921e-03 eta 0:03:42
epoch [5/50] batch [5/26] time 0.154 (0.339) data 0.000 (0.184) loss 2.5586 (2.5887) acc 84.3750 (85.6250) lr 1.9823e-03 eta 0:06:43
epoch [5/50] batch [10/26] time 0.154 (0.247) data 0.000 (0.092) loss 2.4844 (2.5520) acc 90.6250 (85.6250) lr 1.9823e-03 eta 0:04:52
epoch [5/50] batch [15/26] time 0.154 (0.216) data 0.000 (0.062) loss 2.5488 (2.5240) acc 78.1250 (85.0000) lr 1.9823e-03 eta 0:04:14
epoch [5/50] batch [20/26] time 0.152 (0.200) data 0.000 (0.046) loss 2.8242 (2.5205) acc 81.2500 (84.3750) lr 1.9823e-03 eta 0:03:54
epoch [5/50] batch [25/26] time 0.152 (0.190) data 0.000 (0.037) loss 2.8652 (2.5114) acc 84.3750 (85.0000) lr 1.9823e-03 eta 0:03:42
epoch [6/50] batch [5/26] time 0.155 (0.300) data 0.000 (0.146) loss 2.1855 (2.2574) acc 90.6250 (89.3750) lr 1.9686e-03 eta 0:05:49
epoch [6/50] batch [10/26] time 0.154 (0.227) data 0.000 (0.073) loss 2.3242 (2.3244) acc 87.5000 (87.1875) lr 1.9686e-03 eta 0:04:23
epoch [6/50] batch [15/26] time 0.154 (0.202) data 0.000 (0.049) loss 2.6562 (2.3779) acc 78.1250 (86.4583) lr 1.9686e-03 eta 0:03:53
epoch [6/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.037) loss 2.2500 (2.3345) acc 87.5000 (87.3438) lr 1.9686e-03 eta 0:03:38
epoch [6/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.029) loss 2.5488 (2.3423) acc 81.2500 (87.3750) lr 1.9686e-03 eta 0:03:28
epoch [7/50] batch [5/26] time 0.153 (0.299) data 0.000 (0.145) loss 2.2344 (2.4750) acc 87.5000 (86.8750) lr 1.9511e-03 eta 0:05:40
epoch [7/50] batch [10/26] time 0.153 (0.226) data 0.000 (0.073) loss 2.3848 (2.4328) acc 84.3750 (85.3125) lr 1.9511e-03 eta 0:04:16
epoch [7/50] batch [15/26] time 0.152 (0.201) data 0.000 (0.049) loss 2.5566 (2.3557) acc 81.2500 (86.0417) lr 1.9511e-03 eta 0:03:47
epoch [7/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.036) loss 1.9316 (2.3342) acc 96.8750 (86.8750) lr 1.9511e-03 eta 0:03:32
epoch [7/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.029) loss 2.4668 (2.3304) acc 84.3750 (87.0000) lr 1.9511e-03 eta 0:03:23
epoch [8/50] batch [5/26] time 0.154 (0.324) data 0.000 (0.170) loss 1.9893 (2.1814) acc 93.7500 (89.3750) lr 1.9298e-03 eta 0:06:00
epoch [8/50] batch [10/26] time 0.154 (0.239) data 0.000 (0.085) loss 2.1523 (2.1858) acc 84.3750 (88.7500) lr 1.9298e-03 eta 0:04:24
epoch [8/50] batch [15/26] time 0.152 (0.210) data 0.000 (0.057) loss 1.9883 (2.2231) acc 93.7500 (88.7500) lr 1.9298e-03 eta 0:03:51
epoch [8/50] batch [20/26] time 0.153 (0.196) data 0.000 (0.043) loss 2.0859 (2.2111) acc 90.6250 (89.0625) lr 1.9298e-03 eta 0:03:34
epoch [8/50] batch [25/26] time 0.152 (0.187) data 0.000 (0.034) loss 2.1152 (2.1805) acc 96.8750 (89.8750) lr 1.9298e-03 eta 0:03:24
epoch [9/50] batch [5/26] time 0.153 (0.295) data 0.000 (0.141) loss 1.6572 (1.9982) acc 100.0000 (93.1250) lr 1.9048e-03 eta 0:05:20
epoch [9/50] batch [10/26] time 0.156 (0.224) data 0.000 (0.071) loss 1.8848 (2.0204) acc 90.6250 (90.9375) lr 1.9048e-03 eta 0:04:02
epoch [9/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.047) loss 2.4727 (2.0890) acc 84.3750 (90.0000) lr 1.9048e-03 eta 0:03:35
epoch [9/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.035) loss 2.0234 (2.0617) acc 93.7500 (91.2500) lr 1.9048e-03 eta 0:03:21
epoch [9/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.028) loss 1.8477 (2.0512) acc 96.8750 (91.6250) lr 1.9048e-03 eta 0:03:13
epoch [10/50] batch [5/26] time 0.153 (0.295) data 0.000 (0.141) loss 1.9727 (2.0959) acc 90.6250 (89.3750) lr 1.8763e-03 eta 0:05:13
epoch [10/50] batch [10/26] time 0.154 (0.224) data 0.000 (0.071) loss 1.8037 (1.9854) acc 96.8750 (92.8125) lr 1.8763e-03 eta 0:03:56
epoch [10/50] batch [15/26] time 0.152 (0.200) data 0.000 (0.047) loss 1.7197 (1.9857) acc 100.0000 (92.7083) lr 1.8763e-03 eta 0:03:30
epoch [10/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.035) loss 1.7578 (1.9913) acc 100.0000 (92.0312) lr 1.8763e-03 eta 0:03:17
epoch [10/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.028) loss 2.2617 (2.0187) acc 90.6250 (91.5000) lr 1.8763e-03 eta 0:03:08
epoch [11/50] batch [5/26] time 0.154 (0.282) data 0.000 (0.128) loss 1.8164 (1.9248) acc 96.8750 (95.0000) lr 1.8443e-03 eta 0:04:51
epoch [11/50] batch [10/26] time 0.155 (0.218) data 0.000 (0.064) loss 2.6016 (1.9895) acc 75.0000 (92.1875) lr 1.8443e-03 eta 0:03:44
epoch [11/50] batch [15/26] time 0.156 (0.197) data 0.000 (0.043) loss 1.9121 (2.0535) acc 96.8750 (91.0417) lr 1.8443e-03 eta 0:03:22
epoch [11/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 2.3711 (2.0536) acc 84.3750 (90.6250) lr 1.8443e-03 eta 0:03:10
epoch [11/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 2.1738 (2.0415) acc 90.6250 (90.7500) lr 1.8443e-03 eta 0:03:02
epoch [12/50] batch [5/26] time 0.153 (0.286) data 0.000 (0.133) loss 2.1016 (1.9557) acc 90.6250 (90.6250) lr 1.8090e-03 eta 0:04:48
epoch [12/50] batch [10/26] time 0.152 (0.220) data 0.000 (0.067) loss 1.8330 (1.9901) acc 87.5000 (90.9375) lr 1.8090e-03 eta 0:03:40
epoch [12/50] batch [15/26] time 0.154 (0.198) data 0.000 (0.044) loss 1.6738 (1.9391) acc 100.0000 (92.0833) lr 1.8090e-03 eta 0:03:17
epoch [12/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.033) loss 1.8574 (1.9728) acc 93.7500 (92.0312) lr 1.8090e-03 eta 0:03:05
epoch [12/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.8467 (1.9660) acc 96.8750 (91.8750) lr 1.8090e-03 eta 0:02:57
epoch [13/50] batch [5/26] time 0.153 (0.284) data 0.000 (0.129) loss 1.5625 (1.9545) acc 96.8750 (91.2500) lr 1.7705e-03 eta 0:04:38
epoch [13/50] batch [10/26] time 0.154 (0.219) data 0.001 (0.065) loss 1.7148 (1.9010) acc 96.8750 (92.5000) lr 1.7705e-03 eta 0:03:33
epoch [13/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.8535 (1.8989) acc 90.6250 (92.7083) lr 1.7705e-03 eta 0:03:11
epoch [13/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.9043 (1.8952) acc 87.5000 (92.3438) lr 1.7705e-03 eta 0:02:59
epoch [13/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 2.0391 (1.8905) acc 87.5000 (92.1250) lr 1.7705e-03 eta 0:02:52
epoch [14/50] batch [5/26] time 0.153 (0.303) data 0.000 (0.148) loss 1.8652 (1.7414) acc 96.8750 (95.0000) lr 1.7290e-03 eta 0:04:49
epoch [14/50] batch [10/26] time 0.153 (0.228) data 0.000 (0.074) loss 1.9375 (1.9375) acc 93.7500 (91.8750) lr 1.7290e-03 eta 0:03:36
epoch [14/50] batch [15/26] time 0.152 (0.203) data 0.000 (0.050) loss 1.8408 (1.9494) acc 93.7500 (91.4583) lr 1.7290e-03 eta 0:03:11
epoch [14/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.037) loss 2.2500 (1.9596) acc 81.2500 (91.5625) lr 1.7290e-03 eta 0:02:59
epoch [14/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.030) loss 1.9551 (1.9750) acc 90.6250 (91.5000) lr 1.7290e-03 eta 0:02:51
epoch [15/50] batch [5/26] time 0.152 (0.285) data 0.000 (0.132) loss 1.7744 (1.7764) acc 93.7500 (95.6250) lr 1.6845e-03 eta 0:04:25
epoch [15/50] batch [10/26] time 0.155 (0.219) data 0.000 (0.066) loss 2.6406 (1.9643) acc 84.3750 (92.5000) lr 1.6845e-03 eta 0:03:22
epoch [15/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.044) loss 2.0234 (1.9753) acc 84.3750 (91.6667) lr 1.6845e-03 eta 0:03:01
epoch [15/50] batch [20/26] time 0.154 (0.186) data 0.000 (0.033) loss 1.7246 (1.9399) acc 96.8750 (92.5000) lr 1.6845e-03 eta 0:02:50
epoch [15/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.8184 (1.9203) acc 93.7500 (92.6250) lr 1.6845e-03 eta 0:02:43
epoch [16/50] batch [5/26] time 0.154 (0.302) data 0.000 (0.148) loss 1.3965 (1.9473) acc 100.0000 (89.3750) lr 1.6374e-03 eta 0:04:33
epoch [16/50] batch [10/26] time 0.157 (0.228) data 0.000 (0.074) loss 1.9824 (1.9822) acc 93.7500 (90.3125) lr 1.6374e-03 eta 0:03:25
epoch [16/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 1.6641 (1.9080) acc 96.8750 (91.6667) lr 1.6374e-03 eta 0:03:01
epoch [16/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 1.9434 (1.9121) acc 93.7500 (91.7188) lr 1.6374e-03 eta 0:02:49
epoch [16/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.9492 (1.8926) acc 93.7500 (92.1250) lr 1.6374e-03 eta 0:02:41
epoch [17/50] batch [5/26] time 0.153 (0.322) data 0.000 (0.165) loss 1.7783 (1.9035) acc 90.6250 (90.0000) lr 1.5878e-03 eta 0:04:42
epoch [17/50] batch [10/26] time 0.155 (0.238) data 0.000 (0.083) loss 1.7012 (1.9534) acc 100.0000 (90.6250) lr 1.5878e-03 eta 0:03:27
epoch [17/50] batch [15/26] time 0.154 (0.210) data 0.000 (0.055) loss 1.7588 (1.9215) acc 93.7500 (92.0833) lr 1.5878e-03 eta 0:03:02
epoch [17/50] batch [20/26] time 0.155 (0.196) data 0.000 (0.041) loss 1.7402 (1.9029) acc 93.7500 (92.3438) lr 1.5878e-03 eta 0:02:49
epoch [17/50] batch [25/26] time 0.157 (0.188) data 0.000 (0.033) loss 1.4785 (1.8802) acc 96.8750 (92.2500) lr 1.5878e-03 eta 0:02:41
epoch [18/50] batch [5/26] time 0.153 (0.287) data 0.000 (0.133) loss 1.4531 (1.6986) acc 100.0000 (96.2500) lr 1.5358e-03 eta 0:04:05
epoch [18/50] batch [10/26] time 0.159 (0.221) data 0.000 (0.067) loss 1.7363 (1.7767) acc 93.7500 (93.7500) lr 1.5358e-03 eta 0:03:07
epoch [18/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.045) loss 2.1133 (1.8104) acc 87.5000 (93.1250) lr 1.5358e-03 eta 0:02:47
epoch [18/50] batch [20/26] time 0.154 (0.187) data 0.001 (0.033) loss 1.9434 (1.8441) acc 90.6250 (92.8125) lr 1.5358e-03 eta 0:02:37
epoch [18/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 2.4727 (1.8844) acc 87.5000 (92.3750) lr 1.5358e-03 eta 0:02:30
epoch [19/50] batch [5/26] time 0.154 (0.326) data 0.000 (0.171) loss 1.6787 (1.8352) acc 96.8750 (91.8750) lr 1.4818e-03 eta 0:04:29
epoch [19/50] batch [10/26] time 0.156 (0.240) data 0.000 (0.086) loss 1.7461 (1.8007) acc 93.7500 (93.1250) lr 1.4818e-03 eta 0:03:17
epoch [19/50] batch [15/26] time 0.152 (0.211) data 0.000 (0.057) loss 1.9375 (1.8290) acc 84.3750 (92.7083) lr 1.4818e-03 eta 0:02:52
epoch [19/50] batch [20/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.9688 (1.8141) acc 90.6250 (93.1250) lr 1.4818e-03 eta 0:02:39
epoch [19/50] batch [25/26] time 0.152 (0.188) data 0.000 (0.034) loss 1.7754 (1.8329) acc 93.7500 (93.1250) lr 1.4818e-03 eta 0:02:31
epoch [20/50] batch [5/26] time 0.154 (0.325) data 0.000 (0.169) loss 1.6152 (1.6771) acc 96.8750 (95.6250) lr 1.4258e-03 eta 0:04:20
epoch [20/50] batch [10/26] time 0.154 (0.240) data 0.000 (0.085) loss 1.6523 (1.7260) acc 100.0000 (95.6250) lr 1.4258e-03 eta 0:03:10
epoch [20/50] batch [15/26] time 0.152 (0.211) data 0.000 (0.057) loss 1.9580 (1.7903) acc 87.5000 (94.1667) lr 1.4258e-03 eta 0:02:46
epoch [20/50] batch [20/26] time 0.153 (0.196) data 0.000 (0.042) loss 1.8916 (1.7837) acc 87.5000 (93.9062) lr 1.4258e-03 eta 0:02:34
epoch [20/50] batch [25/26] time 0.155 (0.188) data 0.000 (0.034) loss 1.6973 (1.8109) acc 93.7500 (93.2500) lr 1.4258e-03 eta 0:02:26
epoch [21/50] batch [5/26] time 0.153 (0.288) data 0.000 (0.134) loss 1.8262 (1.7838) acc 93.7500 (93.7500) lr 1.3681e-03 eta 0:03:43
epoch [21/50] batch [10/26] time 0.154 (0.221) data 0.000 (0.067) loss 1.8721 (1.7506) acc 90.6250 (93.4375) lr 1.3681e-03 eta 0:02:49
epoch [21/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.045) loss 1.6211 (1.7688) acc 96.8750 (93.1250) lr 1.3681e-03 eta 0:02:31
epoch [21/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.034) loss 1.3359 (1.7800) acc 100.0000 (93.1250) lr 1.3681e-03 eta 0:02:22
epoch [21/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 2.0664 (1.7974) acc 90.6250 (93.2500) lr 1.3681e-03 eta 0:02:15
epoch [22/50] batch [5/26] time 0.154 (0.317) data 0.000 (0.163) loss 1.8525 (1.7512) acc 93.7500 (93.7500) lr 1.3090e-03 eta 0:03:57
epoch [22/50] batch [10/26] time 0.156 (0.236) data 0.000 (0.082) loss 1.9922 (1.7979) acc 87.5000 (94.3750) lr 1.3090e-03 eta 0:02:55
epoch [22/50] batch [15/26] time 0.153 (0.208) data 0.000 (0.054) loss 1.9102 (1.7891) acc 90.6250 (94.1667) lr 1.3090e-03 eta 0:02:33
epoch [22/50] batch [20/26] time 0.153 (0.194) data 0.000 (0.041) loss 1.5742 (1.7797) acc 96.8750 (94.5312) lr 1.3090e-03 eta 0:02:22
epoch [22/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.033) loss 2.0000 (1.7843) acc 90.6250 (94.6250) lr 1.3090e-03 eta 0:02:15
epoch [23/50] batch [5/26] time 0.154 (0.309) data 0.000 (0.155) loss 1.9033 (1.7656) acc 93.7500 (96.2500) lr 1.2487e-03 eta 0:03:43
epoch [23/50] batch [10/26] time 0.153 (0.231) data 0.000 (0.077) loss 1.8008 (1.7831) acc 100.0000 (95.6250) lr 1.2487e-03 eta 0:02:45
epoch [23/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.052) loss 1.8691 (1.7982) acc 96.8750 (95.0000) lr 1.2487e-03 eta 0:02:26
epoch [23/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.039) loss 1.6094 (1.8322) acc 100.0000 (93.5938) lr 1.2487e-03 eta 0:02:15
epoch [23/50] batch [25/26] time 0.154 (0.184) data 0.000 (0.031) loss 1.9941 (1.8189) acc 93.7500 (94.0000) lr 1.2487e-03 eta 0:02:09
epoch [24/50] batch [5/26] time 0.155 (0.328) data 0.000 (0.172) loss 1.4785 (1.7400) acc 100.0000 (95.6250) lr 1.1874e-03 eta 0:03:48
epoch [24/50] batch [10/26] time 0.154 (0.241) data 0.000 (0.086) loss 1.5742 (1.7103) acc 93.7500 (94.6875) lr 1.1874e-03 eta 0:02:46
epoch [24/50] batch [15/26] time 0.154 (0.212) data 0.000 (0.058) loss 1.5146 (1.6791) acc 96.8750 (95.6250) lr 1.1874e-03 eta 0:02:25
epoch [24/50] batch [20/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.9736 (1.7688) acc 93.7500 (94.2188) lr 1.1874e-03 eta 0:02:14
epoch [24/50] batch [25/26] time 0.155 (0.189) data 0.000 (0.035) loss 1.8086 (1.8097) acc 100.0000 (94.3750) lr 1.1874e-03 eta 0:02:07
epoch [25/50] batch [5/26] time 0.154 (0.292) data 0.000 (0.138) loss 2.4160 (1.8766) acc 81.2500 (93.1250) lr 1.1253e-03 eta 0:03:16
epoch [25/50] batch [10/26] time 0.155 (0.223) data 0.000 (0.069) loss 1.5625 (1.7181) acc 96.8750 (95.3125) lr 1.1253e-03 eta 0:02:28
epoch [25/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 1.8730 (1.7480) acc 90.6250 (94.1667) lr 1.1253e-03 eta 0:02:12
epoch [25/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.035) loss 1.9531 (1.7593) acc 90.6250 (93.7500) lr 1.1253e-03 eta 0:02:03
epoch [25/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.028) loss 2.2910 (1.7817) acc 87.5000 (93.6250) lr 1.1253e-03 eta 0:01:57
epoch [26/50] batch [5/26] time 0.154 (0.325) data 0.000 (0.170) loss 1.9502 (1.7924) acc 87.5000 (93.1250) lr 1.0628e-03 eta 0:03:29
epoch [26/50] batch [10/26] time 0.157 (0.240) data 0.000 (0.085) loss 1.7891 (1.8451) acc 93.7500 (91.8750) lr 1.0628e-03 eta 0:02:33
epoch [26/50] batch [15/26] time 0.155 (0.211) data 0.000 (0.057) loss 1.8057 (1.7928) acc 93.7500 (93.1250) lr 1.0628e-03 eta 0:02:13
epoch [26/50] batch [20/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.4238 (1.7824) acc 100.0000 (93.7500) lr 1.0628e-03 eta 0:02:04
epoch [26/50] batch [25/26] time 0.152 (0.188) data 0.000 (0.034) loss 1.7637 (1.7732) acc 96.8750 (94.3750) lr 1.0628e-03 eta 0:01:57
epoch [27/50] batch [5/26] time 0.152 (0.292) data 0.000 (0.137) loss 1.7266 (1.7803) acc 96.8750 (96.2500) lr 1.0000e-03 eta 0:03:00
epoch [27/50] batch [10/26] time 0.153 (0.222) data 0.000 (0.069) loss 1.7871 (1.8084) acc 90.6250 (93.4375) lr 1.0000e-03 eta 0:02:16
epoch [27/50] batch [15/26] time 0.155 (0.200) data 0.000 (0.046) loss 1.8438 (1.7717) acc 90.6250 (93.5417) lr 1.0000e-03 eta 0:02:01
epoch [27/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.034) loss 1.3291 (1.7499) acc 100.0000 (93.7500) lr 1.0000e-03 eta 0:01:53
epoch [27/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.028) loss 1.7373 (1.7329) acc 96.8750 (94.2500) lr 1.0000e-03 eta 0:01:48
epoch [28/50] batch [5/26] time 0.153 (0.273) data 0.000 (0.119) loss 1.7324 (1.6619) acc 93.7500 (96.8750) lr 9.3721e-04 eta 0:02:41
epoch [28/50] batch [10/26] time 0.153 (0.214) data 0.000 (0.060) loss 1.8574 (1.6621) acc 90.6250 (95.9375) lr 9.3721e-04 eta 0:02:05
epoch [28/50] batch [15/26] time 0.153 (0.193) data 0.000 (0.040) loss 1.7695 (1.6796) acc 96.8750 (95.0000) lr 9.3721e-04 eta 0:01:52
epoch [28/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.030) loss 1.7520 (1.7306) acc 87.5000 (94.2188) lr 9.3721e-04 eta 0:01:45
epoch [28/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.024) loss 1.6289 (1.7549) acc 96.8750 (93.8750) lr 9.3721e-04 eta 0:01:41
epoch [29/50] batch [5/26] time 0.155 (0.316) data 0.000 (0.162) loss 1.8945 (1.6994) acc 93.7500 (95.6250) lr 8.7467e-04 eta 0:02:59
epoch [29/50] batch [10/26] time 0.156 (0.235) data 0.000 (0.081) loss 1.8066 (1.7708) acc 93.7500 (94.3750) lr 8.7467e-04 eta 0:02:12
epoch [29/50] batch [15/26] time 0.154 (0.208) data 0.000 (0.054) loss 1.7129 (1.7650) acc 93.7500 (94.1667) lr 8.7467e-04 eta 0:01:56
epoch [29/50] batch [20/26] time 0.153 (0.195) data 0.000 (0.041) loss 2.0684 (1.7632) acc 84.3750 (93.9062) lr 8.7467e-04 eta 0:01:47
epoch [29/50] batch [25/26] time 0.153 (0.187) data 0.000 (0.033) loss 2.0898 (1.7713) acc 90.6250 (93.6250) lr 8.7467e-04 eta 0:01:42
epoch [30/50] batch [5/26] time 0.154 (0.311) data 0.000 (0.156) loss 1.8359 (1.7969) acc 90.6250 (94.3750) lr 8.1262e-04 eta 0:02:48
epoch [30/50] batch [10/26] time 0.158 (0.233) data 0.000 (0.078) loss 1.8008 (1.7355) acc 96.8750 (95.3125) lr 8.1262e-04 eta 0:02:04
epoch [30/50] batch [15/26] time 0.156 (0.207) data 0.000 (0.052) loss 1.9414 (1.7493) acc 90.6250 (94.7917) lr 8.1262e-04 eta 0:01:49
epoch [30/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.039) loss 1.9004 (1.7641) acc 93.7500 (94.3750) lr 8.1262e-04 eta 0:01:41
epoch [30/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.7803 (1.7373) acc 96.8750 (94.6250) lr 8.1262e-04 eta 0:01:36
epoch [31/50] batch [5/26] time 0.154 (0.277) data 0.000 (0.122) loss 1.5078 (1.6371) acc 96.8750 (97.5000) lr 7.5131e-04 eta 0:02:22
epoch [31/50] batch [10/26] time 0.160 (0.216) data 0.000 (0.061) loss 1.7148 (1.6719) acc 93.7500 (95.9375) lr 7.5131e-04 eta 0:01:50
epoch [31/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.041) loss 1.5664 (1.6872) acc 96.8750 (95.4167) lr 7.5131e-04 eta 0:01:38
epoch [31/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.031) loss 1.5381 (1.7139) acc 96.8750 (95.1562) lr 7.5131e-04 eta 0:01:32
epoch [31/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.025) loss 1.9922 (1.7536) acc 90.6250 (94.6250) lr 7.5131e-04 eta 0:01:28
epoch [32/50] batch [5/26] time 0.155 (0.307) data 0.000 (0.152) loss 1.5811 (1.5678) acc 96.8750 (97.5000) lr 6.9098e-04 eta 0:02:30
epoch [32/50] batch [10/26] time 0.154 (0.231) data 0.000 (0.076) loss 1.7393 (1.6383) acc 93.7500 (95.9375) lr 6.9098e-04 eta 0:01:51
epoch [32/50] batch [15/26] time 0.153 (0.205) data 0.000 (0.051) loss 1.8066 (1.6519) acc 93.7500 (95.8333) lr 6.9098e-04 eta 0:01:38
epoch [32/50] batch [20/26] time 0.154 (0.192) data 0.000 (0.038) loss 2.0371 (1.6718) acc 93.7500 (95.6250) lr 6.9098e-04 eta 0:01:31
epoch [32/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.7109 (1.6900) acc 100.0000 (95.5000) lr 6.9098e-04 eta 0:01:26
epoch [33/50] batch [5/26] time 0.154 (0.315) data 0.000 (0.160) loss 1.3848 (1.6080) acc 100.0000 (95.6250) lr 6.3188e-04 eta 0:02:25
epoch [33/50] batch [10/26] time 0.154 (0.234) data 0.000 (0.080) loss 1.5332 (1.7213) acc 96.8750 (94.0625) lr 6.3188e-04 eta 0:01:47
epoch [33/50] batch [15/26] time 0.155 (0.208) data 0.000 (0.053) loss 2.0117 (1.7419) acc 90.6250 (94.1667) lr 6.3188e-04 eta 0:01:34
epoch [33/50] batch [20/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.7568 (1.7245) acc 96.8750 (94.5312) lr 6.3188e-04 eta 0:01:26
epoch [33/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.5020 (1.7010) acc 96.8750 (94.8750) lr 6.3188e-04 eta 0:01:22
epoch [34/50] batch [5/26] time 0.153 (0.309) data 0.000 (0.154) loss 1.6299 (1.6902) acc 96.8750 (93.7500) lr 5.7422e-04 eta 0:02:15
epoch [34/50] batch [10/26] time 0.156 (0.232) data 0.000 (0.077) loss 1.5908 (1.6683) acc 96.8750 (95.0000) lr 5.7422e-04 eta 0:01:40
epoch [34/50] batch [15/26] time 0.153 (0.206) data 0.000 (0.051) loss 1.6016 (1.6852) acc 100.0000 (95.2083) lr 5.7422e-04 eta 0:01:27
epoch [34/50] batch [20/26] time 0.153 (0.192) data 0.000 (0.039) loss 1.4521 (1.6675) acc 100.0000 (95.6250) lr 5.7422e-04 eta 0:01:21
epoch [34/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.031) loss 1.7617 (1.6776) acc 93.7500 (95.1250) lr 5.7422e-04 eta 0:01:16
epoch [35/50] batch [5/26] time 0.154 (0.284) data 0.000 (0.130) loss 1.6025 (1.6170) acc 96.8750 (97.5000) lr 5.1825e-04 eta 0:01:56
epoch [35/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.065) loss 1.6367 (1.6196) acc 93.7500 (95.9375) lr 5.1825e-04 eta 0:01:28
epoch [35/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.8320 (1.6900) acc 93.7500 (94.7917) lr 5.1825e-04 eta 0:01:18
epoch [35/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.033) loss 1.8486 (1.6831) acc 93.7500 (95.1562) lr 5.1825e-04 eta 0:01:13
epoch [35/50] batch [25/26] time 0.152 (0.179) data 0.000 (0.026) loss 1.8340 (1.7051) acc 96.8750 (95.0000) lr 5.1825e-04 eta 0:01:10
epoch [36/50] batch [5/26] time 0.152 (0.273) data 0.000 (0.119) loss 1.6992 (1.7680) acc 96.8750 (93.7500) lr 4.6417e-04 eta 0:01:45
epoch [36/50] batch [10/26] time 0.155 (0.213) data 0.000 (0.060) loss 1.7734 (1.7382) acc 100.0000 (95.3125) lr 4.6417e-04 eta 0:01:21
epoch [36/50] batch [15/26] time 0.154 (0.193) data 0.000 (0.040) loss 1.6250 (1.6879) acc 96.8750 (96.0417) lr 4.6417e-04 eta 0:01:12
epoch [36/50] batch [20/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.0195 (1.7020) acc 93.7500 (96.2500) lr 4.6417e-04 eta 0:01:07
epoch [36/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.024) loss 2.0039 (1.6991) acc 93.7500 (96.2500) lr 4.6417e-04 eta 0:01:04
epoch [37/50] batch [5/26] time 0.154 (0.310) data 0.000 (0.155) loss 1.5254 (1.7531) acc 100.0000 (96.8750) lr 4.1221e-04 eta 0:01:51
epoch [37/50] batch [10/26] time 0.155 (0.232) data 0.000 (0.078) loss 1.4570 (1.6225) acc 100.0000 (97.1875) lr 4.1221e-04 eta 0:01:22
epoch [37/50] batch [15/26] time 0.153 (0.206) data 0.000 (0.052) loss 1.9736 (1.6537) acc 96.8750 (96.8750) lr 4.1221e-04 eta 0:01:11
epoch [37/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.039) loss 1.5625 (1.6952) acc 96.8750 (95.6250) lr 4.1221e-04 eta 0:01:06
epoch [37/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.5576 (1.7155) acc 96.8750 (95.6250) lr 4.1221e-04 eta 0:01:02
epoch [38/50] batch [5/26] time 0.153 (0.300) data 0.000 (0.144) loss 1.5596 (1.5869) acc 100.0000 (98.1250) lr 3.6258e-04 eta 0:01:39
epoch [38/50] batch [10/26] time 0.155 (0.227) data 0.000 (0.072) loss 1.4971 (1.6257) acc 96.8750 (97.1875) lr 3.6258e-04 eta 0:01:14
epoch [38/50] batch [15/26] time 0.154 (0.202) data 0.000 (0.048) loss 1.8018 (1.6604) acc 93.7500 (96.4583) lr 3.6258e-04 eta 0:01:05
epoch [38/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.036) loss 1.7764 (1.6718) acc 90.6250 (95.4688) lr 3.6258e-04 eta 0:01:00
epoch [38/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.6582 (1.6763) acc 96.8750 (95.6250) lr 3.6258e-04 eta 0:00:57
epoch [39/50] batch [5/26] time 0.153 (0.275) data 0.000 (0.121) loss 1.6494 (1.5883) acc 100.0000 (98.1250) lr 3.1545e-04 eta 0:01:24
epoch [39/50] batch [10/26] time 0.156 (0.215) data 0.000 (0.061) loss 1.5352 (1.5879) acc 96.8750 (97.1875) lr 3.1545e-04 eta 0:01:04
epoch [39/50] batch [15/26] time 0.154 (0.194) data 0.000 (0.041) loss 1.8516 (1.6023) acc 96.8750 (97.0833) lr 3.1545e-04 eta 0:00:57
epoch [39/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.031) loss 1.5586 (1.6458) acc 100.0000 (96.4062) lr 3.1545e-04 eta 0:00:53
epoch [39/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.024) loss 1.3457 (1.6353) acc 96.8750 (96.5000) lr 3.1545e-04 eta 0:00:51
epoch [40/50] batch [5/26] time 0.154 (0.283) data 0.000 (0.129) loss 1.7109 (1.6348) acc 93.7500 (95.0000) lr 2.7103e-04 eta 0:01:19
epoch [40/50] batch [10/26] time 0.155 (0.219) data 0.000 (0.065) loss 1.5566 (1.7352) acc 96.8750 (94.3750) lr 2.7103e-04 eta 0:01:00
epoch [40/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.7461 (1.7046) acc 96.8750 (94.5833) lr 2.7103e-04 eta 0:00:53
epoch [40/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 1.4805 (1.7080) acc 100.0000 (95.1562) lr 2.7103e-04 eta 0:00:49
epoch [40/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.7090 (1.7109) acc 93.7500 (94.8750) lr 2.7103e-04 eta 0:00:46
epoch [41/50] batch [5/26] time 0.154 (0.314) data 0.000 (0.159) loss 1.3584 (1.6258) acc 100.0000 (96.8750) lr 2.2949e-04 eta 0:01:19
epoch [41/50] batch [10/26] time 0.153 (0.233) data 0.000 (0.080) loss 1.9219 (1.6978) acc 84.3750 (95.0000) lr 2.2949e-04 eta 0:00:58
epoch [41/50] batch [15/26] time 0.152 (0.206) data 0.000 (0.053) loss 1.9746 (1.6977) acc 90.6250 (95.2083) lr 2.2949e-04 eta 0:00:50
epoch [41/50] batch [20/26] time 0.154 (0.193) data 0.000 (0.040) loss 1.4033 (1.6838) acc 100.0000 (95.4688) lr 2.2949e-04 eta 0:00:46
epoch [41/50] batch [25/26] time 0.154 (0.185) data 0.000 (0.032) loss 1.5615 (1.6673) acc 93.7500 (95.5000) lr 2.2949e-04 eta 0:00:43
epoch [42/50] batch [5/26] time 0.155 (0.300) data 0.000 (0.145) loss 1.9160 (1.7461) acc 93.7500 (92.5000) lr 1.9098e-04 eta 0:01:08
epoch [42/50] batch [10/26] time 0.154 (0.227) data 0.000 (0.073) loss 1.7773 (1.6998) acc 93.7500 (94.0625) lr 1.9098e-04 eta 0:00:50
epoch [42/50] batch [15/26] time 0.152 (0.202) data 0.000 (0.049) loss 2.0273 (1.6837) acc 93.7500 (94.7917) lr 1.9098e-04 eta 0:00:44
epoch [42/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.036) loss 1.6348 (1.7047) acc 96.8750 (94.5312) lr 1.9098e-04 eta 0:00:40
epoch [42/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.029) loss 1.4258 (1.6805) acc 100.0000 (95.0000) lr 1.9098e-04 eta 0:00:38
epoch [43/50] batch [5/26] time 0.154 (0.287) data 0.000 (0.134) loss 1.6660 (1.5512) acc 96.8750 (97.5000) lr 1.5567e-04 eta 0:00:58
epoch [43/50] batch [10/26] time 0.154 (0.220) data 0.000 (0.067) loss 1.6680 (1.6026) acc 96.8750 (96.5625) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [15/26] time 0.152 (0.198) data 0.000 (0.045) loss 1.7031 (1.6526) acc 96.8750 (96.0417) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [20/26] time 0.152 (0.186) data 0.000 (0.034) loss 1.5742 (1.6641) acc 100.0000 (96.2500) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.5098 (1.6604) acc 100.0000 (95.8750) lr 1.5567e-04 eta 0:00:32
epoch [44/50] batch [5/26] time 0.152 (0.312) data 0.000 (0.157) loss 1.8135 (1.5924) acc 90.6250 (96.8750) lr 1.2369e-04 eta 0:00:55
epoch [44/50] batch [10/26] time 0.155 (0.233) data 0.000 (0.079) loss 1.4502 (1.5903) acc 93.7500 (96.5625) lr 1.2369e-04 eta 0:00:40
epoch [44/50] batch [15/26] time 0.152 (0.206) data 0.000 (0.053) loss 1.5801 (1.5976) acc 96.8750 (96.2500) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [20/26] time 0.152 (0.193) data 0.000 (0.039) loss 1.5938 (1.6015) acc 93.7500 (96.8750) lr 1.2369e-04 eta 0:00:31
epoch [44/50] batch [25/26] time 0.154 (0.185) data 0.000 (0.032) loss 2.0312 (1.6233) acc 90.6250 (95.7500) lr 1.2369e-04 eta 0:00:29
epoch [45/50] batch [5/26] time 0.156 (0.285) data 0.000 (0.129) loss 1.5820 (1.5215) acc 100.0000 (99.3750) lr 9.5173e-05 eta 0:00:43
epoch [45/50] batch [10/26] time 0.158 (0.221) data 0.000 (0.064) loss 1.7812 (1.5600) acc 93.7500 (97.5000) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [15/26] time 0.155 (0.199) data 0.000 (0.043) loss 1.4375 (1.5715) acc 100.0000 (97.2917) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.032) loss 1.6406 (1.5626) acc 93.7500 (97.5000) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.026) loss 1.6182 (1.5695) acc 100.0000 (97.1250) lr 9.5173e-05 eta 0:00:23
epoch [46/50] batch [5/26] time 0.155 (0.305) data 0.000 (0.150) loss 1.8301 (1.6596) acc 93.7500 (95.6250) lr 7.0224e-05 eta 0:00:38
epoch [46/50] batch [10/26] time 0.154 (0.229) data 0.000 (0.075) loss 1.6172 (1.6497) acc 96.8750 (95.9375) lr 7.0224e-05 eta 0:00:27
epoch [46/50] batch [15/26] time 0.152 (0.204) data 0.000 (0.050) loss 1.3193 (1.6663) acc 96.8750 (95.2083) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [20/26] time 0.152 (0.191) data 0.000 (0.038) loss 1.8418 (1.6816) acc 93.7500 (95.3125) lr 7.0224e-05 eta 0:00:21
epoch [46/50] batch [25/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.9189 (1.6803) acc 90.6250 (95.6250) lr 7.0224e-05 eta 0:00:19
epoch [47/50] batch [5/26] time 0.154 (0.312) data 0.000 (0.157) loss 1.6484 (1.8168) acc 96.8750 (93.1250) lr 4.8943e-05 eta 0:00:30
epoch [47/50] batch [10/26] time 0.156 (0.233) data 0.000 (0.078) loss 1.5566 (1.7084) acc 96.8750 (95.0000) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [15/26] time 0.152 (0.206) data 0.000 (0.052) loss 1.5918 (1.6813) acc 100.0000 (95.2083) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.039) loss 1.6328 (1.6619) acc 93.7500 (95.3125) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [25/26] time 0.154 (0.185) data 0.000 (0.031) loss 1.8867 (1.6829) acc 90.6250 (95.0000) lr 4.8943e-05 eta 0:00:14
epoch [48/50] batch [5/26] time 0.154 (0.313) data 0.000 (0.159) loss 1.5771 (1.5807) acc 96.8750 (96.8750) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [10/26] time 0.155 (0.234) data 0.000 (0.080) loss 1.8867 (1.6394) acc 93.7500 (95.6250) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [15/26] time 0.154 (0.207) data 0.000 (0.053) loss 1.5957 (1.6409) acc 100.0000 (96.0417) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [20/26] time 0.152 (0.193) data 0.000 (0.040) loss 1.4639 (1.6627) acc 100.0000 (95.4688) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [25/26] time 0.152 (0.185) data 0.000 (0.032) loss 2.0625 (1.6616) acc 90.6250 (95.7500) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.155 (0.296) data 0.000 (0.140) loss 1.4014 (1.5791) acc 100.0000 (97.5000) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [10/26] time 0.156 (0.225) data 0.001 (0.070) loss 1.6230 (1.6268) acc 100.0000 (96.5625) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/26] time 0.153 (0.202) data 0.000 (0.047) loss 1.6484 (1.6438) acc 93.7500 (96.2500) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.035) loss 1.5625 (1.6874) acc 93.7500 (95.3125) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.5537 (1.6818) acc 100.0000 (95.6250) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.154 (0.288) data 0.000 (0.133) loss 1.7500 (1.7033) acc 93.7500 (95.0000) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [10/26] time 0.156 (0.221) data 0.000 (0.067) loss 1.4512 (1.6735) acc 100.0000 (94.6875) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.044) loss 1.6367 (1.6501) acc 100.0000 (95.6250) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.033) loss 1.3301 (1.6393) acc 100.0000 (96.0938) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.027) loss 1.7871 (1.6700) acc 90.6250 (95.5000) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:09<00:18,  9.26s/it] 67%|██████▋   | 2/3 [00:10<00:04,  4.48s/it]100%|██████████| 3/3 [00:14<00:00,  4.37s/it]100%|██████████| 3/3 [00:14<00:00,  4.91s/it]
=> result
* total: 1,053
* correct: 1,031
* accuracy: 97.9%
* error: 2.1%
* macro_f1: 97.8%
Elapsed: 0:04:14
Run this job and save the output to output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,410
---------  -------------
['wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X wild pansy, a type of flower.', 'X X X X primula, a type of flower.', 'X X X X sunflower, a type of flower.', 'X X X X pelargonium, a type of flower.', 'X X X X bishop of llandaff, a type of flower.', 'X X X X gaura, a type of flower.', 'X X X X geranium, a type of flower.', 'X X X X orange dahlia, a type of flower.', 'X X X X pink-yellow dahlia, a type of flower.', 'X X X X cautleya spicata, a type of flower.', 'X X X X japanese anemone, a type of flower.', 'X X X X black-eyed susan, a type of flower.', 'X X X X silverbush, a type of flower.', 'X X X X californian poppy, a type of flower.', 'X X X X osteospermum, a type of flower.', 'X X X X spring crocus, a type of flower.', 'X X X X bearded iris, a type of flower.', 'X X X X windflower, a type of flower.', 'X X X X tree poppy, a type of flower.', 'X X X X gazania, a type of flower.', 'X X X X azalea, a type of flower.', 'X X X X water lily, a type of flower.', 'X X X X rose, a type of flower.', 'X X X X thorn apple, a type of flower.', 'X X X X morning glory, a type of flower.', 'X X X X passion flower, a type of flower.', 'X X X X lotus, a type of flower.', 'X X X X toad lily, a type of flower.', 'X X X X anthurium, a type of flower.', 'X X X X frangipani, a type of flower.', 'X X X X clematis, a type of flower.', 'X X X X hibiscus, a type of flower.', 'X X X X columbine, a type of flower.', 'X X X X desert-rose, a type of flower.', 'X X X X tree mallow, a type of flower.', 'X X X X magnolia, a type of flower.', 'X X X X cyclamen, a type of flower.', 'X X X X watercress, a type of flower.', 'X X X X canna lily, a type of flower.', 'X X X X hippeastrum, a type of flower.', 'X X X X bee balm, a type of flower.', 'X X X X ball moss, a type of flower.', 'X X X X foxglove, a type of flower.', 'X X X X bougainvillea, a type of flower.', 'X X X X camellia, a type of flower.', 'X X X X mallow, a type of flower.', 'X X X X mexican petunia, a type of flower.', 'X X X X bromelia, a type of flower.', 'X X X X blanket flower, a type of flower.', 'X X X X trumpet creeper, a type of flower.', 'X X X X blackberry lily, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:06<00:13,  6.56s/it] 67%|██████▋   | 2/3 [00:07<00:03,  3.36s/it]100%|██████████| 3/3 [00:12<00:00,  3.99s/it]100%|██████████| 3/3 [00:12<00:00,  4.18s/it]
=> result
* total: 1,410
* correct: 1,065
* accuracy: 75.5%
* error: 24.5%
* macro_f1: 70.5%
Run this job and save the output to output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,410
---------  -------------
['wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X wild pansy, a type of flower.', 'X X X X primula, a type of flower.', 'X X X X sunflower, a type of flower.', 'X X X X pelargonium, a type of flower.', 'X X X X bishop of llandaff, a type of flower.', 'X X X X gaura, a type of flower.', 'X X X X geranium, a type of flower.', 'X X X X orange dahlia, a type of flower.', 'X X X X pink-yellow dahlia, a type of flower.', 'X X X X cautleya spicata, a type of flower.', 'X X X X japanese anemone, a type of flower.', 'X X X X black-eyed susan, a type of flower.', 'X X X X silverbush, a type of flower.', 'X X X X californian poppy, a type of flower.', 'X X X X osteospermum, a type of flower.', 'X X X X spring crocus, a type of flower.', 'X X X X bearded iris, a type of flower.', 'X X X X windflower, a type of flower.', 'X X X X tree poppy, a type of flower.', 'X X X X gazania, a type of flower.', 'X X X X azalea, a type of flower.', 'X X X X water lily, a type of flower.', 'X X X X rose, a type of flower.', 'X X X X thorn apple, a type of flower.', 'X X X X morning glory, a type of flower.', 'X X X X passion flower, a type of flower.', 'X X X X lotus, a type of flower.', 'X X X X toad lily, a type of flower.', 'X X X X anthurium, a type of flower.', 'X X X X frangipani, a type of flower.', 'X X X X clematis, a type of flower.', 'X X X X hibiscus, a type of flower.', 'X X X X columbine, a type of flower.', 'X X X X desert-rose, a type of flower.', 'X X X X tree mallow, a type of flower.', 'X X X X magnolia, a type of flower.', 'X X X X cyclamen, a type of flower.', 'X X X X watercress, a type of flower.', 'X X X X canna lily, a type of flower.', 'X X X X hippeastrum, a type of flower.', 'X X X X bee balm, a type of flower.', 'X X X X ball moss, a type of flower.', 'X X X X foxglove, a type of flower.', 'X X X X bougainvillea, a type of flower.', 'X X X X camellia, a type of flower.', 'X X X X mallow, a type of flower.', 'X X X X mexican petunia, a type of flower.', 'X X X X bromelia, a type of flower.', 'X X X X blanket flower, a type of flower.', 'X X X X trumpet creeper, a type of flower.', 'X X X X blackberry lily, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:10,  5.44s/it] 67%|██████▋   | 2/3 [00:06<00:02,  2.90s/it]100%|██████████| 3/3 [00:07<00:00,  2.03s/it]100%|██████████| 3/3 [00:07<00:00,  2.55s/it]
=> result
* total: 1,410
* correct: 1,069
* accuracy: 75.8%
* error: 24.2%
* macro_f1: 70.6%
Run this job and save the output to output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_flowers.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordFlowers
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordFlowers
Reading split from /data/yht/data/cl/data/oxford_flowers/split_zhou_OxfordFlowers.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_flowers/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  -------------
Dataset    OxfordFlowers
# classes  51
# train_x  816
# val      204
# test     1,410
---------  -------------
['wild pansy', 'primula', 'sunflower', 'pelargonium', 'bishop of llandaff', 'gaura', 'geranium', 'orange dahlia', 'pink-yellow dahlia', 'cautleya spicata', 'japanese anemone', 'black-eyed susan', 'silverbush', 'californian poppy', 'osteospermum', 'spring crocus', 'bearded iris', 'windflower', 'tree poppy', 'gazania', 'azalea', 'water lily', 'rose', 'thorn apple', 'morning glory', 'passion flower', 'lotus', 'toad lily', 'anthurium', 'frangipani', 'clematis', 'hibiscus', 'columbine', 'desert-rose', 'tree mallow', 'magnolia', 'cyclamen', 'watercress', 'canna lily', 'hippeastrum', 'bee balm', 'ball moss', 'foxglove', 'bougainvillea', 'camellia', 'mallow', 'mexican petunia', 'bromelia', 'blanket flower', 'trumpet creeper', 'blackberry lily']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X wild pansy, a type of flower.', 'X X X X primula, a type of flower.', 'X X X X sunflower, a type of flower.', 'X X X X pelargonium, a type of flower.', 'X X X X bishop of llandaff, a type of flower.', 'X X X X gaura, a type of flower.', 'X X X X geranium, a type of flower.', 'X X X X orange dahlia, a type of flower.', 'X X X X pink-yellow dahlia, a type of flower.', 'X X X X cautleya spicata, a type of flower.', 'X X X X japanese anemone, a type of flower.', 'X X X X black-eyed susan, a type of flower.', 'X X X X silverbush, a type of flower.', 'X X X X californian poppy, a type of flower.', 'X X X X osteospermum, a type of flower.', 'X X X X spring crocus, a type of flower.', 'X X X X bearded iris, a type of flower.', 'X X X X windflower, a type of flower.', 'X X X X tree poppy, a type of flower.', 'X X X X gazania, a type of flower.', 'X X X X azalea, a type of flower.', 'X X X X water lily, a type of flower.', 'X X X X rose, a type of flower.', 'X X X X thorn apple, a type of flower.', 'X X X X morning glory, a type of flower.', 'X X X X passion flower, a type of flower.', 'X X X X lotus, a type of flower.', 'X X X X toad lily, a type of flower.', 'X X X X anthurium, a type of flower.', 'X X X X frangipani, a type of flower.', 'X X X X clematis, a type of flower.', 'X X X X hibiscus, a type of flower.', 'X X X X columbine, a type of flower.', 'X X X X desert-rose, a type of flower.', 'X X X X tree mallow, a type of flower.', 'X X X X magnolia, a type of flower.', 'X X X X cyclamen, a type of flower.', 'X X X X watercress, a type of flower.', 'X X X X canna lily, a type of flower.', 'X X X X hippeastrum, a type of flower.', 'X X X X bee balm, a type of flower.', 'X X X X ball moss, a type of flower.', 'X X X X foxglove, a type of flower.', 'X X X X bougainvillea, a type of flower.', 'X X X X camellia, a type of flower.', 'X X X X mallow, a type of flower.', 'X X X X mexican petunia, a type of flower.', 'X X X X bromelia, a type of flower.', 'X X X X blanket flower, a type of flower.', 'X X X X trumpet creeper, a type of flower.', 'X X X X blackberry lily, a type of flower.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/oxford_flowers/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/3 [00:00<?, ?it/s] 33%|███▎      | 1/3 [00:05<00:11,  5.54s/it] 67%|██████▋   | 2/3 [00:06<00:02,  2.94s/it]100%|██████████| 3/3 [00:07<00:00,  2.05s/it]100%|██████████| 3/3 [00:07<00:00,  2.59s/it]
=> result
* total: 1,410
* correct: 1,073
* accuracy: 76.1%
* error: 23.9%
* macro_f1: 71.3%
Run this job and save the output to output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  19
# train_x  304
# val      76
# test     1,881
---------  ----------
['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair', 'chihuahua', 'egyptian_mau', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abyssinian, a type of pet.', 'X X X X american bulldog, a type of pet.', 'X X X X american pit bull terrier, a type of pet.', 'X X X X basset hound, a type of pet.', 'X X X X beagle, a type of pet.', 'X X X X bengal, a type of pet.', 'X X X X birman, a type of pet.', 'X X X X bombay, a type of pet.', 'X X X X boxer, a type of pet.', 'X X X X british shorthair, a type of pet.', 'X X X X chihuahua, a type of pet.', 'X X X X egyptian mau, a type of pet.', 'X X X X english cocker spaniel, a type of pet.', 'X X X X english setter, a type of pet.', 'X X X X german shorthaired, a type of pet.', 'X X X X great pyrenees, a type of pet.', 'X X X X havanese, a type of pet.', 'X X X X japanese chin, a type of pet.', 'X X X X keeshond, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([19, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/10] time 0.144 (0.314) data 0.000 (0.167) loss 4.8398 (4.8711) acc 62.5000 (69.3750) lr 1.0000e-05 eta 0:02:35
epoch [1/50] batch [10/10] time 0.086 (0.224) data 0.000 (0.083) loss 4.7109 (4.8438) acc 68.7500 (67.5000) lr 2.0000e-03 eta 0:01:49
epoch [2/50] batch [5/10] time 0.141 (0.267) data 0.000 (0.126) loss 3.3184 (3.5465) acc 75.0000 (77.5000) lr 2.0000e-03 eta 0:02:09
epoch [2/50] batch [10/10] time 0.087 (0.199) data 0.000 (0.063) loss 3.3398 (3.2406) acc 68.7500 (79.6875) lr 1.9980e-03 eta 0:01:35
epoch [3/50] batch [5/10] time 0.141 (0.287) data 0.000 (0.144) loss 2.8145 (2.6977) acc 68.7500 (81.8750) lr 1.9980e-03 eta 0:02:16
epoch [3/50] batch [10/10] time 0.088 (0.208) data 0.000 (0.072) loss 2.3086 (2.5695) acc 87.5000 (84.0625) lr 1.9921e-03 eta 0:01:37
epoch [4/50] batch [5/10] time 0.140 (0.273) data 0.000 (0.131) loss 2.3789 (2.3414) acc 87.5000 (85.6250) lr 1.9921e-03 eta 0:02:07
epoch [4/50] batch [10/10] time 0.088 (0.202) data 0.000 (0.066) loss 1.8516 (2.3311) acc 100.0000 (84.0625) lr 1.9823e-03 eta 0:01:32
epoch [5/50] batch [5/10] time 0.143 (0.288) data 0.000 (0.145) loss 2.0215 (2.2500) acc 87.5000 (83.7500) lr 1.9823e-03 eta 0:02:11
epoch [5/50] batch [10/10] time 0.088 (0.210) data 0.001 (0.073) loss 2.0391 (2.1520) acc 81.2500 (83.7500) lr 1.9686e-03 eta 0:01:34
epoch [6/50] batch [5/10] time 0.140 (0.277) data 0.000 (0.136) loss 1.9727 (1.9539) acc 84.3750 (86.8750) lr 1.9686e-03 eta 0:02:03
epoch [6/50] batch [10/10] time 0.087 (0.204) data 0.000 (0.068) loss 2.1777 (2.0184) acc 75.0000 (83.4375) lr 1.9511e-03 eta 0:01:29
epoch [7/50] batch [5/10] time 0.141 (0.281) data 0.000 (0.138) loss 1.7207 (1.9508) acc 96.8750 (86.2500) lr 1.9511e-03 eta 0:02:02
epoch [7/50] batch [10/10] time 0.087 (0.206) data 0.000 (0.069) loss 1.8281 (1.9323) acc 93.7500 (87.5000) lr 1.9298e-03 eta 0:01:28
epoch [8/50] batch [5/10] time 0.141 (0.271) data 0.000 (0.129) loss 1.8262 (1.7832) acc 90.6250 (87.5000) lr 1.9298e-03 eta 0:01:55
epoch [8/50] batch [10/10] time 0.087 (0.201) data 0.000 (0.064) loss 2.4883 (1.9285) acc 87.5000 (86.2500) lr 1.9048e-03 eta 0:01:24
epoch [9/50] batch [5/10] time 0.141 (0.278) data 0.000 (0.136) loss 1.9531 (1.9461) acc 78.1250 (81.8750) lr 1.9048e-03 eta 0:01:55
epoch [9/50] batch [10/10] time 0.086 (0.205) data 0.000 (0.068) loss 1.7168 (1.8614) acc 87.5000 (85.6250) lr 1.8763e-03 eta 0:01:23
epoch [10/50] batch [5/10] time 0.141 (0.273) data 0.000 (0.130) loss 1.9824 (1.9066) acc 81.2500 (82.5000) lr 1.8763e-03 eta 0:01:50
epoch [10/50] batch [10/10] time 0.087 (0.202) data 0.000 (0.065) loss 2.2773 (1.9471) acc 75.0000 (81.2500) lr 1.8443e-03 eta 0:01:20
epoch [11/50] batch [5/10] time 0.141 (0.282) data 0.000 (0.140) loss 1.7158 (1.6672) acc 84.3750 (87.5000) lr 1.8443e-03 eta 0:01:51
epoch [11/50] batch [10/10] time 0.087 (0.207) data 0.000 (0.070) loss 2.2617 (1.7871) acc 75.0000 (86.2500) lr 1.8090e-03 eta 0:01:20
epoch [12/50] batch [5/10] time 0.142 (0.286) data 0.000 (0.144) loss 1.5068 (1.5752) acc 87.5000 (89.3750) lr 1.8090e-03 eta 0:01:50
epoch [12/50] batch [10/10] time 0.086 (0.209) data 0.000 (0.072) loss 1.8652 (1.6328) acc 87.5000 (89.0625) lr 1.7705e-03 eta 0:01:19
epoch [13/50] batch [5/10] time 0.142 (0.271) data 0.000 (0.130) loss 1.5498 (1.7525) acc 81.2500 (83.7500) lr 1.7705e-03 eta 0:01:41
epoch [13/50] batch [10/10] time 0.085 (0.201) data 0.000 (0.065) loss 1.4424 (1.6340) acc 87.5000 (87.5000) lr 1.7290e-03 eta 0:01:14
epoch [14/50] batch [5/10] time 0.142 (0.279) data 0.000 (0.137) loss 1.8555 (1.6973) acc 87.5000 (85.6250) lr 1.7290e-03 eta 0:01:41
epoch [14/50] batch [10/10] time 0.087 (0.205) data 0.000 (0.068) loss 1.6074 (1.6035) acc 81.2500 (87.1875) lr 1.6845e-03 eta 0:01:13
epoch [15/50] batch [5/10] time 0.143 (0.290) data 0.000 (0.147) loss 1.7783 (1.6861) acc 90.6250 (88.7500) lr 1.6845e-03 eta 0:01:42
epoch [15/50] batch [10/10] time 0.088 (0.211) data 0.000 (0.073) loss 1.2295 (1.5890) acc 100.0000 (90.3125) lr 1.6374e-03 eta 0:01:13
epoch [16/50] batch [5/10] time 0.140 (0.269) data 0.000 (0.127) loss 1.7578 (1.5199) acc 90.6250 (91.8750) lr 1.6374e-03 eta 0:01:32
epoch [16/50] batch [10/10] time 0.086 (0.200) data 0.000 (0.064) loss 1.8535 (1.5688) acc 81.2500 (90.0000) lr 1.5878e-03 eta 0:01:07
epoch [17/50] batch [5/10] time 0.140 (0.262) data 0.000 (0.121) loss 1.7178 (1.8070) acc 90.6250 (83.7500) lr 1.5878e-03 eta 0:01:27
epoch [17/50] batch [10/10] time 0.086 (0.197) data 0.000 (0.061) loss 1.6934 (1.6637) acc 75.0000 (85.0000) lr 1.5358e-03 eta 0:01:04
epoch [18/50] batch [5/10] time 0.142 (0.306) data 0.000 (0.162) loss 1.7285 (1.5223) acc 84.3750 (88.7500) lr 1.5358e-03 eta 0:01:39
epoch [18/50] batch [10/10] time 0.089 (0.219) data 0.000 (0.081) loss 1.9375 (1.6003) acc 75.0000 (88.1250) lr 1.4818e-03 eta 0:01:10
epoch [19/50] batch [5/10] time 0.147 (0.309) data 0.000 (0.160) loss 1.6377 (1.5838) acc 87.5000 (88.1250) lr 1.4818e-03 eta 0:01:37
epoch [19/50] batch [10/10] time 0.090 (0.222) data 0.000 (0.080) loss 1.6641 (1.6378) acc 81.2500 (87.8125) lr 1.4258e-03 eta 0:01:08
epoch [20/50] batch [5/10] time 0.141 (0.275) data 0.000 (0.132) loss 1.4160 (1.5195) acc 93.7500 (90.6250) lr 1.4258e-03 eta 0:01:23
epoch [20/50] batch [10/10] time 0.088 (0.202) data 0.000 (0.066) loss 1.1162 (1.4466) acc 100.0000 (92.1875) lr 1.3681e-03 eta 0:01:00
epoch [21/50] batch [5/10] time 0.141 (0.346) data 0.000 (0.204) loss 1.5732 (1.5932) acc 87.5000 (91.2500) lr 1.3681e-03 eta 0:01:42
epoch [21/50] batch [10/10] time 0.088 (0.238) data 0.000 (0.102) loss 1.4062 (1.5400) acc 100.0000 (90.0000) lr 1.3090e-03 eta 0:01:09
epoch [22/50] batch [5/10] time 0.143 (0.267) data 0.000 (0.125) loss 1.6035 (1.5959) acc 87.5000 (89.3750) lr 1.3090e-03 eta 0:01:16
epoch [22/50] batch [10/10] time 0.087 (0.199) data 0.000 (0.062) loss 1.5703 (1.5705) acc 93.7500 (90.3125) lr 1.2487e-03 eta 0:00:55
epoch [23/50] batch [5/10] time 0.141 (0.285) data 0.000 (0.143) loss 1.8398 (1.6139) acc 84.3750 (88.7500) lr 1.2487e-03 eta 0:01:18
epoch [23/50] batch [10/10] time 0.087 (0.208) data 0.000 (0.072) loss 1.1553 (1.5537) acc 100.0000 (90.3125) lr 1.1874e-03 eta 0:00:56
epoch [24/50] batch [5/10] time 0.142 (0.275) data 0.000 (0.133) loss 1.3682 (1.5193) acc 96.8750 (88.7500) lr 1.1874e-03 eta 0:01:12
epoch [24/50] batch [10/10] time 0.088 (0.204) data 0.000 (0.067) loss 1.3809 (1.4806) acc 93.7500 (89.6875) lr 1.1253e-03 eta 0:00:52
epoch [25/50] batch [5/10] time 0.143 (0.280) data 0.000 (0.136) loss 1.7021 (1.4438) acc 87.5000 (92.5000) lr 1.1253e-03 eta 0:01:11
epoch [25/50] batch [10/10] time 0.088 (0.205) data 0.000 (0.068) loss 2.0547 (1.4614) acc 75.0000 (90.9375) lr 1.0628e-03 eta 0:00:51
epoch [26/50] batch [5/10] time 0.147 (0.278) data 0.000 (0.133) loss 1.3691 (1.4859) acc 96.8750 (93.1250) lr 1.0628e-03 eta 0:01:08
epoch [26/50] batch [10/10] time 0.090 (0.206) data 0.000 (0.067) loss 1.6943 (1.4505) acc 87.5000 (92.5000) lr 1.0000e-03 eta 0:00:49
epoch [27/50] batch [5/10] time 0.146 (0.268) data 0.000 (0.121) loss 1.1895 (1.3789) acc 96.8750 (91.2500) lr 1.0000e-03 eta 0:01:02
epoch [27/50] batch [10/10] time 0.089 (0.201) data 0.000 (0.061) loss 2.5586 (1.5784) acc 75.0000 (89.3750) lr 9.3721e-04 eta 0:00:46
epoch [28/50] batch [5/10] time 0.146 (0.299) data 0.000 (0.152) loss 1.5996 (1.5449) acc 90.6250 (90.0000) lr 9.3721e-04 eta 0:01:07
epoch [28/50] batch [10/10] time 0.091 (0.217) data 0.000 (0.076) loss 1.6621 (1.5234) acc 87.5000 (89.3750) lr 8.7467e-04 eta 0:00:47
epoch [29/50] batch [5/10] time 0.145 (0.281) data 0.000 (0.136) loss 1.2402 (1.3033) acc 100.0000 (95.0000) lr 8.7467e-04 eta 0:01:00
epoch [29/50] batch [10/10] time 0.089 (0.207) data 0.000 (0.068) loss 1.7969 (1.4697) acc 81.2500 (90.6250) lr 8.1262e-04 eta 0:00:43
epoch [30/50] batch [5/10] time 0.141 (0.288) data 0.000 (0.145) loss 1.3945 (1.3568) acc 93.7500 (93.7500) lr 8.1262e-04 eta 0:00:58
epoch [30/50] batch [10/10] time 0.087 (0.209) data 0.000 (0.073) loss 1.1230 (1.3657) acc 100.0000 (94.0625) lr 7.5131e-04 eta 0:00:41
epoch [31/50] batch [5/10] time 0.144 (0.281) data 0.000 (0.138) loss 1.2715 (1.6363) acc 96.8750 (87.5000) lr 7.5131e-04 eta 0:00:54
epoch [31/50] batch [10/10] time 0.087 (0.206) data 0.000 (0.069) loss 1.6982 (1.5619) acc 87.5000 (88.4375) lr 6.9098e-04 eta 0:00:39
epoch [32/50] batch [5/10] time 0.142 (0.304) data 0.000 (0.162) loss 1.2754 (1.4125) acc 96.8750 (90.6250) lr 6.9098e-04 eta 0:00:56
epoch [32/50] batch [10/10] time 0.087 (0.217) data 0.000 (0.081) loss 1.1436 (1.4829) acc 93.7500 (89.3750) lr 6.3188e-04 eta 0:00:39
epoch [33/50] batch [5/10] time 0.142 (0.271) data 0.000 (0.129) loss 1.1709 (1.4793) acc 96.8750 (89.3750) lr 6.3188e-04 eta 0:00:47
epoch [33/50] batch [10/10] time 0.088 (0.201) data 0.000 (0.065) loss 1.9658 (1.4845) acc 75.0000 (90.3125) lr 5.7422e-04 eta 0:00:34
epoch [34/50] batch [5/10] time 0.141 (0.287) data 0.000 (0.144) loss 1.2988 (1.5068) acc 96.8750 (90.6250) lr 5.7422e-04 eta 0:00:47
epoch [34/50] batch [10/10] time 0.085 (0.209) data 0.000 (0.072) loss 1.1826 (1.4341) acc 93.7500 (90.9375) lr 5.1825e-04 eta 0:00:33
epoch [35/50] batch [5/10] time 0.141 (0.284) data 0.000 (0.142) loss 1.2344 (1.4264) acc 93.7500 (88.7500) lr 5.1825e-04 eta 0:00:43
epoch [35/50] batch [10/10] time 0.087 (0.207) data 0.000 (0.071) loss 0.9136 (1.3797) acc 100.0000 (90.3125) lr 4.6417e-04 eta 0:00:31
epoch [36/50] batch [5/10] time 0.144 (0.278) data 0.000 (0.134) loss 1.2500 (1.4115) acc 90.6250 (91.8750) lr 4.6417e-04 eta 0:00:40
epoch [36/50] batch [10/10] time 0.087 (0.204) data 0.000 (0.067) loss 1.2100 (1.3890) acc 100.0000 (92.5000) lr 4.1221e-04 eta 0:00:28
epoch [37/50] batch [5/10] time 0.146 (0.281) data 0.000 (0.135) loss 1.4316 (1.3459) acc 90.6250 (93.1250) lr 4.1221e-04 eta 0:00:37
epoch [37/50] batch [10/10] time 0.095 (0.207) data 0.000 (0.068) loss 1.1279 (1.4055) acc 93.7500 (90.9375) lr 3.6258e-04 eta 0:00:26
epoch [38/50] batch [5/10] time 0.142 (0.275) data 0.000 (0.132) loss 1.6729 (1.4346) acc 81.2500 (89.3750) lr 3.6258e-04 eta 0:00:34
epoch [38/50] batch [10/10] time 0.087 (0.203) data 0.000 (0.066) loss 1.1230 (1.3735) acc 100.0000 (91.2500) lr 3.1545e-04 eta 0:00:24
epoch [39/50] batch [5/10] time 0.143 (0.291) data 0.000 (0.148) loss 1.3018 (1.3094) acc 96.8750 (97.5000) lr 3.1545e-04 eta 0:00:33
epoch [39/50] batch [10/10] time 0.089 (0.212) data 0.000 (0.074) loss 1.1826 (1.3949) acc 93.7500 (93.4375) lr 2.7103e-04 eta 0:00:23
epoch [40/50] batch [5/10] time 0.141 (0.267) data 0.000 (0.125) loss 1.2314 (1.3279) acc 96.8750 (93.7500) lr 2.7103e-04 eta 0:00:28
epoch [40/50] batch [10/10] time 0.087 (0.199) data 0.000 (0.062) loss 1.1992 (1.3358) acc 93.7500 (93.4375) lr 2.2949e-04 eta 0:00:19
epoch [41/50] batch [5/10] time 0.142 (0.277) data 0.000 (0.134) loss 1.4229 (1.3275) acc 90.6250 (93.1250) lr 2.2949e-04 eta 0:00:26
epoch [41/50] batch [10/10] time 0.087 (0.204) data 0.000 (0.067) loss 1.6055 (1.3436) acc 81.2500 (92.8125) lr 1.9098e-04 eta 0:00:18
epoch [42/50] batch [5/10] time 0.141 (0.282) data 0.000 (0.140) loss 1.4883 (1.4945) acc 90.6250 (91.2500) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [10/10] time 0.087 (0.206) data 0.000 (0.070) loss 1.4922 (1.4572) acc 93.7500 (91.5625) lr 1.5567e-04 eta 0:00:16
epoch [43/50] batch [5/10] time 0.143 (0.275) data 0.000 (0.131) loss 1.6465 (1.3707) acc 84.3750 (93.7500) lr 1.5567e-04 eta 0:00:20
epoch [43/50] batch [10/10] time 0.088 (0.203) data 0.000 (0.065) loss 1.7100 (1.4225) acc 87.5000 (92.1875) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [5/10] time 0.141 (0.278) data 0.000 (0.137) loss 1.3027 (1.3078) acc 90.6250 (95.0000) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [10/10] time 0.087 (0.204) data 0.000 (0.068) loss 1.1562 (1.3964) acc 93.7500 (92.1875) lr 9.5173e-05 eta 0:00:12
epoch [45/50] batch [5/10] time 0.141 (0.297) data 0.000 (0.154) loss 1.6182 (1.4355) acc 87.5000 (91.2500) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [10/10] time 0.086 (0.214) data 0.000 (0.077) loss 1.1582 (1.3902) acc 100.0000 (92.8125) lr 7.0224e-05 eta 0:00:10
epoch [46/50] batch [5/10] time 0.141 (0.290) data 0.000 (0.148) loss 1.6611 (1.4203) acc 90.6250 (91.2500) lr 7.0224e-05 eta 0:00:13
epoch [46/50] batch [10/10] time 0.087 (0.210) data 0.000 (0.074) loss 1.3574 (1.3600) acc 93.7500 (93.4375) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [5/10] time 0.143 (0.265) data 0.000 (0.121) loss 1.4043 (1.2908) acc 90.6250 (93.7500) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [10/10] time 0.088 (0.198) data 0.000 (0.061) loss 1.2051 (1.3044) acc 93.7500 (93.7500) lr 3.1417e-05 eta 0:00:05
epoch [48/50] batch [5/10] time 0.143 (0.275) data 0.000 (0.132) loss 1.1387 (1.4193) acc 93.7500 (93.1250) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [10/10] time 0.088 (0.204) data 0.000 (0.066) loss 1.3789 (1.4158) acc 100.0000 (93.1250) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [5/10] time 0.141 (0.275) data 0.000 (0.133) loss 1.3525 (1.4094) acc 87.5000 (90.0000) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [10/10] time 0.087 (0.203) data 0.000 (0.067) loss 1.6680 (1.3988) acc 87.5000 (90.9375) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/10] time 0.143 (0.280) data 0.000 (0.135) loss 1.5889 (1.5557) acc 90.6250 (90.6250) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/10] time 0.088 (0.206) data 0.000 (0.068) loss 1.4414 (1.4754) acc 93.7500 (90.9375) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:16,  5.46s/it] 50%|█████     | 2/4 [00:06<00:05,  2.91s/it] 75%|███████▌  | 3/4 [00:07<00:02,  2.09s/it]100%|██████████| 4/4 [00:08<00:00,  1.64s/it]100%|██████████| 4/4 [00:08<00:00,  2.19s/it]
=> result
* total: 1,881
* correct: 1,784
* accuracy: 94.8%
* error: 5.2%
* macro_f1: 94.8%
Elapsed: 0:01:58
Run this job and save the output to output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  19
# train_x  304
# val      76
# test     1,881
---------  ----------
['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair', 'chihuahua', 'egyptian_mau', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abyssinian, a type of pet.', 'X X X X american bulldog, a type of pet.', 'X X X X american pit bull terrier, a type of pet.', 'X X X X basset hound, a type of pet.', 'X X X X beagle, a type of pet.', 'X X X X bengal, a type of pet.', 'X X X X birman, a type of pet.', 'X X X X bombay, a type of pet.', 'X X X X boxer, a type of pet.', 'X X X X british shorthair, a type of pet.', 'X X X X chihuahua, a type of pet.', 'X X X X egyptian mau, a type of pet.', 'X X X X english cocker spaniel, a type of pet.', 'X X X X english setter, a type of pet.', 'X X X X german shorthaired, a type of pet.', 'X X X X great pyrenees, a type of pet.', 'X X X X havanese, a type of pet.', 'X X X X japanese chin, a type of pet.', 'X X X X keeshond, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([19, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/10] time 0.142 (0.277) data 0.000 (0.130) loss 4.7734 (4.2660) acc 68.7500 (78.1250) lr 1.0000e-05 eta 0:02:16
epoch [1/50] batch [10/10] time 0.087 (0.203) data 0.000 (0.065) loss 3.8164 (4.1693) acc 87.5000 (79.3750) lr 2.0000e-03 eta 0:01:39
epoch [2/50] batch [5/10] time 0.141 (0.276) data 0.000 (0.134) loss 2.9434 (3.4074) acc 93.7500 (83.7500) lr 2.0000e-03 eta 0:02:14
epoch [2/50] batch [10/10] time 0.086 (0.203) data 0.000 (0.067) loss 2.5859 (3.1844) acc 87.5000 (83.7500) lr 1.9980e-03 eta 0:01:37
epoch [3/50] batch [5/10] time 0.141 (0.283) data 0.000 (0.140) loss 2.6094 (2.5266) acc 81.2500 (85.0000) lr 1.9980e-03 eta 0:02:14
epoch [3/50] batch [10/10] time 0.086 (0.206) data 0.000 (0.070) loss 2.2031 (2.4699) acc 87.5000 (84.0625) lr 1.9921e-03 eta 0:01:37
epoch [4/50] batch [5/10] time 0.142 (0.276) data 0.000 (0.133) loss 2.5742 (2.3602) acc 81.2500 (83.1250) lr 1.9921e-03 eta 0:02:08
epoch [4/50] batch [10/10] time 0.088 (0.203) data 0.000 (0.067) loss 2.4844 (2.1960) acc 81.2500 (86.5625) lr 1.9823e-03 eta 0:01:33
epoch [5/50] batch [5/10] time 0.141 (0.276) data 0.000 (0.133) loss 2.1562 (2.0809) acc 84.3750 (86.8750) lr 1.9823e-03 eta 0:02:05
epoch [5/50] batch [10/10] time 0.088 (0.203) data 0.000 (0.067) loss 1.9160 (2.0160) acc 87.5000 (86.8750) lr 1.9686e-03 eta 0:01:31
epoch [6/50] batch [5/10] time 0.141 (0.279) data 0.000 (0.136) loss 2.2695 (2.0264) acc 81.2500 (85.6250) lr 1.9686e-03 eta 0:02:04
epoch [6/50] batch [10/10] time 0.087 (0.204) data 0.000 (0.068) loss 1.3750 (1.9125) acc 93.7500 (86.8750) lr 1.9511e-03 eta 0:01:29
epoch [7/50] batch [5/10] time 0.141 (0.288) data 0.000 (0.146) loss 1.6816 (1.7688) acc 90.6250 (90.6250) lr 1.9511e-03 eta 0:02:05
epoch [7/50] batch [10/10] time 0.087 (0.209) data 0.000 (0.073) loss 1.9746 (1.8547) acc 87.5000 (90.0000) lr 1.9298e-03 eta 0:01:29
epoch [8/50] batch [5/10] time 0.142 (0.291) data 0.000 (0.148) loss 1.6719 (1.7811) acc 87.5000 (87.5000) lr 1.9298e-03 eta 0:02:03
epoch [8/50] batch [10/10] time 0.087 (0.211) data 0.000 (0.074) loss 1.6680 (1.8023) acc 93.7500 (86.5625) lr 1.9048e-03 eta 0:01:28
epoch [9/50] batch [5/10] time 0.141 (0.284) data 0.000 (0.142) loss 1.9766 (1.7770) acc 90.6250 (89.3750) lr 1.9048e-03 eta 0:01:57
epoch [9/50] batch [10/10] time 0.086 (0.207) data 0.000 (0.071) loss 1.3760 (1.7371) acc 100.0000 (90.0000) lr 1.8763e-03 eta 0:01:24
epoch [10/50] batch [5/10] time 0.141 (0.275) data 0.000 (0.133) loss 1.8906 (1.7705) acc 90.6250 (87.5000) lr 1.8763e-03 eta 0:01:51
epoch [10/50] batch [10/10] time 0.087 (0.203) data 0.000 (0.067) loss 2.3105 (1.7721) acc 75.0000 (87.1875) lr 1.8443e-03 eta 0:01:21
epoch [11/50] batch [5/10] time 0.141 (0.273) data 0.000 (0.130) loss 2.0078 (1.7732) acc 78.1250 (86.8750) lr 1.8443e-03 eta 0:01:47
epoch [11/50] batch [10/10] time 0.087 (0.201) data 0.000 (0.065) loss 1.5479 (1.6852) acc 93.7500 (89.3750) lr 1.8090e-03 eta 0:01:18
epoch [12/50] batch [5/10] time 0.141 (0.287) data 0.000 (0.144) loss 1.2598 (1.4307) acc 100.0000 (95.6250) lr 1.8090e-03 eta 0:01:50
epoch [12/50] batch [10/10] time 0.087 (0.209) data 0.000 (0.072) loss 1.2324 (1.4806) acc 100.0000 (92.5000) lr 1.7705e-03 eta 0:01:19
epoch [13/50] batch [5/10] time 0.142 (0.274) data 0.000 (0.131) loss 1.7744 (1.6098) acc 81.2500 (89.3750) lr 1.7705e-03 eta 0:01:42
epoch [13/50] batch [10/10] time 0.087 (0.202) data 0.000 (0.066) loss 1.5332 (1.6164) acc 93.7500 (88.7500) lr 1.7290e-03 eta 0:01:14
epoch [14/50] batch [5/10] time 0.141 (0.294) data 0.000 (0.152) loss 1.6855 (1.4930) acc 87.5000 (91.8750) lr 1.7290e-03 eta 0:01:47
epoch [14/50] batch [10/10] time 0.087 (0.212) data 0.000 (0.076) loss 1.4199 (1.5130) acc 87.5000 (90.9375) lr 1.6845e-03 eta 0:01:16
epoch [15/50] batch [5/10] time 0.144 (0.304) data 0.000 (0.160) loss 1.3564 (1.6949) acc 93.7500 (87.5000) lr 1.6845e-03 eta 0:01:47
epoch [15/50] batch [10/10] time 0.087 (0.218) data 0.000 (0.080) loss 2.0352 (1.7288) acc 81.2500 (86.5625) lr 1.6374e-03 eta 0:01:16
epoch [16/50] batch [5/10] time 0.141 (0.281) data 0.000 (0.138) loss 1.6504 (1.7279) acc 90.6250 (87.5000) lr 1.6374e-03 eta 0:01:36
epoch [16/50] batch [10/10] time 0.086 (0.206) data 0.000 (0.069) loss 1.0283 (1.6123) acc 100.0000 (89.6875) lr 1.5878e-03 eta 0:01:10
epoch [17/50] batch [5/10] time 0.142 (0.292) data 0.000 (0.148) loss 1.6250 (1.4260) acc 87.5000 (93.7500) lr 1.5878e-03 eta 0:01:37
epoch [17/50] batch [10/10] time 0.086 (0.211) data 0.000 (0.074) loss 2.3848 (1.5144) acc 81.2500 (92.5000) lr 1.5358e-03 eta 0:01:09
epoch [18/50] batch [5/10] time 0.142 (0.269) data 0.000 (0.127) loss 1.5303 (1.4850) acc 87.5000 (91.8750) lr 1.5358e-03 eta 0:01:27
epoch [18/50] batch [10/10] time 0.088 (0.201) data 0.000 (0.063) loss 1.1670 (1.4617) acc 100.0000 (91.8750) lr 1.4818e-03 eta 0:01:04
epoch [19/50] batch [5/10] time 0.148 (0.288) data 0.000 (0.143) loss 1.6504 (1.4967) acc 84.3750 (92.5000) lr 1.4818e-03 eta 0:01:30
epoch [19/50] batch [10/10] time 0.091 (0.211) data 0.000 (0.072) loss 1.3867 (1.5207) acc 93.7500 (91.8750) lr 1.4258e-03 eta 0:01:05
epoch [20/50] batch [5/10] time 0.147 (0.276) data 0.000 (0.129) loss 1.5781 (1.5965) acc 81.2500 (86.2500) lr 1.4258e-03 eta 0:01:24
epoch [20/50] batch [10/10] time 0.091 (0.205) data 0.000 (0.064) loss 1.3750 (1.5439) acc 100.0000 (89.0625) lr 1.3681e-03 eta 0:01:01
epoch [21/50] batch [5/10] time 0.143 (0.279) data 0.000 (0.136) loss 1.5576 (1.4178) acc 87.5000 (90.6250) lr 1.3681e-03 eta 0:01:22
epoch [21/50] batch [10/10] time 0.089 (0.206) data 0.000 (0.068) loss 1.4375 (1.4463) acc 93.7500 (90.9375) lr 1.3090e-03 eta 0:00:59
epoch [22/50] batch [5/10] time 0.141 (0.284) data 0.000 (0.142) loss 1.5176 (1.5768) acc 90.6250 (88.7500) lr 1.3090e-03 eta 0:01:20
epoch [22/50] batch [10/10] time 0.088 (0.207) data 0.000 (0.071) loss 1.3691 (1.5314) acc 87.5000 (88.4375) lr 1.2487e-03 eta 0:00:57
epoch [23/50] batch [5/10] time 0.143 (0.283) data 0.000 (0.140) loss 1.2305 (1.4816) acc 96.8750 (91.2500) lr 1.2487e-03 eta 0:01:17
epoch [23/50] batch [10/10] time 0.089 (0.208) data 0.000 (0.070) loss 1.2930 (1.4582) acc 93.7500 (91.2500) lr 1.1874e-03 eta 0:00:56
epoch [24/50] batch [5/10] time 0.141 (0.274) data 0.000 (0.132) loss 1.2881 (1.4342) acc 93.7500 (91.2500) lr 1.1874e-03 eta 0:01:12
epoch [24/50] batch [10/10] time 0.087 (0.202) data 0.000 (0.066) loss 1.5723 (1.4240) acc 87.5000 (91.5625) lr 1.1253e-03 eta 0:00:52
epoch [25/50] batch [5/10] time 0.142 (0.285) data 0.000 (0.142) loss 1.3789 (1.5133) acc 90.6250 (87.5000) lr 1.1253e-03 eta 0:01:12
epoch [25/50] batch [10/10] time 0.088 (0.208) data 0.000 (0.071) loss 2.5723 (1.5838) acc 68.7500 (87.5000) lr 1.0628e-03 eta 0:00:52
epoch [26/50] batch [5/10] time 0.142 (0.284) data 0.000 (0.142) loss 1.3701 (1.3652) acc 96.8750 (95.6250) lr 1.0628e-03 eta 0:01:09
epoch [26/50] batch [10/10] time 0.087 (0.208) data 0.000 (0.071) loss 1.2891 (1.4113) acc 93.7500 (92.8125) lr 1.0000e-03 eta 0:00:49
epoch [27/50] batch [5/10] time 0.141 (0.309) data 0.000 (0.168) loss 1.2393 (1.4635) acc 96.8750 (90.0000) lr 1.0000e-03 eta 0:01:12
epoch [27/50] batch [10/10] time 0.087 (0.220) data 0.000 (0.084) loss 1.5459 (1.4834) acc 81.2500 (89.6875) lr 9.3721e-04 eta 0:00:50
epoch [28/50] batch [5/10] time 0.141 (0.285) data 0.000 (0.142) loss 1.5469 (1.4238) acc 93.7500 (93.1250) lr 9.3721e-04 eta 0:01:04
epoch [28/50] batch [10/10] time 0.088 (0.208) data 0.000 (0.071) loss 1.4492 (1.4214) acc 87.5000 (92.1875) lr 8.7467e-04 eta 0:00:45
epoch [29/50] batch [5/10] time 0.143 (0.306) data 0.000 (0.163) loss 1.2910 (1.4465) acc 93.7500 (90.6250) lr 8.7467e-04 eta 0:01:05
epoch [29/50] batch [10/10] time 0.087 (0.219) data 0.000 (0.082) loss 1.7070 (1.4918) acc 81.2500 (90.0000) lr 8.1262e-04 eta 0:00:45
epoch [30/50] batch [5/10] time 0.142 (0.306) data 0.000 (0.163) loss 1.4678 (1.4385) acc 87.5000 (92.5000) lr 8.1262e-04 eta 0:01:02
epoch [30/50] batch [10/10] time 0.088 (0.218) data 0.000 (0.082) loss 1.3584 (1.4152) acc 87.5000 (91.8750) lr 7.5131e-04 eta 0:00:43
epoch [31/50] batch [5/10] time 0.142 (0.286) data 0.000 (0.144) loss 1.3262 (1.4604) acc 96.8750 (92.5000) lr 7.5131e-04 eta 0:00:55
epoch [31/50] batch [10/10] time 0.087 (0.209) data 0.000 (0.072) loss 1.3613 (1.4660) acc 93.7500 (92.5000) lr 6.9098e-04 eta 0:00:39
epoch [32/50] batch [5/10] time 0.143 (0.297) data 0.000 (0.153) loss 1.3438 (1.5195) acc 93.7500 (91.2500) lr 6.9098e-04 eta 0:00:54
epoch [32/50] batch [10/10] time 0.093 (0.215) data 0.000 (0.077) loss 0.9409 (1.3991) acc 100.0000 (93.1250) lr 6.3188e-04 eta 0:00:38
epoch [33/50] batch [5/10] time 0.142 (0.298) data 0.000 (0.154) loss 1.2480 (1.3834) acc 96.8750 (91.2500) lr 6.3188e-04 eta 0:00:52
epoch [33/50] batch [10/10] time 0.086 (0.214) data 0.000 (0.077) loss 1.1865 (1.4084) acc 100.0000 (91.8750) lr 5.7422e-04 eta 0:00:36
epoch [34/50] batch [5/10] time 0.143 (0.280) data 0.000 (0.138) loss 1.6914 (1.4127) acc 90.6250 (94.3750) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [10/10] time 0.087 (0.206) data 0.000 (0.069) loss 1.2930 (1.4604) acc 100.0000 (93.4375) lr 5.1825e-04 eta 0:00:32
epoch [35/50] batch [5/10] time 0.144 (0.290) data 0.000 (0.147) loss 1.1221 (1.3566) acc 100.0000 (93.1250) lr 5.1825e-04 eta 0:00:44
epoch [35/50] batch [10/10] time 0.089 (0.212) data 0.000 (0.074) loss 1.6592 (1.4071) acc 87.5000 (92.1875) lr 4.6417e-04 eta 0:00:31
epoch [36/50] batch [5/10] time 0.141 (0.295) data 0.000 (0.152) loss 1.2090 (1.4920) acc 96.8750 (91.2500) lr 4.6417e-04 eta 0:00:42
epoch [36/50] batch [10/10] time 0.081 (0.211) data 0.000 (0.076) loss 1.7305 (1.4077) acc 87.5000 (93.4375) lr 4.1221e-04 eta 0:00:29
epoch [37/50] batch [5/10] time 0.141 (0.290) data 0.000 (0.148) loss 1.5645 (1.4656) acc 90.6250 (90.6250) lr 4.1221e-04 eta 0:00:39
epoch [37/50] batch [10/10] time 0.087 (0.210) data 0.000 (0.074) loss 1.1172 (1.3456) acc 100.0000 (93.7500) lr 3.6258e-04 eta 0:00:27
epoch [38/50] batch [5/10] time 0.142 (0.282) data 0.000 (0.141) loss 1.6016 (1.4563) acc 84.3750 (91.2500) lr 3.6258e-04 eta 0:00:35
epoch [38/50] batch [10/10] time 0.087 (0.206) data 0.000 (0.070) loss 1.4980 (1.4580) acc 87.5000 (90.3125) lr 3.1545e-04 eta 0:00:24
epoch [39/50] batch [5/10] time 0.142 (0.274) data 0.000 (0.130) loss 1.4336 (1.3768) acc 90.6250 (91.2500) lr 3.1545e-04 eta 0:00:31
epoch [39/50] batch [10/10] time 0.088 (0.202) data 0.000 (0.065) loss 1.6543 (1.4311) acc 93.7500 (91.5625) lr 2.7103e-04 eta 0:00:22
epoch [40/50] batch [5/10] time 0.144 (0.270) data 0.000 (0.125) loss 1.6133 (1.3719) acc 84.3750 (92.5000) lr 2.7103e-04 eta 0:00:28
epoch [40/50] batch [10/10] time 0.088 (0.200) data 0.000 (0.063) loss 1.4844 (1.3975) acc 93.7500 (91.5625) lr 2.2949e-04 eta 0:00:20
epoch [41/50] batch [5/10] time 0.138 (0.274) data 0.000 (0.131) loss 1.4824 (1.4475) acc 87.5000 (91.2500) lr 2.2949e-04 eta 0:00:25
epoch [41/50] batch [10/10] time 0.087 (0.202) data 0.000 (0.066) loss 1.0859 (1.3975) acc 93.7500 (91.8750) lr 1.9098e-04 eta 0:00:18
epoch [42/50] batch [5/10] time 0.141 (0.272) data 0.000 (0.129) loss 1.4736 (1.3771) acc 87.5000 (91.2500) lr 1.9098e-04 eta 0:00:23
epoch [42/50] batch [10/10] time 0.086 (0.202) data 0.000 (0.065) loss 1.5244 (1.4008) acc 93.7500 (92.8125) lr 1.5567e-04 eta 0:00:16
epoch [43/50] batch [5/10] time 0.141 (0.304) data 0.000 (0.162) loss 1.4912 (1.4201) acc 90.6250 (92.5000) lr 1.5567e-04 eta 0:00:22
epoch [43/50] batch [10/10] time 0.086 (0.218) data 0.000 (0.081) loss 1.5371 (1.4023) acc 93.7500 (93.7500) lr 1.2369e-04 eta 0:00:15
epoch [44/50] batch [5/10] time 0.144 (0.299) data 0.000 (0.152) loss 1.2070 (1.3594) acc 93.7500 (92.5000) lr 1.2369e-04 eta 0:00:19
epoch [44/50] batch [10/10] time 0.088 (0.216) data 0.000 (0.076) loss 1.4512 (1.4124) acc 93.7500 (92.1875) lr 9.5173e-05 eta 0:00:12
epoch [45/50] batch [5/10] time 0.137 (0.273) data 0.000 (0.130) loss 1.0967 (1.2711) acc 96.8750 (93.7500) lr 9.5173e-05 eta 0:00:15
epoch [45/50] batch [10/10] time 0.087 (0.202) data 0.000 (0.065) loss 1.5244 (1.3774) acc 87.5000 (91.8750) lr 7.0224e-05 eta 0:00:10
epoch [46/50] batch [5/10] time 0.141 (0.289) data 0.000 (0.146) loss 1.1758 (1.4785) acc 96.8750 (90.6250) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [10/10] time 0.086 (0.210) data 0.000 (0.073) loss 1.4131 (1.4398) acc 93.7500 (91.8750) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [5/10] time 0.143 (0.278) data 0.000 (0.135) loss 1.1943 (1.3871) acc 96.8750 (91.2500) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [10/10] time 0.087 (0.205) data 0.000 (0.068) loss 1.8965 (1.3709) acc 75.0000 (91.5625) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [5/10] time 0.142 (0.281) data 0.000 (0.138) loss 1.3477 (1.3199) acc 93.7500 (95.6250) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [10/10] time 0.088 (0.206) data 0.000 (0.069) loss 1.6055 (1.3281) acc 81.2500 (93.4375) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [5/10] time 0.145 (0.275) data 0.000 (0.131) loss 1.3398 (1.4414) acc 93.7500 (91.8750) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [10/10] time 0.088 (0.203) data 0.000 (0.066) loss 1.5303 (1.4181) acc 87.5000 (91.2500) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/10] time 0.142 (0.310) data 0.000 (0.167) loss 1.2871 (1.3014) acc 93.7500 (95.0000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/10] time 0.087 (0.221) data 0.000 (0.084) loss 0.9482 (1.3275) acc 100.0000 (93.7500) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:15,  5.30s/it] 50%|█████     | 2/4 [00:06<00:05,  2.84s/it] 75%|███████▌  | 3/4 [00:07<00:02,  2.06s/it]100%|██████████| 4/4 [00:08<00:00,  1.61s/it]100%|██████████| 4/4 [00:08<00:00,  2.15s/it]
=> result
* total: 1,881
* correct: 1,782
* accuracy: 94.7%
* error: 5.3%
* macro_f1: 94.7%
Elapsed: 0:01:58
Run this job and save the output to output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  19
# train_x  304
# val      76
# test     1,881
---------  ----------
['abyssinian', 'american_bulldog', 'american_pit_bull_terrier', 'basset_hound', 'beagle', 'bengal', 'birman', 'bombay', 'boxer', 'british_shorthair', 'chihuahua', 'egyptian_mau', 'english_cocker_spaniel', 'english_setter', 'german_shorthaired', 'great_pyrenees', 'havanese', 'japanese_chin', 'keeshond']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X abyssinian, a type of pet.', 'X X X X american bulldog, a type of pet.', 'X X X X american pit bull terrier, a type of pet.', 'X X X X basset hound, a type of pet.', 'X X X X beagle, a type of pet.', 'X X X X bengal, a type of pet.', 'X X X X birman, a type of pet.', 'X X X X bombay, a type of pet.', 'X X X X boxer, a type of pet.', 'X X X X british shorthair, a type of pet.', 'X X X X chihuahua, a type of pet.', 'X X X X egyptian mau, a type of pet.', 'X X X X english cocker spaniel, a type of pet.', 'X X X X english setter, a type of pet.', 'X X X X german shorthaired, a type of pet.', 'X X X X great pyrenees, a type of pet.', 'X X X X havanese, a type of pet.', 'X X X X japanese chin, a type of pet.', 'X X X X keeshond, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([19, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/10] time 0.143 (0.315) data 0.000 (0.164) loss 4.1406 (4.0336) acc 75.0000 (80.6250) lr 1.0000e-05 eta 0:02:36
epoch [1/50] batch [10/10] time 0.088 (0.224) data 0.000 (0.082) loss 4.8516 (4.2145) acc 68.7500 (77.1875) lr 2.0000e-03 eta 0:01:49
epoch [2/50] batch [5/10] time 0.142 (0.275) data 0.000 (0.131) loss 3.2852 (3.5484) acc 75.0000 (77.5000) lr 2.0000e-03 eta 0:02:13
epoch [2/50] batch [10/10] time 0.089 (0.204) data 0.000 (0.066) loss 2.6133 (3.2051) acc 87.5000 (80.9375) lr 1.9980e-03 eta 0:01:37
epoch [3/50] batch [5/10] time 0.146 (0.282) data 0.000 (0.136) loss 2.7578 (2.6055) acc 71.8750 (81.8750) lr 1.9980e-03 eta 0:02:14
epoch [3/50] batch [10/10] time 0.088 (0.207) data 0.000 (0.068) loss 2.3789 (2.5305) acc 81.2500 (83.4375) lr 1.9921e-03 eta 0:01:37
epoch [4/50] batch [5/10] time 0.144 (0.291) data 0.000 (0.148) loss 2.0840 (2.1836) acc 84.3750 (88.7500) lr 1.9921e-03 eta 0:02:15
epoch [4/50] batch [10/10] time 0.089 (0.212) data 0.000 (0.074) loss 2.3750 (2.2137) acc 81.2500 (86.5625) lr 1.9823e-03 eta 0:01:37
epoch [5/50] batch [5/10] time 0.142 (0.288) data 0.000 (0.144) loss 2.2148 (2.1672) acc 81.2500 (83.1250) lr 1.9823e-03 eta 0:02:10
epoch [5/50] batch [10/10] time 0.088 (0.210) data 0.000 (0.072) loss 2.2734 (2.0866) acc 87.5000 (84.6875) lr 1.9686e-03 eta 0:01:34
epoch [6/50] batch [5/10] time 0.143 (0.284) data 0.000 (0.140) loss 1.9834 (2.0467) acc 90.6250 (86.2500) lr 1.9686e-03 eta 0:02:06
epoch [6/50] batch [10/10] time 0.089 (0.208) data 0.000 (0.070) loss 1.6689 (1.9419) acc 93.7500 (87.8125) lr 1.9511e-03 eta 0:01:31
epoch [7/50] batch [5/10] time 0.142 (0.289) data 0.000 (0.146) loss 1.8506 (1.8904) acc 93.7500 (86.8750) lr 1.9511e-03 eta 0:02:05
epoch [7/50] batch [10/10] time 0.088 (0.210) data 0.000 (0.073) loss 1.6396 (1.8748) acc 93.7500 (87.1875) lr 1.9298e-03 eta 0:01:30
epoch [8/50] batch [5/10] time 0.143 (0.306) data 0.000 (0.162) loss 2.0605 (2.0357) acc 84.3750 (81.8750) lr 1.9298e-03 eta 0:02:10
epoch [8/50] batch [10/10] time 0.088 (0.219) data 0.000 (0.081) loss 2.4805 (1.9745) acc 68.7500 (82.1875) lr 1.9048e-03 eta 0:01:32
epoch [9/50] batch [5/10] time 0.143 (0.302) data 0.000 (0.159) loss 1.4922 (1.8285) acc 90.6250 (83.7500) lr 1.9048e-03 eta 0:02:05
epoch [9/50] batch [10/10] time 0.088 (0.217) data 0.000 (0.079) loss 1.5156 (1.6915) acc 93.7500 (88.7500) lr 1.8763e-03 eta 0:01:28
epoch [10/50] batch [5/10] time 0.142 (0.285) data 0.000 (0.142) loss 1.4971 (1.6344) acc 90.6250 (89.3750) lr 1.8763e-03 eta 0:01:55
epoch [10/50] batch [10/10] time 0.087 (0.208) data 0.000 (0.071) loss 1.5049 (1.6788) acc 100.0000 (89.6875) lr 1.8443e-03 eta 0:01:23
epoch [11/50] batch [5/10] time 0.145 (0.290) data 0.000 (0.145) loss 1.8135 (1.7621) acc 84.3750 (84.3750) lr 1.8443e-03 eta 0:01:54
epoch [11/50] batch [10/10] time 0.097 (0.212) data 0.000 (0.073) loss 1.4219 (1.6940) acc 100.0000 (88.7500) lr 1.8090e-03 eta 0:01:22
epoch [12/50] batch [5/10] time 0.137 (0.279) data 0.000 (0.136) loss 1.7168 (1.7947) acc 81.2500 (85.0000) lr 1.8090e-03 eta 0:01:47
epoch [12/50] batch [10/10] time 0.089 (0.206) data 0.000 (0.068) loss 1.4941 (1.6966) acc 100.0000 (88.1250) lr 1.7705e-03 eta 0:01:18
epoch [13/50] batch [5/10] time 0.142 (0.309) data 0.000 (0.165) loss 1.7666 (1.7775) acc 81.2500 (83.7500) lr 1.7705e-03 eta 0:01:55
epoch [13/50] batch [10/10] time 0.088 (0.220) data 0.000 (0.082) loss 1.6484 (1.7063) acc 87.5000 (87.1875) lr 1.7290e-03 eta 0:01:21
epoch [14/50] batch [5/10] time 0.142 (0.299) data 0.000 (0.156) loss 1.3242 (1.5988) acc 96.8750 (87.5000) lr 1.7290e-03 eta 0:01:49
epoch [14/50] batch [10/10] time 0.089 (0.216) data 0.000 (0.078) loss 1.4062 (1.5813) acc 93.7500 (89.0625) lr 1.6845e-03 eta 0:01:17
epoch [15/50] batch [5/10] time 0.143 (0.300) data 0.000 (0.156) loss 1.4795 (1.5543) acc 90.6250 (92.5000) lr 1.6845e-03 eta 0:01:46
epoch [15/50] batch [10/10] time 0.089 (0.216) data 0.000 (0.078) loss 2.0488 (1.5845) acc 81.2500 (89.6875) lr 1.6374e-03 eta 0:01:15
epoch [16/50] batch [5/10] time 0.142 (0.307) data 0.000 (0.164) loss 1.3320 (1.5994) acc 96.8750 (91.8750) lr 1.6374e-03 eta 0:01:46
epoch [16/50] batch [10/10] time 0.088 (0.220) data 0.000 (0.082) loss 1.3184 (1.5198) acc 100.0000 (92.5000) lr 1.5878e-03 eta 0:01:14
epoch [17/50] batch [5/10] time 0.143 (0.288) data 0.000 (0.144) loss 1.4746 (1.5484) acc 90.6250 (90.6250) lr 1.5878e-03 eta 0:01:36
epoch [17/50] batch [10/10] time 0.089 (0.210) data 0.000 (0.072) loss 1.3955 (1.5139) acc 93.7500 (91.2500) lr 1.5358e-03 eta 0:01:09
epoch [18/50] batch [5/10] time 0.143 (0.280) data 0.000 (0.137) loss 1.5684 (1.5045) acc 93.7500 (92.5000) lr 1.5358e-03 eta 0:01:31
epoch [18/50] batch [10/10] time 0.088 (0.206) data 0.000 (0.069) loss 1.9258 (1.5433) acc 75.0000 (89.6875) lr 1.4818e-03 eta 0:01:05
epoch [19/50] batch [5/10] time 0.143 (0.285) data 0.000 (0.142) loss 1.9023 (1.5428) acc 87.5000 (90.0000) lr 1.4818e-03 eta 0:01:29
epoch [19/50] batch [10/10] time 0.088 (0.208) data 0.000 (0.071) loss 1.4727 (1.5558) acc 87.5000 (89.0625) lr 1.4258e-03 eta 0:01:04
epoch [20/50] batch [5/10] time 0.145 (0.277) data 0.000 (0.133) loss 1.6650 (1.4605) acc 87.5000 (90.0000) lr 1.4258e-03 eta 0:01:24
epoch [20/50] batch [10/10] time 0.087 (0.205) data 0.000 (0.066) loss 2.0820 (1.5387) acc 81.2500 (90.6250) lr 1.3681e-03 eta 0:01:01
epoch [21/50] batch [5/10] time 0.142 (0.281) data 0.000 (0.137) loss 1.4922 (1.6113) acc 90.6250 (88.7500) lr 1.3681e-03 eta 0:01:22
epoch [21/50] batch [10/10] time 0.087 (0.206) data 0.000 (0.069) loss 1.3271 (1.5673) acc 93.7500 (88.1250) lr 1.3090e-03 eta 0:00:59
epoch [22/50] batch [5/10] time 0.143 (0.280) data 0.000 (0.136) loss 1.3369 (1.4877) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:01:19
epoch [22/50] batch [10/10] time 0.088 (0.206) data 0.000 (0.068) loss 1.6221 (1.5752) acc 93.7500 (88.4375) lr 1.2487e-03 eta 0:00:57
epoch [23/50] batch [5/10] time 0.142 (0.277) data 0.000 (0.133) loss 1.4326 (1.4158) acc 90.6250 (92.5000) lr 1.2487e-03 eta 0:01:16
epoch [23/50] batch [10/10] time 0.088 (0.204) data 0.000 (0.067) loss 0.9580 (1.3946) acc 100.0000 (92.5000) lr 1.1874e-03 eta 0:00:55
epoch [24/50] batch [5/10] time 0.145 (0.285) data 0.000 (0.141) loss 1.3770 (1.4426) acc 93.7500 (92.5000) lr 1.1874e-03 eta 0:01:15
epoch [24/50] batch [10/10] time 0.089 (0.209) data 0.000 (0.071) loss 1.4824 (1.5076) acc 87.5000 (90.0000) lr 1.1253e-03 eta 0:00:54
epoch [25/50] batch [5/10] time 0.146 (0.280) data 0.000 (0.135) loss 1.3184 (1.3686) acc 93.7500 (90.6250) lr 1.1253e-03 eta 0:01:11
epoch [25/50] batch [10/10] time 0.088 (0.206) data 0.000 (0.067) loss 1.5088 (1.4168) acc 100.0000 (91.5625) lr 1.0628e-03 eta 0:00:51
epoch [26/50] batch [5/10] time 0.142 (0.283) data 0.000 (0.139) loss 1.3281 (1.4576) acc 90.6250 (89.3750) lr 1.0628e-03 eta 0:01:09
epoch [26/50] batch [10/10] time 0.088 (0.207) data 0.000 (0.069) loss 1.6836 (1.5063) acc 87.5000 (89.0625) lr 1.0000e-03 eta 0:00:49
epoch [27/50] batch [5/10] time 0.142 (0.271) data 0.000 (0.128) loss 1.0469 (1.4742) acc 100.0000 (93.7500) lr 1.0000e-03 eta 0:01:03
epoch [27/50] batch [10/10] time 0.088 (0.202) data 0.000 (0.064) loss 1.4023 (1.4859) acc 93.7500 (91.8750) lr 9.3721e-04 eta 0:00:46
epoch [28/50] batch [5/10] time 0.142 (0.289) data 0.000 (0.146) loss 1.5283 (1.4836) acc 84.3750 (92.5000) lr 9.3721e-04 eta 0:01:05
epoch [28/50] batch [10/10] time 0.088 (0.211) data 0.000 (0.073) loss 1.3203 (1.4626) acc 93.7500 (90.9375) lr 8.7467e-04 eta 0:00:46
epoch [29/50] batch [5/10] time 0.143 (0.280) data 0.000 (0.137) loss 1.5645 (1.3551) acc 81.2500 (91.2500) lr 8.7467e-04 eta 0:01:00
epoch [29/50] batch [10/10] time 0.088 (0.206) data 0.000 (0.069) loss 1.5352 (1.4570) acc 93.7500 (89.0625) lr 8.1262e-04 eta 0:00:43
epoch [30/50] batch [5/10] time 0.142 (0.282) data 0.000 (0.139) loss 1.1885 (1.3027) acc 96.8750 (95.0000) lr 8.1262e-04 eta 0:00:57
epoch [30/50] batch [10/10] time 0.088 (0.207) data 0.000 (0.069) loss 1.3506 (1.4030) acc 87.5000 (91.5625) lr 7.5131e-04 eta 0:00:41
epoch [31/50] batch [5/10] time 0.142 (0.273) data 0.000 (0.130) loss 1.0908 (1.4607) acc 96.8750 (89.3750) lr 7.5131e-04 eta 0:00:53
epoch [31/50] batch [10/10] time 0.088 (0.203) data 0.000 (0.065) loss 1.7852 (1.5012) acc 87.5000 (90.3125) lr 6.9098e-04 eta 0:00:38
epoch [32/50] batch [5/10] time 0.142 (0.282) data 0.000 (0.139) loss 1.5205 (1.4584) acc 87.5000 (90.6250) lr 6.9098e-04 eta 0:00:52
epoch [32/50] batch [10/10] time 0.088 (0.207) data 0.000 (0.070) loss 1.4893 (1.4100) acc 93.7500 (91.8750) lr 6.3188e-04 eta 0:00:37
epoch [33/50] batch [5/10] time 0.142 (0.276) data 0.000 (0.132) loss 1.1348 (1.4604) acc 96.8750 (90.0000) lr 6.3188e-04 eta 0:00:48
epoch [33/50] batch [10/10] time 0.088 (0.204) data 0.000 (0.066) loss 1.4844 (1.4937) acc 87.5000 (88.1250) lr 5.7422e-04 eta 0:00:34
epoch [34/50] batch [5/10] time 0.143 (0.282) data 0.000 (0.138) loss 1.4912 (1.3689) acc 93.7500 (95.0000) lr 5.7422e-04 eta 0:00:46
epoch [34/50] batch [10/10] time 0.087 (0.207) data 0.000 (0.069) loss 1.9688 (1.5503) acc 75.0000 (88.1250) lr 5.1825e-04 eta 0:00:33
epoch [35/50] batch [5/10] time 0.142 (0.275) data 0.000 (0.131) loss 1.4668 (1.5945) acc 87.5000 (85.6250) lr 5.1825e-04 eta 0:00:42
epoch [35/50] batch [10/10] time 0.089 (0.203) data 0.000 (0.065) loss 1.3301 (1.4975) acc 93.7500 (89.3750) lr 4.6417e-04 eta 0:00:30
epoch [36/50] batch [5/10] time 0.142 (0.283) data 0.000 (0.140) loss 1.3242 (1.3158) acc 87.5000 (91.8750) lr 4.6417e-04 eta 0:00:41
epoch [36/50] batch [10/10] time 0.087 (0.207) data 0.000 (0.070) loss 1.2910 (1.3542) acc 93.7500 (91.2500) lr 4.1221e-04 eta 0:00:28
epoch [37/50] batch [5/10] time 0.142 (0.298) data 0.000 (0.153) loss 1.3789 (1.3504) acc 93.7500 (92.5000) lr 4.1221e-04 eta 0:00:40
epoch [37/50] batch [10/10] time 0.089 (0.215) data 0.000 (0.077) loss 1.7354 (1.3535) acc 87.5000 (92.8125) lr 3.6258e-04 eta 0:00:27
epoch [38/50] batch [5/10] time 0.142 (0.298) data 0.000 (0.154) loss 1.4297 (1.5389) acc 93.7500 (88.1250) lr 3.6258e-04 eta 0:00:37
epoch [38/50] batch [10/10] time 0.088 (0.215) data 0.000 (0.077) loss 1.6777 (1.4894) acc 87.5000 (90.0000) lr 3.1545e-04 eta 0:00:25
epoch [39/50] batch [5/10] time 0.142 (0.283) data 0.000 (0.140) loss 1.7119 (1.4865) acc 87.5000 (89.3750) lr 3.1545e-04 eta 0:00:32
epoch [39/50] batch [10/10] time 0.089 (0.213) data 0.000 (0.070) loss 1.1992 (1.3930) acc 93.7500 (90.9375) lr 2.7103e-04 eta 0:00:23
epoch [40/50] batch [5/10] time 0.145 (0.311) data 0.000 (0.168) loss 1.4658 (1.4600) acc 90.6250 (88.7500) lr 2.7103e-04 eta 0:00:32
epoch [40/50] batch [10/10] time 0.088 (0.222) data 0.000 (0.084) loss 1.4434 (1.4929) acc 87.5000 (88.4375) lr 2.2949e-04 eta 0:00:22
epoch [41/50] batch [5/10] time 0.143 (0.303) data 0.000 (0.160) loss 1.3389 (1.2646) acc 96.8750 (95.6250) lr 2.2949e-04 eta 0:00:28
epoch [41/50] batch [10/10] time 0.087 (0.218) data 0.000 (0.080) loss 1.3047 (1.2970) acc 100.0000 (95.3125) lr 1.9098e-04 eta 0:00:19
epoch [42/50] batch [5/10] time 0.145 (0.290) data 0.000 (0.146) loss 1.3223 (1.4713) acc 90.6250 (91.2500) lr 1.9098e-04 eta 0:00:24
epoch [42/50] batch [10/10] time 0.088 (0.211) data 0.000 (0.073) loss 1.5332 (1.5437) acc 87.5000 (90.3125) lr 1.5567e-04 eta 0:00:16
epoch [43/50] batch [5/10] time 0.144 (0.284) data 0.000 (0.140) loss 1.6016 (1.4791) acc 84.3750 (90.6250) lr 1.5567e-04 eta 0:00:21
epoch [43/50] batch [10/10] time 0.089 (0.208) data 0.000 (0.070) loss 1.3555 (1.4551) acc 93.7500 (91.5625) lr 1.2369e-04 eta 0:00:14
epoch [44/50] batch [5/10] time 0.143 (0.285) data 0.000 (0.141) loss 1.4092 (1.4076) acc 90.6250 (91.2500) lr 1.2369e-04 eta 0:00:18
epoch [44/50] batch [10/10] time 0.089 (0.209) data 0.000 (0.071) loss 1.6396 (1.4322) acc 81.2500 (91.5625) lr 9.5173e-05 eta 0:00:12
epoch [45/50] batch [5/10] time 0.144 (0.305) data 0.000 (0.161) loss 1.2520 (1.5088) acc 93.7500 (90.0000) lr 9.5173e-05 eta 0:00:16
epoch [45/50] batch [10/10] time 0.093 (0.220) data 0.000 (0.081) loss 1.1084 (1.4489) acc 93.7500 (90.0000) lr 7.0224e-05 eta 0:00:11
epoch [46/50] batch [5/10] time 0.144 (0.286) data 0.000 (0.141) loss 1.4863 (1.4906) acc 87.5000 (90.6250) lr 7.0224e-05 eta 0:00:12
epoch [46/50] batch [10/10] time 0.089 (0.210) data 0.000 (0.071) loss 1.2295 (1.4453) acc 93.7500 (91.8750) lr 4.8943e-05 eta 0:00:08
epoch [47/50] batch [5/10] time 0.144 (0.280) data 0.000 (0.136) loss 1.4219 (1.3314) acc 93.7500 (90.6250) lr 4.8943e-05 eta 0:00:09
epoch [47/50] batch [10/10] time 0.089 (0.207) data 0.000 (0.068) loss 1.6885 (1.3937) acc 93.7500 (92.8125) lr 3.1417e-05 eta 0:00:06
epoch [48/50] batch [5/10] time 0.146 (0.303) data 0.000 (0.158) loss 1.5010 (1.4969) acc 84.3750 (90.0000) lr 3.1417e-05 eta 0:00:07
epoch [48/50] batch [10/10] time 0.090 (0.218) data 0.000 (0.079) loss 0.8970 (1.3894) acc 100.0000 (92.1875) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [5/10] time 0.142 (0.303) data 0.000 (0.159) loss 1.4355 (1.4197) acc 90.6250 (90.6250) lr 1.7713e-05 eta 0:00:04
epoch [49/50] batch [10/10] time 0.089 (0.218) data 0.000 (0.080) loss 1.0137 (1.3843) acc 100.0000 (92.5000) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [5/10] time 0.143 (0.285) data 0.000 (0.141) loss 2.1133 (1.7027) acc 78.1250 (89.3750) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [10/10] time 0.087 (0.209) data 0.000 (0.071) loss 1.6191 (1.5870) acc 81.2500 (89.0625) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:16,  5.36s/it] 50%|█████     | 2/4 [00:06<00:05,  2.87s/it] 75%|███████▌  | 3/4 [00:07<00:02,  2.07s/it]100%|██████████| 4/4 [00:08<00:00,  1.62s/it]100%|██████████| 4/4 [00:08<00:00,  2.17s/it]
=> result
* total: 1,881
* correct: 1,795
* accuracy: 95.4%
* error: 4.6%
* macro_f1: 95.4%
Elapsed: 0:02:00
Run this job and save the output to output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  18
# train_x  288
# val      72
# test     1,788
---------  ----------
['leonberger', 'maine_coon', 'miniature_pinscher', 'newfoundland', 'persian', 'pomeranian', 'pug', 'ragdoll', 'russian_blue', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'siamese', 'sphynx', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X leonberger, a type of pet.', 'X X X X maine coon, a type of pet.', 'X X X X miniature pinscher, a type of pet.', 'X X X X newfoundland, a type of pet.', 'X X X X persian, a type of pet.', 'X X X X pomeranian, a type of pet.', 'X X X X pug, a type of pet.', 'X X X X ragdoll, a type of pet.', 'X X X X russian blue, a type of pet.', 'X X X X saint bernard, a type of pet.', 'X X X X samoyed, a type of pet.', 'X X X X scottish terrier, a type of pet.', 'X X X X shiba inu, a type of pet.', 'X X X X siamese, a type of pet.', 'X X X X sphynx, a type of pet.', 'X X X X staffordshire bull terrier, a type of pet.', 'X X X X wheaten terrier, a type of pet.', 'X X X X yorkshire terrier, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([18, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:15,  5.25s/it] 50%|█████     | 2/4 [00:06<00:05,  2.82s/it] 75%|███████▌  | 3/4 [00:07<00:02,  2.05s/it]100%|██████████| 4/4 [00:08<00:00,  1.52s/it]100%|██████████| 4/4 [00:08<00:00,  2.09s/it]
=> result
* total: 1,788
* correct: 1,744
* accuracy: 97.5%
* error: 2.5%
* macro_f1: 97.6%
Run this job and save the output to output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  18
# train_x  288
# val      72
# test     1,788
---------  ----------
['leonberger', 'maine_coon', 'miniature_pinscher', 'newfoundland', 'persian', 'pomeranian', 'pug', 'ragdoll', 'russian_blue', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'siamese', 'sphynx', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X leonberger, a type of pet.', 'X X X X maine coon, a type of pet.', 'X X X X miniature pinscher, a type of pet.', 'X X X X newfoundland, a type of pet.', 'X X X X persian, a type of pet.', 'X X X X pomeranian, a type of pet.', 'X X X X pug, a type of pet.', 'X X X X ragdoll, a type of pet.', 'X X X X russian blue, a type of pet.', 'X X X X saint bernard, a type of pet.', 'X X X X samoyed, a type of pet.', 'X X X X scottish terrier, a type of pet.', 'X X X X shiba inu, a type of pet.', 'X X X X siamese, a type of pet.', 'X X X X sphynx, a type of pet.', 'X X X X staffordshire bull terrier, a type of pet.', 'X X X X wheaten terrier, a type of pet.', 'X X X X yorkshire terrier, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([18, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:11<00:33, 11.23s/it] 50%|█████     | 2/4 [00:12<00:10,  5.28s/it] 75%|███████▌  | 3/4 [00:13<00:03,  3.38s/it]100%|██████████| 4/4 [00:14<00:00,  2.32s/it]100%|██████████| 4/4 [00:14<00:00,  3.57s/it]
=> result
* total: 1,788
* correct: 1,739
* accuracy: 97.3%
* error: 2.7%
* macro_f1: 97.3%
Run this job and save the output to output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/oxford_pets.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: OxfordPets
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: OxfordPets
Reading split from /data/yht/data/cl/data/oxford_pets/split_zhou_OxfordPets.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/oxford_pets/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    OxfordPets
# classes  18
# train_x  288
# val      72
# test     1,788
---------  ----------
['leonberger', 'maine_coon', 'miniature_pinscher', 'newfoundland', 'persian', 'pomeranian', 'pug', 'ragdoll', 'russian_blue', 'saint_bernard', 'samoyed', 'scottish_terrier', 'shiba_inu', 'siamese', 'sphynx', 'staffordshire_bull_terrier', 'wheaten_terrier', 'yorkshire_terrier']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X leonberger, a type of pet.', 'X X X X maine coon, a type of pet.', 'X X X X miniature pinscher, a type of pet.', 'X X X X newfoundland, a type of pet.', 'X X X X persian, a type of pet.', 'X X X X pomeranian, a type of pet.', 'X X X X pug, a type of pet.', 'X X X X ragdoll, a type of pet.', 'X X X X russian blue, a type of pet.', 'X X X X saint bernard, a type of pet.', 'X X X X samoyed, a type of pet.', 'X X X X scottish terrier, a type of pet.', 'X X X X shiba inu, a type of pet.', 'X X X X siamese, a type of pet.', 'X X X X sphynx, a type of pet.', 'X X X X staffordshire bull terrier, a type of pet.', 'X X X X wheaten terrier, a type of pet.', 'X X X X yorkshire terrier, a type of pet.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([18, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/oxford_pets/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:05<00:15,  5.29s/it] 50%|█████     | 2/4 [00:06<00:05,  2.83s/it] 75%|███████▌  | 3/4 [00:07<00:02,  2.05s/it]100%|██████████| 4/4 [00:08<00:00,  1.52s/it]100%|██████████| 4/4 [00:08<00:00,  2.08s/it]
=> result
* total: 1,788
* correct: 1,747
* accuracy: 97.7%
* error: 2.3%
* macro_f1: 97.7%
Run this job and save the output to output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,002
---------  ------------
['2000 AM General Hummer SUV', '2012 Acura RL Sedan', '2012 Acura TL Sedan', '2008 Acura TL Type-S', '2012 Acura TSX Sedan', '2001 Acura Integra Type R', '2012 Acura ZDX Hatchback', '2012 Aston Martin V8 Vantage Convertible', '2012 Aston Martin V8 Vantage Coupe', '2012 Aston Martin Virage Convertible', '2012 Aston Martin Virage Coupe', '2008 Audi RS 4 Convertible', '2012 Audi A5 Coupe', '2012 Audi TTS Coupe', '2012 Audi R8 Coupe', '1994 Audi V8 Sedan', '1994 Audi 100 Sedan', '1994 Audi 100 Wagon', '2011 Audi TT Hatchback', '2011 Audi S6 Sedan', '2012 Audi S5 Convertible', '2012 Audi S5 Coupe', '2012 Audi S4 Sedan', '2007 Audi S4 Sedan', '2012 Audi TT RS Coupe', '2012 BMW ActiveHybrid 5 Sedan', '2012 BMW 1 Series Convertible', '2012 BMW 1 Series Coupe', '2012 BMW 3 Series Sedan', '2012 BMW 3 Series Wagon', '2007 BMW 6 Series Convertible', '2007 BMW X5 SUV', '2012 BMW X6 SUV', '2012 BMW M3 Coupe', '2010 BMW M5 Sedan', '2010 BMW M6 Convertible', '2012 BMW X3 SUV', '2012 BMW Z4 Convertible', '2012 Bentley Continental Supersports Conv. Convertible', '2009 Bentley Arnage Sedan', '2011 Bentley Mulsanne Sedan', '2012 Bentley Continental GT Coupe', '2007 Bentley Continental GT Coupe', '2007 Bentley Continental Flying Spur Sedan', '2009 Bugatti Veyron 16.4 Convertible', '2009 Bugatti Veyron 16.4 Coupe', '2012 Buick Regal GS', '2007 Buick Rainier SUV', '2012 Buick Verano Sedan', '2012 Buick Enclave SUV', '2012 Cadillac CTS-V Sedan', '2012 Cadillac SRX SUV', '2007 Cadillac Escalade EXT Crew Cab', '2012 Chevrolet Silverado 1500 Hybrid Crew Cab', '2012 Chevrolet Corvette Convertible', '2012 Chevrolet Corvette ZR1', '2007 Chevrolet Corvette Ron Fellows Edition Z06', '2012 Chevrolet Traverse SUV', '2012 Chevrolet Camaro Convertible', '2010 Chevrolet HHR SS', '2007 Chevrolet Impala Sedan', '2012 Chevrolet Tahoe Hybrid SUV', '2012 Chevrolet Sonic Sedan', '2007 Chevrolet Express Cargo Van', '2012 Chevrolet Avalanche Crew Cab', '2010 Chevrolet Cobalt SS', '2010 Chevrolet Malibu Hybrid Sedan', '2009 Chevrolet TrailBlazer SS', '2012 Chevrolet Silverado 2500HD Regular Cab', '2007 Chevrolet Silverado 1500 Classic Extended Cab', '2007 Chevrolet Express Van', '2007 Chevrolet Monte Carlo Coupe', '2007 Chevrolet Malibu Sedan', '2012 Chevrolet Silverado 1500 Extended Cab', '2012 Chevrolet Silverado 1500 Regular Cab', '2009 Chrysler Aspen SUV', '2010 Chrysler Sebring Convertible', '2012 Chrysler Town and Country Minivan', '2010 Chrysler 300 SRT-8', '2008 Chrysler Crossfire Convertible', '2008 Chrysler PT Cruiser Convertible', '2002 Daewoo Nubira Wagon', '2012 Dodge Caliber Wagon', '2007 Dodge Caliber Wagon', '1997 Dodge Caravan Minivan', '2010 Dodge Ram Pickup 3500 Crew Cab', '2009 Dodge Ram Pickup 3500 Quad Cab', '2009 Dodge Sprinter Cargo Van', '2012 Dodge Journey SUV', '2010 Dodge Dakota Crew Cab', '2007 Dodge Dakota Club Cab', '2008 Dodge Magnum Wagon', '2011 Dodge Challenger SRT8', '2012 Dodge Durango SUV', '2007 Dodge Durango SUV', '2012 Dodge Charger Sedan', '2009 Dodge Charger SRT-8', '1998 Eagle Talon Hatchback']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2000 AM General Hummer SUV, a type of car', 'X X X X 2012 Acura RL Sedan, a type of car', 'X X X X 2012 Acura TL Sedan, a type of car', 'X X X X 2008 Acura TL Type-S, a type of car', 'X X X X 2012 Acura TSX Sedan, a type of car', 'X X X X 2001 Acura Integra Type R, a type of car', 'X X X X 2012 Acura ZDX Hatchback, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Convertible, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Coupe, a type of car', 'X X X X 2012 Aston Martin Virage Convertible, a type of car', 'X X X X 2012 Aston Martin Virage Coupe, a type of car', 'X X X X 2008 Audi RS 4 Convertible, a type of car', 'X X X X 2012 Audi A5 Coupe, a type of car', 'X X X X 2012 Audi TTS Coupe, a type of car', 'X X X X 2012 Audi R8 Coupe, a type of car', 'X X X X 1994 Audi V8 Sedan, a type of car', 'X X X X 1994 Audi 100 Sedan, a type of car', 'X X X X 1994 Audi 100 Wagon, a type of car', 'X X X X 2011 Audi TT Hatchback, a type of car', 'X X X X 2011 Audi S6 Sedan, a type of car', 'X X X X 2012 Audi S5 Convertible, a type of car', 'X X X X 2012 Audi S5 Coupe, a type of car', 'X X X X 2012 Audi S4 Sedan, a type of car', 'X X X X 2007 Audi S4 Sedan, a type of car', 'X X X X 2012 Audi TT RS Coupe, a type of car', 'X X X X 2012 BMW ActiveHybrid 5 Sedan, a type of car', 'X X X X 2012 BMW 1 Series Convertible, a type of car', 'X X X X 2012 BMW 1 Series Coupe, a type of car', 'X X X X 2012 BMW 3 Series Sedan, a type of car', 'X X X X 2012 BMW 3 Series Wagon, a type of car', 'X X X X 2007 BMW 6 Series Convertible, a type of car', 'X X X X 2007 BMW X5 SUV, a type of car', 'X X X X 2012 BMW X6 SUV, a type of car', 'X X X X 2012 BMW M3 Coupe, a type of car', 'X X X X 2010 BMW M5 Sedan, a type of car', 'X X X X 2010 BMW M6 Convertible, a type of car', 'X X X X 2012 BMW X3 SUV, a type of car', 'X X X X 2012 BMW Z4 Convertible, a type of car', 'X X X X 2012 Bentley Continental Supersports Conv. Convertible, a type of car', 'X X X X 2009 Bentley Arnage Sedan, a type of car', 'X X X X 2011 Bentley Mulsanne Sedan, a type of car', 'X X X X 2012 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental Flying Spur Sedan, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Convertible, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Coupe, a type of car', 'X X X X 2012 Buick Regal GS, a type of car', 'X X X X 2007 Buick Rainier SUV, a type of car', 'X X X X 2012 Buick Verano Sedan, a type of car', 'X X X X 2012 Buick Enclave SUV, a type of car', 'X X X X 2012 Cadillac CTS-V Sedan, a type of car', 'X X X X 2012 Cadillac SRX SUV, a type of car', 'X X X X 2007 Cadillac Escalade EXT Crew Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Hybrid Crew Cab, a type of car', 'X X X X 2012 Chevrolet Corvette Convertible, a type of car', 'X X X X 2012 Chevrolet Corvette ZR1, a type of car', 'X X X X 2007 Chevrolet Corvette Ron Fellows Edition Z06, a type of car', 'X X X X 2012 Chevrolet Traverse SUV, a type of car', 'X X X X 2012 Chevrolet Camaro Convertible, a type of car', 'X X X X 2010 Chevrolet HHR SS, a type of car', 'X X X X 2007 Chevrolet Impala Sedan, a type of car', 'X X X X 2012 Chevrolet Tahoe Hybrid SUV, a type of car', 'X X X X 2012 Chevrolet Sonic Sedan, a type of car', 'X X X X 2007 Chevrolet Express Cargo Van, a type of car', 'X X X X 2012 Chevrolet Avalanche Crew Cab, a type of car', 'X X X X 2010 Chevrolet Cobalt SS, a type of car', 'X X X X 2010 Chevrolet Malibu Hybrid Sedan, a type of car', 'X X X X 2009 Chevrolet TrailBlazer SS, a type of car', 'X X X X 2012 Chevrolet Silverado 2500HD Regular Cab, a type of car', 'X X X X 2007 Chevrolet Silverado 1500 Classic Extended Cab, a type of car', 'X X X X 2007 Chevrolet Express Van, a type of car', 'X X X X 2007 Chevrolet Monte Carlo Coupe, a type of car', 'X X X X 2007 Chevrolet Malibu Sedan, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Extended Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Regular Cab, a type of car', 'X X X X 2009 Chrysler Aspen SUV, a type of car', 'X X X X 2010 Chrysler Sebring Convertible, a type of car', 'X X X X 2012 Chrysler Town and Country Minivan, a type of car', 'X X X X 2010 Chrysler 300 SRT-8, a type of car', 'X X X X 2008 Chrysler Crossfire Convertible, a type of car', 'X X X X 2008 Chrysler PT Cruiser Convertible, a type of car', 'X X X X 2002 Daewoo Nubira Wagon, a type of car', 'X X X X 2012 Dodge Caliber Wagon, a type of car', 'X X X X 2007 Dodge Caliber Wagon, a type of car', 'X X X X 1997 Dodge Caravan Minivan, a type of car', 'X X X X 2010 Dodge Ram Pickup 3500 Crew Cab, a type of car', 'X X X X 2009 Dodge Ram Pickup 3500 Quad Cab, a type of car', 'X X X X 2009 Dodge Sprinter Cargo Van, a type of car', 'X X X X 2012 Dodge Journey SUV, a type of car', 'X X X X 2010 Dodge Dakota Crew Cab, a type of car', 'X X X X 2007 Dodge Dakota Club Cab, a type of car', 'X X X X 2008 Dodge Magnum Wagon, a type of car', 'X X X X 2011 Dodge Challenger SRT8, a type of car', 'X X X X 2012 Dodge Durango SUV, a type of car', 'X X X X 2007 Dodge Durango SUV, a type of car', 'X X X X 2012 Dodge Charger Sedan, a type of car', 'X X X X 2009 Dodge Charger SRT-8, a type of car', 'X X X X 1998 Eagle Talon Hatchback, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([98, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/49] time 0.175 (0.340) data 0.000 (0.157) loss 7.0391 (7.1242) acc 43.7500 (38.1250) lr 1.0000e-05 eta 0:13:51
epoch [1/50] batch [10/49] time 0.176 (0.258) data 0.000 (0.079) loss 7.2695 (7.1020) acc 43.7500 (40.9375) lr 1.0000e-05 eta 0:10:29
epoch [1/50] batch [15/49] time 0.186 (0.231) data 0.000 (0.053) loss 6.8984 (7.0143) acc 56.2500 (43.9583) lr 1.0000e-05 eta 0:09:22
epoch [1/50] batch [20/49] time 0.175 (0.217) data 0.000 (0.040) loss 6.6367 (6.9770) acc 46.8750 (44.5312) lr 1.0000e-05 eta 0:08:48
epoch [1/50] batch [25/49] time 0.176 (0.209) data 0.000 (0.032) loss 7.2930 (6.9778) acc 34.3750 (43.6250) lr 1.0000e-05 eta 0:08:26
epoch [1/50] batch [30/49] time 0.175 (0.203) data 0.000 (0.026) loss 6.6016 (6.9039) acc 46.8750 (44.8958) lr 1.0000e-05 eta 0:08:12
epoch [1/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.023) loss 7.0352 (6.8641) acc 28.1250 (45.0000) lr 1.0000e-05 eta 0:08:01
epoch [1/50] batch [40/49] time 0.174 (0.196) data 0.000 (0.020) loss 7.0703 (6.8240) acc 31.2500 (45.7812) lr 1.0000e-05 eta 0:07:53
epoch [1/50] batch [45/49] time 0.174 (0.194) data 0.000 (0.018) loss 6.4453 (6.7940) acc 53.1250 (46.1111) lr 1.0000e-05 eta 0:07:46
epoch [2/50] batch [5/49] time 0.175 (0.337) data 0.000 (0.162) loss 5.7188 (5.8906) acc 53.1250 (53.1250) lr 2.0000e-03 eta 0:13:27
epoch [2/50] batch [10/49] time 0.176 (0.256) data 0.000 (0.081) loss 5.1250 (5.5754) acc 56.2500 (53.1250) lr 2.0000e-03 eta 0:10:12
epoch [2/50] batch [15/49] time 0.175 (0.229) data 0.000 (0.054) loss 5.5703 (5.4419) acc 37.5000 (52.7083) lr 2.0000e-03 eta 0:09:07
epoch [2/50] batch [20/49] time 0.176 (0.216) data 0.000 (0.041) loss 4.9883 (5.3227) acc 56.2500 (52.5000) lr 2.0000e-03 eta 0:08:34
epoch [2/50] batch [25/49] time 0.176 (0.208) data 0.000 (0.033) loss 5.3047 (5.2492) acc 37.5000 (51.6250) lr 2.0000e-03 eta 0:08:13
epoch [2/50] batch [30/49] time 0.176 (0.203) data 0.000 (0.027) loss 4.6055 (5.1392) acc 56.2500 (52.6042) lr 2.0000e-03 eta 0:08:00
epoch [2/50] batch [35/49] time 0.175 (0.199) data 0.000 (0.023) loss 4.3945 (5.0911) acc 50.0000 (52.0536) lr 2.0000e-03 eta 0:07:49
epoch [2/50] batch [40/49] time 0.176 (0.196) data 0.000 (0.020) loss 4.7148 (5.0526) acc 53.1250 (52.0312) lr 2.0000e-03 eta 0:07:42
epoch [2/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.018) loss 4.8086 (5.0080) acc 50.0000 (51.8750) lr 2.0000e-03 eta 0:07:35
epoch [3/50] batch [5/49] time 0.177 (0.329) data 0.000 (0.152) loss 4.7422 (4.5695) acc 37.5000 (49.3750) lr 1.9980e-03 eta 0:12:52
epoch [3/50] batch [10/49] time 0.187 (0.254) data 0.000 (0.076) loss 4.6055 (4.4986) acc 37.5000 (51.2500) lr 1.9980e-03 eta 0:09:54
epoch [3/50] batch [15/49] time 0.177 (0.228) data 0.000 (0.051) loss 4.2461 (4.4040) acc 43.7500 (52.2917) lr 1.9980e-03 eta 0:08:52
epoch [3/50] batch [20/49] time 0.175 (0.215) data 0.000 (0.038) loss 4.1758 (4.3188) acc 53.1250 (52.8125) lr 1.9980e-03 eta 0:08:21
epoch [3/50] batch [25/49] time 0.179 (0.207) data 0.000 (0.031) loss 3.9492 (4.2837) acc 53.1250 (53.2500) lr 1.9980e-03 eta 0:08:02
epoch [3/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.026) loss 4.1797 (4.2958) acc 59.3750 (53.6458) lr 1.9980e-03 eta 0:07:49
epoch [3/50] batch [35/49] time 0.175 (0.198) data 0.000 (0.022) loss 4.0820 (4.2815) acc 53.1250 (53.1250) lr 1.9980e-03 eta 0:07:39
epoch [3/50] batch [40/49] time 0.175 (0.195) data 0.000 (0.019) loss 4.9609 (4.2907) acc 34.3750 (52.8906) lr 1.9980e-03 eta 0:07:31
epoch [3/50] batch [45/49] time 0.177 (0.193) data 0.000 (0.017) loss 4.6211 (4.2981) acc 43.7500 (52.6389) lr 1.9980e-03 eta 0:07:25
epoch [4/50] batch [5/49] time 0.176 (0.344) data 0.000 (0.168) loss 4.2148 (4.1984) acc 53.1250 (60.0000) lr 1.9921e-03 eta 0:13:11
epoch [4/50] batch [10/49] time 0.177 (0.261) data 0.000 (0.084) loss 4.1133 (4.1367) acc 53.1250 (57.5000) lr 1.9921e-03 eta 0:09:58
epoch [4/50] batch [15/49] time 0.176 (0.233) data 0.000 (0.056) loss 4.0000 (4.1380) acc 62.5000 (56.6667) lr 1.9921e-03 eta 0:08:52
epoch [4/50] batch [20/49] time 0.176 (0.219) data 0.000 (0.042) loss 3.8203 (4.0771) acc 59.3750 (57.6562) lr 1.9921e-03 eta 0:08:19
epoch [4/50] batch [25/49] time 0.176 (0.210) data 0.000 (0.034) loss 4.2734 (4.0829) acc 43.7500 (55.6250) lr 1.9921e-03 eta 0:07:59
epoch [4/50] batch [30/49] time 0.177 (0.205) data 0.000 (0.028) loss 4.4219 (4.0594) acc 53.1250 (55.0000) lr 1.9921e-03 eta 0:07:45
epoch [4/50] batch [35/49] time 0.175 (0.201) data 0.000 (0.024) loss 3.8906 (4.0436) acc 46.8750 (54.5536) lr 1.9921e-03 eta 0:07:35
epoch [4/50] batch [40/49] time 0.175 (0.197) data 0.000 (0.021) loss 4.0898 (4.0494) acc 68.7500 (54.7656) lr 1.9921e-03 eta 0:07:26
epoch [4/50] batch [45/49] time 0.176 (0.195) data 0.000 (0.019) loss 4.1016 (4.0575) acc 62.5000 (54.8611) lr 1.9921e-03 eta 0:07:20
epoch [5/50] batch [5/49] time 0.176 (0.378) data 0.000 (0.202) loss 3.7148 (3.9664) acc 59.3750 (53.7500) lr 1.9823e-03 eta 0:14:11
epoch [5/50] batch [10/49] time 0.177 (0.277) data 0.000 (0.101) loss 4.1680 (4.0855) acc 50.0000 (53.1250) lr 1.9823e-03 eta 0:10:22
epoch [5/50] batch [15/49] time 0.177 (0.244) data 0.000 (0.068) loss 3.7793 (3.9999) acc 53.1250 (54.5833) lr 1.9823e-03 eta 0:09:06
epoch [5/50] batch [20/49] time 0.177 (0.227) data 0.000 (0.051) loss 3.9570 (3.9139) acc 59.3750 (56.2500) lr 1.9823e-03 eta 0:08:27
epoch [5/50] batch [25/49] time 0.176 (0.217) data 0.000 (0.041) loss 3.8633 (3.8841) acc 65.6250 (57.1250) lr 1.9823e-03 eta 0:08:04
epoch [5/50] batch [30/49] time 0.176 (0.210) data 0.000 (0.034) loss 4.3867 (3.9390) acc 46.8750 (56.1458) lr 1.9823e-03 eta 0:07:47
epoch [5/50] batch [35/49] time 0.177 (0.206) data 0.000 (0.029) loss 3.5039 (3.8957) acc 56.2500 (56.5179) lr 1.9823e-03 eta 0:07:36
epoch [5/50] batch [40/49] time 0.175 (0.202) data 0.000 (0.026) loss 3.9336 (3.9346) acc 56.2500 (56.4062) lr 1.9823e-03 eta 0:07:26
epoch [5/50] batch [45/49] time 0.176 (0.199) data 0.000 (0.023) loss 3.9570 (3.9826) acc 56.2500 (55.6944) lr 1.9823e-03 eta 0:07:19
epoch [6/50] batch [5/49] time 0.175 (0.343) data 0.000 (0.166) loss 3.5137 (3.7770) acc 53.1250 (50.0000) lr 1.9686e-03 eta 0:12:33
epoch [6/50] batch [10/49] time 0.176 (0.259) data 0.000 (0.083) loss 3.4336 (3.7623) acc 59.3750 (54.0625) lr 1.9686e-03 eta 0:09:29
epoch [6/50] batch [15/49] time 0.177 (0.232) data 0.000 (0.056) loss 4.2383 (3.8283) acc 56.2500 (55.2083) lr 1.9686e-03 eta 0:08:27
epoch [6/50] batch [20/49] time 0.177 (0.218) data 0.000 (0.042) loss 3.7930 (3.8432) acc 59.3750 (56.0938) lr 1.9686e-03 eta 0:07:56
epoch [6/50] batch [25/49] time 0.177 (0.210) data 0.000 (0.034) loss 4.1953 (3.9206) acc 53.1250 (54.5000) lr 1.9686e-03 eta 0:07:37
epoch [6/50] batch [30/49] time 0.176 (0.204) data 0.000 (0.028) loss 3.9531 (3.8946) acc 53.1250 (54.5833) lr 1.9686e-03 eta 0:07:24
epoch [6/50] batch [35/49] time 0.176 (0.200) data 0.000 (0.024) loss 4.3164 (3.9074) acc 53.1250 (54.4643) lr 1.9686e-03 eta 0:07:14
epoch [6/50] batch [40/49] time 0.177 (0.197) data 0.000 (0.021) loss 3.5723 (3.8787) acc 62.5000 (55.3906) lr 1.9686e-03 eta 0:07:07
epoch [6/50] batch [45/49] time 0.176 (0.195) data 0.000 (0.019) loss 4.1055 (3.8742) acc 46.8750 (55.4861) lr 1.9686e-03 eta 0:07:01
epoch [7/50] batch [5/49] time 0.177 (0.335) data 0.000 (0.157) loss 4.1289 (3.8805) acc 50.0000 (60.0000) lr 1.9511e-03 eta 0:12:00
epoch [7/50] batch [10/49] time 0.176 (0.256) data 0.000 (0.079) loss 3.4883 (3.6898) acc 68.7500 (63.4375) lr 1.9511e-03 eta 0:09:09
epoch [7/50] batch [15/49] time 0.176 (0.229) data 0.000 (0.053) loss 3.9238 (3.7608) acc 43.7500 (58.5417) lr 1.9511e-03 eta 0:08:11
epoch [7/50] batch [20/49] time 0.178 (0.216) data 0.000 (0.040) loss 3.8262 (3.8134) acc 50.0000 (56.8750) lr 1.9511e-03 eta 0:07:42
epoch [7/50] batch [25/49] time 0.177 (0.208) data 0.000 (0.032) loss 3.6152 (3.7874) acc 65.6250 (57.5000) lr 1.9511e-03 eta 0:07:24
epoch [7/50] batch [30/49] time 0.177 (0.203) data 0.000 (0.026) loss 3.7227 (3.7689) acc 65.6250 (57.8125) lr 1.9511e-03 eta 0:07:11
epoch [7/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.023) loss 3.7695 (3.7864) acc 62.5000 (57.3214) lr 1.9511e-03 eta 0:07:02
epoch [7/50] batch [40/49] time 0.175 (0.196) data 0.000 (0.020) loss 3.8477 (3.7992) acc 43.7500 (57.5000) lr 1.9511e-03 eta 0:06:55
epoch [7/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.7773 (3.7834) acc 68.7500 (57.5000) lr 1.9511e-03 eta 0:06:49
epoch [8/50] batch [5/49] time 0.176 (0.325) data 0.000 (0.148) loss 3.4688 (3.8156) acc 75.0000 (59.3750) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [10/49] time 0.178 (0.251) data 0.000 (0.074) loss 3.5566 (3.7479) acc 59.3750 (60.0000) lr 1.9298e-03 eta 0:08:46
epoch [8/50] batch [15/49] time 0.176 (0.227) data 0.000 (0.050) loss 3.4688 (3.6911) acc 53.1250 (58.7500) lr 1.9298e-03 eta 0:07:54
epoch [8/50] batch [20/49] time 0.192 (0.215) data 0.016 (0.038) loss 3.8438 (3.7117) acc 53.1250 (58.5938) lr 1.9298e-03 eta 0:07:28
epoch [8/50] batch [25/49] time 0.177 (0.207) data 0.000 (0.031) loss 4.0547 (3.7163) acc 43.7500 (57.7500) lr 1.9298e-03 eta 0:07:11
epoch [8/50] batch [30/49] time 0.177 (0.202) data 0.000 (0.025) loss 3.1641 (3.7220) acc 62.5000 (56.7708) lr 1.9298e-03 eta 0:07:00
epoch [8/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.022) loss 4.2500 (3.7643) acc 46.8750 (56.4286) lr 1.9298e-03 eta 0:06:51
epoch [8/50] batch [40/49] time 0.176 (0.196) data 0.000 (0.019) loss 3.2461 (3.7689) acc 87.5000 (57.1875) lr 1.9298e-03 eta 0:06:44
epoch [8/50] batch [45/49] time 0.176 (0.194) data 0.000 (0.017) loss 3.9238 (3.8240) acc 56.2500 (56.9444) lr 1.9298e-03 eta 0:06:39
epoch [9/50] batch [5/49] time 0.177 (0.329) data 0.000 (0.151) loss 3.5430 (3.7902) acc 65.6250 (56.8750) lr 1.9048e-03 eta 0:11:14
epoch [9/50] batch [10/49] time 0.176 (0.252) data 0.000 (0.075) loss 3.7305 (3.7254) acc 53.1250 (60.6250) lr 1.9048e-03 eta 0:08:36
epoch [9/50] batch [15/49] time 0.177 (0.227) data 0.000 (0.050) loss 2.9961 (3.5678) acc 65.6250 (62.2917) lr 1.9048e-03 eta 0:07:44
epoch [9/50] batch [20/49] time 0.177 (0.215) data 0.000 (0.038) loss 3.8008 (3.5355) acc 59.3750 (61.2500) lr 1.9048e-03 eta 0:07:17
epoch [9/50] batch [25/49] time 0.176 (0.207) data 0.000 (0.030) loss 3.7539 (3.5602) acc 59.3750 (60.7500) lr 1.9048e-03 eta 0:07:01
epoch [9/50] batch [30/49] time 0.178 (0.202) data 0.000 (0.025) loss 3.5664 (3.5857) acc 65.6250 (61.1458) lr 1.9048e-03 eta 0:06:50
epoch [9/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.022) loss 3.5664 (3.5989) acc 53.1250 (60.5357) lr 1.9048e-03 eta 0:06:41
epoch [9/50] batch [40/49] time 0.176 (0.196) data 0.000 (0.019) loss 3.8789 (3.6150) acc 59.3750 (60.2344) lr 1.9048e-03 eta 0:06:35
epoch [9/50] batch [45/49] time 0.177 (0.194) data 0.000 (0.017) loss 3.7695 (3.6365) acc 53.1250 (59.6528) lr 1.9048e-03 eta 0:06:30
epoch [10/50] batch [5/49] time 0.177 (0.348) data 0.000 (0.171) loss 2.9219 (3.3816) acc 75.0000 (65.6250) lr 1.8763e-03 eta 0:11:36
epoch [10/50] batch [10/49] time 0.180 (0.263) data 0.000 (0.086) loss 3.5469 (3.5178) acc 62.5000 (64.0625) lr 1.8763e-03 eta 0:08:45
epoch [10/50] batch [15/49] time 0.176 (0.234) data 0.000 (0.057) loss 3.8145 (3.4927) acc 53.1250 (63.5417) lr 1.8763e-03 eta 0:07:46
epoch [10/50] batch [20/49] time 0.176 (0.220) data 0.000 (0.043) loss 3.4492 (3.5414) acc 56.2500 (61.2500) lr 1.8763e-03 eta 0:07:16
epoch [10/50] batch [25/49] time 0.177 (0.211) data 0.000 (0.034) loss 3.4434 (3.5425) acc 59.3750 (61.5000) lr 1.8763e-03 eta 0:06:58
epoch [10/50] batch [30/49] time 0.176 (0.205) data 0.000 (0.029) loss 4.0078 (3.5717) acc 59.3750 (60.7292) lr 1.8763e-03 eta 0:06:46
epoch [10/50] batch [35/49] time 0.175 (0.201) data 0.000 (0.025) loss 3.5469 (3.5773) acc 65.6250 (60.2679) lr 1.8763e-03 eta 0:06:37
epoch [10/50] batch [40/49] time 0.175 (0.198) data 0.000 (0.022) loss 4.0000 (3.5815) acc 56.2500 (60.7031) lr 1.8763e-03 eta 0:06:29
epoch [10/50] batch [45/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.5527 (3.5898) acc 59.3750 (60.3472) lr 1.8763e-03 eta 0:06:23
epoch [11/50] batch [5/49] time 0.181 (0.334) data 0.004 (0.158) loss 3.5449 (3.6867) acc 65.6250 (60.6250) lr 1.8443e-03 eta 0:10:53
epoch [11/50] batch [10/49] time 0.175 (0.255) data 0.000 (0.079) loss 3.4961 (3.6688) acc 65.6250 (61.8750) lr 1.8443e-03 eta 0:08:16
epoch [11/50] batch [15/49] time 0.175 (0.229) data 0.000 (0.053) loss 3.7480 (3.6868) acc 65.6250 (62.2917) lr 1.8443e-03 eta 0:07:24
epoch [11/50] batch [20/49] time 0.175 (0.215) data 0.000 (0.040) loss 3.9414 (3.6524) acc 65.6250 (63.1250) lr 1.8443e-03 eta 0:06:57
epoch [11/50] batch [25/49] time 0.175 (0.207) data 0.000 (0.032) loss 4.2930 (3.6643) acc 43.7500 (61.8750) lr 1.8443e-03 eta 0:06:40
epoch [11/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.026) loss 3.3379 (3.6271) acc 62.5000 (62.6042) lr 1.8443e-03 eta 0:06:29
epoch [11/50] batch [35/49] time 0.175 (0.198) data 0.000 (0.023) loss 3.8008 (3.6311) acc 65.6250 (62.7679) lr 1.8443e-03 eta 0:06:21
epoch [11/50] batch [40/49] time 0.175 (0.195) data 0.000 (0.020) loss 4.1992 (3.6628) acc 53.1250 (61.5625) lr 1.8443e-03 eta 0:06:14
epoch [11/50] batch [45/49] time 0.176 (0.193) data 0.000 (0.018) loss 3.6602 (3.6444) acc 50.0000 (61.4583) lr 1.8443e-03 eta 0:06:09
epoch [12/50] batch [5/49] time 0.179 (0.330) data 0.000 (0.153) loss 3.5938 (3.5406) acc 46.8750 (60.6250) lr 1.8090e-03 eta 0:10:29
epoch [12/50] batch [10/49] time 0.176 (0.254) data 0.000 (0.077) loss 3.0234 (3.5088) acc 65.6250 (63.4375) lr 1.8090e-03 eta 0:08:02
epoch [12/50] batch [15/49] time 0.178 (0.229) data 0.000 (0.051) loss 3.4082 (3.5152) acc 68.7500 (62.9167) lr 1.8090e-03 eta 0:07:13
epoch [12/50] batch [20/49] time 0.177 (0.216) data 0.000 (0.038) loss 3.3711 (3.4517) acc 78.1250 (64.3750) lr 1.8090e-03 eta 0:06:47
epoch [12/50] batch [25/49] time 0.178 (0.208) data 0.000 (0.031) loss 4.1562 (3.4550) acc 56.2500 (63.7500) lr 1.8090e-03 eta 0:06:32
epoch [12/50] batch [30/49] time 0.178 (0.203) data 0.000 (0.026) loss 3.9297 (3.5247) acc 59.3750 (63.0208) lr 1.8090e-03 eta 0:06:21
epoch [12/50] batch [35/49] time 0.179 (0.199) data 0.000 (0.022) loss 3.3945 (3.5525) acc 71.8750 (63.2143) lr 1.8090e-03 eta 0:06:13
epoch [12/50] batch [40/49] time 0.176 (0.196) data 0.000 (0.019) loss 3.7617 (3.5445) acc 50.0000 (62.9688) lr 1.8090e-03 eta 0:06:07
epoch [12/50] batch [45/49] time 0.176 (0.194) data 0.000 (0.017) loss 3.2461 (3.5537) acc 59.3750 (62.7778) lr 1.8090e-03 eta 0:06:02
epoch [13/50] batch [5/49] time 0.178 (0.376) data 0.000 (0.198) loss 3.7168 (3.5176) acc 56.2500 (63.1250) lr 1.7705e-03 eta 0:11:37
epoch [13/50] batch [10/49] time 0.178 (0.276) data 0.000 (0.099) loss 2.9922 (3.4266) acc 71.8750 (63.7500) lr 1.7705e-03 eta 0:08:31
epoch [13/50] batch [15/49] time 0.176 (0.243) data 0.000 (0.066) loss 3.4688 (3.4591) acc 68.7500 (63.1250) lr 1.7705e-03 eta 0:07:28
epoch [13/50] batch [20/49] time 0.177 (0.227) data 0.000 (0.050) loss 3.7461 (3.4946) acc 59.3750 (63.7500) lr 1.7705e-03 eta 0:06:57
epoch [13/50] batch [25/49] time 0.176 (0.217) data 0.000 (0.040) loss 3.1719 (3.5552) acc 65.6250 (61.7500) lr 1.7705e-03 eta 0:06:37
epoch [13/50] batch [30/49] time 0.176 (0.210) data 0.000 (0.033) loss 3.1289 (3.5387) acc 75.0000 (61.7708) lr 1.7705e-03 eta 0:06:24
epoch [13/50] batch [35/49] time 0.175 (0.205) data 0.000 (0.029) loss 3.6289 (3.5625) acc 59.3750 (61.3393) lr 1.7705e-03 eta 0:06:14
epoch [13/50] batch [40/49] time 0.177 (0.201) data 0.000 (0.025) loss 4.0938 (3.5563) acc 43.7500 (61.4844) lr 1.7705e-03 eta 0:06:06
epoch [13/50] batch [45/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.6094 (3.5773) acc 75.0000 (61.3194) lr 1.7705e-03 eta 0:06:00
epoch [14/50] batch [5/49] time 0.177 (0.355) data 0.000 (0.177) loss 3.9492 (3.5676) acc 59.3750 (64.3750) lr 1.7290e-03 eta 0:10:40
epoch [14/50] batch [10/49] time 0.176 (0.266) data 0.000 (0.089) loss 3.8477 (3.4723) acc 46.8750 (64.0625) lr 1.7290e-03 eta 0:07:59
epoch [14/50] batch [15/49] time 0.176 (0.236) data 0.000 (0.059) loss 3.4531 (3.4573) acc 53.1250 (63.5417) lr 1.7290e-03 eta 0:07:04
epoch [14/50] batch [20/49] time 0.176 (0.221) data 0.000 (0.044) loss 3.5195 (3.4712) acc 50.0000 (62.8125) lr 1.7290e-03 eta 0:06:36
epoch [14/50] batch [25/49] time 0.176 (0.212) data 0.000 (0.036) loss 4.0039 (3.5848) acc 50.0000 (60.1250) lr 1.7290e-03 eta 0:06:19
epoch [14/50] batch [30/49] time 0.175 (0.206) data 0.000 (0.030) loss 3.4766 (3.5613) acc 65.6250 (61.5625) lr 1.7290e-03 eta 0:06:07
epoch [14/50] batch [35/49] time 0.175 (0.202) data 0.000 (0.026) loss 3.1719 (3.5411) acc 75.0000 (61.7857) lr 1.7290e-03 eta 0:05:58
epoch [14/50] batch [40/49] time 0.175 (0.198) data 0.000 (0.022) loss 4.3242 (3.5962) acc 46.8750 (61.6406) lr 1.7290e-03 eta 0:05:51
epoch [14/50] batch [45/49] time 0.176 (0.196) data 0.000 (0.020) loss 3.5059 (3.5719) acc 59.3750 (61.6667) lr 1.7290e-03 eta 0:05:46
epoch [15/50] batch [5/49] time 0.177 (0.308) data 0.000 (0.130) loss 3.9141 (3.6172) acc 62.5000 (65.6250) lr 1.6845e-03 eta 0:09:01
epoch [15/50] batch [10/49] time 0.183 (0.243) data 0.004 (0.065) loss 4.3867 (3.6357) acc 43.7500 (63.4375) lr 1.6845e-03 eta 0:07:06
epoch [15/50] batch [15/49] time 0.176 (0.221) data 0.000 (0.044) loss 3.4023 (3.5089) acc 65.6250 (65.6250) lr 1.6845e-03 eta 0:06:25
epoch [15/50] batch [20/49] time 0.175 (0.209) data 0.000 (0.033) loss 3.5137 (3.4461) acc 62.5000 (66.5625) lr 1.6845e-03 eta 0:06:05
epoch [15/50] batch [25/49] time 0.176 (0.203) data 0.000 (0.026) loss 3.5430 (3.4631) acc 56.2500 (66.0000) lr 1.6845e-03 eta 0:05:52
epoch [15/50] batch [30/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.9492 (3.5104) acc 56.2500 (64.8958) lr 1.6845e-03 eta 0:05:43
epoch [15/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.019) loss 4.3242 (3.5526) acc 56.2500 (64.6429) lr 1.6845e-03 eta 0:05:37
epoch [15/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.016) loss 4.0195 (3.5406) acc 56.2500 (65.2344) lr 1.6845e-03 eta 0:05:31
epoch [15/50] batch [45/49] time 0.176 (0.191) data 0.000 (0.015) loss 3.3320 (3.5483) acc 65.6250 (64.7917) lr 1.6845e-03 eta 0:05:27
epoch [16/50] batch [5/49] time 0.175 (0.335) data 0.000 (0.159) loss 3.5273 (3.4215) acc 71.8750 (65.6250) lr 1.6374e-03 eta 0:09:32
epoch [16/50] batch [10/49] time 0.175 (0.255) data 0.000 (0.079) loss 3.4414 (3.4020) acc 62.5000 (65.0000) lr 1.6374e-03 eta 0:07:15
epoch [16/50] batch [15/49] time 0.175 (0.229) data 0.000 (0.053) loss 3.4102 (3.4988) acc 59.3750 (63.7500) lr 1.6374e-03 eta 0:06:28
epoch [16/50] batch [20/49] time 0.175 (0.216) data 0.000 (0.040) loss 3.2734 (3.4901) acc 68.7500 (64.2188) lr 1.6374e-03 eta 0:06:06
epoch [16/50] batch [25/49] time 0.176 (0.208) data 0.000 (0.032) loss 3.5000 (3.5090) acc 46.8750 (63.8750) lr 1.6374e-03 eta 0:05:51
epoch [16/50] batch [30/49] time 0.176 (0.203) data 0.000 (0.027) loss 4.4766 (3.5262) acc 46.8750 (64.0625) lr 1.6374e-03 eta 0:05:41
epoch [16/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.023) loss 3.1309 (3.5315) acc 62.5000 (62.9464) lr 1.6374e-03 eta 0:05:34
epoch [16/50] batch [40/49] time 0.177 (0.196) data 0.000 (0.020) loss 3.1250 (3.5416) acc 62.5000 (62.8125) lr 1.6374e-03 eta 0:05:28
epoch [16/50] batch [45/49] time 0.175 (0.194) data 0.001 (0.018) loss 3.6191 (3.5406) acc 68.7500 (62.8472) lr 1.6374e-03 eta 0:05:23
epoch [17/50] batch [5/49] time 0.176 (0.329) data 0.000 (0.151) loss 3.4629 (3.4949) acc 62.5000 (65.6250) lr 1.5878e-03 eta 0:09:06
epoch [17/50] batch [10/49] time 0.177 (0.253) data 0.000 (0.076) loss 3.5156 (3.4314) acc 62.5000 (67.1875) lr 1.5878e-03 eta 0:06:59
epoch [17/50] batch [15/49] time 0.177 (0.228) data 0.000 (0.050) loss 3.8008 (3.4960) acc 68.7500 (67.2917) lr 1.5878e-03 eta 0:06:16
epoch [17/50] batch [20/49] time 0.178 (0.215) data 0.000 (0.038) loss 3.3477 (3.4559) acc 68.7500 (68.4375) lr 1.5878e-03 eta 0:05:54
epoch [17/50] batch [25/49] time 0.176 (0.207) data 0.000 (0.030) loss 3.4199 (3.4495) acc 65.6250 (67.3750) lr 1.5878e-03 eta 0:05:40
epoch [17/50] batch [30/49] time 0.179 (0.202) data 0.000 (0.025) loss 3.6172 (3.4685) acc 53.1250 (65.8333) lr 1.5878e-03 eta 0:05:30
epoch [17/50] batch [35/49] time 0.176 (0.198) data 0.000 (0.022) loss 3.7031 (3.4847) acc 56.2500 (65.0893) lr 1.5878e-03 eta 0:05:23
epoch [17/50] batch [40/49] time 0.177 (0.196) data 0.000 (0.019) loss 3.7168 (3.5064) acc 62.5000 (65.1562) lr 1.5878e-03 eta 0:05:18
epoch [17/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.017) loss 3.2656 (3.5014) acc 78.1250 (65.0694) lr 1.5878e-03 eta 0:05:13
epoch [18/50] batch [5/49] time 0.177 (0.317) data 0.000 (0.139) loss 3.7930 (3.5418) acc 53.1250 (66.8750) lr 1.5358e-03 eta 0:08:30
epoch [18/50] batch [10/49] time 0.176 (0.247) data 0.000 (0.070) loss 4.0156 (3.5131) acc 53.1250 (67.1875) lr 1.5358e-03 eta 0:06:36
epoch [18/50] batch [15/49] time 0.177 (0.223) data 0.000 (0.047) loss 2.9570 (3.4177) acc 62.5000 (68.1250) lr 1.5358e-03 eta 0:05:57
epoch [18/50] batch [20/49] time 0.177 (0.212) data 0.000 (0.035) loss 3.7480 (3.4512) acc 62.5000 (66.4062) lr 1.5358e-03 eta 0:05:38
epoch [18/50] batch [25/49] time 0.176 (0.205) data 0.000 (0.028) loss 3.6758 (3.4741) acc 53.1250 (66.3750) lr 1.5358e-03 eta 0:05:25
epoch [18/50] batch [30/49] time 0.175 (0.200) data 0.000 (0.023) loss 3.8320 (3.4895) acc 53.1250 (65.8333) lr 1.5358e-03 eta 0:05:17
epoch [18/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.020) loss 3.5527 (3.5036) acc 59.3750 (65.6250) lr 1.5358e-03 eta 0:05:10
epoch [18/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.0469 (3.4684) acc 75.0000 (66.3281) lr 1.5358e-03 eta 0:05:05
epoch [18/50] batch [45/49] time 0.177 (0.192) data 0.000 (0.016) loss 3.7207 (3.4812) acc 56.2500 (66.0417) lr 1.5358e-03 eta 0:05:01
epoch [19/50] batch [5/49] time 0.175 (0.302) data 0.000 (0.124) loss 3.1836 (3.7355) acc 68.7500 (60.0000) lr 1.4818e-03 eta 0:07:51
epoch [19/50] batch [10/49] time 0.177 (0.239) data 0.000 (0.062) loss 3.2910 (3.6584) acc 62.5000 (60.9375) lr 1.4818e-03 eta 0:06:12
epoch [19/50] batch [15/49] time 0.178 (0.219) data 0.000 (0.042) loss 3.3750 (3.4797) acc 75.0000 (64.3750) lr 1.4818e-03 eta 0:05:39
epoch [19/50] batch [20/49] time 0.175 (0.208) data 0.000 (0.031) loss 2.9727 (3.4343) acc 75.0000 (64.5312) lr 1.4818e-03 eta 0:05:22
epoch [19/50] batch [25/49] time 0.175 (0.201) data 0.000 (0.025) loss 3.4102 (3.4978) acc 78.1250 (64.8750) lr 1.4818e-03 eta 0:05:10
epoch [19/50] batch [30/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.2070 (3.4650) acc 65.6250 (65.9375) lr 1.4818e-03 eta 0:05:03
epoch [19/50] batch [35/49] time 0.176 (0.194) data 0.000 (0.018) loss 3.5039 (3.4589) acc 65.6250 (65.9821) lr 1.4818e-03 eta 0:04:57
epoch [19/50] batch [40/49] time 0.176 (0.192) data 0.002 (0.016) loss 3.6328 (3.4739) acc 59.3750 (65.4688) lr 1.4818e-03 eta 0:04:53
epoch [19/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.014) loss 3.3613 (3.4563) acc 68.7500 (65.9028) lr 1.4818e-03 eta 0:04:49
epoch [20/50] batch [5/49] time 0.176 (0.379) data 0.000 (0.202) loss 3.5957 (3.4793) acc 71.8750 (66.8750) lr 1.4258e-03 eta 0:09:33
epoch [20/50] batch [10/49] time 0.177 (0.278) data 0.000 (0.101) loss 3.1758 (3.4162) acc 75.0000 (69.0625) lr 1.4258e-03 eta 0:06:59
epoch [20/50] batch [15/49] time 0.177 (0.244) data 0.000 (0.068) loss 3.7734 (3.4141) acc 59.3750 (67.9167) lr 1.4258e-03 eta 0:06:07
epoch [20/50] batch [20/49] time 0.177 (0.227) data 0.000 (0.051) loss 3.8516 (3.4716) acc 62.5000 (67.3438) lr 1.4258e-03 eta 0:05:40
epoch [20/50] batch [25/49] time 0.176 (0.217) data 0.000 (0.041) loss 2.9316 (3.4725) acc 71.8750 (67.5000) lr 1.4258e-03 eta 0:05:24
epoch [20/50] batch [30/49] time 0.176 (0.210) data 0.000 (0.034) loss 3.2832 (3.4417) acc 65.6250 (67.3958) lr 1.4258e-03 eta 0:05:13
epoch [20/50] batch [35/49] time 0.176 (0.205) data 0.000 (0.029) loss 3.4609 (3.4122) acc 65.6250 (67.5000) lr 1.4258e-03 eta 0:05:04
epoch [20/50] batch [40/49] time 0.176 (0.202) data 0.000 (0.025) loss 3.4883 (3.4179) acc 53.1250 (67.0312) lr 1.4258e-03 eta 0:04:58
epoch [20/50] batch [45/49] time 0.176 (0.199) data 0.000 (0.023) loss 3.2188 (3.3905) acc 71.8750 (67.4306) lr 1.4258e-03 eta 0:04:53
epoch [21/50] batch [5/49] time 0.176 (0.311) data 0.000 (0.134) loss 4.1680 (3.4223) acc 56.2500 (69.3750) lr 1.3681e-03 eta 0:07:35
epoch [21/50] batch [10/49] time 0.175 (0.243) data 0.000 (0.067) loss 3.8457 (3.4574) acc 56.2500 (66.8750) lr 1.3681e-03 eta 0:05:55
epoch [21/50] batch [15/49] time 0.176 (0.221) data 0.000 (0.045) loss 3.2188 (3.4137) acc 65.6250 (67.7083) lr 1.3681e-03 eta 0:05:20
epoch [21/50] batch [20/49] time 0.175 (0.209) data 0.000 (0.034) loss 3.0293 (3.3558) acc 68.7500 (67.6562) lr 1.3681e-03 eta 0:05:03
epoch [21/50] batch [25/49] time 0.177 (0.203) data 0.000 (0.027) loss 2.9941 (3.3170) acc 71.8750 (68.0000) lr 1.3681e-03 eta 0:04:52
epoch [21/50] batch [30/49] time 0.177 (0.198) data 0.000 (0.023) loss 3.5430 (3.3755) acc 68.7500 (66.9792) lr 1.3681e-03 eta 0:04:45
epoch [21/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.3555 (3.3549) acc 59.3750 (67.2321) lr 1.3681e-03 eta 0:04:40
epoch [21/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.017) loss 3.6562 (3.3502) acc 75.0000 (67.7344) lr 1.3681e-03 eta 0:04:35
epoch [21/50] batch [45/49] time 0.176 (0.191) data 0.000 (0.015) loss 2.9062 (3.3580) acc 81.2500 (67.4306) lr 1.3681e-03 eta 0:04:32
epoch [22/50] batch [5/49] time 0.175 (0.356) data 0.000 (0.178) loss 2.9688 (3.2797) acc 81.2500 (67.5000) lr 1.3090e-03 eta 0:08:24
epoch [22/50] batch [10/49] time 0.179 (0.267) data 0.000 (0.089) loss 3.9102 (3.3975) acc 46.8750 (65.9375) lr 1.3090e-03 eta 0:06:16
epoch [22/50] batch [15/49] time 0.178 (0.237) data 0.000 (0.060) loss 3.7344 (3.3745) acc 65.6250 (67.2917) lr 1.3090e-03 eta 0:05:33
epoch [22/50] batch [20/49] time 0.176 (0.222) data 0.000 (0.045) loss 3.3574 (3.3716) acc 56.2500 (66.8750) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [25/49] time 0.176 (0.213) data 0.000 (0.036) loss 3.8379 (3.3715) acc 65.6250 (67.0000) lr 1.3090e-03 eta 0:04:57
epoch [22/50] batch [30/49] time 0.176 (0.207) data 0.000 (0.030) loss 3.3867 (3.3327) acc 56.2500 (67.9167) lr 1.3090e-03 eta 0:04:47
epoch [22/50] batch [35/49] time 0.177 (0.203) data 0.000 (0.026) loss 3.1875 (3.2962) acc 68.7500 (68.4821) lr 1.3090e-03 eta 0:04:40
epoch [22/50] batch [40/49] time 0.176 (0.199) data 0.000 (0.023) loss 3.7598 (3.3253) acc 62.5000 (67.8125) lr 1.3090e-03 eta 0:04:35
epoch [22/50] batch [45/49] time 0.175 (0.197) data 0.000 (0.020) loss 2.9609 (3.3058) acc 78.1250 (68.2639) lr 1.3090e-03 eta 0:04:30
epoch [23/50] batch [5/49] time 0.176 (0.308) data 0.000 (0.130) loss 3.5215 (3.2195) acc 56.2500 (69.3750) lr 1.2487e-03 eta 0:07:01
epoch [23/50] batch [10/49] time 0.176 (0.242) data 0.000 (0.065) loss 3.0977 (3.1777) acc 78.1250 (71.8750) lr 1.2487e-03 eta 0:05:29
epoch [23/50] batch [15/49] time 0.175 (0.220) data 0.000 (0.044) loss 4.0820 (3.1922) acc 68.7500 (71.0417) lr 1.2487e-03 eta 0:04:58
epoch [23/50] batch [20/49] time 0.176 (0.209) data 0.000 (0.033) loss 3.5293 (3.2610) acc 59.3750 (67.6562) lr 1.2487e-03 eta 0:04:42
epoch [23/50] batch [25/49] time 0.174 (0.202) data 0.000 (0.026) loss 3.0664 (3.2762) acc 75.0000 (67.1250) lr 1.2487e-03 eta 0:04:32
epoch [23/50] batch [30/49] time 0.176 (0.198) data 0.000 (0.022) loss 3.7695 (3.2936) acc 65.6250 (67.6042) lr 1.2487e-03 eta 0:04:25
epoch [23/50] batch [35/49] time 0.174 (0.195) data 0.000 (0.019) loss 3.4824 (3.3047) acc 59.3750 (67.2321) lr 1.2487e-03 eta 0:04:20
epoch [23/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.016) loss 2.9336 (3.2687) acc 71.8750 (68.2031) lr 1.2487e-03 eta 0:04:15
epoch [23/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.015) loss 3.2832 (3.2697) acc 75.0000 (68.8889) lr 1.2487e-03 eta 0:04:12
epoch [24/50] batch [5/49] time 0.176 (0.320) data 0.000 (0.143) loss 3.3789 (3.2199) acc 59.3750 (70.0000) lr 1.1874e-03 eta 0:07:01
epoch [24/50] batch [10/49] time 0.177 (0.248) data 0.000 (0.072) loss 3.2227 (3.2660) acc 59.3750 (66.2500) lr 1.1874e-03 eta 0:05:25
epoch [24/50] batch [15/49] time 0.177 (0.224) data 0.000 (0.048) loss 3.4297 (3.3108) acc 65.6250 (66.4583) lr 1.1874e-03 eta 0:04:53
epoch [24/50] batch [20/49] time 0.177 (0.212) data 0.000 (0.036) loss 2.9863 (3.2917) acc 87.5000 (69.2188) lr 1.1874e-03 eta 0:04:36
epoch [24/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.029) loss 3.0742 (3.2977) acc 75.0000 (68.7500) lr 1.1874e-03 eta 0:04:26
epoch [24/50] batch [30/49] time 0.177 (0.200) data 0.000 (0.024) loss 3.4082 (3.2999) acc 68.7500 (69.3750) lr 1.1874e-03 eta 0:04:18
epoch [24/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.021) loss 3.4004 (3.3185) acc 65.6250 (69.1071) lr 1.1874e-03 eta 0:04:13
epoch [24/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 2.2852 (3.2913) acc 87.5000 (69.3750) lr 1.1874e-03 eta 0:04:09
epoch [24/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.8027 (3.3043) acc 56.2500 (69.0278) lr 1.1874e-03 eta 0:04:05
epoch [25/50] batch [5/49] time 0.177 (0.321) data 0.000 (0.141) loss 3.2266 (3.0285) acc 68.7500 (73.7500) lr 1.1253e-03 eta 0:06:46
epoch [25/50] batch [10/49] time 0.177 (0.249) data 0.000 (0.071) loss 3.4648 (3.2416) acc 65.6250 (71.5625) lr 1.1253e-03 eta 0:05:14
epoch [25/50] batch [15/49] time 0.176 (0.225) data 0.000 (0.047) loss 2.9160 (3.2453) acc 71.8750 (70.0000) lr 1.1253e-03 eta 0:04:42
epoch [25/50] batch [20/49] time 0.179 (0.213) data 0.000 (0.036) loss 3.2109 (3.2510) acc 59.3750 (70.1562) lr 1.1253e-03 eta 0:04:26
epoch [25/50] batch [25/49] time 0.177 (0.206) data 0.000 (0.028) loss 3.7656 (3.3011) acc 50.0000 (69.1250) lr 1.1253e-03 eta 0:04:16
epoch [25/50] batch [30/49] time 0.180 (0.201) data 0.000 (0.024) loss 2.7715 (3.3018) acc 81.2500 (69.1667) lr 1.1253e-03 eta 0:04:10
epoch [25/50] batch [35/49] time 0.177 (0.198) data 0.000 (0.020) loss 3.6445 (3.3392) acc 65.6250 (68.4821) lr 1.1253e-03 eta 0:04:04
epoch [25/50] batch [40/49] time 0.176 (0.195) data 0.000 (0.018) loss 2.8438 (3.3257) acc 81.2500 (69.1406) lr 1.1253e-03 eta 0:04:00
epoch [25/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.016) loss 3.7988 (3.3302) acc 59.3750 (69.3750) lr 1.1253e-03 eta 0:03:56
epoch [26/50] batch [5/49] time 0.177 (0.323) data 0.000 (0.145) loss 3.4844 (3.2168) acc 71.8750 (72.5000) lr 1.0628e-03 eta 0:06:34
epoch [26/50] batch [10/49] time 0.177 (0.250) data 0.000 (0.072) loss 3.2148 (3.3188) acc 71.8750 (70.9375) lr 1.0628e-03 eta 0:05:03
epoch [26/50] batch [15/49] time 0.176 (0.226) data 0.000 (0.048) loss 3.5859 (3.2661) acc 68.7500 (71.2500) lr 1.0628e-03 eta 0:04:33
epoch [26/50] batch [20/49] time 0.177 (0.214) data 0.000 (0.036) loss 3.7266 (3.2944) acc 59.3750 (70.7812) lr 1.0628e-03 eta 0:04:17
epoch [26/50] batch [25/49] time 0.178 (0.206) data 0.000 (0.029) loss 3.0527 (3.2977) acc 75.0000 (70.3750) lr 1.0628e-03 eta 0:04:07
epoch [26/50] batch [30/49] time 0.177 (0.202) data 0.000 (0.024) loss 3.4609 (3.3066) acc 65.6250 (69.8958) lr 1.0628e-03 eta 0:04:00
epoch [26/50] batch [35/49] time 0.176 (0.198) data 0.000 (0.021) loss 3.0391 (3.2917) acc 68.7500 (70.3571) lr 1.0628e-03 eta 0:03:55
epoch [26/50] batch [40/49] time 0.177 (0.195) data 0.000 (0.018) loss 3.9570 (3.2992) acc 62.5000 (70.3906) lr 1.0628e-03 eta 0:03:51
epoch [26/50] batch [45/49] time 0.176 (0.193) data 0.000 (0.016) loss 3.0859 (3.2871) acc 78.1250 (70.6250) lr 1.0628e-03 eta 0:03:47
epoch [27/50] batch [5/49] time 0.179 (0.309) data 0.000 (0.131) loss 2.8477 (3.1203) acc 75.0000 (71.8750) lr 1.0000e-03 eta 0:06:01
epoch [27/50] batch [10/49] time 0.179 (0.243) data 0.000 (0.065) loss 3.2773 (3.2131) acc 75.0000 (72.1875) lr 1.0000e-03 eta 0:04:43
epoch [27/50] batch [15/49] time 0.179 (0.222) data 0.000 (0.044) loss 3.3477 (3.2490) acc 68.7500 (71.6667) lr 1.0000e-03 eta 0:04:17
epoch [27/50] batch [20/49] time 0.177 (0.210) data 0.000 (0.033) loss 3.2637 (3.2186) acc 78.1250 (72.1875) lr 1.0000e-03 eta 0:04:03
epoch [27/50] batch [25/49] time 0.176 (0.204) data 0.000 (0.026) loss 3.4023 (3.2272) acc 62.5000 (72.1250) lr 1.0000e-03 eta 0:03:54
epoch [27/50] batch [30/49] time 0.175 (0.199) data 0.000 (0.022) loss 3.7051 (3.2529) acc 71.8750 (71.5625) lr 1.0000e-03 eta 0:03:47
epoch [27/50] batch [35/49] time 0.176 (0.196) data 0.000 (0.019) loss 3.4258 (3.2698) acc 65.6250 (71.4286) lr 1.0000e-03 eta 0:03:43
epoch [27/50] batch [40/49] time 0.176 (0.193) data 0.000 (0.017) loss 2.7012 (3.2513) acc 75.0000 (71.6406) lr 1.0000e-03 eta 0:03:39
epoch [27/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.015) loss 2.5859 (3.2703) acc 87.5000 (71.3194) lr 1.0000e-03 eta 0:03:36
epoch [28/50] batch [5/49] time 0.176 (0.306) data 0.000 (0.127) loss 2.8535 (3.3980) acc 81.2500 (66.8750) lr 9.3721e-04 eta 0:05:42
epoch [28/50] batch [10/49] time 0.177 (0.241) data 0.000 (0.064) loss 3.9883 (3.4943) acc 62.5000 (65.3125) lr 9.3721e-04 eta 0:04:29
epoch [28/50] batch [15/49] time 0.176 (0.220) data 0.000 (0.043) loss 3.5703 (3.4534) acc 56.2500 (65.8333) lr 9.3721e-04 eta 0:04:04
epoch [28/50] batch [20/49] time 0.176 (0.209) data 0.000 (0.032) loss 3.0195 (3.4121) acc 59.3750 (66.5625) lr 9.3721e-04 eta 0:03:50
epoch [28/50] batch [25/49] time 0.177 (0.202) data 0.000 (0.026) loss 3.4160 (3.4155) acc 71.8750 (67.2500) lr 9.3721e-04 eta 0:03:42
epoch [28/50] batch [30/49] time 0.176 (0.198) data 0.000 (0.021) loss 3.3125 (3.3876) acc 71.8750 (68.7500) lr 9.3721e-04 eta 0:03:37
epoch [28/50] batch [35/49] time 0.176 (0.195) data 0.000 (0.018) loss 3.2305 (3.3580) acc 71.8750 (69.3750) lr 9.3721e-04 eta 0:03:32
epoch [28/50] batch [40/49] time 0.176 (0.192) data 0.000 (0.016) loss 2.9766 (3.3491) acc 78.1250 (70.0000) lr 9.3721e-04 eta 0:03:29
epoch [28/50] batch [45/49] time 0.177 (0.191) data 0.000 (0.014) loss 3.1680 (3.3367) acc 71.8750 (69.7222) lr 9.3721e-04 eta 0:03:26
epoch [29/50] batch [5/49] time 0.176 (0.312) data 0.000 (0.133) loss 2.8945 (3.1211) acc 78.1250 (71.8750) lr 8.7467e-04 eta 0:05:35
epoch [29/50] batch [10/49] time 0.182 (0.245) data 0.000 (0.067) loss 3.1602 (3.0988) acc 68.7500 (72.8125) lr 8.7467e-04 eta 0:04:21
epoch [29/50] batch [15/49] time 0.178 (0.223) data 0.001 (0.045) loss 2.8262 (3.0633) acc 81.2500 (74.3750) lr 8.7467e-04 eta 0:03:56
epoch [29/50] batch [20/49] time 0.176 (0.211) data 0.000 (0.034) loss 3.3047 (3.0714) acc 71.8750 (74.3750) lr 8.7467e-04 eta 0:03:43
epoch [29/50] batch [25/49] time 0.178 (0.204) data 0.000 (0.027) loss 3.9297 (3.1041) acc 53.1250 (73.0000) lr 8.7467e-04 eta 0:03:35
epoch [29/50] batch [30/49] time 0.178 (0.200) data 0.000 (0.022) loss 4.0469 (3.1559) acc 56.2500 (72.5000) lr 8.7467e-04 eta 0:03:29
epoch [29/50] batch [35/49] time 0.177 (0.197) data 0.000 (0.019) loss 3.0430 (3.1801) acc 62.5000 (71.7857) lr 8.7467e-04 eta 0:03:25
epoch [29/50] batch [40/49] time 0.177 (0.194) data 0.000 (0.017) loss 2.8906 (3.1765) acc 78.1250 (72.0312) lr 8.7467e-04 eta 0:03:21
epoch [29/50] batch [45/49] time 0.181 (0.193) data 0.000 (0.015) loss 3.6172 (3.1918) acc 71.8750 (72.0833) lr 8.7467e-04 eta 0:03:18
epoch [30/50] batch [5/49] time 0.175 (0.339) data 0.000 (0.163) loss 3.5234 (3.4562) acc 65.6250 (71.2500) lr 8.1262e-04 eta 0:05:47
epoch [30/50] batch [10/49] time 0.175 (0.257) data 0.000 (0.081) loss 3.3203 (3.2803) acc 62.5000 (74.0625) lr 8.1262e-04 eta 0:04:22
epoch [30/50] batch [15/49] time 0.178 (0.230) data 0.000 (0.054) loss 2.7578 (3.3243) acc 71.8750 (71.8750) lr 8.1262e-04 eta 0:03:53
epoch [30/50] batch [20/49] time 0.177 (0.217) data 0.000 (0.041) loss 3.0371 (3.3356) acc 71.8750 (70.9375) lr 8.1262e-04 eta 0:03:38
epoch [30/50] batch [25/49] time 0.176 (0.209) data 0.000 (0.033) loss 2.5664 (3.2892) acc 93.7500 (71.0000) lr 8.1262e-04 eta 0:03:29
epoch [30/50] batch [30/49] time 0.178 (0.203) data 0.004 (0.027) loss 3.0898 (3.2609) acc 75.0000 (71.6667) lr 8.1262e-04 eta 0:03:23
epoch [30/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.024) loss 3.4258 (3.2941) acc 68.7500 (71.0714) lr 8.1262e-04 eta 0:03:18
epoch [30/50] batch [40/49] time 0.177 (0.197) data 0.000 (0.021) loss 3.0859 (3.2786) acc 75.0000 (71.6406) lr 8.1262e-04 eta 0:03:14
epoch [30/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.8125 (3.2934) acc 59.3750 (71.2500) lr 8.1262e-04 eta 0:03:11
epoch [31/50] batch [5/49] time 0.177 (0.327) data 0.000 (0.150) loss 3.4102 (3.1582) acc 75.0000 (74.3750) lr 7.5131e-04 eta 0:05:18
epoch [31/50] batch [10/49] time 0.177 (0.252) data 0.000 (0.075) loss 2.8516 (3.0922) acc 75.0000 (74.6875) lr 7.5131e-04 eta 0:04:04
epoch [31/50] batch [15/49] time 0.175 (0.227) data 0.000 (0.050) loss 3.8535 (3.1391) acc 56.2500 (73.1250) lr 7.5131e-04 eta 0:03:38
epoch [31/50] batch [20/49] time 0.176 (0.214) data 0.000 (0.038) loss 3.3359 (3.1825) acc 75.0000 (72.9688) lr 7.5131e-04 eta 0:03:25
epoch [31/50] batch [25/49] time 0.177 (0.207) data 0.000 (0.030) loss 3.1953 (3.2344) acc 81.2500 (72.6250) lr 7.5131e-04 eta 0:03:17
epoch [31/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.025) loss 3.7109 (3.2456) acc 59.3750 (72.0833) lr 7.5131e-04 eta 0:03:11
epoch [31/50] batch [35/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.2266 (3.2491) acc 78.1250 (72.1429) lr 7.5131e-04 eta 0:03:07
epoch [31/50] batch [40/49] time 0.176 (0.195) data 0.000 (0.019) loss 2.8945 (3.2556) acc 87.5000 (72.8906) lr 7.5131e-04 eta 0:03:03
epoch [31/50] batch [45/49] time 0.177 (0.193) data 0.000 (0.017) loss 2.9375 (3.2480) acc 81.2500 (73.1250) lr 7.5131e-04 eta 0:03:00
epoch [32/50] batch [5/49] time 0.176 (0.313) data 0.000 (0.136) loss 3.2578 (3.2262) acc 71.8750 (76.2500) lr 6.9098e-04 eta 0:04:49
epoch [32/50] batch [10/49] time 0.177 (0.245) data 0.000 (0.069) loss 2.7422 (3.0979) acc 84.3750 (77.5000) lr 6.9098e-04 eta 0:03:45
epoch [32/50] batch [15/49] time 0.177 (0.222) data 0.000 (0.046) loss 3.4375 (3.1240) acc 68.7500 (76.2500) lr 6.9098e-04 eta 0:03:23
epoch [32/50] batch [20/49] time 0.176 (0.211) data 0.000 (0.034) loss 3.7031 (3.2252) acc 62.5000 (73.4375) lr 6.9098e-04 eta 0:03:12
epoch [32/50] batch [25/49] time 0.176 (0.204) data 0.000 (0.028) loss 2.7461 (3.1433) acc 71.8750 (74.1250) lr 6.9098e-04 eta 0:03:04
epoch [32/50] batch [30/49] time 0.176 (0.199) data 0.000 (0.023) loss 2.7539 (3.1141) acc 78.1250 (73.9583) lr 6.9098e-04 eta 0:02:59
epoch [32/50] batch [35/49] time 0.176 (0.196) data 0.000 (0.020) loss 3.4258 (3.1148) acc 56.2500 (73.5714) lr 6.9098e-04 eta 0:02:55
epoch [32/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.017) loss 3.2305 (3.1212) acc 62.5000 (73.1250) lr 6.9098e-04 eta 0:02:52
epoch [32/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.015) loss 2.9844 (3.1208) acc 71.8750 (73.2639) lr 6.9098e-04 eta 0:02:49
epoch [33/50] batch [5/49] time 0.179 (0.320) data 0.000 (0.142) loss 2.6953 (3.1629) acc 78.1250 (76.8750) lr 6.3188e-04 eta 0:04:40
epoch [33/50] batch [10/49] time 0.178 (0.248) data 0.000 (0.071) loss 3.3398 (3.2008) acc 62.5000 (73.1250) lr 6.3188e-04 eta 0:03:36
epoch [33/50] batch [15/49] time 0.176 (0.225) data 0.000 (0.048) loss 3.1484 (3.1612) acc 65.6250 (73.7500) lr 6.3188e-04 eta 0:03:14
epoch [33/50] batch [20/49] time 0.177 (0.213) data 0.000 (0.036) loss 2.6602 (3.1180) acc 81.2500 (74.0625) lr 6.3188e-04 eta 0:03:03
epoch [33/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.029) loss 3.2773 (3.0801) acc 78.1250 (75.2500) lr 6.3188e-04 eta 0:02:56
epoch [33/50] batch [30/49] time 0.177 (0.201) data 0.000 (0.024) loss 3.1035 (3.1296) acc 71.8750 (74.3750) lr 6.3188e-04 eta 0:02:51
epoch [33/50] batch [35/49] time 0.176 (0.198) data 0.000 (0.020) loss 3.3125 (3.1367) acc 71.8750 (74.0179) lr 6.3188e-04 eta 0:02:47
epoch [33/50] batch [40/49] time 0.176 (0.195) data 0.000 (0.018) loss 3.3281 (3.1291) acc 78.1250 (74.0625) lr 6.3188e-04 eta 0:02:44
epoch [33/50] batch [45/49] time 0.176 (0.193) data 0.000 (0.016) loss 3.5078 (3.1366) acc 56.2500 (73.6806) lr 6.3188e-04 eta 0:02:41
epoch [34/50] batch [5/49] time 0.177 (0.312) data 0.000 (0.134) loss 2.8242 (3.4438) acc 84.3750 (66.2500) lr 5.7422e-04 eta 0:04:18
epoch [34/50] batch [10/49] time 0.181 (0.245) data 0.000 (0.067) loss 3.2109 (3.2896) acc 78.1250 (70.3125) lr 5.7422e-04 eta 0:03:21
epoch [34/50] batch [15/49] time 0.181 (0.223) data 0.000 (0.045) loss 3.4102 (3.3487) acc 65.6250 (70.8333) lr 5.7422e-04 eta 0:03:02
epoch [34/50] batch [20/49] time 0.179 (0.212) data 0.000 (0.034) loss 3.0273 (3.3555) acc 71.8750 (70.9375) lr 5.7422e-04 eta 0:02:51
epoch [34/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.027) loss 2.9922 (3.3204) acc 78.1250 (72.0000) lr 5.7422e-04 eta 0:02:45
epoch [34/50] batch [30/49] time 0.177 (0.200) data 0.000 (0.023) loss 3.4570 (3.3436) acc 71.8750 (72.0833) lr 5.7422e-04 eta 0:02:40
epoch [34/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.019) loss 2.8340 (3.3403) acc 87.5000 (72.9464) lr 5.7422e-04 eta 0:02:36
epoch [34/50] batch [40/49] time 0.179 (0.194) data 0.000 (0.017) loss 3.4219 (3.3347) acc 68.7500 (72.8125) lr 5.7422e-04 eta 0:02:33
epoch [34/50] batch [45/49] time 0.178 (0.192) data 0.002 (0.015) loss 3.2969 (3.3054) acc 78.1250 (73.1944) lr 5.7422e-04 eta 0:02:31
epoch [35/50] batch [5/49] time 0.177 (0.304) data 0.000 (0.125) loss 3.4297 (3.0949) acc 71.8750 (74.3750) lr 5.1825e-04 eta 0:03:56
epoch [35/50] batch [10/49] time 0.177 (0.241) data 0.000 (0.063) loss 3.1582 (3.1949) acc 78.1250 (75.6250) lr 5.1825e-04 eta 0:03:06
epoch [35/50] batch [15/49] time 0.178 (0.220) data 0.000 (0.042) loss 2.9980 (3.2065) acc 81.2500 (73.5417) lr 5.1825e-04 eta 0:02:48
epoch [35/50] batch [20/49] time 0.177 (0.209) data 0.000 (0.031) loss 3.3125 (3.1738) acc 78.1250 (74.6875) lr 5.1825e-04 eta 0:02:39
epoch [35/50] batch [25/49] time 0.177 (0.203) data 0.000 (0.025) loss 3.5273 (3.2325) acc 75.0000 (74.1250) lr 5.1825e-04 eta 0:02:33
epoch [35/50] batch [30/49] time 0.176 (0.198) data 0.000 (0.021) loss 2.8496 (3.2172) acc 78.1250 (73.7500) lr 5.1825e-04 eta 0:02:29
epoch [35/50] batch [35/49] time 0.176 (0.195) data 0.000 (0.018) loss 2.5449 (3.2174) acc 78.1250 (73.4821) lr 5.1825e-04 eta 0:02:26
epoch [35/50] batch [40/49] time 0.178 (0.193) data 0.000 (0.016) loss 3.1953 (3.2242) acc 68.7500 (73.7500) lr 5.1825e-04 eta 0:02:23
epoch [35/50] batch [45/49] time 0.177 (0.191) data 0.000 (0.014) loss 3.0078 (3.2077) acc 75.0000 (73.7500) lr 5.1825e-04 eta 0:02:21
epoch [36/50] batch [5/49] time 0.177 (0.304) data 0.000 (0.125) loss 2.9023 (3.2672) acc 81.2500 (74.3750) lr 4.6417e-04 eta 0:03:41
epoch [36/50] batch [10/49] time 0.180 (0.240) data 0.000 (0.062) loss 3.5352 (3.3158) acc 65.6250 (72.8125) lr 4.6417e-04 eta 0:02:54
epoch [36/50] batch [15/49] time 0.176 (0.219) data 0.000 (0.042) loss 3.3242 (3.2499) acc 75.0000 (73.1250) lr 4.6417e-04 eta 0:02:37
epoch [36/50] batch [20/49] time 0.180 (0.208) data 0.004 (0.032) loss 2.7734 (3.2280) acc 78.1250 (74.3750) lr 4.6417e-04 eta 0:02:29
epoch [36/50] batch [25/49] time 0.177 (0.202) data 0.000 (0.025) loss 3.0176 (3.2034) acc 78.1250 (75.2500) lr 4.6417e-04 eta 0:02:23
epoch [36/50] batch [30/49] time 0.179 (0.198) data 0.000 (0.021) loss 3.0430 (3.1844) acc 75.0000 (75.2083) lr 4.6417e-04 eta 0:02:19
epoch [36/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.018) loss 2.9844 (3.1675) acc 68.7500 (74.7321) lr 4.6417e-04 eta 0:02:16
epoch [36/50] batch [40/49] time 0.176 (0.192) data 0.000 (0.016) loss 2.6133 (3.1330) acc 93.7500 (75.6250) lr 4.6417e-04 eta 0:02:13
epoch [36/50] batch [45/49] time 0.176 (0.190) data 0.000 (0.014) loss 3.2988 (3.1524) acc 75.0000 (75.1389) lr 4.6417e-04 eta 0:02:11
epoch [37/50] batch [5/49] time 0.177 (0.320) data 0.000 (0.142) loss 2.5879 (3.0434) acc 84.3750 (73.7500) lr 4.1221e-04 eta 0:03:37
epoch [37/50] batch [10/49] time 0.177 (0.248) data 0.000 (0.071) loss 3.0156 (3.1229) acc 87.5000 (73.7500) lr 4.1221e-04 eta 0:02:47
epoch [37/50] batch [15/49] time 0.179 (0.225) data 0.000 (0.048) loss 3.5312 (3.2384) acc 65.6250 (72.7083) lr 4.1221e-04 eta 0:02:30
epoch [37/50] batch [20/49] time 0.177 (0.213) data 0.000 (0.036) loss 3.1992 (3.2008) acc 65.6250 (72.9688) lr 4.1221e-04 eta 0:02:21
epoch [37/50] batch [25/49] time 0.177 (0.206) data 0.000 (0.029) loss 2.9434 (3.1700) acc 71.8750 (73.7500) lr 4.1221e-04 eta 0:02:15
epoch [37/50] batch [30/49] time 0.177 (0.201) data 0.000 (0.024) loss 3.0801 (3.1715) acc 87.5000 (74.0625) lr 4.1221e-04 eta 0:02:11
epoch [37/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.021) loss 2.6172 (3.1338) acc 81.2500 (75.0893) lr 4.1221e-04 eta 0:02:08
epoch [37/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.1211 (3.1429) acc 78.1250 (75.1562) lr 4.1221e-04 eta 0:02:05
epoch [37/50] batch [45/49] time 0.177 (0.192) data 0.000 (0.016) loss 3.3281 (3.1586) acc 68.7500 (74.7222) lr 4.1221e-04 eta 0:02:03
epoch [38/50] batch [5/49] time 0.177 (0.335) data 0.000 (0.158) loss 3.0957 (2.9289) acc 71.8750 (79.3750) lr 3.6258e-04 eta 0:03:31
epoch [38/50] batch [10/49] time 0.176 (0.256) data 0.000 (0.079) loss 2.9980 (2.9211) acc 84.3750 (77.5000) lr 3.6258e-04 eta 0:02:40
epoch [38/50] batch [15/49] time 0.178 (0.230) data 0.000 (0.053) loss 3.3906 (3.0612) acc 75.0000 (76.0417) lr 3.6258e-04 eta 0:02:23
epoch [38/50] batch [20/49] time 0.176 (0.217) data 0.000 (0.040) loss 2.7520 (3.0281) acc 78.1250 (76.7188) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [25/49] time 0.175 (0.209) data 0.000 (0.032) loss 3.1699 (3.0231) acc 59.3750 (76.5000) lr 3.6258e-04 eta 0:02:07
epoch [38/50] batch [30/49] time 0.181 (0.204) data 0.000 (0.027) loss 3.0469 (3.0255) acc 71.8750 (76.2500) lr 3.6258e-04 eta 0:02:03
epoch [38/50] batch [35/49] time 0.176 (0.200) data 0.000 (0.023) loss 3.5508 (3.0475) acc 75.0000 (76.6964) lr 3.6258e-04 eta 0:02:00
epoch [38/50] batch [40/49] time 0.176 (0.197) data 0.000 (0.020) loss 2.7773 (3.0495) acc 84.3750 (76.8750) lr 3.6258e-04 eta 0:01:57
epoch [38/50] batch [45/49] time 0.176 (0.195) data 0.000 (0.018) loss 3.6523 (3.0796) acc 68.7500 (76.3889) lr 3.6258e-04 eta 0:01:55
epoch [39/50] batch [5/49] time 0.177 (0.348) data 0.000 (0.171) loss 3.5566 (3.1527) acc 62.5000 (73.7500) lr 3.1545e-04 eta 0:03:22
epoch [39/50] batch [10/49] time 0.176 (0.262) data 0.000 (0.086) loss 2.8008 (3.1283) acc 81.2500 (73.1250) lr 3.1545e-04 eta 0:02:31
epoch [39/50] batch [15/49] time 0.177 (0.234) data 0.000 (0.057) loss 3.3789 (3.0473) acc 71.8750 (75.6250) lr 3.1545e-04 eta 0:02:13
epoch [39/50] batch [20/49] time 0.177 (0.219) data 0.000 (0.043) loss 3.0840 (3.0262) acc 78.1250 (76.4062) lr 3.1545e-04 eta 0:02:04
epoch [39/50] batch [25/49] time 0.178 (0.211) data 0.000 (0.034) loss 3.0586 (3.0411) acc 81.2500 (76.6250) lr 3.1545e-04 eta 0:01:58
epoch [39/50] batch [30/49] time 0.178 (0.205) data 0.000 (0.029) loss 3.1543 (3.0452) acc 71.8750 (76.7708) lr 3.1545e-04 eta 0:01:54
epoch [39/50] batch [35/49] time 0.177 (0.201) data 0.000 (0.025) loss 3.1133 (3.0631) acc 78.1250 (76.7857) lr 3.1545e-04 eta 0:01:51
epoch [39/50] batch [40/49] time 0.176 (0.198) data 0.000 (0.022) loss 3.0137 (3.0577) acc 71.8750 (76.7188) lr 3.1545e-04 eta 0:01:48
epoch [39/50] batch [45/49] time 0.177 (0.196) data 0.000 (0.019) loss 3.5020 (3.0828) acc 71.8750 (76.0417) lr 3.1545e-04 eta 0:01:46
epoch [40/50] batch [5/49] time 0.177 (0.367) data 0.000 (0.188) loss 2.5625 (3.2188) acc 90.6250 (75.6250) lr 2.7103e-04 eta 0:03:15
epoch [40/50] batch [10/49] time 0.177 (0.272) data 0.000 (0.094) loss 3.2617 (3.1057) acc 75.0000 (78.1250) lr 2.7103e-04 eta 0:02:23
epoch [40/50] batch [15/49] time 0.177 (0.240) data 0.000 (0.063) loss 3.5312 (3.1138) acc 68.7500 (77.2917) lr 2.7103e-04 eta 0:02:05
epoch [40/50] batch [20/49] time 0.176 (0.225) data 0.000 (0.047) loss 3.0742 (3.0772) acc 81.2500 (78.1250) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [25/49] time 0.178 (0.215) data 0.000 (0.038) loss 3.0000 (3.0263) acc 75.0000 (78.6250) lr 2.7103e-04 eta 0:01:50
epoch [40/50] batch [30/49] time 0.178 (0.209) data 0.000 (0.032) loss 3.0977 (3.0408) acc 78.1250 (78.6458) lr 2.7103e-04 eta 0:01:46
epoch [40/50] batch [35/49] time 0.178 (0.205) data 0.000 (0.027) loss 3.2715 (3.0600) acc 78.1250 (78.3036) lr 2.7103e-04 eta 0:01:43
epoch [40/50] batch [40/49] time 0.179 (0.201) data 0.000 (0.024) loss 2.5273 (3.0187) acc 75.0000 (78.6719) lr 2.7103e-04 eta 0:01:40
epoch [40/50] batch [45/49] time 0.177 (0.199) data 0.000 (0.021) loss 3.2578 (3.0195) acc 75.0000 (78.6806) lr 2.7103e-04 eta 0:01:38
epoch [41/50] batch [5/49] time 0.178 (0.315) data 0.000 (0.137) loss 3.2148 (2.8687) acc 75.0000 (78.1250) lr 2.2949e-04 eta 0:02:32
epoch [41/50] batch [10/49] time 0.178 (0.247) data 0.000 (0.069) loss 2.7109 (2.8484) acc 81.2500 (77.8125) lr 2.2949e-04 eta 0:01:58
epoch [41/50] batch [15/49] time 0.177 (0.224) data 0.000 (0.046) loss 3.1660 (2.9371) acc 78.1250 (77.7083) lr 2.2949e-04 eta 0:01:46
epoch [41/50] batch [20/49] time 0.177 (0.212) data 0.000 (0.034) loss 3.7812 (2.9851) acc 75.0000 (78.1250) lr 2.2949e-04 eta 0:01:39
epoch [41/50] batch [25/49] time 0.178 (0.205) data 0.001 (0.028) loss 3.0020 (3.0234) acc 78.1250 (77.0000) lr 2.2949e-04 eta 0:01:35
epoch [41/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.023) loss 3.1250 (3.0214) acc 81.2500 (77.5000) lr 2.2949e-04 eta 0:01:32
epoch [41/50] batch [35/49] time 0.177 (0.197) data 0.000 (0.020) loss 2.8828 (3.0022) acc 75.0000 (77.5000) lr 2.2949e-04 eta 0:01:29
epoch [41/50] batch [40/49] time 0.177 (0.195) data 0.000 (0.017) loss 3.4453 (3.0364) acc 65.6250 (76.7969) lr 2.2949e-04 eta 0:01:27
epoch [41/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.015) loss 3.5469 (3.0534) acc 68.7500 (76.5278) lr 2.2949e-04 eta 0:01:25
epoch [42/50] batch [5/49] time 0.177 (0.320) data 0.000 (0.142) loss 3.4141 (3.0992) acc 75.0000 (76.8750) lr 1.9098e-04 eta 0:02:19
epoch [42/50] batch [10/49] time 0.177 (0.248) data 0.000 (0.071) loss 2.8496 (3.1486) acc 87.5000 (76.8750) lr 1.9098e-04 eta 0:01:47
epoch [42/50] batch [15/49] time 0.177 (0.225) data 0.000 (0.048) loss 2.6445 (3.0249) acc 87.5000 (78.9583) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [20/49] time 0.177 (0.213) data 0.000 (0.036) loss 3.1465 (2.9957) acc 68.7500 (78.5938) lr 1.9098e-04 eta 0:01:29
epoch [42/50] batch [25/49] time 0.176 (0.205) data 0.000 (0.029) loss 3.1875 (3.0079) acc 75.0000 (79.0000) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [30/49] time 0.177 (0.201) data 0.000 (0.024) loss 2.8848 (3.0204) acc 84.3750 (78.8542) lr 1.9098e-04 eta 0:01:22
epoch [42/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.021) loss 3.4902 (3.0716) acc 71.8750 (77.9464) lr 1.9098e-04 eta 0:01:20
epoch [42/50] batch [40/49] time 0.176 (0.195) data 0.000 (0.018) loss 3.1836 (3.0884) acc 84.3750 (77.8906) lr 1.9098e-04 eta 0:01:18
epoch [42/50] batch [45/49] time 0.176 (0.193) data 0.000 (0.016) loss 2.9375 (3.0750) acc 71.8750 (77.7083) lr 1.9098e-04 eta 0:01:16
epoch [43/50] batch [5/49] time 0.177 (0.299) data 0.000 (0.121) loss 2.9160 (3.0270) acc 84.3750 (77.5000) lr 1.5567e-04 eta 0:01:55
epoch [43/50] batch [10/49] time 0.177 (0.238) data 0.000 (0.061) loss 3.2305 (3.0674) acc 93.7500 (77.8125) lr 1.5567e-04 eta 0:01:30
epoch [43/50] batch [15/49] time 0.178 (0.218) data 0.000 (0.041) loss 3.1504 (3.1051) acc 81.2500 (77.0833) lr 1.5567e-04 eta 0:01:22
epoch [43/50] batch [20/49] time 0.177 (0.208) data 0.000 (0.031) loss 3.2285 (3.1334) acc 87.5000 (76.5625) lr 1.5567e-04 eta 0:01:17
epoch [43/50] batch [25/49] time 0.177 (0.201) data 0.000 (0.024) loss 3.7656 (3.1361) acc 71.8750 (76.5000) lr 1.5567e-04 eta 0:01:13
epoch [43/50] batch [30/49] time 0.178 (0.197) data 0.000 (0.020) loss 2.4199 (3.1036) acc 93.7500 (76.2500) lr 1.5567e-04 eta 0:01:11
epoch [43/50] batch [35/49] time 0.176 (0.195) data 0.000 (0.018) loss 3.7734 (3.0968) acc 68.7500 (76.3393) lr 1.5567e-04 eta 0:01:09
epoch [43/50] batch [40/49] time 0.176 (0.192) data 0.000 (0.015) loss 3.1328 (3.0904) acc 78.1250 (76.9531) lr 1.5567e-04 eta 0:01:07
epoch [43/50] batch [45/49] time 0.176 (0.190) data 0.000 (0.014) loss 2.8086 (3.0481) acc 87.5000 (77.5694) lr 1.5567e-04 eta 0:01:06
epoch [44/50] batch [5/49] time 0.176 (0.317) data 0.000 (0.139) loss 2.8633 (3.0195) acc 87.5000 (78.1250) lr 1.2369e-04 eta 0:01:47
epoch [44/50] batch [10/49] time 0.178 (0.247) data 0.000 (0.070) loss 2.9531 (3.0510) acc 84.3750 (79.0625) lr 1.2369e-04 eta 0:01:22
epoch [44/50] batch [15/49] time 0.176 (0.224) data 0.000 (0.047) loss 3.2383 (3.1333) acc 75.0000 (77.0833) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [20/49] time 0.178 (0.212) data 0.000 (0.035) loss 3.2266 (3.1058) acc 78.1250 (77.8125) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [25/49] time 0.177 (0.206) data 0.000 (0.028) loss 2.7148 (3.1648) acc 81.2500 (76.2500) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [30/49] time 0.178 (0.201) data 0.001 (0.023) loss 3.2852 (3.1502) acc 78.1250 (76.2500) lr 1.2369e-04 eta 0:01:02
epoch [44/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.020) loss 2.8379 (3.1056) acc 78.1250 (76.6071) lr 1.2369e-04 eta 0:01:00
epoch [44/50] batch [40/49] time 0.176 (0.195) data 0.000 (0.018) loss 3.2285 (3.1282) acc 71.8750 (76.1719) lr 1.2369e-04 eta 0:00:58
epoch [44/50] batch [45/49] time 0.176 (0.193) data 0.000 (0.016) loss 3.3320 (3.1534) acc 75.0000 (75.9028) lr 1.2369e-04 eta 0:00:57
epoch [45/50] batch [5/49] time 0.177 (0.319) data 0.000 (0.141) loss 2.6992 (2.6867) acc 87.5000 (86.8750) lr 9.5173e-05 eta 0:01:32
epoch [45/50] batch [10/49] time 0.178 (0.249) data 0.000 (0.071) loss 2.8750 (2.7768) acc 81.2500 (83.4375) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [15/49] time 0.178 (0.225) data 0.000 (0.047) loss 2.8555 (2.9324) acc 78.1250 (80.6250) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [20/49] time 0.177 (0.213) data 0.000 (0.035) loss 2.8789 (2.8969) acc 81.2500 (80.6250) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [25/49] time 0.177 (0.206) data 0.000 (0.028) loss 3.3906 (2.9989) acc 65.6250 (78.7500) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [30/49] time 0.177 (0.201) data 0.000 (0.024) loss 3.1387 (2.9903) acc 81.2500 (78.7500) lr 9.5173e-05 eta 0:00:53
epoch [45/50] batch [35/49] time 0.177 (0.198) data 0.000 (0.020) loss 3.2773 (3.0069) acc 75.0000 (78.0357) lr 9.5173e-05 eta 0:00:51
epoch [45/50] batch [40/49] time 0.175 (0.195) data 0.000 (0.018) loss 3.9922 (3.0271) acc 65.6250 (77.8125) lr 9.5173e-05 eta 0:00:49
epoch [45/50] batch [45/49] time 0.176 (0.193) data 0.000 (0.016) loss 2.7383 (3.0206) acc 84.3750 (77.8472) lr 9.5173e-05 eta 0:00:47
epoch [46/50] batch [5/49] time 0.176 (0.333) data 0.000 (0.156) loss 3.2246 (3.0137) acc 78.1250 (80.0000) lr 7.0224e-05 eta 0:01:19
epoch [46/50] batch [10/49] time 0.177 (0.255) data 0.000 (0.078) loss 3.4805 (3.1523) acc 71.8750 (76.5625) lr 7.0224e-05 eta 0:00:59
epoch [46/50] batch [15/49] time 0.179 (0.229) data 0.000 (0.052) loss 2.5078 (3.1549) acc 81.2500 (75.6250) lr 7.0224e-05 eta 0:00:52
epoch [46/50] batch [20/49] time 0.176 (0.216) data 0.000 (0.039) loss 3.0215 (3.1507) acc 81.2500 (77.1875) lr 7.0224e-05 eta 0:00:48
epoch [46/50] batch [25/49] time 0.177 (0.208) data 0.000 (0.031) loss 3.1152 (3.1436) acc 75.0000 (76.2500) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [30/49] time 0.177 (0.203) data 0.000 (0.026) loss 2.9648 (3.1011) acc 78.1250 (76.6667) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.022) loss 2.9609 (3.1243) acc 84.3750 (76.9643) lr 7.0224e-05 eta 0:00:41
epoch [46/50] batch [40/49] time 0.176 (0.196) data 0.000 (0.020) loss 3.1641 (3.1266) acc 71.8750 (76.7188) lr 7.0224e-05 eta 0:00:40
epoch [46/50] batch [45/49] time 0.176 (0.194) data 0.000 (0.018) loss 2.8164 (3.1261) acc 81.2500 (76.3194) lr 7.0224e-05 eta 0:00:38
epoch [47/50] batch [5/49] time 0.176 (0.317) data 0.000 (0.139) loss 2.7148 (2.8996) acc 87.5000 (78.7500) lr 4.8943e-05 eta 0:01:00
epoch [47/50] batch [10/49] time 0.177 (0.247) data 0.000 (0.070) loss 3.1895 (2.9531) acc 75.0000 (76.2500) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [15/49] time 0.178 (0.224) data 0.000 (0.046) loss 2.6230 (2.9884) acc 84.3750 (75.4167) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [20/49] time 0.177 (0.212) data 0.000 (0.035) loss 3.7852 (3.0438) acc 46.8750 (74.3750) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [25/49] time 0.177 (0.205) data 0.000 (0.028) loss 2.6543 (3.0101) acc 90.6250 (75.7500) lr 4.8943e-05 eta 0:00:35
epoch [47/50] batch [30/49] time 0.180 (0.201) data 0.000 (0.023) loss 3.1348 (3.0040) acc 75.0000 (76.1458) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [35/49] time 0.176 (0.198) data 0.000 (0.020) loss 2.8770 (3.0112) acc 81.2500 (76.3393) lr 4.8943e-05 eta 0:00:31
epoch [47/50] batch [40/49] time 0.177 (0.195) data 0.000 (0.018) loss 3.0938 (3.0154) acc 90.6250 (76.7188) lr 4.8943e-05 eta 0:00:30
epoch [47/50] batch [45/49] time 0.177 (0.193) data 0.000 (0.016) loss 3.5527 (3.0355) acc 68.7500 (76.1806) lr 4.8943e-05 eta 0:00:29
epoch [48/50] batch [5/49] time 0.176 (0.295) data 0.000 (0.118) loss 2.9688 (3.3539) acc 81.2500 (76.2500) lr 3.1417e-05 eta 0:00:41
epoch [48/50] batch [10/49] time 0.177 (0.236) data 0.000 (0.059) loss 2.7812 (3.2217) acc 81.2500 (76.5625) lr 3.1417e-05 eta 0:00:32
epoch [48/50] batch [15/49] time 0.176 (0.216) data 0.000 (0.039) loss 2.8535 (3.2128) acc 87.5000 (76.8750) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [20/49] time 0.176 (0.207) data 0.000 (0.030) loss 3.3633 (3.2194) acc 68.7500 (76.2500) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [25/49] time 0.177 (0.201) data 0.000 (0.024) loss 3.4453 (3.2166) acc 71.8750 (76.0000) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [30/49] time 0.177 (0.197) data 0.000 (0.020) loss 3.0664 (3.2021) acc 78.1250 (76.4583) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [35/49] time 0.176 (0.194) data 0.000 (0.017) loss 3.3203 (3.1841) acc 78.1250 (76.8750) lr 3.1417e-05 eta 0:00:21
epoch [48/50] batch [40/49] time 0.176 (0.192) data 0.000 (0.015) loss 3.0371 (3.1669) acc 68.7500 (76.7188) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [45/49] time 0.176 (0.190) data 0.000 (0.013) loss 3.3652 (3.1444) acc 71.8750 (76.9444) lr 3.1417e-05 eta 0:00:19
epoch [49/50] batch [5/49] time 0.177 (0.312) data 0.000 (0.135) loss 2.7773 (2.8195) acc 78.1250 (83.1250) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [10/49] time 0.177 (0.245) data 0.000 (0.067) loss 2.9512 (3.0229) acc 81.2500 (79.3750) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [15/49] time 0.177 (0.222) data 0.000 (0.045) loss 3.1758 (2.9931) acc 78.1250 (78.5417) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [20/49] time 0.177 (0.211) data 0.000 (0.034) loss 3.0684 (3.0283) acc 78.1250 (77.5000) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [25/49] time 0.177 (0.204) data 0.000 (0.027) loss 3.0059 (3.0200) acc 75.0000 (77.1250) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.023) loss 3.1719 (3.0329) acc 81.2500 (77.2917) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [35/49] time 0.176 (0.196) data 0.000 (0.019) loss 3.0430 (3.0373) acc 87.5000 (77.1429) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [40/49] time 0.177 (0.194) data 0.000 (0.017) loss 3.0781 (3.0520) acc 78.1250 (76.7188) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [45/49] time 0.176 (0.192) data 0.000 (0.015) loss 3.3184 (3.0395) acc 71.8750 (77.0833) lr 1.7713e-05 eta 0:00:10
epoch [50/50] batch [5/49] time 0.175 (0.338) data 0.000 (0.162) loss 2.5762 (3.1277) acc 84.3750 (74.3750) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [10/49] time 0.177 (0.257) data 0.000 (0.081) loss 3.4766 (3.0768) acc 81.2500 (76.8750) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [15/49] time 0.176 (0.230) data 0.000 (0.054) loss 2.4883 (3.0339) acc 81.2500 (77.7083) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [20/49] time 0.177 (0.217) data 0.000 (0.041) loss 2.5977 (3.0067) acc 87.5000 (77.8125) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [25/49] time 0.180 (0.209) data 0.000 (0.032) loss 3.3613 (3.0120) acc 75.0000 (78.1250) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [30/49] time 0.176 (0.204) data 0.000 (0.027) loss 3.9375 (3.0184) acc 59.3750 (78.0208) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [35/49] time 0.176 (0.200) data 0.000 (0.023) loss 3.2383 (3.0056) acc 68.7500 (78.5714) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [40/49] time 0.176 (0.197) data 0.000 (0.020) loss 3.2266 (3.0025) acc 71.8750 (78.3594) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [45/49] time 0.176 (0.194) data 0.000 (0.018) loss 3.6582 (3.0249) acc 59.3750 (77.9167) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:07<00:57,  7.21s/it] 22%|██▏       | 2/9 [00:08<00:25,  3.64s/it] 33%|███▎      | 3/9 [00:09<00:15,  2.50s/it] 44%|████▍     | 4/9 [00:10<00:09,  1.97s/it] 56%|█████▌    | 5/9 [00:11<00:06,  1.67s/it] 67%|██████▋   | 6/9 [00:12<00:04,  1.49s/it] 78%|███████▊  | 7/9 [00:14<00:02,  1.38s/it] 89%|████████▉ | 8/9 [00:15<00:01,  1.31s/it]100%|██████████| 9/9 [00:15<00:00,  1.71s/it]
=> result
* total: 4,002
* correct: 3,244
* accuracy: 81.1%
* error: 18.9%
* macro_f1: 80.7%
Elapsed: 0:08:11
Run this job and save the output to output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,002
---------  ------------
['2000 AM General Hummer SUV', '2012 Acura RL Sedan', '2012 Acura TL Sedan', '2008 Acura TL Type-S', '2012 Acura TSX Sedan', '2001 Acura Integra Type R', '2012 Acura ZDX Hatchback', '2012 Aston Martin V8 Vantage Convertible', '2012 Aston Martin V8 Vantage Coupe', '2012 Aston Martin Virage Convertible', '2012 Aston Martin Virage Coupe', '2008 Audi RS 4 Convertible', '2012 Audi A5 Coupe', '2012 Audi TTS Coupe', '2012 Audi R8 Coupe', '1994 Audi V8 Sedan', '1994 Audi 100 Sedan', '1994 Audi 100 Wagon', '2011 Audi TT Hatchback', '2011 Audi S6 Sedan', '2012 Audi S5 Convertible', '2012 Audi S5 Coupe', '2012 Audi S4 Sedan', '2007 Audi S4 Sedan', '2012 Audi TT RS Coupe', '2012 BMW ActiveHybrid 5 Sedan', '2012 BMW 1 Series Convertible', '2012 BMW 1 Series Coupe', '2012 BMW 3 Series Sedan', '2012 BMW 3 Series Wagon', '2007 BMW 6 Series Convertible', '2007 BMW X5 SUV', '2012 BMW X6 SUV', '2012 BMW M3 Coupe', '2010 BMW M5 Sedan', '2010 BMW M6 Convertible', '2012 BMW X3 SUV', '2012 BMW Z4 Convertible', '2012 Bentley Continental Supersports Conv. Convertible', '2009 Bentley Arnage Sedan', '2011 Bentley Mulsanne Sedan', '2012 Bentley Continental GT Coupe', '2007 Bentley Continental GT Coupe', '2007 Bentley Continental Flying Spur Sedan', '2009 Bugatti Veyron 16.4 Convertible', '2009 Bugatti Veyron 16.4 Coupe', '2012 Buick Regal GS', '2007 Buick Rainier SUV', '2012 Buick Verano Sedan', '2012 Buick Enclave SUV', '2012 Cadillac CTS-V Sedan', '2012 Cadillac SRX SUV', '2007 Cadillac Escalade EXT Crew Cab', '2012 Chevrolet Silverado 1500 Hybrid Crew Cab', '2012 Chevrolet Corvette Convertible', '2012 Chevrolet Corvette ZR1', '2007 Chevrolet Corvette Ron Fellows Edition Z06', '2012 Chevrolet Traverse SUV', '2012 Chevrolet Camaro Convertible', '2010 Chevrolet HHR SS', '2007 Chevrolet Impala Sedan', '2012 Chevrolet Tahoe Hybrid SUV', '2012 Chevrolet Sonic Sedan', '2007 Chevrolet Express Cargo Van', '2012 Chevrolet Avalanche Crew Cab', '2010 Chevrolet Cobalt SS', '2010 Chevrolet Malibu Hybrid Sedan', '2009 Chevrolet TrailBlazer SS', '2012 Chevrolet Silverado 2500HD Regular Cab', '2007 Chevrolet Silverado 1500 Classic Extended Cab', '2007 Chevrolet Express Van', '2007 Chevrolet Monte Carlo Coupe', '2007 Chevrolet Malibu Sedan', '2012 Chevrolet Silverado 1500 Extended Cab', '2012 Chevrolet Silverado 1500 Regular Cab', '2009 Chrysler Aspen SUV', '2010 Chrysler Sebring Convertible', '2012 Chrysler Town and Country Minivan', '2010 Chrysler 300 SRT-8', '2008 Chrysler Crossfire Convertible', '2008 Chrysler PT Cruiser Convertible', '2002 Daewoo Nubira Wagon', '2012 Dodge Caliber Wagon', '2007 Dodge Caliber Wagon', '1997 Dodge Caravan Minivan', '2010 Dodge Ram Pickup 3500 Crew Cab', '2009 Dodge Ram Pickup 3500 Quad Cab', '2009 Dodge Sprinter Cargo Van', '2012 Dodge Journey SUV', '2010 Dodge Dakota Crew Cab', '2007 Dodge Dakota Club Cab', '2008 Dodge Magnum Wagon', '2011 Dodge Challenger SRT8', '2012 Dodge Durango SUV', '2007 Dodge Durango SUV', '2012 Dodge Charger Sedan', '2009 Dodge Charger SRT-8', '1998 Eagle Talon Hatchback']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2000 AM General Hummer SUV, a type of car', 'X X X X 2012 Acura RL Sedan, a type of car', 'X X X X 2012 Acura TL Sedan, a type of car', 'X X X X 2008 Acura TL Type-S, a type of car', 'X X X X 2012 Acura TSX Sedan, a type of car', 'X X X X 2001 Acura Integra Type R, a type of car', 'X X X X 2012 Acura ZDX Hatchback, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Convertible, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Coupe, a type of car', 'X X X X 2012 Aston Martin Virage Convertible, a type of car', 'X X X X 2012 Aston Martin Virage Coupe, a type of car', 'X X X X 2008 Audi RS 4 Convertible, a type of car', 'X X X X 2012 Audi A5 Coupe, a type of car', 'X X X X 2012 Audi TTS Coupe, a type of car', 'X X X X 2012 Audi R8 Coupe, a type of car', 'X X X X 1994 Audi V8 Sedan, a type of car', 'X X X X 1994 Audi 100 Sedan, a type of car', 'X X X X 1994 Audi 100 Wagon, a type of car', 'X X X X 2011 Audi TT Hatchback, a type of car', 'X X X X 2011 Audi S6 Sedan, a type of car', 'X X X X 2012 Audi S5 Convertible, a type of car', 'X X X X 2012 Audi S5 Coupe, a type of car', 'X X X X 2012 Audi S4 Sedan, a type of car', 'X X X X 2007 Audi S4 Sedan, a type of car', 'X X X X 2012 Audi TT RS Coupe, a type of car', 'X X X X 2012 BMW ActiveHybrid 5 Sedan, a type of car', 'X X X X 2012 BMW 1 Series Convertible, a type of car', 'X X X X 2012 BMW 1 Series Coupe, a type of car', 'X X X X 2012 BMW 3 Series Sedan, a type of car', 'X X X X 2012 BMW 3 Series Wagon, a type of car', 'X X X X 2007 BMW 6 Series Convertible, a type of car', 'X X X X 2007 BMW X5 SUV, a type of car', 'X X X X 2012 BMW X6 SUV, a type of car', 'X X X X 2012 BMW M3 Coupe, a type of car', 'X X X X 2010 BMW M5 Sedan, a type of car', 'X X X X 2010 BMW M6 Convertible, a type of car', 'X X X X 2012 BMW X3 SUV, a type of car', 'X X X X 2012 BMW Z4 Convertible, a type of car', 'X X X X 2012 Bentley Continental Supersports Conv. Convertible, a type of car', 'X X X X 2009 Bentley Arnage Sedan, a type of car', 'X X X X 2011 Bentley Mulsanne Sedan, a type of car', 'X X X X 2012 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental Flying Spur Sedan, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Convertible, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Coupe, a type of car', 'X X X X 2012 Buick Regal GS, a type of car', 'X X X X 2007 Buick Rainier SUV, a type of car', 'X X X X 2012 Buick Verano Sedan, a type of car', 'X X X X 2012 Buick Enclave SUV, a type of car', 'X X X X 2012 Cadillac CTS-V Sedan, a type of car', 'X X X X 2012 Cadillac SRX SUV, a type of car', 'X X X X 2007 Cadillac Escalade EXT Crew Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Hybrid Crew Cab, a type of car', 'X X X X 2012 Chevrolet Corvette Convertible, a type of car', 'X X X X 2012 Chevrolet Corvette ZR1, a type of car', 'X X X X 2007 Chevrolet Corvette Ron Fellows Edition Z06, a type of car', 'X X X X 2012 Chevrolet Traverse SUV, a type of car', 'X X X X 2012 Chevrolet Camaro Convertible, a type of car', 'X X X X 2010 Chevrolet HHR SS, a type of car', 'X X X X 2007 Chevrolet Impala Sedan, a type of car', 'X X X X 2012 Chevrolet Tahoe Hybrid SUV, a type of car', 'X X X X 2012 Chevrolet Sonic Sedan, a type of car', 'X X X X 2007 Chevrolet Express Cargo Van, a type of car', 'X X X X 2012 Chevrolet Avalanche Crew Cab, a type of car', 'X X X X 2010 Chevrolet Cobalt SS, a type of car', 'X X X X 2010 Chevrolet Malibu Hybrid Sedan, a type of car', 'X X X X 2009 Chevrolet TrailBlazer SS, a type of car', 'X X X X 2012 Chevrolet Silverado 2500HD Regular Cab, a type of car', 'X X X X 2007 Chevrolet Silverado 1500 Classic Extended Cab, a type of car', 'X X X X 2007 Chevrolet Express Van, a type of car', 'X X X X 2007 Chevrolet Monte Carlo Coupe, a type of car', 'X X X X 2007 Chevrolet Malibu Sedan, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Extended Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Regular Cab, a type of car', 'X X X X 2009 Chrysler Aspen SUV, a type of car', 'X X X X 2010 Chrysler Sebring Convertible, a type of car', 'X X X X 2012 Chrysler Town and Country Minivan, a type of car', 'X X X X 2010 Chrysler 300 SRT-8, a type of car', 'X X X X 2008 Chrysler Crossfire Convertible, a type of car', 'X X X X 2008 Chrysler PT Cruiser Convertible, a type of car', 'X X X X 2002 Daewoo Nubira Wagon, a type of car', 'X X X X 2012 Dodge Caliber Wagon, a type of car', 'X X X X 2007 Dodge Caliber Wagon, a type of car', 'X X X X 1997 Dodge Caravan Minivan, a type of car', 'X X X X 2010 Dodge Ram Pickup 3500 Crew Cab, a type of car', 'X X X X 2009 Dodge Ram Pickup 3500 Quad Cab, a type of car', 'X X X X 2009 Dodge Sprinter Cargo Van, a type of car', 'X X X X 2012 Dodge Journey SUV, a type of car', 'X X X X 2010 Dodge Dakota Crew Cab, a type of car', 'X X X X 2007 Dodge Dakota Club Cab, a type of car', 'X X X X 2008 Dodge Magnum Wagon, a type of car', 'X X X X 2011 Dodge Challenger SRT8, a type of car', 'X X X X 2012 Dodge Durango SUV, a type of car', 'X X X X 2007 Dodge Durango SUV, a type of car', 'X X X X 2012 Dodge Charger Sedan, a type of car', 'X X X X 2009 Dodge Charger SRT-8, a type of car', 'X X X X 1998 Eagle Talon Hatchback, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([98, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/49] time 0.177 (0.405) data 0.000 (0.217) loss 6.8125 (6.7938) acc 43.7500 (45.0000) lr 1.0000e-05 eta 0:16:29
epoch [1/50] batch [10/49] time 0.176 (0.290) data 0.000 (0.109) loss 7.3867 (6.8145) acc 34.3750 (44.3750) lr 1.0000e-05 eta 0:11:47
epoch [1/50] batch [15/49] time 0.174 (0.251) data 0.000 (0.073) loss 6.5625 (6.8258) acc 46.8750 (42.9167) lr 1.0000e-05 eta 0:10:11
epoch [1/50] batch [20/49] time 0.176 (0.233) data 0.000 (0.054) loss 6.3203 (6.7553) acc 40.6250 (45.6250) lr 1.0000e-05 eta 0:09:25
epoch [1/50] batch [25/49] time 0.175 (0.221) data 0.000 (0.044) loss 6.4688 (6.7408) acc 43.7500 (44.8750) lr 1.0000e-05 eta 0:08:56
epoch [1/50] batch [30/49] time 0.175 (0.214) data 0.000 (0.036) loss 6.7031 (6.7331) acc 43.7500 (45.1042) lr 1.0000e-05 eta 0:08:37
epoch [1/50] batch [35/49] time 0.174 (0.208) data 0.000 (0.031) loss 6.9453 (6.7326) acc 46.8750 (45.8036) lr 1.0000e-05 eta 0:08:22
epoch [1/50] batch [40/49] time 0.174 (0.204) data 0.000 (0.027) loss 6.7461 (6.6865) acc 46.8750 (46.5625) lr 1.0000e-05 eta 0:08:11
epoch [1/50] batch [45/49] time 0.178 (0.201) data 0.000 (0.024) loss 5.9102 (6.6763) acc 65.6250 (46.1111) lr 1.0000e-05 eta 0:08:03
epoch [2/50] batch [5/49] time 0.176 (0.316) data 0.000 (0.140) loss 5.6328 (5.9234) acc 40.6250 (42.5000) lr 2.0000e-03 eta 0:12:37
epoch [2/50] batch [10/49] time 0.175 (0.246) data 0.000 (0.070) loss 5.7109 (5.6211) acc 43.7500 (44.6875) lr 2.0000e-03 eta 0:09:47
epoch [2/50] batch [15/49] time 0.175 (0.222) data 0.000 (0.047) loss 5.1875 (5.5060) acc 40.6250 (45.2083) lr 2.0000e-03 eta 0:08:50
epoch [2/50] batch [20/49] time 0.175 (0.211) data 0.000 (0.035) loss 4.7812 (5.4055) acc 43.7500 (45.3125) lr 2.0000e-03 eta 0:08:21
epoch [2/50] batch [25/49] time 0.176 (0.204) data 0.000 (0.028) loss 4.4727 (5.2375) acc 65.6250 (47.8750) lr 2.0000e-03 eta 0:08:03
epoch [2/50] batch [30/49] time 0.175 (0.199) data 0.000 (0.024) loss 4.8203 (5.1707) acc 53.1250 (48.0208) lr 2.0000e-03 eta 0:07:52
epoch [2/50] batch [35/49] time 0.175 (0.196) data 0.000 (0.020) loss 4.8359 (5.1865) acc 62.5000 (47.5000) lr 2.0000e-03 eta 0:07:43
epoch [2/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.018) loss 4.7734 (5.1204) acc 53.1250 (47.8906) lr 2.0000e-03 eta 0:07:35
epoch [2/50] batch [45/49] time 0.174 (0.191) data 0.000 (0.016) loss 5.1016 (5.0460) acc 40.6250 (48.7500) lr 2.0000e-03 eta 0:07:30
epoch [3/50] batch [5/49] time 0.176 (0.326) data 0.000 (0.150) loss 5.1562 (4.4121) acc 43.7500 (48.1250) lr 1.9980e-03 eta 0:12:45
epoch [3/50] batch [10/49] time 0.176 (0.251) data 0.000 (0.075) loss 4.2031 (4.3533) acc 53.1250 (53.7500) lr 1.9980e-03 eta 0:09:47
epoch [3/50] batch [15/49] time 0.177 (0.226) data 0.001 (0.050) loss 4.4297 (4.3194) acc 59.3750 (56.2500) lr 1.9980e-03 eta 0:08:48
epoch [3/50] batch [20/49] time 0.174 (0.214) data 0.000 (0.038) loss 3.9707 (4.2919) acc 71.8750 (57.3438) lr 1.9980e-03 eta 0:08:18
epoch [3/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.030) loss 4.3320 (4.2716) acc 53.1250 (57.2500) lr 1.9980e-03 eta 0:07:59
epoch [3/50] batch [30/49] time 0.175 (0.201) data 0.000 (0.025) loss 3.8379 (4.2986) acc 71.8750 (57.3958) lr 1.9980e-03 eta 0:07:46
epoch [3/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.022) loss 4.0156 (4.2966) acc 50.0000 (56.6964) lr 1.9980e-03 eta 0:07:37
epoch [3/50] batch [40/49] time 0.176 (0.195) data 0.000 (0.019) loss 4.2852 (4.2785) acc 46.8750 (56.4844) lr 1.9980e-03 eta 0:07:29
epoch [3/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.017) loss 3.9414 (4.2842) acc 59.3750 (55.3472) lr 1.9980e-03 eta 0:07:24
epoch [4/50] batch [5/49] time 0.176 (0.329) data 0.000 (0.152) loss 4.6172 (4.2996) acc 46.8750 (54.3750) lr 1.9921e-03 eta 0:12:35
epoch [4/50] batch [10/49] time 0.175 (0.252) data 0.000 (0.076) loss 3.3125 (4.1227) acc 62.5000 (55.3125) lr 1.9921e-03 eta 0:09:38
epoch [4/50] batch [15/49] time 0.175 (0.227) data 0.000 (0.051) loss 4.0195 (4.1823) acc 65.6250 (56.2500) lr 1.9921e-03 eta 0:08:38
epoch [4/50] batch [20/49] time 0.178 (0.214) data 0.000 (0.038) loss 4.1406 (4.1656) acc 50.0000 (55.0000) lr 1.9921e-03 eta 0:08:08
epoch [4/50] batch [25/49] time 0.175 (0.206) data 0.000 (0.031) loss 3.9883 (4.1348) acc 56.2500 (54.6250) lr 1.9921e-03 eta 0:07:49
epoch [4/50] batch [30/49] time 0.174 (0.201) data 0.000 (0.026) loss 4.3125 (4.1367) acc 56.2500 (54.3750) lr 1.9921e-03 eta 0:07:36
epoch [4/50] batch [35/49] time 0.174 (0.197) data 0.000 (0.022) loss 4.2852 (4.1253) acc 31.2500 (54.2857) lr 1.9921e-03 eta 0:07:27
epoch [4/50] batch [40/49] time 0.174 (0.194) data 0.000 (0.019) loss 4.1367 (4.1095) acc 43.7500 (54.0625) lr 1.9921e-03 eta 0:07:19
epoch [4/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.017) loss 4.0820 (4.1023) acc 59.3750 (54.4444) lr 1.9921e-03 eta 0:07:13
epoch [5/50] batch [5/49] time 0.174 (0.318) data 0.000 (0.142) loss 4.3281 (3.7367) acc 59.3750 (58.1250) lr 1.9823e-03 eta 0:11:54
epoch [5/50] batch [10/49] time 0.175 (0.246) data 0.000 (0.071) loss 3.7090 (3.7781) acc 53.1250 (55.0000) lr 1.9823e-03 eta 0:09:13
epoch [5/50] batch [15/49] time 0.180 (0.223) data 0.000 (0.047) loss 4.2461 (3.8096) acc 53.1250 (55.6250) lr 1.9823e-03 eta 0:08:19
epoch [5/50] batch [20/49] time 0.175 (0.212) data 0.000 (0.036) loss 3.6133 (3.8670) acc 56.2500 (56.0938) lr 1.9823e-03 eta 0:07:52
epoch [5/50] batch [25/49] time 0.175 (0.204) data 0.000 (0.029) loss 3.8555 (3.8795) acc 56.2500 (57.1250) lr 1.9823e-03 eta 0:07:35
epoch [5/50] batch [30/49] time 0.175 (0.200) data 0.000 (0.024) loss 3.6934 (3.8917) acc 65.6250 (56.8750) lr 1.9823e-03 eta 0:07:23
epoch [5/50] batch [35/49] time 0.175 (0.196) data 0.000 (0.020) loss 4.0078 (3.8970) acc 56.2500 (56.6964) lr 1.9823e-03 eta 0:07:15
epoch [5/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.4531 (3.8890) acc 68.7500 (56.6406) lr 1.9823e-03 eta 0:07:08
epoch [5/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.016) loss 4.5117 (3.8975) acc 40.6250 (56.1806) lr 1.9823e-03 eta 0:07:03
epoch [6/50] batch [5/49] time 0.176 (0.345) data 0.000 (0.168) loss 3.7461 (3.9477) acc 53.1250 (52.5000) lr 1.9686e-03 eta 0:12:38
epoch [6/50] batch [10/49] time 0.176 (0.260) data 0.000 (0.084) loss 3.8867 (3.8672) acc 53.1250 (55.6250) lr 1.9686e-03 eta 0:09:31
epoch [6/50] batch [15/49] time 0.176 (0.232) data 0.000 (0.056) loss 3.6016 (3.8357) acc 46.8750 (56.2500) lr 1.9686e-03 eta 0:08:28
epoch [6/50] batch [20/49] time 0.176 (0.218) data 0.000 (0.042) loss 3.7832 (3.8842) acc 43.7500 (55.6250) lr 1.9686e-03 eta 0:07:56
epoch [6/50] batch [25/49] time 0.177 (0.210) data 0.000 (0.034) loss 3.8145 (3.8826) acc 53.1250 (55.3750) lr 1.9686e-03 eta 0:07:36
epoch [6/50] batch [30/49] time 0.175 (0.204) data 0.000 (0.028) loss 3.9707 (3.8729) acc 50.0000 (55.3125) lr 1.9686e-03 eta 0:07:23
epoch [6/50] batch [35/49] time 0.175 (0.200) data 0.000 (0.024) loss 3.5781 (3.8746) acc 59.3750 (54.9107) lr 1.9686e-03 eta 0:07:13
epoch [6/50] batch [40/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.7656 (3.8883) acc 56.2500 (54.4531) lr 1.9686e-03 eta 0:07:05
epoch [6/50] batch [45/49] time 0.176 (0.194) data 0.000 (0.019) loss 3.7305 (3.8849) acc 65.6250 (54.9306) lr 1.9686e-03 eta 0:06:59
epoch [7/50] batch [5/49] time 0.176 (0.341) data 0.000 (0.163) loss 3.7305 (3.7637) acc 46.8750 (54.3750) lr 1.9511e-03 eta 0:12:13
epoch [7/50] batch [10/49] time 0.177 (0.259) data 0.001 (0.082) loss 4.1602 (3.8295) acc 40.6250 (54.3750) lr 1.9511e-03 eta 0:09:15
epoch [7/50] batch [15/49] time 0.176 (0.231) data 0.000 (0.055) loss 2.8398 (3.6935) acc 75.0000 (57.2917) lr 1.9511e-03 eta 0:08:15
epoch [7/50] batch [20/49] time 0.176 (0.217) data 0.000 (0.041) loss 3.4180 (3.7354) acc 62.5000 (57.1875) lr 1.9511e-03 eta 0:07:44
epoch [7/50] batch [25/49] time 0.177 (0.209) data 0.000 (0.033) loss 3.6289 (3.7459) acc 71.8750 (57.5000) lr 1.9511e-03 eta 0:07:25
epoch [7/50] batch [30/49] time 0.177 (0.204) data 0.000 (0.027) loss 3.7930 (3.7364) acc 65.6250 (58.6458) lr 1.9511e-03 eta 0:07:13
epoch [7/50] batch [35/49] time 0.175 (0.200) data 0.000 (0.024) loss 4.3633 (3.7718) acc 56.2500 (57.3214) lr 1.9511e-03 eta 0:07:03
epoch [7/50] batch [40/49] time 0.174 (0.197) data 0.000 (0.021) loss 3.6172 (3.7762) acc 53.1250 (56.9531) lr 1.9511e-03 eta 0:06:55
epoch [7/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.8457 (3.7686) acc 68.7500 (58.0556) lr 1.9511e-03 eta 0:06:49
epoch [8/50] batch [5/49] time 0.176 (0.325) data 0.000 (0.148) loss 3.5703 (3.5863) acc 62.5000 (57.5000) lr 1.9298e-03 eta 0:11:23
epoch [8/50] batch [10/49] time 0.176 (0.251) data 0.000 (0.074) loss 3.8867 (3.5861) acc 46.8750 (56.8750) lr 1.9298e-03 eta 0:08:45
epoch [8/50] batch [15/49] time 0.174 (0.225) data 0.000 (0.050) loss 4.3477 (3.7103) acc 56.2500 (56.2500) lr 1.9298e-03 eta 0:07:51
epoch [8/50] batch [20/49] time 0.179 (0.213) data 0.000 (0.037) loss 4.0742 (3.7695) acc 46.8750 (55.1562) lr 1.9298e-03 eta 0:07:24
epoch [8/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.030) loss 4.0195 (3.7600) acc 43.7500 (56.1250) lr 1.9298e-03 eta 0:07:07
epoch [8/50] batch [30/49] time 0.174 (0.200) data 0.000 (0.025) loss 3.2812 (3.7344) acc 56.2500 (56.7708) lr 1.9298e-03 eta 0:06:56
epoch [8/50] batch [35/49] time 0.174 (0.197) data 0.000 (0.021) loss 3.8887 (3.7222) acc 65.6250 (57.2321) lr 1.9298e-03 eta 0:06:48
epoch [8/50] batch [40/49] time 0.176 (0.194) data 0.000 (0.019) loss 3.8633 (3.7499) acc 53.1250 (56.8750) lr 1.9298e-03 eta 0:06:41
epoch [8/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.017) loss 3.9121 (3.7700) acc 56.2500 (56.3194) lr 1.9298e-03 eta 0:06:35
epoch [9/50] batch [5/49] time 0.176 (0.320) data 0.000 (0.143) loss 3.5430 (3.3723) acc 65.6250 (66.8750) lr 1.9048e-03 eta 0:10:56
epoch [9/50] batch [10/49] time 0.176 (0.248) data 0.000 (0.071) loss 4.1719 (3.4777) acc 46.8750 (64.0625) lr 1.9048e-03 eta 0:08:27
epoch [9/50] batch [15/49] time 0.176 (0.224) data 0.000 (0.048) loss 3.1484 (3.5210) acc 65.6250 (63.3333) lr 1.9048e-03 eta 0:07:36
epoch [9/50] batch [20/49] time 0.175 (0.212) data 0.000 (0.036) loss 3.6211 (3.5451) acc 50.0000 (62.5000) lr 1.9048e-03 eta 0:07:11
epoch [9/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.029) loss 3.5352 (3.5880) acc 68.7500 (61.3750) lr 1.9048e-03 eta 0:06:55
epoch [9/50] batch [30/49] time 0.174 (0.200) data 0.000 (0.024) loss 4.4883 (3.6464) acc 53.1250 (60.9375) lr 1.9048e-03 eta 0:06:45
epoch [9/50] batch [35/49] time 0.175 (0.196) data 0.000 (0.021) loss 3.8164 (3.6432) acc 56.2500 (61.2500) lr 1.9048e-03 eta 0:06:36
epoch [9/50] batch [40/49] time 0.174 (0.193) data 0.000 (0.018) loss 3.5703 (3.6648) acc 59.3750 (60.3125) lr 1.9048e-03 eta 0:06:30
epoch [9/50] batch [45/49] time 0.174 (0.191) data 0.000 (0.016) loss 3.3496 (3.6677) acc 59.3750 (60.2778) lr 1.9048e-03 eta 0:06:25
epoch [10/50] batch [5/49] time 0.175 (0.346) data 0.000 (0.170) loss 3.1309 (3.4895) acc 75.0000 (61.8750) lr 1.8763e-03 eta 0:11:32
epoch [10/50] batch [10/49] time 0.176 (0.260) data 0.000 (0.085) loss 3.7832 (3.5555) acc 59.3750 (60.3125) lr 1.8763e-03 eta 0:08:40
epoch [10/50] batch [15/49] time 0.174 (0.232) data 0.000 (0.057) loss 3.2695 (3.5501) acc 68.7500 (61.8750) lr 1.8763e-03 eta 0:07:41
epoch [10/50] batch [20/49] time 0.175 (0.217) data 0.000 (0.043) loss 3.7422 (3.5339) acc 68.7500 (61.7188) lr 1.8763e-03 eta 0:07:12
epoch [10/50] batch [25/49] time 0.174 (0.209) data 0.000 (0.034) loss 3.3281 (3.5416) acc 81.2500 (62.7500) lr 1.8763e-03 eta 0:06:54
epoch [10/50] batch [30/49] time 0.175 (0.203) data 0.000 (0.028) loss 4.4180 (3.5740) acc 53.1250 (62.2917) lr 1.8763e-03 eta 0:06:42
epoch [10/50] batch [35/49] time 0.174 (0.199) data 0.000 (0.025) loss 3.7852 (3.5819) acc 59.3750 (62.5000) lr 1.8763e-03 eta 0:06:33
epoch [10/50] batch [40/49] time 0.174 (0.196) data 0.000 (0.021) loss 3.3477 (3.5874) acc 59.3750 (62.0312) lr 1.8763e-03 eta 0:06:26
epoch [10/50] batch [45/49] time 0.174 (0.194) data 0.000 (0.019) loss 3.3496 (3.5994) acc 62.5000 (61.5278) lr 1.8763e-03 eta 0:06:20
epoch [11/50] batch [5/49] time 0.176 (0.347) data 0.000 (0.170) loss 4.0508 (3.5922) acc 53.1250 (56.8750) lr 1.8443e-03 eta 0:11:18
epoch [11/50] batch [10/49] time 0.176 (0.261) data 0.000 (0.085) loss 3.9297 (3.6293) acc 43.7500 (55.6250) lr 1.8443e-03 eta 0:08:29
epoch [11/50] batch [15/49] time 0.176 (0.233) data 0.000 (0.057) loss 3.9551 (3.6290) acc 59.3750 (57.5000) lr 1.8443e-03 eta 0:07:32
epoch [11/50] batch [20/49] time 0.176 (0.219) data 0.000 (0.043) loss 3.2891 (3.5564) acc 68.7500 (59.8438) lr 1.8443e-03 eta 0:07:03
epoch [11/50] batch [25/49] time 0.176 (0.210) data 0.000 (0.034) loss 3.0898 (3.5096) acc 68.7500 (60.0000) lr 1.8443e-03 eta 0:06:46
epoch [11/50] batch [30/49] time 0.176 (0.204) data 0.000 (0.029) loss 3.0547 (3.5449) acc 62.5000 (59.3750) lr 1.8443e-03 eta 0:06:34
epoch [11/50] batch [35/49] time 0.174 (0.200) data 0.000 (0.025) loss 3.2578 (3.5675) acc 68.7500 (59.9107) lr 1.8443e-03 eta 0:06:25
epoch [11/50] batch [40/49] time 0.176 (0.197) data 0.000 (0.022) loss 3.4766 (3.5680) acc 62.5000 (59.6094) lr 1.8443e-03 eta 0:06:18
epoch [11/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.019) loss 4.0859 (3.6082) acc 46.8750 (59.1667) lr 1.8443e-03 eta 0:06:12
epoch [12/50] batch [5/49] time 0.176 (0.326) data 0.000 (0.140) loss 4.2734 (3.7105) acc 43.7500 (62.5000) lr 1.8090e-03 eta 0:10:21
epoch [12/50] batch [10/49] time 0.176 (0.251) data 0.000 (0.070) loss 3.1055 (3.6027) acc 68.7500 (64.6875) lr 1.8090e-03 eta 0:07:57
epoch [12/50] batch [15/49] time 0.177 (0.226) data 0.000 (0.047) loss 3.7148 (3.6187) acc 59.3750 (63.5417) lr 1.8090e-03 eta 0:07:09
epoch [12/50] batch [20/49] time 0.176 (0.214) data 0.000 (0.035) loss 3.2891 (3.6327) acc 62.5000 (63.1250) lr 1.8090e-03 eta 0:06:44
epoch [12/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.028) loss 3.3984 (3.5925) acc 59.3750 (64.0000) lr 1.8090e-03 eta 0:06:29
epoch [12/50] batch [30/49] time 0.176 (0.201) data 0.000 (0.024) loss 3.6445 (3.5778) acc 65.6250 (63.5417) lr 1.8090e-03 eta 0:06:18
epoch [12/50] batch [35/49] time 0.174 (0.198) data 0.000 (0.020) loss 3.0215 (3.5401) acc 71.8750 (63.4821) lr 1.8090e-03 eta 0:06:10
epoch [12/50] batch [40/49] time 0.174 (0.195) data 0.000 (0.018) loss 3.6055 (3.5195) acc 46.8750 (63.4375) lr 1.8090e-03 eta 0:06:04
epoch [12/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.016) loss 3.1738 (3.5301) acc 59.3750 (62.7778) lr 1.8090e-03 eta 0:05:59
epoch [13/50] batch [5/49] time 0.176 (0.331) data 0.000 (0.154) loss 3.2168 (3.5555) acc 65.6250 (61.2500) lr 1.7705e-03 eta 0:10:15
epoch [13/50] batch [10/49] time 0.176 (0.254) data 0.000 (0.077) loss 3.5977 (3.6057) acc 62.5000 (61.2500) lr 1.7705e-03 eta 0:07:49
epoch [13/50] batch [15/49] time 0.175 (0.228) data 0.000 (0.052) loss 3.5977 (3.6316) acc 62.5000 (60.4167) lr 1.7705e-03 eta 0:07:00
epoch [13/50] batch [20/49] time 0.179 (0.215) data 0.000 (0.039) loss 3.7266 (3.6068) acc 65.6250 (63.1250) lr 1.7705e-03 eta 0:06:36
epoch [13/50] batch [25/49] time 0.177 (0.207) data 0.000 (0.031) loss 3.2969 (3.5885) acc 65.6250 (63.1250) lr 1.7705e-03 eta 0:06:21
epoch [13/50] batch [30/49] time 0.175 (0.202) data 0.000 (0.026) loss 3.7188 (3.5635) acc 62.5000 (63.7500) lr 1.7705e-03 eta 0:06:10
epoch [13/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.022) loss 3.0332 (3.5217) acc 68.7500 (64.4643) lr 1.7705e-03 eta 0:06:02
epoch [13/50] batch [40/49] time 0.177 (0.196) data 0.000 (0.020) loss 3.6016 (3.5586) acc 56.2500 (63.7500) lr 1.7705e-03 eta 0:05:57
epoch [13/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.017) loss 3.3047 (3.5444) acc 65.6250 (63.4722) lr 1.7705e-03 eta 0:05:51
epoch [14/50] batch [5/49] time 0.177 (0.323) data 0.000 (0.144) loss 3.3184 (3.2742) acc 71.8750 (67.5000) lr 1.7290e-03 eta 0:09:43
epoch [14/50] batch [10/49] time 0.176 (0.250) data 0.000 (0.072) loss 3.0605 (3.2719) acc 78.1250 (68.1250) lr 1.7290e-03 eta 0:07:30
epoch [14/50] batch [15/49] time 0.178 (0.225) data 0.000 (0.048) loss 3.2109 (3.3335) acc 68.7500 (66.4583) lr 1.7290e-03 eta 0:06:45
epoch [14/50] batch [20/49] time 0.176 (0.213) data 0.000 (0.036) loss 2.8516 (3.3509) acc 62.5000 (65.3125) lr 1.7290e-03 eta 0:06:22
epoch [14/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.029) loss 3.3203 (3.3488) acc 65.6250 (66.0000) lr 1.7290e-03 eta 0:06:08
epoch [14/50] batch [30/49] time 0.176 (0.201) data 0.000 (0.024) loss 3.9414 (3.3983) acc 59.3750 (65.7292) lr 1.7290e-03 eta 0:05:58
epoch [14/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.2227 (3.4035) acc 65.6250 (66.2500) lr 1.7290e-03 eta 0:05:50
epoch [14/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.9102 (3.4273) acc 50.0000 (65.6250) lr 1.7290e-03 eta 0:05:44
epoch [14/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.016) loss 3.1113 (3.4183) acc 75.0000 (66.2500) lr 1.7290e-03 eta 0:05:39
epoch [15/50] batch [5/49] time 0.176 (0.365) data 0.000 (0.188) loss 3.6504 (3.3984) acc 65.6250 (66.8750) lr 1.6845e-03 eta 0:10:41
epoch [15/50] batch [10/49] time 0.179 (0.271) data 0.000 (0.094) loss 3.5469 (3.3607) acc 59.3750 (67.8125) lr 1.6845e-03 eta 0:07:54
epoch [15/50] batch [15/49] time 0.176 (0.239) data 0.000 (0.063) loss 3.6582 (3.3811) acc 62.5000 (67.5000) lr 1.6845e-03 eta 0:06:58
epoch [15/50] batch [20/49] time 0.178 (0.223) data 0.000 (0.047) loss 3.1914 (3.3769) acc 68.7500 (66.5625) lr 1.6845e-03 eta 0:06:29
epoch [15/50] batch [25/49] time 0.176 (0.214) data 0.000 (0.038) loss 3.5723 (3.3648) acc 68.7500 (67.0000) lr 1.6845e-03 eta 0:06:12
epoch [15/50] batch [30/49] time 0.177 (0.208) data 0.000 (0.032) loss 3.1562 (3.3594) acc 59.3750 (66.4583) lr 1.6845e-03 eta 0:06:00
epoch [15/50] batch [35/49] time 0.175 (0.204) data 0.000 (0.027) loss 3.8281 (3.3477) acc 50.0000 (66.1607) lr 1.6845e-03 eta 0:05:52
epoch [15/50] batch [40/49] time 0.175 (0.200) data 0.000 (0.024) loss 3.9258 (3.3618) acc 59.3750 (66.7969) lr 1.6845e-03 eta 0:05:44
epoch [15/50] batch [45/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.7578 (3.3624) acc 62.5000 (67.2917) lr 1.6845e-03 eta 0:05:39
epoch [16/50] batch [5/49] time 0.175 (0.308) data 0.000 (0.131) loss 3.5430 (3.4023) acc 68.7500 (66.2500) lr 1.6374e-03 eta 0:08:45
epoch [16/50] batch [10/49] time 0.175 (0.241) data 0.000 (0.066) loss 3.7656 (3.5041) acc 56.2500 (62.8125) lr 1.6374e-03 eta 0:06:51
epoch [16/50] batch [15/49] time 0.176 (0.219) data 0.000 (0.044) loss 4.1406 (3.4730) acc 46.8750 (63.7500) lr 1.6374e-03 eta 0:06:12
epoch [16/50] batch [20/49] time 0.175 (0.208) data 0.000 (0.033) loss 2.9883 (3.4905) acc 71.8750 (63.7500) lr 1.6374e-03 eta 0:05:53
epoch [16/50] batch [25/49] time 0.176 (0.202) data 0.000 (0.027) loss 3.3867 (3.4137) acc 59.3750 (65.2500) lr 1.6374e-03 eta 0:05:41
epoch [16/50] batch [30/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.5508 (3.4048) acc 62.5000 (64.6875) lr 1.6374e-03 eta 0:05:32
epoch [16/50] batch [35/49] time 0.175 (0.194) data 0.000 (0.019) loss 4.1406 (3.4648) acc 46.8750 (63.9286) lr 1.6374e-03 eta 0:05:26
epoch [16/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.017) loss 4.2422 (3.4916) acc 50.0000 (63.5156) lr 1.6374e-03 eta 0:05:21
epoch [16/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.015) loss 3.0898 (3.4731) acc 84.3750 (64.0278) lr 1.6374e-03 eta 0:05:17
epoch [17/50] batch [5/49] time 0.177 (0.296) data 0.000 (0.119) loss 3.6133 (3.3410) acc 62.5000 (63.7500) lr 1.5878e-03 eta 0:08:11
epoch [17/50] batch [10/49] time 0.176 (0.236) data 0.000 (0.060) loss 3.4043 (3.2473) acc 65.6250 (67.5000) lr 1.5878e-03 eta 0:06:30
epoch [17/50] batch [15/49] time 0.176 (0.216) data 0.000 (0.040) loss 3.8926 (3.2913) acc 65.6250 (67.7083) lr 1.5878e-03 eta 0:05:57
epoch [17/50] batch [20/49] time 0.176 (0.206) data 0.000 (0.030) loss 3.5430 (3.3087) acc 68.7500 (67.0312) lr 1.5878e-03 eta 0:05:39
epoch [17/50] batch [25/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.8027 (3.2764) acc 78.1250 (68.2500) lr 1.5878e-03 eta 0:05:28
epoch [17/50] batch [30/49] time 0.176 (0.196) data 0.000 (0.020) loss 3.8789 (3.3544) acc 59.3750 (66.8750) lr 1.5878e-03 eta 0:05:21
epoch [17/50] batch [35/49] time 0.176 (0.193) data 0.000 (0.017) loss 3.7598 (3.3834) acc 59.3750 (66.0714) lr 1.5878e-03 eta 0:05:15
epoch [17/50] batch [40/49] time 0.174 (0.191) data 0.000 (0.015) loss 3.5664 (3.3816) acc 62.5000 (66.2500) lr 1.5878e-03 eta 0:05:10
epoch [17/50] batch [45/49] time 0.175 (0.189) data 0.000 (0.013) loss 3.1465 (3.4036) acc 84.3750 (66.4583) lr 1.5878e-03 eta 0:05:06
epoch [18/50] batch [5/49] time 0.177 (0.326) data 0.000 (0.148) loss 3.0449 (3.3574) acc 71.8750 (66.2500) lr 1.5358e-03 eta 0:08:45
epoch [18/50] batch [10/49] time 0.177 (0.252) data 0.000 (0.074) loss 4.0312 (3.4502) acc 65.6250 (67.8125) lr 1.5358e-03 eta 0:06:44
epoch [18/50] batch [15/49] time 0.176 (0.227) data 0.000 (0.050) loss 3.1367 (3.3967) acc 65.6250 (68.5417) lr 1.5358e-03 eta 0:06:03
epoch [18/50] batch [20/49] time 0.176 (0.214) data 0.000 (0.037) loss 3.2031 (3.3132) acc 68.7500 (70.0000) lr 1.5358e-03 eta 0:05:41
epoch [18/50] batch [25/49] time 0.176 (0.207) data 0.000 (0.030) loss 3.6484 (3.3705) acc 78.1250 (69.0000) lr 1.5358e-03 eta 0:05:29
epoch [18/50] batch [30/49] time 0.177 (0.202) data 0.000 (0.025) loss 3.3164 (3.3643) acc 78.1250 (69.4792) lr 1.5358e-03 eta 0:05:20
epoch [18/50] batch [35/49] time 0.175 (0.198) data 0.000 (0.021) loss 3.9609 (3.3814) acc 59.3750 (68.9286) lr 1.5358e-03 eta 0:05:13
epoch [18/50] batch [40/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.7559 (3.3740) acc 65.6250 (68.6719) lr 1.5358e-03 eta 0:05:07
epoch [18/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.017) loss 2.9688 (3.3758) acc 75.0000 (68.4722) lr 1.5358e-03 eta 0:05:03
epoch [19/50] batch [5/49] time 0.176 (0.319) data 0.000 (0.141) loss 3.2969 (3.5684) acc 62.5000 (60.0000) lr 1.4818e-03 eta 0:08:17
epoch [19/50] batch [10/49] time 0.175 (0.247) data 0.000 (0.071) loss 3.3281 (3.3287) acc 62.5000 (65.6250) lr 1.4818e-03 eta 0:06:25
epoch [19/50] batch [15/49] time 0.175 (0.223) data 0.000 (0.047) loss 2.8457 (3.4125) acc 78.1250 (66.0417) lr 1.4818e-03 eta 0:05:47
epoch [19/50] batch [20/49] time 0.177 (0.212) data 0.000 (0.036) loss 3.6484 (3.4378) acc 65.6250 (65.9375) lr 1.4818e-03 eta 0:05:27
epoch [19/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.028) loss 3.2930 (3.4042) acc 75.0000 (67.6250) lr 1.4818e-03 eta 0:05:15
epoch [19/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.9531 (3.4143) acc 81.2500 (67.8125) lr 1.4818e-03 eta 0:05:07
epoch [19/50] batch [35/49] time 0.175 (0.196) data 0.000 (0.020) loss 3.1016 (3.3996) acc 68.7500 (67.5000) lr 1.4818e-03 eta 0:05:00
epoch [19/50] batch [40/49] time 0.176 (0.194) data 0.000 (0.018) loss 3.2578 (3.4048) acc 65.6250 (67.6562) lr 1.4818e-03 eta 0:04:55
epoch [19/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.2539 (3.3760) acc 75.0000 (68.0556) lr 1.4818e-03 eta 0:04:51
epoch [20/50] batch [5/49] time 0.178 (0.360) data 0.000 (0.182) loss 3.6445 (3.3102) acc 62.5000 (65.6250) lr 1.4258e-03 eta 0:09:05
epoch [20/50] batch [10/49] time 0.179 (0.268) data 0.000 (0.091) loss 3.2207 (3.4160) acc 78.1250 (66.8750) lr 1.4258e-03 eta 0:06:44
epoch [20/50] batch [15/49] time 0.176 (0.237) data 0.000 (0.061) loss 3.5000 (3.4118) acc 78.1250 (67.7083) lr 1.4258e-03 eta 0:05:57
epoch [20/50] batch [20/49] time 0.176 (0.222) data 0.000 (0.046) loss 3.1250 (3.3354) acc 68.7500 (68.7500) lr 1.4258e-03 eta 0:05:32
epoch [20/50] batch [25/49] time 0.177 (0.213) data 0.000 (0.037) loss 3.8027 (3.3359) acc 65.6250 (68.3750) lr 1.4258e-03 eta 0:05:17
epoch [20/50] batch [30/49] time 0.176 (0.207) data 0.000 (0.030) loss 3.0078 (3.3215) acc 68.7500 (69.2708) lr 1.4258e-03 eta 0:05:07
epoch [20/50] batch [35/49] time 0.175 (0.202) data 0.000 (0.026) loss 3.6172 (3.3419) acc 71.8750 (69.1071) lr 1.4258e-03 eta 0:05:00
epoch [20/50] batch [40/49] time 0.174 (0.199) data 0.000 (0.023) loss 3.1055 (3.3623) acc 78.1250 (68.5938) lr 1.4258e-03 eta 0:04:53
epoch [20/50] batch [45/49] time 0.175 (0.196) data 0.000 (0.020) loss 3.3984 (3.3490) acc 68.7500 (68.8889) lr 1.4258e-03 eta 0:04:49
epoch [21/50] batch [5/49] time 0.175 (0.336) data 0.000 (0.160) loss 3.5449 (3.3234) acc 65.6250 (68.1250) lr 1.3681e-03 eta 0:08:11
epoch [21/50] batch [10/49] time 0.176 (0.256) data 0.000 (0.080) loss 3.4922 (3.2754) acc 50.0000 (67.5000) lr 1.3681e-03 eta 0:06:13
epoch [21/50] batch [15/49] time 0.176 (0.229) data 0.000 (0.053) loss 2.6641 (3.3184) acc 81.2500 (67.5000) lr 1.3681e-03 eta 0:05:33
epoch [21/50] batch [20/49] time 0.175 (0.216) data 0.000 (0.040) loss 2.7891 (3.3060) acc 84.3750 (67.6562) lr 1.3681e-03 eta 0:05:13
epoch [21/50] batch [25/49] time 0.175 (0.208) data 0.000 (0.032) loss 3.5625 (3.3151) acc 68.7500 (68.2500) lr 1.3681e-03 eta 0:05:00
epoch [21/50] batch [30/49] time 0.177 (0.203) data 0.000 (0.027) loss 3.2188 (3.3257) acc 75.0000 (68.2292) lr 1.3681e-03 eta 0:04:51
epoch [21/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.023) loss 3.2812 (3.3195) acc 68.7500 (68.9286) lr 1.3681e-03 eta 0:04:45
epoch [21/50] batch [40/49] time 0.175 (0.196) data 0.000 (0.020) loss 3.4668 (3.3094) acc 68.7500 (69.6094) lr 1.3681e-03 eta 0:04:39
epoch [21/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.018) loss 3.8164 (3.3316) acc 62.5000 (69.5833) lr 1.3681e-03 eta 0:04:35
epoch [22/50] batch [5/49] time 0.177 (0.311) data 0.000 (0.133) loss 3.3906 (3.4434) acc 71.8750 (68.1250) lr 1.3090e-03 eta 0:07:20
epoch [22/50] batch [10/49] time 0.177 (0.244) data 0.000 (0.067) loss 3.9688 (3.4332) acc 62.5000 (68.1250) lr 1.3090e-03 eta 0:05:43
epoch [22/50] batch [15/49] time 0.177 (0.221) data 0.000 (0.045) loss 3.1836 (3.3391) acc 71.8750 (68.7500) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [20/49] time 0.177 (0.210) data 0.000 (0.033) loss 3.0625 (3.2686) acc 75.0000 (70.3125) lr 1.3090e-03 eta 0:04:54
epoch [22/50] batch [25/49] time 0.178 (0.204) data 0.000 (0.027) loss 2.6934 (3.2482) acc 81.2500 (71.1250) lr 1.3090e-03 eta 0:04:44
epoch [22/50] batch [30/49] time 0.177 (0.199) data 0.000 (0.022) loss 3.6367 (3.2812) acc 68.7500 (71.0417) lr 1.3090e-03 eta 0:04:37
epoch [22/50] batch [35/49] time 0.176 (0.196) data 0.000 (0.019) loss 3.1152 (3.2937) acc 71.8750 (70.6250) lr 1.3090e-03 eta 0:04:31
epoch [22/50] batch [40/49] time 0.176 (0.194) data 0.000 (0.017) loss 3.7227 (3.3171) acc 62.5000 (69.8438) lr 1.3090e-03 eta 0:04:27
epoch [22/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.015) loss 3.4668 (3.3192) acc 65.6250 (70.0000) lr 1.3090e-03 eta 0:04:23
epoch [23/50] batch [5/49] time 0.176 (0.378) data 0.000 (0.202) loss 3.5273 (3.3000) acc 68.7500 (71.2500) lr 1.2487e-03 eta 0:08:37
epoch [23/50] batch [10/49] time 0.176 (0.277) data 0.000 (0.101) loss 3.3164 (3.2703) acc 65.6250 (71.2500) lr 1.2487e-03 eta 0:06:17
epoch [23/50] batch [15/49] time 0.176 (0.244) data 0.000 (0.068) loss 3.3906 (3.3749) acc 65.6250 (70.4167) lr 1.2487e-03 eta 0:05:30
epoch [23/50] batch [20/49] time 0.176 (0.227) data 0.000 (0.051) loss 2.8398 (3.3256) acc 71.8750 (70.7812) lr 1.2487e-03 eta 0:05:06
epoch [23/50] batch [25/49] time 0.176 (0.217) data 0.000 (0.041) loss 2.9082 (3.2992) acc 75.0000 (70.3750) lr 1.2487e-03 eta 0:04:52
epoch [23/50] batch [30/49] time 0.176 (0.210) data 0.000 (0.034) loss 3.3594 (3.3285) acc 68.7500 (69.3750) lr 1.2487e-03 eta 0:04:42
epoch [23/50] batch [35/49] time 0.174 (0.205) data 0.000 (0.029) loss 2.8262 (3.3531) acc 84.3750 (68.5714) lr 1.2487e-03 eta 0:04:34
epoch [23/50] batch [40/49] time 0.175 (0.201) data 0.000 (0.026) loss 2.7734 (3.3684) acc 78.1250 (68.8281) lr 1.2487e-03 eta 0:04:28
epoch [23/50] batch [45/49] time 0.174 (0.198) data 0.000 (0.023) loss 2.9629 (3.3462) acc 75.0000 (68.8889) lr 1.2487e-03 eta 0:04:23
epoch [24/50] batch [5/49] time 0.174 (0.326) data 0.000 (0.149) loss 3.2266 (3.2652) acc 71.8750 (68.7500) lr 1.1874e-03 eta 0:07:09
epoch [24/50] batch [10/49] time 0.174 (0.250) data 0.000 (0.075) loss 3.0547 (3.1600) acc 75.0000 (72.1875) lr 1.1874e-03 eta 0:05:28
epoch [24/50] batch [15/49] time 0.176 (0.225) data 0.000 (0.050) loss 3.4746 (3.2387) acc 62.5000 (71.0417) lr 1.1874e-03 eta 0:04:54
epoch [24/50] batch [20/49] time 0.174 (0.213) data 0.000 (0.037) loss 3.4395 (3.2136) acc 68.7500 (72.0312) lr 1.1874e-03 eta 0:04:37
epoch [24/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.030) loss 2.8848 (3.1933) acc 84.3750 (72.8750) lr 1.1874e-03 eta 0:04:26
epoch [24/50] batch [30/49] time 0.175 (0.200) data 0.000 (0.025) loss 3.4219 (3.2027) acc 71.8750 (72.7083) lr 1.1874e-03 eta 0:04:18
epoch [24/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.022) loss 3.3242 (3.2129) acc 65.6250 (72.3214) lr 1.1874e-03 eta 0:04:13
epoch [24/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.019) loss 2.6602 (3.2117) acc 78.1250 (72.3438) lr 1.1874e-03 eta 0:04:08
epoch [24/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.017) loss 3.1562 (3.2211) acc 78.1250 (72.1528) lr 1.1874e-03 eta 0:04:04
epoch [25/50] batch [5/49] time 0.176 (0.331) data 0.000 (0.154) loss 3.2266 (3.0938) acc 71.8750 (75.0000) lr 1.1253e-03 eta 0:06:59
epoch [25/50] batch [10/49] time 0.176 (0.253) data 0.000 (0.077) loss 2.9941 (3.1549) acc 78.1250 (73.4375) lr 1.1253e-03 eta 0:05:19
epoch [25/50] batch [15/49] time 0.174 (0.227) data 0.000 (0.051) loss 4.1562 (3.1977) acc 62.5000 (73.7500) lr 1.1253e-03 eta 0:04:45
epoch [25/50] batch [20/49] time 0.175 (0.214) data 0.000 (0.039) loss 3.0703 (3.2116) acc 71.8750 (73.9062) lr 1.1253e-03 eta 0:04:28
epoch [25/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.031) loss 3.7539 (3.2146) acc 59.3750 (73.7500) lr 1.1253e-03 eta 0:04:17
epoch [25/50] batch [30/49] time 0.175 (0.201) data 0.000 (0.026) loss 2.6191 (3.2054) acc 87.5000 (74.3750) lr 1.1253e-03 eta 0:04:10
epoch [25/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.022) loss 3.1445 (3.2079) acc 65.6250 (73.6607) lr 1.1253e-03 eta 0:04:04
epoch [25/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.019) loss 3.2188 (3.2364) acc 75.0000 (73.5156) lr 1.1253e-03 eta 0:03:59
epoch [25/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.017) loss 3.6055 (3.2608) acc 65.6250 (72.6389) lr 1.1253e-03 eta 0:03:56
epoch [26/50] batch [5/49] time 0.176 (0.343) data 0.000 (0.166) loss 2.8418 (3.1133) acc 75.0000 (75.0000) lr 1.0628e-03 eta 0:06:58
epoch [26/50] batch [10/49] time 0.175 (0.260) data 0.000 (0.084) loss 2.7969 (3.1262) acc 78.1250 (75.0000) lr 1.0628e-03 eta 0:05:15
epoch [26/50] batch [15/49] time 0.176 (0.232) data 0.000 (0.056) loss 3.3223 (3.2238) acc 65.6250 (74.1667) lr 1.0628e-03 eta 0:04:40
epoch [26/50] batch [20/49] time 0.175 (0.218) data 0.000 (0.042) loss 3.1680 (3.2376) acc 75.0000 (73.4375) lr 1.0628e-03 eta 0:04:22
epoch [26/50] batch [25/49] time 0.176 (0.209) data 0.000 (0.034) loss 3.6133 (3.2638) acc 65.6250 (72.3750) lr 1.0628e-03 eta 0:04:11
epoch [26/50] batch [30/49] time 0.176 (0.204) data 0.000 (0.028) loss 3.1641 (3.2888) acc 68.7500 (71.4583) lr 1.0628e-03 eta 0:04:03
epoch [26/50] batch [35/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.9160 (3.2667) acc 75.0000 (71.6964) lr 1.0628e-03 eta 0:03:57
epoch [26/50] batch [40/49] time 0.175 (0.197) data 0.000 (0.021) loss 2.8770 (3.2240) acc 78.1250 (72.1875) lr 1.0628e-03 eta 0:03:53
epoch [26/50] batch [45/49] time 0.174 (0.194) data 0.000 (0.019) loss 3.9102 (3.2357) acc 50.0000 (72.1528) lr 1.0628e-03 eta 0:03:49
epoch [27/50] batch [5/49] time 0.176 (0.329) data 0.000 (0.152) loss 2.8594 (3.2016) acc 81.2500 (70.0000) lr 1.0000e-03 eta 0:06:25
epoch [27/50] batch [10/49] time 0.176 (0.252) data 0.000 (0.076) loss 3.5430 (3.1768) acc 68.7500 (72.1875) lr 1.0000e-03 eta 0:04:54
epoch [27/50] batch [15/49] time 0.176 (0.227) data 0.000 (0.051) loss 2.8789 (3.0939) acc 75.0000 (73.1250) lr 1.0000e-03 eta 0:04:23
epoch [27/50] batch [20/49] time 0.176 (0.215) data 0.000 (0.038) loss 3.3555 (3.1465) acc 68.7500 (72.9688) lr 1.0000e-03 eta 0:04:08
epoch [27/50] batch [25/49] time 0.177 (0.207) data 0.000 (0.031) loss 3.5195 (3.1531) acc 71.8750 (74.1250) lr 1.0000e-03 eta 0:03:58
epoch [27/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.026) loss 3.2461 (3.1797) acc 68.7500 (73.1250) lr 1.0000e-03 eta 0:03:51
epoch [27/50] batch [35/49] time 0.176 (0.198) data 0.000 (0.022) loss 3.4531 (3.1945) acc 71.8750 (72.9464) lr 1.0000e-03 eta 0:03:46
epoch [27/50] batch [40/49] time 0.177 (0.195) data 0.000 (0.019) loss 3.1270 (3.1890) acc 75.0000 (72.9688) lr 1.0000e-03 eta 0:03:42
epoch [27/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.017) loss 3.4746 (3.1859) acc 65.6250 (72.7083) lr 1.0000e-03 eta 0:03:38
epoch [28/50] batch [5/49] time 0.176 (0.343) data 0.000 (0.167) loss 3.3711 (3.4094) acc 78.1250 (70.6250) lr 9.3721e-04 eta 0:06:25
epoch [28/50] batch [10/49] time 0.176 (0.260) data 0.000 (0.083) loss 2.8242 (3.2590) acc 84.3750 (73.4375) lr 9.3721e-04 eta 0:04:50
epoch [28/50] batch [15/49] time 0.177 (0.232) data 0.000 (0.056) loss 2.4727 (3.1547) acc 90.6250 (75.4167) lr 9.3721e-04 eta 0:04:18
epoch [28/50] batch [20/49] time 0.175 (0.218) data 0.000 (0.042) loss 3.1953 (3.1983) acc 68.7500 (73.5938) lr 9.3721e-04 eta 0:04:01
epoch [28/50] batch [25/49] time 0.180 (0.210) data 0.003 (0.034) loss 3.0762 (3.1538) acc 81.2500 (74.1250) lr 9.3721e-04 eta 0:03:51
epoch [28/50] batch [30/49] time 0.176 (0.204) data 0.000 (0.028) loss 3.0977 (3.2049) acc 65.6250 (72.6042) lr 9.3721e-04 eta 0:03:44
epoch [28/50] batch [35/49] time 0.178 (0.201) data 0.000 (0.024) loss 3.7344 (3.2436) acc 62.5000 (71.6964) lr 9.3721e-04 eta 0:03:39
epoch [28/50] batch [40/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.9688 (3.2533) acc 56.2500 (71.7188) lr 9.3721e-04 eta 0:03:34
epoch [28/50] batch [45/49] time 0.175 (0.195) data 0.000 (0.019) loss 2.7812 (3.2224) acc 81.2500 (72.4306) lr 9.3721e-04 eta 0:03:30
epoch [29/50] batch [5/49] time 0.176 (0.308) data 0.000 (0.132) loss 3.7891 (3.2105) acc 62.5000 (73.1250) lr 8.7467e-04 eta 0:05:30
epoch [29/50] batch [10/49] time 0.176 (0.242) data 0.000 (0.066) loss 3.2656 (3.1975) acc 62.5000 (73.4375) lr 8.7467e-04 eta 0:04:18
epoch [29/50] batch [15/49] time 0.176 (0.221) data 0.000 (0.044) loss 3.2734 (3.1961) acc 71.8750 (74.1667) lr 8.7467e-04 eta 0:03:54
epoch [29/50] batch [20/49] time 0.177 (0.210) data 0.000 (0.033) loss 3.3477 (3.2476) acc 68.7500 (72.1875) lr 8.7467e-04 eta 0:03:41
epoch [29/50] batch [25/49] time 0.177 (0.203) data 0.000 (0.027) loss 3.8945 (3.3048) acc 59.3750 (71.8750) lr 8.7467e-04 eta 0:03:33
epoch [29/50] batch [30/49] time 0.174 (0.198) data 0.000 (0.022) loss 2.7852 (3.2867) acc 87.5000 (71.9792) lr 8.7467e-04 eta 0:03:28
epoch [29/50] batch [35/49] time 0.176 (0.195) data 0.000 (0.019) loss 3.1973 (3.2742) acc 62.5000 (71.5179) lr 8.7467e-04 eta 0:03:23
epoch [29/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.017) loss 3.0840 (3.2623) acc 75.0000 (72.1875) lr 8.7467e-04 eta 0:03:20
epoch [29/50] batch [45/49] time 0.174 (0.191) data 0.000 (0.015) loss 2.7930 (3.2654) acc 75.0000 (71.7361) lr 8.7467e-04 eta 0:03:17
epoch [30/50] batch [5/49] time 0.176 (0.304) data 0.001 (0.127) loss 3.2539 (2.9453) acc 62.5000 (72.5000) lr 8.1262e-04 eta 0:05:11
epoch [30/50] batch [10/49] time 0.175 (0.240) data 0.000 (0.064) loss 3.3281 (3.0068) acc 56.2500 (73.1250) lr 8.1262e-04 eta 0:04:04
epoch [30/50] batch [15/49] time 0.175 (0.218) data 0.000 (0.043) loss 3.0000 (3.0551) acc 75.0000 (72.5000) lr 8.1262e-04 eta 0:03:41
epoch [30/50] batch [20/49] time 0.175 (0.208) data 0.000 (0.032) loss 3.0000 (3.1471) acc 78.1250 (71.7188) lr 8.1262e-04 eta 0:03:29
epoch [30/50] batch [25/49] time 0.175 (0.201) data 0.000 (0.026) loss 3.3438 (3.1496) acc 68.7500 (72.6250) lr 8.1262e-04 eta 0:03:21
epoch [30/50] batch [30/49] time 0.178 (0.197) data 0.000 (0.021) loss 3.2324 (3.0981) acc 71.8750 (73.9583) lr 8.1262e-04 eta 0:03:16
epoch [30/50] batch [35/49] time 0.174 (0.194) data 0.000 (0.018) loss 2.7656 (3.0939) acc 84.3750 (74.1964) lr 8.1262e-04 eta 0:03:12
epoch [30/50] batch [40/49] time 0.174 (0.191) data 0.000 (0.016) loss 2.6504 (3.1039) acc 78.1250 (74.3750) lr 8.1262e-04 eta 0:03:09
epoch [30/50] batch [45/49] time 0.176 (0.190) data 0.000 (0.014) loss 3.4395 (3.1347) acc 71.8750 (74.0972) lr 8.1262e-04 eta 0:03:06
epoch [31/50] batch [5/49] time 0.176 (0.308) data 0.000 (0.131) loss 3.4570 (3.2570) acc 71.8750 (75.0000) lr 7.5131e-04 eta 0:05:00
epoch [31/50] batch [10/49] time 0.176 (0.242) data 0.000 (0.066) loss 3.0957 (3.2418) acc 81.2500 (74.6875) lr 7.5131e-04 eta 0:03:54
epoch [31/50] batch [15/49] time 0.176 (0.220) data 0.000 (0.044) loss 2.3691 (3.1470) acc 81.2500 (74.5833) lr 7.5131e-04 eta 0:03:32
epoch [31/50] batch [20/49] time 0.175 (0.209) data 0.000 (0.033) loss 3.5684 (3.2142) acc 68.7500 (74.0625) lr 7.5131e-04 eta 0:03:20
epoch [31/50] batch [25/49] time 0.177 (0.202) data 0.000 (0.026) loss 3.3594 (3.1980) acc 68.7500 (74.2500) lr 7.5131e-04 eta 0:03:13
epoch [31/50] batch [30/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.1367 (3.1768) acc 68.7500 (74.0625) lr 7.5131e-04 eta 0:03:08
epoch [31/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.5566 (3.1858) acc 65.6250 (73.8393) lr 7.5131e-04 eta 0:03:04
epoch [31/50] batch [40/49] time 0.176 (0.192) data 0.000 (0.017) loss 2.9629 (3.1991) acc 62.5000 (73.5938) lr 7.5131e-04 eta 0:03:00
epoch [31/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.015) loss 3.4062 (3.2082) acc 75.0000 (73.6806) lr 7.5131e-04 eta 0:02:58
epoch [32/50] batch [5/49] time 0.177 (0.328) data 0.000 (0.152) loss 2.6992 (2.9488) acc 81.2500 (81.8750) lr 6.9098e-04 eta 0:05:03
epoch [32/50] batch [10/49] time 0.176 (0.252) data 0.000 (0.076) loss 3.5430 (3.1373) acc 62.5000 (76.5625) lr 6.9098e-04 eta 0:03:52
epoch [32/50] batch [15/49] time 0.175 (0.227) data 0.000 (0.051) loss 3.0586 (3.1095) acc 78.1250 (76.2500) lr 6.9098e-04 eta 0:03:27
epoch [32/50] batch [20/49] time 0.175 (0.214) data 0.000 (0.038) loss 3.5801 (3.1483) acc 59.3750 (75.4688) lr 6.9098e-04 eta 0:03:14
epoch [32/50] batch [25/49] time 0.175 (0.206) data 0.000 (0.031) loss 3.6406 (3.1685) acc 65.6250 (74.7500) lr 6.9098e-04 eta 0:03:06
epoch [32/50] batch [30/49] time 0.176 (0.201) data 0.000 (0.025) loss 2.7656 (3.1440) acc 84.3750 (75.7292) lr 6.9098e-04 eta 0:03:01
epoch [32/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.022) loss 3.1270 (3.1585) acc 81.2500 (75.8036) lr 6.9098e-04 eta 0:02:56
epoch [32/50] batch [40/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.6250 (3.1841) acc 71.8750 (74.9219) lr 6.9098e-04 eta 0:02:53
epoch [32/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.017) loss 3.4590 (3.1799) acc 59.3750 (74.7917) lr 6.9098e-04 eta 0:02:50
epoch [33/50] batch [5/49] time 0.176 (0.331) data 0.000 (0.153) loss 2.8105 (3.1133) acc 87.5000 (77.5000) lr 6.3188e-04 eta 0:04:50
epoch [33/50] batch [10/49] time 0.176 (0.254) data 0.000 (0.077) loss 2.6641 (3.1426) acc 87.5000 (75.3125) lr 6.3188e-04 eta 0:03:41
epoch [33/50] batch [15/49] time 0.177 (0.228) data 0.000 (0.051) loss 2.8379 (3.1009) acc 84.3750 (75.2083) lr 6.3188e-04 eta 0:03:17
epoch [33/50] batch [20/49] time 0.177 (0.215) data 0.000 (0.038) loss 3.1172 (3.0705) acc 71.8750 (75.3125) lr 6.3188e-04 eta 0:03:05
epoch [33/50] batch [25/49] time 0.176 (0.207) data 0.000 (0.031) loss 3.2422 (3.0755) acc 81.2500 (74.8750) lr 6.3188e-04 eta 0:02:57
epoch [33/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.026) loss 2.7480 (3.0943) acc 84.3750 (74.6875) lr 6.3188e-04 eta 0:02:52
epoch [33/50] batch [35/49] time 0.176 (0.199) data 0.000 (0.022) loss 2.9219 (3.0797) acc 78.1250 (75.3571) lr 6.3188e-04 eta 0:02:48
epoch [33/50] batch [40/49] time 0.174 (0.196) data 0.000 (0.019) loss 3.2363 (3.0998) acc 81.2500 (75.3125) lr 6.3188e-04 eta 0:02:44
epoch [33/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.017) loss 3.2734 (3.1486) acc 71.8750 (74.4444) lr 6.3188e-04 eta 0:02:41
epoch [34/50] batch [5/49] time 0.176 (0.302) data 0.000 (0.125) loss 2.9551 (3.0598) acc 78.1250 (76.2500) lr 5.7422e-04 eta 0:04:10
epoch [34/50] batch [10/49] time 0.176 (0.239) data 0.000 (0.063) loss 3.3789 (3.0553) acc 78.1250 (78.1250) lr 5.7422e-04 eta 0:03:16
epoch [34/50] batch [15/49] time 0.176 (0.218) data 0.000 (0.042) loss 2.8555 (3.0618) acc 78.1250 (78.1250) lr 5.7422e-04 eta 0:02:58
epoch [34/50] batch [20/49] time 0.176 (0.207) data 0.000 (0.031) loss 3.0664 (3.0644) acc 75.0000 (76.8750) lr 5.7422e-04 eta 0:02:48
epoch [34/50] batch [25/49] time 0.176 (0.201) data 0.000 (0.025) loss 2.8535 (3.0327) acc 81.2500 (77.1250) lr 5.7422e-04 eta 0:02:42
epoch [34/50] batch [30/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.5078 (3.0449) acc 65.6250 (76.4583) lr 5.7422e-04 eta 0:02:38
epoch [34/50] batch [35/49] time 0.175 (0.194) data 0.000 (0.018) loss 2.8711 (3.0359) acc 71.8750 (76.2500) lr 5.7422e-04 eta 0:02:34
epoch [34/50] batch [40/49] time 0.176 (0.192) data 0.000 (0.016) loss 3.0703 (3.0358) acc 75.0000 (76.4062) lr 5.7422e-04 eta 0:02:31
epoch [34/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.014) loss 3.0664 (3.0523) acc 78.1250 (75.6250) lr 5.7422e-04 eta 0:02:29
epoch [35/50] batch [5/49] time 0.176 (0.335) data 0.000 (0.158) loss 3.7363 (3.1273) acc 62.5000 (78.7500) lr 5.1825e-04 eta 0:04:21
epoch [35/50] batch [10/49] time 0.176 (0.256) data 0.000 (0.079) loss 2.9062 (3.0686) acc 87.5000 (79.3750) lr 5.1825e-04 eta 0:03:18
epoch [35/50] batch [15/49] time 0.176 (0.230) data 0.000 (0.053) loss 3.0410 (3.0931) acc 71.8750 (77.7083) lr 5.1825e-04 eta 0:02:56
epoch [35/50] batch [20/49] time 0.175 (0.216) data 0.000 (0.040) loss 2.6523 (3.0411) acc 84.3750 (77.6562) lr 5.1825e-04 eta 0:02:45
epoch [35/50] batch [25/49] time 0.176 (0.208) data 0.000 (0.032) loss 3.2285 (3.0513) acc 71.8750 (76.7500) lr 5.1825e-04 eta 0:02:37
epoch [35/50] batch [30/49] time 0.176 (0.203) data 0.000 (0.027) loss 2.9062 (3.1003) acc 84.3750 (76.5625) lr 5.1825e-04 eta 0:02:32
epoch [35/50] batch [35/49] time 0.175 (0.199) data 0.000 (0.023) loss 3.1934 (3.1021) acc 81.2500 (76.6964) lr 5.1825e-04 eta 0:02:29
epoch [35/50] batch [40/49] time 0.175 (0.196) data 0.000 (0.020) loss 2.8652 (3.0773) acc 78.1250 (77.2656) lr 5.1825e-04 eta 0:02:25
epoch [35/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.5176 (3.0823) acc 68.7500 (77.0139) lr 5.1825e-04 eta 0:02:23
epoch [36/50] batch [5/49] time 0.175 (0.332) data 0.000 (0.155) loss 2.7930 (3.0504) acc 81.2500 (74.3750) lr 4.6417e-04 eta 0:04:02
epoch [36/50] batch [10/49] time 0.177 (0.254) data 0.000 (0.077) loss 3.1719 (3.1824) acc 68.7500 (74.0625) lr 4.6417e-04 eta 0:03:04
epoch [36/50] batch [15/49] time 0.177 (0.228) data 0.000 (0.052) loss 2.7422 (3.1471) acc 78.1250 (76.2500) lr 4.6417e-04 eta 0:02:44
epoch [36/50] batch [20/49] time 0.177 (0.216) data 0.000 (0.039) loss 3.2617 (3.1431) acc 71.8750 (76.2500) lr 4.6417e-04 eta 0:02:34
epoch [36/50] batch [25/49] time 0.177 (0.208) data 0.000 (0.031) loss 3.5742 (3.1524) acc 68.7500 (75.2500) lr 4.6417e-04 eta 0:02:27
epoch [36/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.026) loss 2.9414 (3.1126) acc 78.1250 (76.2500) lr 4.6417e-04 eta 0:02:22
epoch [36/50] batch [35/49] time 0.175 (0.199) data 0.000 (0.022) loss 3.5273 (3.1170) acc 65.6250 (76.1607) lr 4.6417e-04 eta 0:02:19
epoch [36/50] batch [40/49] time 0.177 (0.196) data 0.000 (0.020) loss 3.7305 (3.1419) acc 68.7500 (75.3125) lr 4.6417e-04 eta 0:02:16
epoch [36/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.017) loss 3.4258 (3.1328) acc 65.6250 (75.5556) lr 4.6417e-04 eta 0:02:13
epoch [37/50] batch [5/49] time 0.177 (0.336) data 0.000 (0.158) loss 2.5449 (2.9945) acc 87.5000 (77.5000) lr 4.1221e-04 eta 0:03:48
epoch [37/50] batch [10/49] time 0.177 (0.256) data 0.000 (0.079) loss 3.2207 (3.0051) acc 81.2500 (78.1250) lr 4.1221e-04 eta 0:02:53
epoch [37/50] batch [15/49] time 0.177 (0.230) data 0.000 (0.053) loss 2.8047 (3.0622) acc 81.2500 (77.2917) lr 4.1221e-04 eta 0:02:34
epoch [37/50] batch [20/49] time 0.177 (0.217) data 0.000 (0.040) loss 2.7656 (3.0535) acc 84.3750 (77.3438) lr 4.1221e-04 eta 0:02:24
epoch [37/50] batch [25/49] time 0.177 (0.209) data 0.000 (0.032) loss 3.4121 (3.0787) acc 84.3750 (77.8750) lr 4.1221e-04 eta 0:02:17
epoch [37/50] batch [30/49] time 0.177 (0.203) data 0.000 (0.027) loss 2.8594 (3.0636) acc 90.6250 (77.9167) lr 4.1221e-04 eta 0:02:13
epoch [37/50] batch [35/49] time 0.176 (0.200) data 0.000 (0.023) loss 3.0938 (3.0903) acc 84.3750 (77.4107) lr 4.1221e-04 eta 0:02:09
epoch [37/50] batch [40/49] time 0.176 (0.197) data 0.000 (0.020) loss 3.2734 (3.1110) acc 78.1250 (77.1875) lr 4.1221e-04 eta 0:02:07
epoch [37/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.0000 (3.1099) acc 78.1250 (77.0833) lr 4.1221e-04 eta 0:02:04
epoch [38/50] batch [5/49] time 0.176 (0.384) data 0.000 (0.206) loss 2.9844 (2.7660) acc 71.8750 (83.1250) lr 3.6258e-04 eta 0:04:02
epoch [38/50] batch [10/49] time 0.176 (0.280) data 0.000 (0.103) loss 3.0488 (2.9180) acc 84.3750 (82.5000) lr 3.6258e-04 eta 0:02:55
epoch [38/50] batch [15/49] time 0.176 (0.246) data 0.000 (0.069) loss 3.5078 (3.0132) acc 53.1250 (78.1250) lr 3.6258e-04 eta 0:02:32
epoch [38/50] batch [20/49] time 0.177 (0.229) data 0.000 (0.052) loss 2.8574 (2.9974) acc 75.0000 (78.2812) lr 3.6258e-04 eta 0:02:21
epoch [38/50] batch [25/49] time 0.176 (0.218) data 0.000 (0.041) loss 3.0273 (3.0080) acc 75.0000 (78.3750) lr 3.6258e-04 eta 0:02:13
epoch [38/50] batch [30/49] time 0.177 (0.211) data 0.000 (0.035) loss 3.1934 (3.0049) acc 68.7500 (78.4375) lr 3.6258e-04 eta 0:02:08
epoch [38/50] batch [35/49] time 0.175 (0.206) data 0.000 (0.030) loss 3.4180 (3.0290) acc 71.8750 (77.6786) lr 3.6258e-04 eta 0:02:04
epoch [38/50] batch [40/49] time 0.175 (0.202) data 0.000 (0.026) loss 2.7578 (3.0351) acc 87.5000 (77.8125) lr 3.6258e-04 eta 0:02:00
epoch [38/50] batch [45/49] time 0.175 (0.199) data 0.000 (0.023) loss 2.6895 (3.0246) acc 87.5000 (78.1944) lr 3.6258e-04 eta 0:01:57
epoch [39/50] batch [5/49] time 0.177 (0.332) data 0.000 (0.154) loss 2.9492 (3.2105) acc 84.3750 (74.3750) lr 3.1545e-04 eta 0:03:13
epoch [39/50] batch [10/49] time 0.177 (0.255) data 0.000 (0.077) loss 3.2539 (3.1494) acc 75.0000 (75.0000) lr 3.1545e-04 eta 0:02:27
epoch [39/50] batch [15/49] time 0.178 (0.229) data 0.000 (0.051) loss 2.9062 (3.1659) acc 75.0000 (74.7917) lr 3.1545e-04 eta 0:02:11
epoch [39/50] batch [20/49] time 0.178 (0.216) data 0.000 (0.039) loss 2.9492 (3.1452) acc 87.5000 (75.3125) lr 3.1545e-04 eta 0:02:02
epoch [39/50] batch [25/49] time 0.177 (0.208) data 0.000 (0.031) loss 3.1055 (3.1377) acc 75.0000 (75.2500) lr 3.1545e-04 eta 0:01:57
epoch [39/50] batch [30/49] time 0.179 (0.203) data 0.000 (0.026) loss 3.1094 (3.1359) acc 71.8750 (75.1042) lr 3.1545e-04 eta 0:01:53
epoch [39/50] batch [35/49] time 0.176 (0.200) data 0.000 (0.022) loss 3.4199 (3.1343) acc 59.3750 (75.1786) lr 3.1545e-04 eta 0:01:50
epoch [39/50] batch [40/49] time 0.175 (0.197) data 0.000 (0.019) loss 2.8086 (3.1359) acc 87.5000 (75.0781) lr 3.1545e-04 eta 0:01:47
epoch [39/50] batch [45/49] time 0.175 (0.194) data 0.000 (0.017) loss 3.0117 (3.1155) acc 87.5000 (75.7639) lr 3.1545e-04 eta 0:01:45
epoch [40/50] batch [5/49] time 0.177 (0.312) data 0.000 (0.134) loss 3.1465 (3.0410) acc 75.0000 (76.2500) lr 2.7103e-04 eta 0:02:46
epoch [40/50] batch [10/49] time 0.177 (0.244) data 0.000 (0.067) loss 3.4824 (3.0742) acc 71.8750 (76.2500) lr 2.7103e-04 eta 0:02:09
epoch [40/50] batch [15/49] time 0.180 (0.222) data 0.000 (0.045) loss 3.0430 (3.1371) acc 87.5000 (76.6667) lr 2.7103e-04 eta 0:01:56
epoch [40/50] batch [20/49] time 0.179 (0.211) data 0.000 (0.034) loss 2.7988 (3.1028) acc 78.1250 (76.2500) lr 2.7103e-04 eta 0:01:49
epoch [40/50] batch [25/49] time 0.176 (0.204) data 0.000 (0.027) loss 3.1465 (3.0940) acc 75.0000 (75.3750) lr 2.7103e-04 eta 0:01:44
epoch [40/50] batch [30/49] time 0.177 (0.199) data 0.000 (0.023) loss 2.7637 (3.0549) acc 81.2500 (75.7292) lr 2.7103e-04 eta 0:01:41
epoch [40/50] batch [35/49] time 0.177 (0.196) data 0.000 (0.019) loss 3.5566 (3.0710) acc 68.7500 (76.0714) lr 2.7103e-04 eta 0:01:38
epoch [40/50] batch [40/49] time 0.176 (0.194) data 0.000 (0.017) loss 2.9258 (3.0606) acc 84.3750 (76.2500) lr 2.7103e-04 eta 0:01:36
epoch [40/50] batch [45/49] time 0.176 (0.192) data 0.000 (0.015) loss 3.1953 (3.0579) acc 62.5000 (75.9028) lr 2.7103e-04 eta 0:01:34
epoch [41/50] batch [5/49] time 0.180 (0.333) data 0.000 (0.155) loss 2.6992 (2.7895) acc 87.5000 (86.2500) lr 2.2949e-04 eta 0:02:41
epoch [41/50] batch [10/49] time 0.177 (0.256) data 0.000 (0.078) loss 2.6738 (2.8645) acc 84.3750 (82.5000) lr 2.2949e-04 eta 0:02:02
epoch [41/50] batch [15/49] time 0.180 (0.230) data 0.000 (0.052) loss 3.0664 (2.9311) acc 81.2500 (81.0417) lr 2.2949e-04 eta 0:01:49
epoch [41/50] batch [20/49] time 0.177 (0.217) data 0.000 (0.039) loss 3.4297 (3.0610) acc 59.3750 (78.2812) lr 2.2949e-04 eta 0:01:41
epoch [41/50] batch [25/49] time 0.177 (0.209) data 0.000 (0.031) loss 3.2656 (3.0448) acc 75.0000 (78.7500) lr 2.2949e-04 eta 0:01:37
epoch [41/50] batch [30/49] time 0.178 (0.204) data 0.000 (0.026) loss 2.9531 (3.0634) acc 68.7500 (77.5000) lr 2.2949e-04 eta 0:01:33
epoch [41/50] batch [35/49] time 0.176 (0.200) data 0.000 (0.022) loss 3.2734 (3.0800) acc 75.0000 (77.6786) lr 2.2949e-04 eta 0:01:30
epoch [41/50] batch [40/49] time 0.177 (0.197) data 0.000 (0.020) loss 3.0879 (3.0902) acc 75.0000 (77.1875) lr 2.2949e-04 eta 0:01:28
epoch [41/50] batch [45/49] time 0.177 (0.195) data 0.000 (0.017) loss 2.9355 (3.1103) acc 84.3750 (76.8750) lr 2.2949e-04 eta 0:01:26
epoch [42/50] batch [5/49] time 0.177 (0.321) data 0.000 (0.144) loss 3.1172 (3.2375) acc 84.3750 (78.1250) lr 1.9098e-04 eta 0:02:19
epoch [42/50] batch [10/49] time 0.177 (0.249) data 0.000 (0.072) loss 2.5781 (3.1570) acc 87.5000 (78.1250) lr 1.9098e-04 eta 0:01:47
epoch [42/50] batch [15/49] time 0.176 (0.225) data 0.000 (0.048) loss 2.8867 (3.0997) acc 78.1250 (77.7083) lr 1.9098e-04 eta 0:01:35
epoch [42/50] batch [20/49] time 0.176 (0.213) data 0.000 (0.036) loss 3.6719 (3.0664) acc 75.0000 (79.3750) lr 1.9098e-04 eta 0:01:29
epoch [42/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.029) loss 2.5957 (3.0459) acc 84.3750 (79.6250) lr 1.9098e-04 eta 0:01:25
epoch [42/50] batch [30/49] time 0.179 (0.201) data 0.004 (0.024) loss 3.0352 (3.0227) acc 75.0000 (79.4792) lr 1.9098e-04 eta 0:01:22
epoch [42/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.021) loss 3.2949 (3.0377) acc 75.0000 (78.7500) lr 1.9098e-04 eta 0:01:20
epoch [42/50] batch [40/49] time 0.176 (0.195) data 0.000 (0.018) loss 2.8281 (3.0246) acc 81.2500 (78.9062) lr 1.9098e-04 eta 0:01:18
epoch [42/50] batch [45/49] time 0.176 (0.192) data 0.000 (0.016) loss 3.0703 (3.0414) acc 75.0000 (78.0556) lr 1.9098e-04 eta 0:01:16
epoch [43/50] batch [5/49] time 0.176 (0.316) data 0.000 (0.138) loss 3.3945 (3.2852) acc 71.8750 (74.3750) lr 1.5567e-04 eta 0:02:02
epoch [43/50] batch [10/49] time 0.176 (0.246) data 0.000 (0.069) loss 2.6562 (3.0535) acc 78.1250 (77.8125) lr 1.5567e-04 eta 0:01:34
epoch [43/50] batch [15/49] time 0.176 (0.224) data 0.000 (0.046) loss 3.3320 (2.9970) acc 65.6250 (78.7500) lr 1.5567e-04 eta 0:01:24
epoch [43/50] batch [20/49] time 0.177 (0.212) data 0.001 (0.035) loss 2.5195 (2.9718) acc 87.5000 (79.2188) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [25/49] time 0.177 (0.205) data 0.000 (0.028) loss 2.8438 (2.9643) acc 84.3750 (79.5000) lr 1.5567e-04 eta 0:01:15
epoch [43/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.023) loss 3.0664 (2.9880) acc 78.1250 (78.6458) lr 1.5567e-04 eta 0:01:12
epoch [43/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.020) loss 2.7656 (2.9999) acc 78.1250 (78.4821) lr 1.5567e-04 eta 0:01:10
epoch [43/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 2.4121 (2.9821) acc 90.6250 (78.9062) lr 1.5567e-04 eta 0:01:08
epoch [43/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.0137 (2.9788) acc 84.3750 (79.0972) lr 1.5567e-04 eta 0:01:06
epoch [44/50] batch [5/49] time 0.175 (0.321) data 0.000 (0.144) loss 2.5742 (2.7574) acc 75.0000 (83.1250) lr 1.2369e-04 eta 0:01:48
epoch [44/50] batch [10/49] time 0.175 (0.249) data 0.000 (0.072) loss 2.2793 (2.7854) acc 93.7500 (85.3125) lr 1.2369e-04 eta 0:01:22
epoch [44/50] batch [15/49] time 0.177 (0.225) data 0.000 (0.048) loss 2.7617 (2.8845) acc 81.2500 (83.1250) lr 1.2369e-04 eta 0:01:13
epoch [44/50] batch [20/49] time 0.176 (0.213) data 0.000 (0.036) loss 3.3516 (2.9506) acc 75.0000 (81.2500) lr 1.2369e-04 eta 0:01:08
epoch [44/50] batch [25/49] time 0.176 (0.205) data 0.000 (0.029) loss 3.3594 (2.9759) acc 71.8750 (80.2500) lr 1.2369e-04 eta 0:01:05
epoch [44/50] batch [30/49] time 0.176 (0.201) data 0.000 (0.024) loss 3.0781 (2.9810) acc 78.1250 (80.6250) lr 1.2369e-04 eta 0:01:02
epoch [44/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.021) loss 2.7148 (3.0182) acc 87.5000 (79.9107) lr 1.2369e-04 eta 0:01:00
epoch [44/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 2.4316 (2.9944) acc 96.8750 (80.3906) lr 1.2369e-04 eta 0:00:58
epoch [44/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.0566 (3.0048) acc 75.0000 (80.1389) lr 1.2369e-04 eta 0:00:57
epoch [45/50] batch [5/49] time 0.176 (0.314) data 0.000 (0.136) loss 3.5391 (3.1637) acc 65.6250 (75.6250) lr 9.5173e-05 eta 0:01:30
epoch [45/50] batch [10/49] time 0.175 (0.245) data 0.000 (0.068) loss 2.4258 (3.0469) acc 87.5000 (75.6250) lr 9.5173e-05 eta 0:01:09
epoch [45/50] batch [15/49] time 0.174 (0.221) data 0.000 (0.045) loss 3.4395 (3.0249) acc 71.8750 (77.0833) lr 9.5173e-05 eta 0:01:01
epoch [45/50] batch [20/49] time 0.176 (0.210) data 0.000 (0.034) loss 2.7070 (3.0058) acc 78.1250 (77.3438) lr 9.5173e-05 eta 0:00:57
epoch [45/50] batch [25/49] time 0.175 (0.203) data 0.000 (0.027) loss 2.6465 (3.0123) acc 87.5000 (78.5000) lr 9.5173e-05 eta 0:00:54
epoch [45/50] batch [30/49] time 0.176 (0.198) data 0.000 (0.023) loss 2.8594 (3.0600) acc 75.0000 (77.6042) lr 9.5173e-05 eta 0:00:52
epoch [45/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.020) loss 2.5547 (3.0602) acc 81.2500 (77.6786) lr 9.5173e-05 eta 0:00:50
epoch [45/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.017) loss 2.4258 (3.0402) acc 90.6250 (78.5156) lr 9.5173e-05 eta 0:00:48
epoch [45/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.015) loss 2.7188 (3.0338) acc 87.5000 (78.6806) lr 9.5173e-05 eta 0:00:47
epoch [46/50] batch [5/49] time 0.177 (0.320) data 0.000 (0.142) loss 3.5625 (3.1184) acc 71.8750 (78.7500) lr 7.0224e-05 eta 0:01:16
epoch [46/50] batch [10/49] time 0.176 (0.248) data 0.000 (0.071) loss 3.4102 (3.1756) acc 78.1250 (76.2500) lr 7.0224e-05 eta 0:00:58
epoch [46/50] batch [15/49] time 0.177 (0.224) data 0.000 (0.047) loss 3.4023 (3.2126) acc 81.2500 (76.4583) lr 7.0224e-05 eta 0:00:51
epoch [46/50] batch [20/49] time 0.179 (0.213) data 0.000 (0.036) loss 2.9492 (3.1580) acc 81.2500 (77.0312) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [25/49] time 0.178 (0.206) data 0.000 (0.029) loss 3.3613 (3.1709) acc 71.8750 (77.3750) lr 7.0224e-05 eta 0:00:45
epoch [46/50] batch [30/49] time 0.176 (0.201) data 0.000 (0.024) loss 3.2227 (3.1822) acc 75.0000 (77.2917) lr 7.0224e-05 eta 0:00:43
epoch [46/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.020) loss 2.5605 (3.1460) acc 87.5000 (78.0357) lr 7.0224e-05 eta 0:00:41
epoch [46/50] batch [40/49] time 0.177 (0.195) data 0.000 (0.018) loss 2.8125 (3.1299) acc 78.1250 (77.9688) lr 7.0224e-05 eta 0:00:39
epoch [46/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.016) loss 2.7188 (3.0977) acc 81.2500 (78.4028) lr 7.0224e-05 eta 0:00:38
epoch [47/50] batch [5/49] time 0.177 (0.298) data 0.000 (0.120) loss 3.2227 (3.0496) acc 71.8750 (80.6250) lr 4.8943e-05 eta 0:00:56
epoch [47/50] batch [10/49] time 0.177 (0.237) data 0.000 (0.060) loss 2.8340 (3.0121) acc 84.3750 (80.0000) lr 4.8943e-05 eta 0:00:44
epoch [47/50] batch [15/49] time 0.176 (0.217) data 0.000 (0.040) loss 3.4414 (3.0158) acc 78.1250 (80.2083) lr 4.8943e-05 eta 0:00:39
epoch [47/50] batch [20/49] time 0.177 (0.207) data 0.000 (0.030) loss 2.9844 (3.0663) acc 71.8750 (78.7500) lr 4.8943e-05 eta 0:00:36
epoch [47/50] batch [25/49] time 0.178 (0.201) data 0.000 (0.024) loss 2.9785 (3.0518) acc 81.2500 (79.7500) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [30/49] time 0.176 (0.197) data 0.000 (0.020) loss 2.8125 (3.0674) acc 81.2500 (79.3750) lr 4.8943e-05 eta 0:00:32
epoch [47/50] batch [35/49] time 0.176 (0.194) data 0.000 (0.017) loss 2.6992 (3.0660) acc 81.2500 (79.5536) lr 4.8943e-05 eta 0:00:31
epoch [47/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.015) loss 2.7383 (3.0368) acc 87.5000 (80.1562) lr 4.8943e-05 eta 0:00:29
epoch [47/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.014) loss 2.9180 (3.0497) acc 75.0000 (79.4444) lr 4.8943e-05 eta 0:00:28
epoch [48/50] batch [5/49] time 0.177 (0.308) data 0.000 (0.131) loss 2.8984 (2.9473) acc 78.1250 (78.7500) lr 3.1417e-05 eta 0:00:43
epoch [48/50] batch [10/49] time 0.176 (0.242) data 0.000 (0.066) loss 2.8730 (2.9688) acc 81.2500 (79.3750) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [15/49] time 0.179 (0.221) data 0.000 (0.044) loss 3.5273 (3.0357) acc 68.7500 (78.9583) lr 3.1417e-05 eta 0:00:29
epoch [48/50] batch [20/49] time 0.177 (0.210) data 0.000 (0.033) loss 3.1113 (3.0297) acc 75.0000 (78.1250) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [25/49] time 0.178 (0.204) data 0.001 (0.027) loss 2.8691 (3.0183) acc 81.2500 (78.5000) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [30/49] time 0.176 (0.199) data 0.000 (0.022) loss 3.1035 (3.0072) acc 87.5000 (79.1667) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [35/49] time 0.176 (0.196) data 0.000 (0.019) loss 2.6465 (2.9825) acc 90.6250 (79.3750) lr 3.1417e-05 eta 0:00:21
epoch [48/50] batch [40/49] time 0.176 (0.193) data 0.000 (0.017) loss 3.6250 (3.0111) acc 71.8750 (79.2188) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.015) loss 2.9219 (3.0052) acc 78.1250 (79.4444) lr 3.1417e-05 eta 0:00:19
epoch [49/50] batch [5/49] time 0.176 (0.301) data 0.000 (0.124) loss 2.5391 (2.9266) acc 87.5000 (81.8750) lr 1.7713e-05 eta 0:00:28
epoch [49/50] batch [10/49] time 0.177 (0.240) data 0.000 (0.062) loss 3.5703 (2.9854) acc 71.8750 (80.9375) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [15/49] time 0.177 (0.219) data 0.000 (0.042) loss 3.3848 (2.9254) acc 65.6250 (81.4583) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [20/49] time 0.177 (0.208) data 0.000 (0.031) loss 2.6348 (2.9053) acc 87.5000 (81.8750) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [25/49] time 0.176 (0.202) data 0.000 (0.025) loss 3.0273 (2.9066) acc 78.1250 (81.0000) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [30/49] time 0.177 (0.198) data 0.000 (0.021) loss 3.0078 (2.9174) acc 87.5000 (82.0833) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.018) loss 3.4883 (2.9546) acc 71.8750 (81.8750) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [40/49] time 0.177 (0.192) data 0.000 (0.016) loss 3.1445 (2.9703) acc 78.1250 (81.4062) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [45/49] time 0.177 (0.191) data 0.000 (0.014) loss 3.0156 (2.9519) acc 81.2500 (81.6667) lr 1.7713e-05 eta 0:00:10
epoch [50/50] batch [5/49] time 0.174 (0.380) data 0.000 (0.203) loss 3.8359 (3.0973) acc 71.8750 (76.8750) lr 7.8853e-06 eta 0:00:16
epoch [50/50] batch [10/49] time 0.179 (0.278) data 0.000 (0.102) loss 2.9180 (3.0617) acc 87.5000 (80.0000) lr 7.8853e-06 eta 0:00:10
epoch [50/50] batch [15/49] time 0.176 (0.244) data 0.000 (0.068) loss 2.7383 (2.9832) acc 84.3750 (81.8750) lr 7.8853e-06 eta 0:00:08
epoch [50/50] batch [20/49] time 0.179 (0.228) data 0.000 (0.051) loss 2.8672 (2.9184) acc 81.2500 (82.3438) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [25/49] time 0.178 (0.218) data 0.000 (0.041) loss 2.7070 (2.9209) acc 84.3750 (81.7500) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [30/49] time 0.179 (0.211) data 0.000 (0.034) loss 2.9570 (2.9688) acc 75.0000 (80.5208) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [35/49] time 0.178 (0.206) data 0.000 (0.029) loss 2.6367 (2.9441) acc 87.5000 (80.2679) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [40/49] time 0.176 (0.203) data 0.000 (0.026) loss 2.7168 (2.9538) acc 84.3750 (79.9219) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [45/49] time 0.176 (0.200) data 0.000 (0.023) loss 3.1660 (2.9549) acc 81.2500 (80.4861) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:06<00:55,  6.98s/it] 22%|██▏       | 2/9 [00:08<00:24,  3.55s/it] 33%|███▎      | 3/9 [00:09<00:14,  2.45s/it] 44%|████▍     | 4/9 [00:10<00:09,  1.94s/it] 56%|█████▌    | 5/9 [00:11<00:06,  1.65s/it] 67%|██████▋   | 6/9 [00:12<00:04,  1.48s/it] 78%|███████▊  | 7/9 [00:13<00:02,  1.37s/it] 89%|████████▉ | 8/9 [00:15<00:01,  1.30s/it]100%|██████████| 9/9 [00:15<00:00,  1.69s/it]
=> result
* total: 4,002
* correct: 3,263
* accuracy: 81.5%
* error: 18.5%
* macro_f1: 81.3%
Elapsed: 0:08:10
Run this job and save the output to output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,002
---------  ------------
['2000 AM General Hummer SUV', '2012 Acura RL Sedan', '2012 Acura TL Sedan', '2008 Acura TL Type-S', '2012 Acura TSX Sedan', '2001 Acura Integra Type R', '2012 Acura ZDX Hatchback', '2012 Aston Martin V8 Vantage Convertible', '2012 Aston Martin V8 Vantage Coupe', '2012 Aston Martin Virage Convertible', '2012 Aston Martin Virage Coupe', '2008 Audi RS 4 Convertible', '2012 Audi A5 Coupe', '2012 Audi TTS Coupe', '2012 Audi R8 Coupe', '1994 Audi V8 Sedan', '1994 Audi 100 Sedan', '1994 Audi 100 Wagon', '2011 Audi TT Hatchback', '2011 Audi S6 Sedan', '2012 Audi S5 Convertible', '2012 Audi S5 Coupe', '2012 Audi S4 Sedan', '2007 Audi S4 Sedan', '2012 Audi TT RS Coupe', '2012 BMW ActiveHybrid 5 Sedan', '2012 BMW 1 Series Convertible', '2012 BMW 1 Series Coupe', '2012 BMW 3 Series Sedan', '2012 BMW 3 Series Wagon', '2007 BMW 6 Series Convertible', '2007 BMW X5 SUV', '2012 BMW X6 SUV', '2012 BMW M3 Coupe', '2010 BMW M5 Sedan', '2010 BMW M6 Convertible', '2012 BMW X3 SUV', '2012 BMW Z4 Convertible', '2012 Bentley Continental Supersports Conv. Convertible', '2009 Bentley Arnage Sedan', '2011 Bentley Mulsanne Sedan', '2012 Bentley Continental GT Coupe', '2007 Bentley Continental GT Coupe', '2007 Bentley Continental Flying Spur Sedan', '2009 Bugatti Veyron 16.4 Convertible', '2009 Bugatti Veyron 16.4 Coupe', '2012 Buick Regal GS', '2007 Buick Rainier SUV', '2012 Buick Verano Sedan', '2012 Buick Enclave SUV', '2012 Cadillac CTS-V Sedan', '2012 Cadillac SRX SUV', '2007 Cadillac Escalade EXT Crew Cab', '2012 Chevrolet Silverado 1500 Hybrid Crew Cab', '2012 Chevrolet Corvette Convertible', '2012 Chevrolet Corvette ZR1', '2007 Chevrolet Corvette Ron Fellows Edition Z06', '2012 Chevrolet Traverse SUV', '2012 Chevrolet Camaro Convertible', '2010 Chevrolet HHR SS', '2007 Chevrolet Impala Sedan', '2012 Chevrolet Tahoe Hybrid SUV', '2012 Chevrolet Sonic Sedan', '2007 Chevrolet Express Cargo Van', '2012 Chevrolet Avalanche Crew Cab', '2010 Chevrolet Cobalt SS', '2010 Chevrolet Malibu Hybrid Sedan', '2009 Chevrolet TrailBlazer SS', '2012 Chevrolet Silverado 2500HD Regular Cab', '2007 Chevrolet Silverado 1500 Classic Extended Cab', '2007 Chevrolet Express Van', '2007 Chevrolet Monte Carlo Coupe', '2007 Chevrolet Malibu Sedan', '2012 Chevrolet Silverado 1500 Extended Cab', '2012 Chevrolet Silverado 1500 Regular Cab', '2009 Chrysler Aspen SUV', '2010 Chrysler Sebring Convertible', '2012 Chrysler Town and Country Minivan', '2010 Chrysler 300 SRT-8', '2008 Chrysler Crossfire Convertible', '2008 Chrysler PT Cruiser Convertible', '2002 Daewoo Nubira Wagon', '2012 Dodge Caliber Wagon', '2007 Dodge Caliber Wagon', '1997 Dodge Caravan Minivan', '2010 Dodge Ram Pickup 3500 Crew Cab', '2009 Dodge Ram Pickup 3500 Quad Cab', '2009 Dodge Sprinter Cargo Van', '2012 Dodge Journey SUV', '2010 Dodge Dakota Crew Cab', '2007 Dodge Dakota Club Cab', '2008 Dodge Magnum Wagon', '2011 Dodge Challenger SRT8', '2012 Dodge Durango SUV', '2007 Dodge Durango SUV', '2012 Dodge Charger Sedan', '2009 Dodge Charger SRT-8', '1998 Eagle Talon Hatchback']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2000 AM General Hummer SUV, a type of car', 'X X X X 2012 Acura RL Sedan, a type of car', 'X X X X 2012 Acura TL Sedan, a type of car', 'X X X X 2008 Acura TL Type-S, a type of car', 'X X X X 2012 Acura TSX Sedan, a type of car', 'X X X X 2001 Acura Integra Type R, a type of car', 'X X X X 2012 Acura ZDX Hatchback, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Convertible, a type of car', 'X X X X 2012 Aston Martin V8 Vantage Coupe, a type of car', 'X X X X 2012 Aston Martin Virage Convertible, a type of car', 'X X X X 2012 Aston Martin Virage Coupe, a type of car', 'X X X X 2008 Audi RS 4 Convertible, a type of car', 'X X X X 2012 Audi A5 Coupe, a type of car', 'X X X X 2012 Audi TTS Coupe, a type of car', 'X X X X 2012 Audi R8 Coupe, a type of car', 'X X X X 1994 Audi V8 Sedan, a type of car', 'X X X X 1994 Audi 100 Sedan, a type of car', 'X X X X 1994 Audi 100 Wagon, a type of car', 'X X X X 2011 Audi TT Hatchback, a type of car', 'X X X X 2011 Audi S6 Sedan, a type of car', 'X X X X 2012 Audi S5 Convertible, a type of car', 'X X X X 2012 Audi S5 Coupe, a type of car', 'X X X X 2012 Audi S4 Sedan, a type of car', 'X X X X 2007 Audi S4 Sedan, a type of car', 'X X X X 2012 Audi TT RS Coupe, a type of car', 'X X X X 2012 BMW ActiveHybrid 5 Sedan, a type of car', 'X X X X 2012 BMW 1 Series Convertible, a type of car', 'X X X X 2012 BMW 1 Series Coupe, a type of car', 'X X X X 2012 BMW 3 Series Sedan, a type of car', 'X X X X 2012 BMW 3 Series Wagon, a type of car', 'X X X X 2007 BMW 6 Series Convertible, a type of car', 'X X X X 2007 BMW X5 SUV, a type of car', 'X X X X 2012 BMW X6 SUV, a type of car', 'X X X X 2012 BMW M3 Coupe, a type of car', 'X X X X 2010 BMW M5 Sedan, a type of car', 'X X X X 2010 BMW M6 Convertible, a type of car', 'X X X X 2012 BMW X3 SUV, a type of car', 'X X X X 2012 BMW Z4 Convertible, a type of car', 'X X X X 2012 Bentley Continental Supersports Conv. Convertible, a type of car', 'X X X X 2009 Bentley Arnage Sedan, a type of car', 'X X X X 2011 Bentley Mulsanne Sedan, a type of car', 'X X X X 2012 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental GT Coupe, a type of car', 'X X X X 2007 Bentley Continental Flying Spur Sedan, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Convertible, a type of car', 'X X X X 2009 Bugatti Veyron 16.4 Coupe, a type of car', 'X X X X 2012 Buick Regal GS, a type of car', 'X X X X 2007 Buick Rainier SUV, a type of car', 'X X X X 2012 Buick Verano Sedan, a type of car', 'X X X X 2012 Buick Enclave SUV, a type of car', 'X X X X 2012 Cadillac CTS-V Sedan, a type of car', 'X X X X 2012 Cadillac SRX SUV, a type of car', 'X X X X 2007 Cadillac Escalade EXT Crew Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Hybrid Crew Cab, a type of car', 'X X X X 2012 Chevrolet Corvette Convertible, a type of car', 'X X X X 2012 Chevrolet Corvette ZR1, a type of car', 'X X X X 2007 Chevrolet Corvette Ron Fellows Edition Z06, a type of car', 'X X X X 2012 Chevrolet Traverse SUV, a type of car', 'X X X X 2012 Chevrolet Camaro Convertible, a type of car', 'X X X X 2010 Chevrolet HHR SS, a type of car', 'X X X X 2007 Chevrolet Impala Sedan, a type of car', 'X X X X 2012 Chevrolet Tahoe Hybrid SUV, a type of car', 'X X X X 2012 Chevrolet Sonic Sedan, a type of car', 'X X X X 2007 Chevrolet Express Cargo Van, a type of car', 'X X X X 2012 Chevrolet Avalanche Crew Cab, a type of car', 'X X X X 2010 Chevrolet Cobalt SS, a type of car', 'X X X X 2010 Chevrolet Malibu Hybrid Sedan, a type of car', 'X X X X 2009 Chevrolet TrailBlazer SS, a type of car', 'X X X X 2012 Chevrolet Silverado 2500HD Regular Cab, a type of car', 'X X X X 2007 Chevrolet Silverado 1500 Classic Extended Cab, a type of car', 'X X X X 2007 Chevrolet Express Van, a type of car', 'X X X X 2007 Chevrolet Monte Carlo Coupe, a type of car', 'X X X X 2007 Chevrolet Malibu Sedan, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Extended Cab, a type of car', 'X X X X 2012 Chevrolet Silverado 1500 Regular Cab, a type of car', 'X X X X 2009 Chrysler Aspen SUV, a type of car', 'X X X X 2010 Chrysler Sebring Convertible, a type of car', 'X X X X 2012 Chrysler Town and Country Minivan, a type of car', 'X X X X 2010 Chrysler 300 SRT-8, a type of car', 'X X X X 2008 Chrysler Crossfire Convertible, a type of car', 'X X X X 2008 Chrysler PT Cruiser Convertible, a type of car', 'X X X X 2002 Daewoo Nubira Wagon, a type of car', 'X X X X 2012 Dodge Caliber Wagon, a type of car', 'X X X X 2007 Dodge Caliber Wagon, a type of car', 'X X X X 1997 Dodge Caravan Minivan, a type of car', 'X X X X 2010 Dodge Ram Pickup 3500 Crew Cab, a type of car', 'X X X X 2009 Dodge Ram Pickup 3500 Quad Cab, a type of car', 'X X X X 2009 Dodge Sprinter Cargo Van, a type of car', 'X X X X 2012 Dodge Journey SUV, a type of car', 'X X X X 2010 Dodge Dakota Crew Cab, a type of car', 'X X X X 2007 Dodge Dakota Club Cab, a type of car', 'X X X X 2008 Dodge Magnum Wagon, a type of car', 'X X X X 2011 Dodge Challenger SRT8, a type of car', 'X X X X 2012 Dodge Durango SUV, a type of car', 'X X X X 2007 Dodge Durango SUV, a type of car', 'X X X X 2012 Dodge Charger Sedan, a type of car', 'X X X X 2009 Dodge Charger SRT-8, a type of car', 'X X X X 1998 Eagle Talon Hatchback, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([98, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/49] time 0.175 (0.323) data 0.000 (0.137) loss 7.0156 (7.3289) acc 50.0000 (47.5000) lr 1.0000e-05 eta 0:13:10
epoch [1/50] batch [10/49] time 0.175 (0.249) data 0.000 (0.069) loss 6.8906 (7.2570) acc 50.0000 (49.0625) lr 1.0000e-05 eta 0:10:07
epoch [1/50] batch [15/49] time 0.174 (0.224) data 0.000 (0.046) loss 7.3281 (7.2021) acc 46.8750 (48.7500) lr 1.0000e-05 eta 0:09:05
epoch [1/50] batch [20/49] time 0.174 (0.212) data 0.000 (0.034) loss 6.9180 (7.1121) acc 43.7500 (49.0625) lr 1.0000e-05 eta 0:08:34
epoch [1/50] batch [25/49] time 0.174 (0.204) data 0.000 (0.028) loss 7.3516 (7.0866) acc 37.5000 (48.8750) lr 1.0000e-05 eta 0:08:14
epoch [1/50] batch [30/49] time 0.174 (0.199) data 0.000 (0.023) loss 7.0469 (7.0557) acc 50.0000 (48.8542) lr 1.0000e-05 eta 0:08:02
epoch [1/50] batch [35/49] time 0.173 (0.196) data 0.000 (0.020) loss 6.6797 (6.9994) acc 56.2500 (48.9286) lr 1.0000e-05 eta 0:07:53
epoch [1/50] batch [40/49] time 0.174 (0.193) data 0.000 (0.017) loss 6.3867 (6.9570) acc 53.1250 (48.9062) lr 1.0000e-05 eta 0:07:45
epoch [1/50] batch [45/49] time 0.174 (0.191) data 0.000 (0.015) loss 6.7109 (6.9246) acc 46.8750 (48.4722) lr 1.0000e-05 eta 0:07:39
epoch [2/50] batch [5/49] time 0.175 (0.321) data 0.000 (0.145) loss 5.4492 (5.8648) acc 56.2500 (58.1250) lr 2.0000e-03 eta 0:12:48
epoch [2/50] batch [10/49] time 0.174 (0.248) data 0.000 (0.073) loss 5.5312 (5.6672) acc 46.8750 (53.7500) lr 2.0000e-03 eta 0:09:53
epoch [2/50] batch [15/49] time 0.174 (0.223) data 0.000 (0.049) loss 5.1875 (5.5193) acc 43.7500 (52.2917) lr 2.0000e-03 eta 0:08:52
epoch [2/50] batch [20/49] time 0.173 (0.211) data 0.000 (0.036) loss 5.0039 (5.4686) acc 53.1250 (50.6250) lr 2.0000e-03 eta 0:08:22
epoch [2/50] batch [25/49] time 0.174 (0.203) data 0.000 (0.029) loss 4.6172 (5.3620) acc 62.5000 (50.7500) lr 2.0000e-03 eta 0:08:03
epoch [2/50] batch [30/49] time 0.174 (0.199) data 0.000 (0.024) loss 4.5977 (5.2727) acc 53.1250 (49.4792) lr 2.0000e-03 eta 0:07:50
epoch [2/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.021) loss 4.5195 (5.1765) acc 56.2500 (50.0000) lr 2.0000e-03 eta 0:07:41
epoch [2/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.018) loss 4.3398 (5.0959) acc 56.2500 (50.3906) lr 2.0000e-03 eta 0:07:34
epoch [2/50] batch [45/49] time 0.173 (0.190) data 0.000 (0.016) loss 4.5234 (5.0116) acc 56.2500 (51.1806) lr 2.0000e-03 eta 0:07:28
epoch [3/50] batch [5/49] time 0.175 (0.314) data 0.000 (0.138) loss 4.1797 (4.2570) acc 46.8750 (53.1250) lr 1.9980e-03 eta 0:12:15
epoch [3/50] batch [10/49] time 0.174 (0.244) data 0.001 (0.069) loss 4.3828 (4.2943) acc 50.0000 (54.6875) lr 1.9980e-03 eta 0:09:31
epoch [3/50] batch [15/49] time 0.175 (0.221) data 0.000 (0.046) loss 4.2734 (4.3074) acc 50.0000 (54.3750) lr 1.9980e-03 eta 0:08:37
epoch [3/50] batch [20/49] time 0.176 (0.210) data 0.000 (0.035) loss 4.4375 (4.2925) acc 59.3750 (54.8438) lr 1.9980e-03 eta 0:08:09
epoch [3/50] batch [25/49] time 0.176 (0.203) data 0.000 (0.028) loss 3.7910 (4.2787) acc 75.0000 (54.1250) lr 1.9980e-03 eta 0:07:52
epoch [3/50] batch [30/49] time 0.175 (0.198) data 0.000 (0.023) loss 4.7930 (4.2850) acc 43.7500 (54.3750) lr 1.9980e-03 eta 0:07:40
epoch [3/50] batch [35/49] time 0.174 (0.195) data 0.000 (0.020) loss 4.2188 (4.2644) acc 40.6250 (55.0000) lr 1.9980e-03 eta 0:07:31
epoch [3/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.017) loss 3.8398 (4.2676) acc 53.1250 (53.9062) lr 1.9980e-03 eta 0:07:24
epoch [3/50] batch [45/49] time 0.174 (0.190) data 0.000 (0.016) loss 4.2773 (4.2794) acc 37.5000 (53.1944) lr 1.9980e-03 eta 0:07:19
epoch [4/50] batch [5/49] time 0.177 (0.333) data 0.000 (0.156) loss 4.3594 (4.1656) acc 53.1250 (51.2500) lr 1.9921e-03 eta 0:12:44
epoch [4/50] batch [10/49] time 0.176 (0.255) data 0.001 (0.078) loss 3.9453 (4.1406) acc 53.1250 (53.7500) lr 1.9921e-03 eta 0:09:44
epoch [4/50] batch [15/49] time 0.176 (0.229) data 0.000 (0.052) loss 4.5391 (4.2281) acc 50.0000 (51.4583) lr 1.9921e-03 eta 0:08:43
epoch [4/50] batch [20/49] time 0.178 (0.216) data 0.001 (0.039) loss 4.1445 (4.2109) acc 46.8750 (51.0938) lr 1.9921e-03 eta 0:08:12
epoch [4/50] batch [25/49] time 0.175 (0.208) data 0.000 (0.032) loss 3.6914 (4.1535) acc 53.1250 (52.6250) lr 1.9921e-03 eta 0:07:53
epoch [4/50] batch [30/49] time 0.175 (0.202) data 0.000 (0.026) loss 3.5918 (4.1620) acc 65.6250 (51.6667) lr 1.9921e-03 eta 0:07:39
epoch [4/50] batch [35/49] time 0.174 (0.199) data 0.000 (0.023) loss 3.4961 (4.1164) acc 68.7500 (52.9464) lr 1.9921e-03 eta 0:07:30
epoch [4/50] batch [40/49] time 0.174 (0.195) data 0.000 (0.020) loss 4.0312 (4.1069) acc 53.1250 (53.0469) lr 1.9921e-03 eta 0:07:22
epoch [4/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.018) loss 4.3164 (4.1149) acc 53.1250 (52.9861) lr 1.9921e-03 eta 0:07:15
epoch [5/50] batch [5/49] time 0.175 (0.357) data 0.000 (0.180) loss 4.0312 (3.7559) acc 43.7500 (58.7500) lr 1.9823e-03 eta 0:13:22
epoch [5/50] batch [10/49] time 0.176 (0.267) data 0.000 (0.090) loss 3.3633 (3.8729) acc 68.7500 (57.8125) lr 1.9823e-03 eta 0:09:58
epoch [5/50] batch [15/49] time 0.175 (0.236) data 0.000 (0.060) loss 3.8613 (3.8565) acc 50.0000 (57.9167) lr 1.9823e-03 eta 0:08:49
epoch [5/50] batch [20/49] time 0.176 (0.221) data 0.000 (0.045) loss 4.1992 (3.9421) acc 40.6250 (56.8750) lr 1.9823e-03 eta 0:08:13
epoch [5/50] batch [25/49] time 0.174 (0.212) data 0.000 (0.036) loss 3.9570 (3.9191) acc 65.6250 (58.2500) lr 1.9823e-03 eta 0:07:52
epoch [5/50] batch [30/49] time 0.177 (0.206) data 0.000 (0.030) loss 4.0352 (3.9176) acc 59.3750 (57.6042) lr 1.9823e-03 eta 0:07:38
epoch [5/50] batch [35/49] time 0.179 (0.202) data 0.000 (0.026) loss 3.9219 (3.9167) acc 56.2500 (57.5893) lr 1.9823e-03 eta 0:07:27
epoch [5/50] batch [40/49] time 0.175 (0.198) data 0.000 (0.023) loss 3.6953 (3.8860) acc 56.2500 (58.2812) lr 1.9823e-03 eta 0:07:19
epoch [5/50] batch [45/49] time 0.175 (0.196) data 0.000 (0.020) loss 3.5684 (3.8832) acc 53.1250 (57.5694) lr 1.9823e-03 eta 0:07:12
epoch [6/50] batch [5/49] time 0.176 (0.314) data 0.000 (0.137) loss 4.3047 (3.9375) acc 46.8750 (53.7500) lr 1.9686e-03 eta 0:11:29
epoch [6/50] batch [10/49] time 0.175 (0.245) data 0.000 (0.069) loss 3.7617 (3.8508) acc 56.2500 (53.7500) lr 1.9686e-03 eta 0:08:57
epoch [6/50] batch [15/49] time 0.175 (0.222) data 0.000 (0.046) loss 3.9648 (3.8146) acc 50.0000 (55.8333) lr 1.9686e-03 eta 0:08:05
epoch [6/50] batch [20/49] time 0.176 (0.210) data 0.000 (0.034) loss 3.4805 (3.8155) acc 65.6250 (56.7188) lr 1.9686e-03 eta 0:07:39
epoch [6/50] batch [25/49] time 0.176 (0.203) data 0.000 (0.028) loss 4.0312 (3.8237) acc 40.6250 (54.8750) lr 1.9686e-03 eta 0:07:23
epoch [6/50] batch [30/49] time 0.175 (0.199) data 0.000 (0.023) loss 4.0547 (3.8378) acc 62.5000 (55.6250) lr 1.9686e-03 eta 0:07:12
epoch [6/50] batch [35/49] time 0.174 (0.195) data 0.000 (0.020) loss 3.8594 (3.7729) acc 56.2500 (57.1429) lr 1.9686e-03 eta 0:07:04
epoch [6/50] batch [40/49] time 0.174 (0.193) data 0.000 (0.017) loss 3.8359 (3.7764) acc 56.2500 (57.4219) lr 1.9686e-03 eta 0:06:57
epoch [6/50] batch [45/49] time 0.176 (0.191) data 0.000 (0.015) loss 4.1094 (3.8023) acc 50.0000 (56.8750) lr 1.9686e-03 eta 0:06:51
epoch [7/50] batch [5/49] time 0.175 (0.328) data 0.000 (0.151) loss 3.6270 (3.6648) acc 65.6250 (58.1250) lr 1.9511e-03 eta 0:11:45
epoch [7/50] batch [10/49] time 0.176 (0.252) data 0.000 (0.076) loss 3.3086 (3.6992) acc 65.6250 (57.8125) lr 1.9511e-03 eta 0:09:01
epoch [7/50] batch [15/49] time 0.175 (0.227) data 0.000 (0.051) loss 2.7891 (3.5949) acc 75.0000 (61.0417) lr 1.9511e-03 eta 0:08:05
epoch [7/50] batch [20/49] time 0.176 (0.214) data 0.000 (0.038) loss 3.4531 (3.5684) acc 62.5000 (60.9375) lr 1.9511e-03 eta 0:07:36
epoch [7/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.030) loss 3.4609 (3.6291) acc 68.7500 (61.0000) lr 1.9511e-03 eta 0:07:19
epoch [7/50] batch [30/49] time 0.178 (0.201) data 0.000 (0.025) loss 3.9648 (3.6590) acc 53.1250 (60.5208) lr 1.9511e-03 eta 0:07:07
epoch [7/50] batch [35/49] time 0.174 (0.197) data 0.000 (0.022) loss 4.1875 (3.6980) acc 53.1250 (60.0000) lr 1.9511e-03 eta 0:06:58
epoch [7/50] batch [40/49] time 0.174 (0.194) data 0.000 (0.019) loss 3.8301 (3.6946) acc 62.5000 (60.3125) lr 1.9511e-03 eta 0:06:51
epoch [7/50] batch [45/49] time 0.173 (0.192) data 0.000 (0.017) loss 3.8223 (3.6866) acc 56.2500 (59.7917) lr 1.9511e-03 eta 0:06:45
epoch [8/50] batch [5/49] time 0.178 (0.348) data 0.000 (0.171) loss 3.7969 (3.3461) acc 56.2500 (65.0000) lr 1.9298e-03 eta 0:12:12
epoch [8/50] batch [10/49] time 0.175 (0.262) data 0.000 (0.086) loss 3.4727 (3.5338) acc 62.5000 (61.2500) lr 1.9298e-03 eta 0:09:10
epoch [8/50] batch [15/49] time 0.176 (0.233) data 0.000 (0.057) loss 3.5273 (3.4837) acc 65.6250 (61.2500) lr 1.9298e-03 eta 0:08:08
epoch [8/50] batch [20/49] time 0.176 (0.219) data 0.000 (0.043) loss 3.7070 (3.5171) acc 59.3750 (61.2500) lr 1.9298e-03 eta 0:07:37
epoch [8/50] batch [25/49] time 0.177 (0.210) data 0.000 (0.034) loss 3.2969 (3.5324) acc 68.7500 (61.8750) lr 1.9298e-03 eta 0:07:18
epoch [8/50] batch [30/49] time 0.177 (0.205) data 0.000 (0.029) loss 3.4766 (3.5521) acc 53.1250 (62.0833) lr 1.9298e-03 eta 0:07:05
epoch [8/50] batch [35/49] time 0.177 (0.201) data 0.000 (0.025) loss 3.7227 (3.5766) acc 56.2500 (61.8750) lr 1.9298e-03 eta 0:06:56
epoch [8/50] batch [40/49] time 0.177 (0.198) data 0.000 (0.022) loss 3.9727 (3.6122) acc 46.8750 (60.9375) lr 1.9298e-03 eta 0:06:48
epoch [8/50] batch [45/49] time 0.177 (0.195) data 0.000 (0.019) loss 3.8516 (3.6196) acc 56.2500 (60.6250) lr 1.9298e-03 eta 0:06:43
epoch [9/50] batch [5/49] time 0.174 (0.323) data 0.000 (0.147) loss 3.3945 (3.3922) acc 68.7500 (70.0000) lr 1.9048e-03 eta 0:11:02
epoch [9/50] batch [10/49] time 0.174 (0.248) data 0.000 (0.074) loss 3.4023 (3.5490) acc 59.3750 (65.9375) lr 1.9048e-03 eta 0:08:28
epoch [9/50] batch [15/49] time 0.176 (0.224) data 0.000 (0.049) loss 3.3398 (3.5374) acc 71.8750 (64.3750) lr 1.9048e-03 eta 0:07:37
epoch [9/50] batch [20/49] time 0.175 (0.212) data 0.000 (0.037) loss 3.6328 (3.5558) acc 68.7500 (64.2188) lr 1.9048e-03 eta 0:07:11
epoch [9/50] batch [25/49] time 0.176 (0.205) data 0.000 (0.030) loss 3.6875 (3.6192) acc 65.6250 (62.6250) lr 1.9048e-03 eta 0:06:56
epoch [9/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.025) loss 3.3496 (3.6048) acc 68.7500 (62.2917) lr 1.9048e-03 eta 0:06:45
epoch [9/50] batch [35/49] time 0.177 (0.197) data 0.000 (0.021) loss 3.6230 (3.6071) acc 59.3750 (61.6071) lr 1.9048e-03 eta 0:06:37
epoch [9/50] batch [40/49] time 0.174 (0.194) data 0.000 (0.019) loss 3.6328 (3.5971) acc 59.3750 (62.0312) lr 1.9048e-03 eta 0:06:31
epoch [9/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.017) loss 3.6875 (3.5686) acc 50.0000 (62.7778) lr 1.9048e-03 eta 0:06:25
epoch [10/50] batch [5/49] time 0.176 (0.314) data 0.000 (0.138) loss 3.5195 (3.5848) acc 65.6250 (64.3750) lr 1.8763e-03 eta 0:10:29
epoch [10/50] batch [10/49] time 0.180 (0.245) data 0.004 (0.069) loss 3.2988 (3.4408) acc 65.6250 (66.8750) lr 1.8763e-03 eta 0:08:10
epoch [10/50] batch [15/49] time 0.176 (0.223) data 0.000 (0.046) loss 3.5566 (3.4146) acc 71.8750 (67.5000) lr 1.8763e-03 eta 0:07:23
epoch [10/50] batch [20/49] time 0.180 (0.211) data 0.000 (0.035) loss 4.0156 (3.4653) acc 46.8750 (66.2500) lr 1.8763e-03 eta 0:06:59
epoch [10/50] batch [25/49] time 0.176 (0.204) data 0.000 (0.028) loss 4.0430 (3.4884) acc 59.3750 (64.8750) lr 1.8763e-03 eta 0:06:45
epoch [10/50] batch [30/49] time 0.175 (0.200) data 0.000 (0.023) loss 3.7598 (3.4917) acc 68.7500 (64.3750) lr 1.8763e-03 eta 0:06:34
epoch [10/50] batch [35/49] time 0.174 (0.196) data 0.000 (0.020) loss 3.7168 (3.5206) acc 68.7500 (64.5536) lr 1.8763e-03 eta 0:06:26
epoch [10/50] batch [40/49] time 0.174 (0.193) data 0.000 (0.018) loss 3.5664 (3.5527) acc 59.3750 (63.2031) lr 1.8763e-03 eta 0:06:20
epoch [10/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.016) loss 4.0820 (3.5984) acc 46.8750 (62.0833) lr 1.8763e-03 eta 0:06:15
epoch [11/50] batch [5/49] time 0.179 (0.356) data 0.001 (0.177) loss 3.8555 (3.6594) acc 53.1250 (59.3750) lr 1.8443e-03 eta 0:11:35
epoch [11/50] batch [10/49] time 0.177 (0.266) data 0.001 (0.089) loss 3.3438 (3.6818) acc 59.3750 (57.5000) lr 1.8443e-03 eta 0:08:39
epoch [11/50] batch [15/49] time 0.174 (0.236) data 0.000 (0.059) loss 2.9980 (3.6358) acc 71.8750 (59.3750) lr 1.8443e-03 eta 0:07:38
epoch [11/50] batch [20/49] time 0.176 (0.221) data 0.000 (0.045) loss 3.3945 (3.6134) acc 68.7500 (60.6250) lr 1.8443e-03 eta 0:07:07
epoch [11/50] batch [25/49] time 0.174 (0.211) data 0.000 (0.036) loss 4.1914 (3.6141) acc 59.3750 (61.5000) lr 1.8443e-03 eta 0:06:48
epoch [11/50] batch [30/49] time 0.174 (0.205) data 0.000 (0.030) loss 3.7188 (3.6081) acc 62.5000 (61.9792) lr 1.8443e-03 eta 0:06:35
epoch [11/50] batch [35/49] time 0.176 (0.201) data 0.000 (0.026) loss 3.7695 (3.6135) acc 43.7500 (61.5179) lr 1.8443e-03 eta 0:06:26
epoch [11/50] batch [40/49] time 0.176 (0.198) data 0.000 (0.022) loss 2.7910 (3.5674) acc 87.5000 (62.4219) lr 1.8443e-03 eta 0:06:19
epoch [11/50] batch [45/49] time 0.176 (0.195) data 0.000 (0.020) loss 3.3301 (3.5509) acc 68.7500 (62.5694) lr 1.8443e-03 eta 0:06:14
epoch [12/50] batch [5/49] time 0.174 (0.325) data 0.000 (0.149) loss 3.1406 (3.4937) acc 75.0000 (71.2500) lr 1.8090e-03 eta 0:10:20
epoch [12/50] batch [10/49] time 0.174 (0.250) data 0.000 (0.074) loss 2.9277 (3.3836) acc 78.1250 (71.8750) lr 1.8090e-03 eta 0:07:55
epoch [12/50] batch [15/49] time 0.174 (0.225) data 0.000 (0.050) loss 3.7070 (3.4100) acc 59.3750 (68.9583) lr 1.8090e-03 eta 0:07:06
epoch [12/50] batch [20/49] time 0.175 (0.212) data 0.000 (0.037) loss 3.0215 (3.3633) acc 71.8750 (68.7500) lr 1.8090e-03 eta 0:06:41
epoch [12/50] batch [25/49] time 0.178 (0.205) data 0.000 (0.030) loss 4.0547 (3.3994) acc 56.2500 (67.7500) lr 1.8090e-03 eta 0:06:27
epoch [12/50] batch [30/49] time 0.175 (0.201) data 0.000 (0.025) loss 3.4727 (3.4716) acc 62.5000 (66.3542) lr 1.8090e-03 eta 0:06:17
epoch [12/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.021) loss 3.2441 (3.4355) acc 68.7500 (66.7857) lr 1.8090e-03 eta 0:06:09
epoch [12/50] batch [40/49] time 0.174 (0.194) data 0.000 (0.019) loss 3.5273 (3.4419) acc 75.0000 (67.1094) lr 1.8090e-03 eta 0:06:03
epoch [12/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.017) loss 3.7891 (3.4493) acc 62.5000 (66.8750) lr 1.8090e-03 eta 0:05:58
epoch [13/50] batch [5/49] time 0.175 (0.335) data 0.000 (0.159) loss 3.4707 (3.3199) acc 62.5000 (65.0000) lr 1.7705e-03 eta 0:10:21
epoch [13/50] batch [10/49] time 0.178 (0.256) data 0.000 (0.080) loss 3.4219 (3.4625) acc 62.5000 (63.7500) lr 1.7705e-03 eta 0:07:53
epoch [13/50] batch [15/49] time 0.178 (0.230) data 0.000 (0.053) loss 3.3203 (3.4464) acc 71.8750 (65.0000) lr 1.7705e-03 eta 0:07:03
epoch [13/50] batch [20/49] time 0.175 (0.216) data 0.000 (0.040) loss 3.3965 (3.4271) acc 68.7500 (66.2500) lr 1.7705e-03 eta 0:06:38
epoch [13/50] batch [25/49] time 0.174 (0.208) data 0.000 (0.032) loss 3.2734 (3.4156) acc 71.8750 (66.2500) lr 1.7705e-03 eta 0:06:22
epoch [13/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.027) loss 3.3359 (3.3587) acc 62.5000 (67.7083) lr 1.7705e-03 eta 0:06:10
epoch [13/50] batch [35/49] time 0.174 (0.198) data 0.000 (0.023) loss 3.1562 (3.3489) acc 65.6250 (67.5893) lr 1.7705e-03 eta 0:06:02
epoch [13/50] batch [40/49] time 0.174 (0.195) data 0.000 (0.020) loss 3.4414 (3.3941) acc 71.8750 (66.8750) lr 1.7705e-03 eta 0:05:56
epoch [13/50] batch [45/49] time 0.176 (0.193) data 0.000 (0.018) loss 3.4160 (3.4102) acc 59.3750 (66.3194) lr 1.7705e-03 eta 0:05:50
epoch [14/50] batch [5/49] time 0.176 (0.332) data 0.000 (0.155) loss 2.7461 (2.9934) acc 62.5000 (76.8750) lr 1.7290e-03 eta 0:09:59
epoch [14/50] batch [10/49] time 0.175 (0.255) data 0.000 (0.078) loss 3.1445 (3.1979) acc 68.7500 (71.5625) lr 1.7290e-03 eta 0:07:38
epoch [14/50] batch [15/49] time 0.175 (0.228) data 0.000 (0.052) loss 3.2266 (3.1904) acc 68.7500 (71.0417) lr 1.7290e-03 eta 0:06:50
epoch [14/50] batch [20/49] time 0.175 (0.215) data 0.000 (0.039) loss 3.2852 (3.2292) acc 62.5000 (69.8438) lr 1.7290e-03 eta 0:06:25
epoch [14/50] batch [25/49] time 0.176 (0.207) data 0.000 (0.031) loss 3.4043 (3.2533) acc 65.6250 (69.8750) lr 1.7290e-03 eta 0:06:10
epoch [14/50] batch [30/49] time 0.177 (0.202) data 0.000 (0.026) loss 4.0742 (3.3279) acc 65.6250 (68.2292) lr 1.7290e-03 eta 0:06:00
epoch [14/50] batch [35/49] time 0.175 (0.198) data 0.000 (0.023) loss 3.7070 (3.3718) acc 65.6250 (68.2143) lr 1.7290e-03 eta 0:05:52
epoch [14/50] batch [40/49] time 0.177 (0.196) data 0.000 (0.020) loss 3.6719 (3.3658) acc 71.8750 (68.3594) lr 1.7290e-03 eta 0:05:46
epoch [14/50] batch [45/49] time 0.175 (0.193) data 0.000 (0.018) loss 2.8711 (3.3799) acc 81.2500 (68.1944) lr 1.7290e-03 eta 0:05:42
epoch [15/50] batch [5/49] time 0.176 (0.304) data 0.000 (0.127) loss 3.3203 (3.5598) acc 68.7500 (60.6250) lr 1.6845e-03 eta 0:08:54
epoch [15/50] batch [10/49] time 0.175 (0.240) data 0.000 (0.064) loss 2.8945 (3.4654) acc 78.1250 (64.6875) lr 1.6845e-03 eta 0:07:00
epoch [15/50] batch [15/49] time 0.175 (0.218) data 0.000 (0.043) loss 2.8125 (3.4328) acc 75.0000 (66.2500) lr 1.6845e-03 eta 0:06:21
epoch [15/50] batch [20/49] time 0.176 (0.208) data 0.000 (0.032) loss 3.4688 (3.4333) acc 68.7500 (66.2500) lr 1.6845e-03 eta 0:06:01
epoch [15/50] batch [25/49] time 0.178 (0.202) data 0.000 (0.026) loss 3.3145 (3.4452) acc 53.1250 (66.5000) lr 1.6845e-03 eta 0:05:50
epoch [15/50] batch [30/49] time 0.176 (0.197) data 0.000 (0.021) loss 2.9121 (3.4120) acc 75.0000 (66.9792) lr 1.6845e-03 eta 0:05:42
epoch [15/50] batch [35/49] time 0.175 (0.194) data 0.000 (0.018) loss 2.9238 (3.4126) acc 71.8750 (66.5179) lr 1.6845e-03 eta 0:05:35
epoch [15/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.016) loss 3.9434 (3.4473) acc 59.3750 (65.7031) lr 1.6845e-03 eta 0:05:30
epoch [15/50] batch [45/49] time 0.174 (0.190) data 0.000 (0.014) loss 3.6250 (3.4801) acc 68.7500 (65.5556) lr 1.6845e-03 eta 0:05:26
epoch [16/50] batch [5/49] time 0.178 (0.284) data 0.000 (0.107) loss 2.6250 (3.3008) acc 81.2500 (73.7500) lr 1.6374e-03 eta 0:08:04
epoch [16/50] batch [10/49] time 0.177 (0.230) data 0.000 (0.054) loss 2.9395 (3.3287) acc 78.1250 (72.1875) lr 1.6374e-03 eta 0:06:31
epoch [16/50] batch [15/49] time 0.177 (0.212) data 0.000 (0.036) loss 3.4395 (3.3232) acc 68.7500 (71.4583) lr 1.6374e-03 eta 0:06:00
epoch [16/50] batch [20/49] time 0.176 (0.203) data 0.000 (0.027) loss 3.1758 (3.2821) acc 65.6250 (69.6875) lr 1.6374e-03 eta 0:05:44
epoch [16/50] batch [25/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.9512 (3.3605) acc 56.2500 (68.7500) lr 1.6374e-03 eta 0:05:34
epoch [16/50] batch [30/49] time 0.176 (0.194) data 0.000 (0.018) loss 3.5391 (3.3694) acc 68.7500 (69.2708) lr 1.6374e-03 eta 0:05:26
epoch [16/50] batch [35/49] time 0.174 (0.191) data 0.000 (0.016) loss 4.4297 (3.3835) acc 56.2500 (68.4821) lr 1.6374e-03 eta 0:05:21
epoch [16/50] batch [40/49] time 0.174 (0.189) data 0.000 (0.014) loss 3.8184 (3.3886) acc 56.2500 (68.3594) lr 1.6374e-03 eta 0:05:16
epoch [16/50] batch [45/49] time 0.174 (0.187) data 0.000 (0.012) loss 3.4805 (3.3965) acc 71.8750 (68.5417) lr 1.6374e-03 eta 0:05:13
epoch [17/50] batch [5/49] time 0.176 (0.327) data 0.000 (0.151) loss 3.2461 (3.3832) acc 65.6250 (64.3750) lr 1.5878e-03 eta 0:09:03
epoch [17/50] batch [10/49] time 0.175 (0.252) data 0.000 (0.076) loss 3.2500 (3.3100) acc 78.1250 (68.7500) lr 1.5878e-03 eta 0:06:56
epoch [17/50] batch [15/49] time 0.176 (0.226) data 0.000 (0.051) loss 3.3281 (3.2771) acc 62.5000 (68.7500) lr 1.5878e-03 eta 0:06:13
epoch [17/50] batch [20/49] time 0.175 (0.214) data 0.000 (0.038) loss 3.4609 (3.3256) acc 68.7500 (67.8125) lr 1.5878e-03 eta 0:05:51
epoch [17/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.030) loss 3.1445 (3.3539) acc 68.7500 (67.3750) lr 1.5878e-03 eta 0:05:37
epoch [17/50] batch [30/49] time 0.178 (0.201) data 0.000 (0.025) loss 3.1074 (3.4014) acc 78.1250 (67.7083) lr 1.5878e-03 eta 0:05:28
epoch [17/50] batch [35/49] time 0.174 (0.197) data 0.000 (0.022) loss 3.6797 (3.4237) acc 62.5000 (67.2321) lr 1.5878e-03 eta 0:05:21
epoch [17/50] batch [40/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.0527 (3.4208) acc 75.0000 (67.0312) lr 1.5878e-03 eta 0:05:16
epoch [17/50] batch [45/49] time 0.176 (0.192) data 0.000 (0.017) loss 3.0586 (3.3997) acc 65.6250 (67.3611) lr 1.5878e-03 eta 0:05:11
epoch [18/50] batch [5/49] time 0.174 (0.305) data 0.000 (0.129) loss 3.1641 (3.2012) acc 84.3750 (73.1250) lr 1.5358e-03 eta 0:08:11
epoch [18/50] batch [10/49] time 0.178 (0.240) data 0.004 (0.065) loss 2.3008 (3.1191) acc 84.3750 (72.8125) lr 1.5358e-03 eta 0:06:25
epoch [18/50] batch [15/49] time 0.174 (0.218) data 0.000 (0.044) loss 2.9551 (3.2367) acc 81.2500 (69.7917) lr 1.5358e-03 eta 0:05:49
epoch [18/50] batch [20/49] time 0.176 (0.207) data 0.000 (0.033) loss 3.5273 (3.2732) acc 65.6250 (69.5312) lr 1.5358e-03 eta 0:05:31
epoch [18/50] batch [25/49] time 0.176 (0.201) data 0.000 (0.026) loss 3.1875 (3.2882) acc 68.7500 (69.6250) lr 1.5358e-03 eta 0:05:20
epoch [18/50] batch [30/49] time 0.176 (0.197) data 0.000 (0.022) loss 4.1055 (3.3127) acc 62.5000 (70.0000) lr 1.5358e-03 eta 0:05:12
epoch [18/50] batch [35/49] time 0.175 (0.194) data 0.000 (0.019) loss 2.9902 (3.3213) acc 65.6250 (69.5536) lr 1.5358e-03 eta 0:05:07
epoch [18/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.016) loss 3.1992 (3.3072) acc 81.2500 (69.5312) lr 1.5358e-03 eta 0:05:02
epoch [18/50] batch [45/49] time 0.174 (0.190) data 0.000 (0.015) loss 3.8047 (3.3033) acc 68.7500 (69.1667) lr 1.5358e-03 eta 0:04:58
epoch [19/50] batch [5/49] time 0.176 (0.316) data 0.000 (0.141) loss 3.5156 (3.4125) acc 71.8750 (66.2500) lr 1.4818e-03 eta 0:08:14
epoch [19/50] batch [10/49] time 0.177 (0.247) data 0.000 (0.071) loss 2.8711 (3.3432) acc 81.2500 (67.5000) lr 1.4818e-03 eta 0:06:24
epoch [19/50] batch [15/49] time 0.176 (0.223) data 0.000 (0.047) loss 2.8418 (3.2628) acc 71.8750 (69.3750) lr 1.4818e-03 eta 0:05:46
epoch [19/50] batch [20/49] time 0.176 (0.212) data 0.000 (0.035) loss 3.0430 (3.2199) acc 81.2500 (71.5625) lr 1.4818e-03 eta 0:05:27
epoch [19/50] batch [25/49] time 0.174 (0.204) data 0.000 (0.028) loss 2.7695 (3.2281) acc 71.8750 (71.6250) lr 1.4818e-03 eta 0:05:15
epoch [19/50] batch [30/49] time 0.177 (0.200) data 0.000 (0.024) loss 3.4219 (3.2713) acc 62.5000 (70.4167) lr 1.4818e-03 eta 0:05:07
epoch [19/50] batch [35/49] time 0.174 (0.196) data 0.000 (0.020) loss 3.3203 (3.2493) acc 68.7500 (70.2679) lr 1.4818e-03 eta 0:05:00
epoch [19/50] batch [40/49] time 0.174 (0.193) data 0.000 (0.018) loss 3.3008 (3.2413) acc 65.6250 (70.3906) lr 1.4818e-03 eta 0:04:55
epoch [19/50] batch [45/49] time 0.174 (0.191) data 0.000 (0.016) loss 3.5078 (3.2677) acc 75.0000 (70.0694) lr 1.4818e-03 eta 0:04:51
epoch [20/50] batch [5/49] time 0.179 (0.322) data 0.000 (0.145) loss 3.1133 (3.1293) acc 65.6250 (70.6250) lr 1.4258e-03 eta 0:08:08
epoch [20/50] batch [10/49] time 0.176 (0.249) data 0.000 (0.073) loss 3.1504 (3.3207) acc 71.8750 (68.7500) lr 1.4258e-03 eta 0:06:16
epoch [20/50] batch [15/49] time 0.175 (0.225) data 0.000 (0.049) loss 2.9004 (3.2523) acc 84.3750 (70.2083) lr 1.4258e-03 eta 0:05:37
epoch [20/50] batch [20/49] time 0.175 (0.212) data 0.000 (0.037) loss 3.5156 (3.2567) acc 65.6250 (69.0625) lr 1.4258e-03 eta 0:05:18
epoch [20/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.029) loss 3.2539 (3.2683) acc 62.5000 (68.8750) lr 1.4258e-03 eta 0:05:06
epoch [20/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.8613 (3.2632) acc 87.5000 (70.1042) lr 1.4258e-03 eta 0:04:58
epoch [20/50] batch [35/49] time 0.177 (0.197) data 0.000 (0.021) loss 2.8750 (3.2891) acc 71.8750 (69.6429) lr 1.4258e-03 eta 0:04:52
epoch [20/50] batch [40/49] time 0.176 (0.194) data 0.000 (0.018) loss 3.5000 (3.3079) acc 71.8750 (69.7656) lr 1.4258e-03 eta 0:04:47
epoch [20/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.016) loss 2.9980 (3.2893) acc 78.1250 (70.0000) lr 1.4258e-03 eta 0:04:42
epoch [21/50] batch [5/49] time 0.175 (0.320) data 0.000 (0.143) loss 3.4102 (3.2703) acc 75.0000 (75.6250) lr 1.3681e-03 eta 0:07:48
epoch [21/50] batch [10/49] time 0.176 (0.248) data 0.000 (0.072) loss 2.7734 (3.3486) acc 81.2500 (71.8750) lr 1.3681e-03 eta 0:06:01
epoch [21/50] batch [15/49] time 0.175 (0.223) data 0.000 (0.048) loss 3.3789 (3.3861) acc 65.6250 (70.6250) lr 1.3681e-03 eta 0:05:25
epoch [21/50] batch [20/49] time 0.176 (0.212) data 0.000 (0.036) loss 3.0273 (3.3327) acc 81.2500 (70.7812) lr 1.3681e-03 eta 0:05:07
epoch [21/50] batch [25/49] time 0.176 (0.205) data 0.000 (0.029) loss 4.0273 (3.3382) acc 62.5000 (70.3750) lr 1.3681e-03 eta 0:04:55
epoch [21/50] batch [30/49] time 0.175 (0.200) data 0.000 (0.024) loss 4.0742 (3.3430) acc 56.2500 (70.3125) lr 1.3681e-03 eta 0:04:47
epoch [21/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.2773 (3.3444) acc 68.7500 (69.5536) lr 1.3681e-03 eta 0:04:42
epoch [21/50] batch [40/49] time 0.174 (0.194) data 0.000 (0.018) loss 3.4355 (3.3446) acc 71.8750 (69.0625) lr 1.3681e-03 eta 0:04:37
epoch [21/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.016) loss 2.8594 (3.3218) acc 87.5000 (69.7917) lr 1.3681e-03 eta 0:04:33
epoch [22/50] batch [5/49] time 0.175 (0.313) data 0.000 (0.138) loss 3.6133 (3.1820) acc 65.6250 (69.3750) lr 1.3090e-03 eta 0:07:23
epoch [22/50] batch [10/49] time 0.177 (0.245) data 0.000 (0.069) loss 3.3750 (3.1299) acc 65.6250 (70.9375) lr 1.3090e-03 eta 0:05:45
epoch [22/50] batch [15/49] time 0.174 (0.222) data 0.000 (0.046) loss 2.9668 (3.1721) acc 78.1250 (71.2500) lr 1.3090e-03 eta 0:05:11
epoch [22/50] batch [20/49] time 0.176 (0.210) data 0.000 (0.035) loss 3.3848 (3.1815) acc 65.6250 (70.7812) lr 1.3090e-03 eta 0:04:54
epoch [22/50] batch [25/49] time 0.175 (0.203) data 0.000 (0.028) loss 3.3613 (3.1500) acc 68.7500 (71.8750) lr 1.3090e-03 eta 0:04:43
epoch [22/50] batch [30/49] time 0.174 (0.198) data 0.000 (0.023) loss 2.9453 (3.1333) acc 84.3750 (72.6042) lr 1.3090e-03 eta 0:04:35
epoch [22/50] batch [35/49] time 0.174 (0.195) data 0.000 (0.020) loss 3.8789 (3.1592) acc 68.7500 (72.6786) lr 1.3090e-03 eta 0:04:30
epoch [22/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.017) loss 3.7578 (3.1972) acc 56.2500 (72.1094) lr 1.3090e-03 eta 0:04:25
epoch [22/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.015) loss 3.8750 (3.2080) acc 56.2500 (71.9444) lr 1.3090e-03 eta 0:04:22
epoch [23/50] batch [5/49] time 0.176 (0.317) data 0.000 (0.141) loss 3.4219 (3.6156) acc 65.6250 (65.6250) lr 1.2487e-03 eta 0:07:13
epoch [23/50] batch [10/49] time 0.175 (0.247) data 0.000 (0.071) loss 3.4473 (3.4777) acc 59.3750 (67.5000) lr 1.2487e-03 eta 0:05:36
epoch [23/50] batch [15/49] time 0.174 (0.223) data 0.000 (0.047) loss 2.7891 (3.3098) acc 75.0000 (69.3750) lr 1.2487e-03 eta 0:05:02
epoch [23/50] batch [20/49] time 0.174 (0.211) data 0.000 (0.035) loss 2.7793 (3.2749) acc 87.5000 (70.1562) lr 1.2487e-03 eta 0:04:45
epoch [23/50] batch [25/49] time 0.175 (0.204) data 0.000 (0.028) loss 3.6680 (3.2378) acc 62.5000 (70.6250) lr 1.2487e-03 eta 0:04:34
epoch [23/50] batch [30/49] time 0.176 (0.199) data 0.000 (0.024) loss 3.1250 (3.2013) acc 71.8750 (70.9375) lr 1.2487e-03 eta 0:04:26
epoch [23/50] batch [35/49] time 0.176 (0.196) data 0.000 (0.020) loss 3.4688 (3.1987) acc 53.1250 (70.5357) lr 1.2487e-03 eta 0:04:21
epoch [23/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.018) loss 3.0059 (3.2078) acc 78.1250 (70.6250) lr 1.2487e-03 eta 0:04:17
epoch [23/50] batch [45/49] time 0.176 (0.191) data 0.000 (0.016) loss 3.7383 (3.2446) acc 68.7500 (70.0000) lr 1.2487e-03 eta 0:04:13
epoch [24/50] batch [5/49] time 0.174 (0.302) data 0.000 (0.126) loss 2.8066 (3.1035) acc 84.3750 (73.7500) lr 1.1874e-03 eta 0:06:38
epoch [24/50] batch [10/49] time 0.176 (0.239) data 0.000 (0.063) loss 2.9883 (3.0941) acc 75.0000 (72.8125) lr 1.1874e-03 eta 0:05:13
epoch [24/50] batch [15/49] time 0.175 (0.218) data 0.000 (0.042) loss 3.0645 (3.2491) acc 81.2500 (71.8750) lr 1.1874e-03 eta 0:04:44
epoch [24/50] batch [20/49] time 0.175 (0.207) data 0.000 (0.032) loss 3.6602 (3.2197) acc 71.8750 (72.3438) lr 1.1874e-03 eta 0:04:29
epoch [24/50] batch [25/49] time 0.178 (0.201) data 0.000 (0.026) loss 3.6504 (3.2503) acc 59.3750 (71.6250) lr 1.1874e-03 eta 0:04:21
epoch [24/50] batch [30/49] time 0.177 (0.197) data 0.000 (0.021) loss 2.8594 (3.2174) acc 84.3750 (72.5000) lr 1.1874e-03 eta 0:04:14
epoch [24/50] batch [35/49] time 0.176 (0.194) data 0.000 (0.018) loss 2.9199 (3.1925) acc 81.2500 (73.0357) lr 1.1874e-03 eta 0:04:10
epoch [24/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.3262 (3.1805) acc 71.8750 (73.2812) lr 1.1874e-03 eta 0:04:06
epoch [24/50] batch [45/49] time 0.176 (0.190) data 0.000 (0.014) loss 2.8574 (3.1949) acc 84.3750 (72.3611) lr 1.1874e-03 eta 0:04:02
epoch [25/50] batch [5/49] time 0.176 (0.324) data 0.000 (0.148) loss 3.0820 (3.1488) acc 78.1250 (76.8750) lr 1.1253e-03 eta 0:06:51
epoch [25/50] batch [10/49] time 0.176 (0.251) data 0.000 (0.074) loss 3.9414 (3.2262) acc 53.1250 (72.1875) lr 1.1253e-03 eta 0:05:16
epoch [25/50] batch [15/49] time 0.177 (0.226) data 0.000 (0.049) loss 2.6426 (3.1805) acc 93.7500 (74.7917) lr 1.1253e-03 eta 0:04:44
epoch [25/50] batch [20/49] time 0.177 (0.213) data 0.000 (0.037) loss 3.4648 (3.1531) acc 65.6250 (74.8438) lr 1.1253e-03 eta 0:04:27
epoch [25/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.030) loss 3.8281 (3.2303) acc 56.2500 (73.3750) lr 1.1253e-03 eta 0:04:17
epoch [25/50] batch [30/49] time 0.176 (0.201) data 0.000 (0.025) loss 3.6719 (3.2565) acc 62.5000 (72.9167) lr 1.1253e-03 eta 0:04:09
epoch [25/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.021) loss 3.2266 (3.2432) acc 71.8750 (72.6786) lr 1.1253e-03 eta 0:04:04
epoch [25/50] batch [40/49] time 0.174 (0.194) data 0.000 (0.019) loss 3.6562 (3.2521) acc 71.8750 (72.5781) lr 1.1253e-03 eta 0:03:59
epoch [25/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.017) loss 3.2090 (3.2429) acc 68.7500 (72.0833) lr 1.1253e-03 eta 0:03:56
epoch [26/50] batch [5/49] time 0.176 (0.357) data 0.000 (0.180) loss 3.5703 (3.0348) acc 65.6250 (75.0000) lr 1.0628e-03 eta 0:07:15
epoch [26/50] batch [10/49] time 0.176 (0.267) data 0.000 (0.090) loss 2.9004 (3.2379) acc 75.0000 (72.5000) lr 1.0628e-03 eta 0:05:24
epoch [26/50] batch [15/49] time 0.176 (0.237) data 0.000 (0.060) loss 3.0000 (3.2668) acc 68.7500 (71.2500) lr 1.0628e-03 eta 0:04:46
epoch [26/50] batch [20/49] time 0.176 (0.222) data 0.000 (0.045) loss 3.3359 (3.2495) acc 78.1250 (72.0312) lr 1.0628e-03 eta 0:04:26
epoch [26/50] batch [25/49] time 0.177 (0.213) data 0.000 (0.036) loss 3.4102 (3.2322) acc 62.5000 (72.2500) lr 1.0628e-03 eta 0:04:15
epoch [26/50] batch [30/49] time 0.178 (0.207) data 0.000 (0.030) loss 3.3555 (3.2536) acc 68.7500 (71.2500) lr 1.0628e-03 eta 0:04:07
epoch [26/50] batch [35/49] time 0.176 (0.203) data 0.000 (0.026) loss 3.1582 (3.2540) acc 71.8750 (71.6071) lr 1.0628e-03 eta 0:04:00
epoch [26/50] batch [40/49] time 0.176 (0.199) data 0.000 (0.023) loss 2.9414 (3.2410) acc 84.3750 (72.0312) lr 1.0628e-03 eta 0:03:56
epoch [26/50] batch [45/49] time 0.177 (0.197) data 0.000 (0.020) loss 3.4766 (3.2601) acc 75.0000 (71.8056) lr 1.0628e-03 eta 0:03:52
epoch [27/50] batch [5/49] time 0.177 (0.346) data 0.000 (0.168) loss 2.8594 (3.2934) acc 75.0000 (67.5000) lr 1.0000e-03 eta 0:06:44
epoch [27/50] batch [10/49] time 0.176 (0.261) data 0.000 (0.084) loss 3.6797 (3.2053) acc 62.5000 (70.3125) lr 1.0000e-03 eta 0:05:03
epoch [27/50] batch [15/49] time 0.176 (0.233) data 0.000 (0.056) loss 2.5977 (3.1393) acc 78.1250 (72.7083) lr 1.0000e-03 eta 0:04:30
epoch [27/50] batch [20/49] time 0.177 (0.219) data 0.000 (0.042) loss 3.2168 (3.1946) acc 68.7500 (72.1875) lr 1.0000e-03 eta 0:04:13
epoch [27/50] batch [25/49] time 0.177 (0.211) data 0.000 (0.034) loss 3.3027 (3.1700) acc 71.8750 (73.5000) lr 1.0000e-03 eta 0:04:02
epoch [27/50] batch [30/49] time 0.177 (0.205) data 0.000 (0.028) loss 2.9902 (3.1549) acc 84.3750 (73.9583) lr 1.0000e-03 eta 0:03:54
epoch [27/50] batch [35/49] time 0.175 (0.201) data 0.000 (0.024) loss 2.7168 (3.1425) acc 81.2500 (73.9286) lr 1.0000e-03 eta 0:03:49
epoch [27/50] batch [40/49] time 0.175 (0.198) data 0.000 (0.021) loss 3.4941 (3.1650) acc 65.6250 (73.5938) lr 1.0000e-03 eta 0:03:44
epoch [27/50] batch [45/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.3711 (3.1901) acc 68.7500 (73.3333) lr 1.0000e-03 eta 0:03:40
epoch [28/50] batch [5/49] time 0.177 (0.310) data 0.000 (0.132) loss 2.9258 (2.8699) acc 78.1250 (77.5000) lr 9.3721e-04 eta 0:05:47
epoch [28/50] batch [10/49] time 0.176 (0.243) data 0.000 (0.066) loss 3.4590 (3.0611) acc 71.8750 (75.6250) lr 9.3721e-04 eta 0:04:31
epoch [28/50] batch [15/49] time 0.176 (0.221) data 0.000 (0.044) loss 3.4121 (3.1082) acc 78.1250 (75.6250) lr 9.3721e-04 eta 0:04:05
epoch [28/50] batch [20/49] time 0.176 (0.209) data 0.000 (0.033) loss 3.7891 (3.1336) acc 59.3750 (75.3125) lr 9.3721e-04 eta 0:03:51
epoch [28/50] batch [25/49] time 0.176 (0.203) data 0.000 (0.027) loss 2.9941 (3.1242) acc 81.2500 (75.3750) lr 9.3721e-04 eta 0:03:43
epoch [28/50] batch [30/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.2891 (3.1695) acc 68.7500 (74.4792) lr 9.3721e-04 eta 0:03:37
epoch [28/50] batch [35/49] time 0.174 (0.195) data 0.000 (0.019) loss 3.5625 (3.1475) acc 68.7500 (75.0000) lr 9.3721e-04 eta 0:03:32
epoch [28/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.017) loss 2.9160 (3.1346) acc 81.2500 (74.9219) lr 9.3721e-04 eta 0:03:29
epoch [28/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.015) loss 3.1289 (3.1577) acc 84.3750 (74.6528) lr 9.3721e-04 eta 0:03:26
epoch [29/50] batch [5/49] time 0.177 (0.304) data 0.000 (0.127) loss 3.2461 (3.2820) acc 68.7500 (70.0000) lr 8.7467e-04 eta 0:05:26
epoch [29/50] batch [10/49] time 0.176 (0.240) data 0.000 (0.064) loss 3.0527 (3.2215) acc 81.2500 (73.7500) lr 8.7467e-04 eta 0:04:16
epoch [29/50] batch [15/49] time 0.176 (0.219) data 0.000 (0.043) loss 2.9844 (3.2590) acc 75.0000 (72.5000) lr 8.7467e-04 eta 0:03:52
epoch [29/50] batch [20/49] time 0.178 (0.208) data 0.000 (0.032) loss 3.4277 (3.2582) acc 78.1250 (74.2188) lr 8.7467e-04 eta 0:03:40
epoch [29/50] batch [25/49] time 0.176 (0.202) data 0.000 (0.026) loss 3.4824 (3.3071) acc 71.8750 (73.8750) lr 8.7467e-04 eta 0:03:32
epoch [29/50] batch [30/49] time 0.176 (0.198) data 0.000 (0.022) loss 3.2793 (3.2874) acc 75.0000 (74.0625) lr 8.7467e-04 eta 0:03:27
epoch [29/50] batch [35/49] time 0.176 (0.195) data 0.000 (0.018) loss 3.2344 (3.2860) acc 78.1250 (74.2857) lr 8.7467e-04 eta 0:03:23
epoch [29/50] batch [40/49] time 0.177 (0.192) data 0.000 (0.016) loss 3.3457 (3.2814) acc 68.7500 (74.4531) lr 8.7467e-04 eta 0:03:19
epoch [29/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.014) loss 3.0215 (3.2731) acc 68.7500 (73.9583) lr 8.7467e-04 eta 0:03:16
epoch [30/50] batch [5/49] time 0.176 (0.304) data 0.000 (0.127) loss 3.0977 (2.9172) acc 87.5000 (81.8750) lr 8.1262e-04 eta 0:05:10
epoch [30/50] batch [10/49] time 0.176 (0.240) data 0.000 (0.063) loss 3.2539 (2.9789) acc 59.3750 (77.5000) lr 8.1262e-04 eta 0:04:04
epoch [30/50] batch [15/49] time 0.176 (0.218) data 0.000 (0.042) loss 3.3203 (2.9767) acc 68.7500 (76.4583) lr 8.1262e-04 eta 0:03:41
epoch [30/50] batch [20/49] time 0.176 (0.208) data 0.000 (0.032) loss 2.7773 (2.9883) acc 81.2500 (76.7188) lr 8.1262e-04 eta 0:03:29
epoch [30/50] batch [25/49] time 0.176 (0.202) data 0.000 (0.026) loss 3.2422 (2.9739) acc 68.7500 (77.1250) lr 8.1262e-04 eta 0:03:22
epoch [30/50] batch [30/49] time 0.179 (0.197) data 0.000 (0.021) loss 2.8887 (3.0301) acc 78.1250 (76.1458) lr 8.1262e-04 eta 0:03:17
epoch [30/50] batch [35/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.6562 (3.0649) acc 68.7500 (75.3571) lr 8.1262e-04 eta 0:03:13
epoch [30/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.016) loss 2.7734 (3.0574) acc 78.1250 (75.7812) lr 8.1262e-04 eta 0:03:09
epoch [30/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.014) loss 2.9375 (3.0645) acc 71.8750 (75.5556) lr 8.1262e-04 eta 0:03:06
epoch [31/50] batch [5/49] time 0.174 (0.313) data 0.000 (0.138) loss 3.4375 (3.1680) acc 68.7500 (75.0000) lr 7.5131e-04 eta 0:05:05
epoch [31/50] batch [10/49] time 0.176 (0.245) data 0.000 (0.069) loss 3.1895 (3.1602) acc 78.1250 (77.1875) lr 7.5131e-04 eta 0:03:57
epoch [31/50] batch [15/49] time 0.174 (0.221) data 0.000 (0.046) loss 3.4570 (3.1530) acc 62.5000 (76.6667) lr 7.5131e-04 eta 0:03:33
epoch [31/50] batch [20/49] time 0.175 (0.210) data 0.000 (0.035) loss 2.9414 (3.1559) acc 78.1250 (75.3125) lr 7.5131e-04 eta 0:03:21
epoch [31/50] batch [25/49] time 0.176 (0.203) data 0.000 (0.028) loss 3.2188 (3.1196) acc 75.0000 (75.3750) lr 7.5131e-04 eta 0:03:13
epoch [31/50] batch [30/49] time 0.173 (0.198) data 0.000 (0.023) loss 2.7168 (3.1331) acc 87.5000 (75.7292) lr 7.5131e-04 eta 0:03:08
epoch [31/50] batch [35/49] time 0.174 (0.195) data 0.000 (0.020) loss 3.6094 (3.1820) acc 68.7500 (75.0893) lr 7.5131e-04 eta 0:03:04
epoch [31/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.017) loss 3.4336 (3.1859) acc 65.6250 (74.9219) lr 7.5131e-04 eta 0:03:00
epoch [31/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.015) loss 3.0586 (3.1899) acc 75.0000 (74.5833) lr 7.5131e-04 eta 0:02:58
epoch [32/50] batch [5/49] time 0.175 (0.310) data 0.000 (0.133) loss 3.4727 (3.1211) acc 68.7500 (76.8750) lr 6.9098e-04 eta 0:04:47
epoch [32/50] batch [10/49] time 0.175 (0.243) data 0.000 (0.067) loss 3.3672 (3.1045) acc 75.0000 (75.9375) lr 6.9098e-04 eta 0:03:43
epoch [32/50] batch [15/49] time 0.177 (0.220) data 0.000 (0.044) loss 3.4492 (3.1477) acc 68.7500 (75.4167) lr 6.9098e-04 eta 0:03:21
epoch [32/50] batch [20/49] time 0.176 (0.209) data 0.000 (0.033) loss 3.2266 (3.1827) acc 75.0000 (75.7812) lr 6.9098e-04 eta 0:03:10
epoch [32/50] batch [25/49] time 0.175 (0.202) data 0.000 (0.027) loss 2.9160 (3.1595) acc 84.3750 (76.0000) lr 6.9098e-04 eta 0:03:03
epoch [32/50] batch [30/49] time 0.175 (0.198) data 0.000 (0.022) loss 3.0254 (3.1972) acc 68.7500 (75.2083) lr 6.9098e-04 eta 0:02:58
epoch [32/50] batch [35/49] time 0.174 (0.194) data 0.000 (0.019) loss 3.2012 (3.1836) acc 75.0000 (75.4464) lr 6.9098e-04 eta 0:02:54
epoch [32/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.017) loss 3.6562 (3.2220) acc 71.8750 (74.9219) lr 6.9098e-04 eta 0:02:51
epoch [32/50] batch [45/49] time 0.174 (0.190) data 0.000 (0.015) loss 3.3691 (3.2325) acc 65.6250 (74.6528) lr 6.9098e-04 eta 0:02:48
epoch [33/50] batch [5/49] time 0.176 (0.304) data 0.000 (0.127) loss 3.3613 (2.9355) acc 68.7500 (79.3750) lr 6.3188e-04 eta 0:04:26
epoch [33/50] batch [10/49] time 0.179 (0.240) data 0.003 (0.064) loss 2.9922 (3.0461) acc 75.0000 (76.8750) lr 6.3188e-04 eta 0:03:29
epoch [33/50] batch [15/49] time 0.175 (0.219) data 0.000 (0.043) loss 3.2031 (3.1118) acc 62.5000 (75.6250) lr 6.3188e-04 eta 0:03:10
epoch [33/50] batch [20/49] time 0.176 (0.208) data 0.000 (0.032) loss 3.0840 (3.0929) acc 78.1250 (76.0938) lr 6.3188e-04 eta 0:02:59
epoch [33/50] batch [25/49] time 0.175 (0.202) data 0.000 (0.026) loss 2.8359 (3.0616) acc 84.3750 (76.7500) lr 6.3188e-04 eta 0:02:53
epoch [33/50] batch [30/49] time 0.176 (0.198) data 0.000 (0.022) loss 2.9102 (3.0503) acc 81.2500 (77.1875) lr 6.3188e-04 eta 0:02:48
epoch [33/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.018) loss 2.4570 (3.0597) acc 90.6250 (77.2321) lr 6.3188e-04 eta 0:02:45
epoch [33/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.2812 (3.1013) acc 78.1250 (76.2500) lr 6.3188e-04 eta 0:02:41
epoch [33/50] batch [45/49] time 0.174 (0.190) data 0.000 (0.014) loss 3.2500 (3.0957) acc 71.8750 (76.3194) lr 6.3188e-04 eta 0:02:39
epoch [34/50] batch [5/49] time 0.178 (0.318) data 0.000 (0.140) loss 2.7305 (2.9707) acc 81.2500 (76.8750) lr 5.7422e-04 eta 0:04:22
epoch [34/50] batch [10/49] time 0.178 (0.247) data 0.000 (0.070) loss 3.4648 (3.1854) acc 78.1250 (74.3750) lr 5.7422e-04 eta 0:03:23
epoch [34/50] batch [15/49] time 0.177 (0.224) data 0.000 (0.047) loss 2.6953 (3.1195) acc 87.5000 (76.0417) lr 5.7422e-04 eta 0:03:03
epoch [34/50] batch [20/49] time 0.176 (0.212) data 0.000 (0.035) loss 3.0234 (3.1497) acc 78.1250 (76.2500) lr 5.7422e-04 eta 0:02:52
epoch [34/50] batch [25/49] time 0.176 (0.205) data 0.000 (0.028) loss 2.4883 (3.1845) acc 90.6250 (75.6250) lr 5.7422e-04 eta 0:02:45
epoch [34/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.9121 (3.1753) acc 68.7500 (75.3125) lr 5.7422e-04 eta 0:02:40
epoch [34/50] batch [35/49] time 0.176 (0.197) data 0.000 (0.020) loss 2.9590 (3.1474) acc 81.2500 (75.5357) lr 5.7422e-04 eta 0:02:37
epoch [34/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 2.8672 (3.1433) acc 81.2500 (75.0781) lr 5.7422e-04 eta 0:02:34
epoch [34/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.0293 (3.1760) acc 81.2500 (74.7917) lr 5.7422e-04 eta 0:02:31
epoch [35/50] batch [5/49] time 0.179 (0.303) data 0.000 (0.125) loss 2.8398 (3.0941) acc 81.2500 (76.2500) lr 5.1825e-04 eta 0:03:56
epoch [35/50] batch [10/49] time 0.177 (0.240) data 0.000 (0.063) loss 2.5820 (3.1635) acc 87.5000 (76.2500) lr 5.1825e-04 eta 0:03:05
epoch [35/50] batch [15/49] time 0.177 (0.219) data 0.000 (0.042) loss 3.4277 (3.2115) acc 71.8750 (75.6250) lr 5.1825e-04 eta 0:02:48
epoch [35/50] batch [20/49] time 0.176 (0.208) data 0.000 (0.031) loss 3.1055 (3.1742) acc 75.0000 (75.4688) lr 5.1825e-04 eta 0:02:39
epoch [35/50] batch [25/49] time 0.176 (0.202) data 0.000 (0.025) loss 2.9102 (3.1154) acc 78.1250 (76.7500) lr 5.1825e-04 eta 0:02:33
epoch [35/50] batch [30/49] time 0.176 (0.198) data 0.000 (0.021) loss 2.9570 (3.1225) acc 81.2500 (77.0833) lr 5.1825e-04 eta 0:02:28
epoch [35/50] batch [35/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.1191 (3.1219) acc 71.8750 (77.1429) lr 5.1825e-04 eta 0:02:25
epoch [35/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.016) loss 2.5664 (3.1024) acc 84.3750 (77.1875) lr 5.1825e-04 eta 0:02:22
epoch [35/50] batch [45/49] time 0.174 (0.190) data 0.000 (0.014) loss 3.0625 (3.0861) acc 75.0000 (77.0833) lr 5.1825e-04 eta 0:02:20
epoch [36/50] batch [5/49] time 0.175 (0.301) data 0.000 (0.124) loss 3.1230 (3.1816) acc 78.1250 (74.3750) lr 4.6417e-04 eta 0:03:39
epoch [36/50] batch [10/49] time 0.176 (0.239) data 0.000 (0.062) loss 2.7637 (3.1178) acc 87.5000 (75.9375) lr 4.6417e-04 eta 0:02:53
epoch [36/50] batch [15/49] time 0.177 (0.218) data 0.000 (0.041) loss 3.0684 (3.1518) acc 78.1250 (75.6250) lr 4.6417e-04 eta 0:02:36
epoch [36/50] batch [20/49] time 0.176 (0.208) data 0.000 (0.031) loss 2.7148 (3.1302) acc 81.2500 (75.7812) lr 4.6417e-04 eta 0:02:28
epoch [36/50] batch [25/49] time 0.176 (0.201) data 0.000 (0.025) loss 2.7734 (3.0702) acc 78.1250 (76.7500) lr 4.6417e-04 eta 0:02:22
epoch [36/50] batch [30/49] time 0.176 (0.197) data 0.000 (0.021) loss 3.2305 (3.0584) acc 75.0000 (77.3958) lr 4.6417e-04 eta 0:02:18
epoch [36/50] batch [35/49] time 0.176 (0.194) data 0.000 (0.018) loss 2.8145 (3.0570) acc 84.3750 (77.5000) lr 4.6417e-04 eta 0:02:15
epoch [36/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.0078 (3.0589) acc 81.2500 (77.7344) lr 4.6417e-04 eta 0:02:13
epoch [36/50] batch [45/49] time 0.174 (0.190) data 0.000 (0.014) loss 3.2812 (3.0543) acc 65.6250 (77.3611) lr 4.6417e-04 eta 0:02:10
epoch [37/50] batch [5/49] time 0.176 (0.309) data 0.000 (0.131) loss 3.6621 (2.9648) acc 65.6250 (79.3750) lr 4.1221e-04 eta 0:03:30
epoch [37/50] batch [10/49] time 0.176 (0.242) data 0.000 (0.066) loss 2.4102 (2.9738) acc 90.6250 (78.4375) lr 4.1221e-04 eta 0:02:43
epoch [37/50] batch [15/49] time 0.176 (0.220) data 0.000 (0.044) loss 3.2031 (3.0199) acc 75.0000 (78.9583) lr 4.1221e-04 eta 0:02:27
epoch [37/50] batch [20/49] time 0.177 (0.210) data 0.000 (0.033) loss 3.1055 (3.0233) acc 75.0000 (79.0625) lr 4.1221e-04 eta 0:02:19
epoch [37/50] batch [25/49] time 0.176 (0.203) data 0.000 (0.026) loss 3.2246 (3.0328) acc 71.8750 (79.0000) lr 4.1221e-04 eta 0:02:14
epoch [37/50] batch [30/49] time 0.177 (0.199) data 0.000 (0.022) loss 3.6602 (3.0455) acc 59.3750 (78.8542) lr 4.1221e-04 eta 0:02:10
epoch [37/50] batch [35/49] time 0.175 (0.196) data 0.000 (0.019) loss 3.3887 (3.0609) acc 62.5000 (78.3929) lr 4.1221e-04 eta 0:02:07
epoch [37/50] batch [40/49] time 0.175 (0.193) data 0.000 (0.017) loss 2.1758 (3.0027) acc 90.6250 (79.2188) lr 4.1221e-04 eta 0:02:04
epoch [37/50] batch [45/49] time 0.174 (0.191) data 0.000 (0.015) loss 3.0117 (2.9919) acc 68.7500 (79.4444) lr 4.1221e-04 eta 0:02:02
epoch [38/50] batch [5/49] time 0.176 (0.304) data 0.000 (0.127) loss 3.3398 (2.9832) acc 68.7500 (77.5000) lr 3.6258e-04 eta 0:03:11
epoch [38/50] batch [10/49] time 0.177 (0.240) data 0.000 (0.064) loss 3.0000 (3.1137) acc 87.5000 (76.2500) lr 3.6258e-04 eta 0:02:30
epoch [38/50] batch [15/49] time 0.177 (0.219) data 0.000 (0.043) loss 3.4297 (3.0617) acc 81.2500 (76.8750) lr 3.6258e-04 eta 0:02:16
epoch [38/50] batch [20/49] time 0.176 (0.208) data 0.000 (0.032) loss 3.1113 (3.0356) acc 81.2500 (78.1250) lr 3.6258e-04 eta 0:02:08
epoch [38/50] batch [25/49] time 0.176 (0.202) data 0.000 (0.026) loss 2.9336 (3.0346) acc 78.1250 (78.0000) lr 3.6258e-04 eta 0:02:03
epoch [38/50] batch [30/49] time 0.178 (0.198) data 0.000 (0.022) loss 3.4141 (3.0354) acc 78.1250 (77.9167) lr 3.6258e-04 eta 0:02:00
epoch [38/50] batch [35/49] time 0.177 (0.195) data 0.000 (0.019) loss 2.8438 (3.0447) acc 75.0000 (77.8571) lr 3.6258e-04 eta 0:01:57
epoch [38/50] batch [40/49] time 0.174 (0.192) data 0.000 (0.016) loss 3.3750 (3.0826) acc 87.5000 (77.4219) lr 3.6258e-04 eta 0:01:54
epoch [38/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.015) loss 3.3164 (3.0781) acc 75.0000 (77.2917) lr 3.6258e-04 eta 0:01:52
epoch [39/50] batch [5/49] time 0.176 (0.294) data 0.000 (0.117) loss 3.2383 (3.1195) acc 78.1250 (80.0000) lr 3.1545e-04 eta 0:02:51
epoch [39/50] batch [10/49] time 0.176 (0.235) data 0.000 (0.059) loss 2.9746 (3.1439) acc 75.0000 (77.1875) lr 3.1545e-04 eta 0:02:15
epoch [39/50] batch [15/49] time 0.176 (0.215) data 0.000 (0.039) loss 3.3828 (3.1504) acc 68.7500 (76.4583) lr 3.1545e-04 eta 0:02:03
epoch [39/50] batch [20/49] time 0.177 (0.206) data 0.000 (0.029) loss 3.4590 (3.2026) acc 75.0000 (75.7812) lr 3.1545e-04 eta 0:01:56
epoch [39/50] batch [25/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.8477 (3.1558) acc 81.2500 (76.5000) lr 3.1545e-04 eta 0:01:52
epoch [39/50] batch [30/49] time 0.176 (0.196) data 0.000 (0.020) loss 2.9141 (3.1180) acc 78.1250 (76.7708) lr 3.1545e-04 eta 0:01:49
epoch [39/50] batch [35/49] time 0.174 (0.193) data 0.000 (0.017) loss 3.2754 (3.0901) acc 68.7500 (77.5000) lr 3.1545e-04 eta 0:01:46
epoch [39/50] batch [40/49] time 0.177 (0.191) data 0.000 (0.015) loss 3.3633 (3.1208) acc 65.6250 (76.7969) lr 3.1545e-04 eta 0:01:44
epoch [39/50] batch [45/49] time 0.176 (0.190) data 0.000 (0.013) loss 3.1973 (3.1010) acc 75.0000 (77.2917) lr 3.1545e-04 eta 0:01:42
epoch [40/50] batch [5/49] time 0.177 (0.300) data 0.000 (0.122) loss 2.8047 (3.0094) acc 78.1250 (76.8750) lr 2.7103e-04 eta 0:02:40
epoch [40/50] batch [10/49] time 0.176 (0.239) data 0.000 (0.061) loss 2.9805 (3.0934) acc 84.3750 (78.7500) lr 2.7103e-04 eta 0:02:06
epoch [40/50] batch [15/49] time 0.176 (0.218) data 0.000 (0.041) loss 3.1367 (3.1177) acc 68.7500 (77.7083) lr 2.7103e-04 eta 0:01:54
epoch [40/50] batch [20/49] time 0.177 (0.208) data 0.000 (0.031) loss 2.7969 (3.0824) acc 84.3750 (77.8125) lr 2.7103e-04 eta 0:01:47
epoch [40/50] batch [25/49] time 0.176 (0.202) data 0.000 (0.025) loss 3.7266 (3.0966) acc 65.6250 (77.1250) lr 2.7103e-04 eta 0:01:43
epoch [40/50] batch [30/49] time 0.176 (0.197) data 0.000 (0.021) loss 2.6602 (3.1249) acc 87.5000 (76.4583) lr 2.7103e-04 eta 0:01:40
epoch [40/50] batch [35/49] time 0.175 (0.194) data 0.000 (0.018) loss 3.1699 (3.1122) acc 65.6250 (76.5179) lr 2.7103e-04 eta 0:01:37
epoch [40/50] batch [40/49] time 0.176 (0.192) data 0.000 (0.016) loss 3.1758 (3.1245) acc 81.2500 (76.3281) lr 2.7103e-04 eta 0:01:35
epoch [40/50] batch [45/49] time 0.175 (0.190) data 0.000 (0.014) loss 2.8164 (3.1181) acc 90.6250 (76.9444) lr 2.7103e-04 eta 0:01:33
epoch [41/50] batch [5/49] time 0.177 (0.323) data 0.000 (0.147) loss 3.0410 (3.0535) acc 75.0000 (76.8750) lr 2.2949e-04 eta 0:02:36
epoch [41/50] batch [10/49] time 0.175 (0.249) data 0.000 (0.074) loss 2.9180 (3.0428) acc 78.1250 (78.7500) lr 2.2949e-04 eta 0:01:59
epoch [41/50] batch [15/49] time 0.177 (0.225) data 0.000 (0.049) loss 2.5117 (3.0072) acc 81.2500 (79.1667) lr 2.2949e-04 eta 0:01:46
epoch [41/50] batch [20/49] time 0.176 (0.213) data 0.000 (0.037) loss 2.7266 (2.9741) acc 84.3750 (79.5312) lr 2.2949e-04 eta 0:01:40
epoch [41/50] batch [25/49] time 0.176 (0.206) data 0.000 (0.030) loss 2.7734 (2.9839) acc 84.3750 (79.7500) lr 2.2949e-04 eta 0:01:35
epoch [41/50] batch [30/49] time 0.176 (0.201) data 0.000 (0.025) loss 2.5762 (2.9757) acc 78.1250 (79.7917) lr 2.2949e-04 eta 0:01:32
epoch [41/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.021) loss 2.8359 (2.9829) acc 84.3750 (79.8214) lr 2.2949e-04 eta 0:01:29
epoch [41/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.019) loss 3.4844 (3.0008) acc 68.7500 (79.3750) lr 2.2949e-04 eta 0:01:27
epoch [41/50] batch [45/49] time 0.174 (0.192) data 0.000 (0.017) loss 2.9805 (2.9965) acc 75.0000 (79.5139) lr 2.2949e-04 eta 0:01:25
epoch [42/50] batch [5/49] time 0.178 (0.334) data 0.000 (0.154) loss 2.7539 (2.7969) acc 78.1250 (81.8750) lr 1.9098e-04 eta 0:02:25
epoch [42/50] batch [10/49] time 0.177 (0.256) data 0.000 (0.077) loss 3.6172 (2.9961) acc 71.8750 (80.9375) lr 1.9098e-04 eta 0:01:50
epoch [42/50] batch [15/49] time 0.175 (0.230) data 0.000 (0.052) loss 3.3320 (3.0686) acc 65.6250 (79.5833) lr 1.9098e-04 eta 0:01:37
epoch [42/50] batch [20/49] time 0.177 (0.216) data 0.000 (0.039) loss 3.0352 (3.0757) acc 81.2500 (78.7500) lr 1.9098e-04 eta 0:01:31
epoch [42/50] batch [25/49] time 0.175 (0.208) data 0.000 (0.031) loss 2.7910 (3.0498) acc 81.2500 (79.0000) lr 1.9098e-04 eta 0:01:26
epoch [42/50] batch [30/49] time 0.176 (0.203) data 0.000 (0.026) loss 3.5000 (3.0902) acc 71.8750 (77.9167) lr 1.9098e-04 eta 0:01:23
epoch [42/50] batch [35/49] time 0.175 (0.199) data 0.000 (0.022) loss 2.7520 (3.0569) acc 84.3750 (78.6607) lr 1.9098e-04 eta 0:01:20
epoch [42/50] batch [40/49] time 0.177 (0.196) data 0.000 (0.020) loss 2.6914 (3.0353) acc 75.0000 (78.4375) lr 1.9098e-04 eta 0:01:18
epoch [42/50] batch [45/49] time 0.176 (0.194) data 0.000 (0.017) loss 3.0625 (3.0176) acc 81.2500 (79.1667) lr 1.9098e-04 eta 0:01:16
epoch [43/50] batch [5/49] time 0.176 (0.312) data 0.000 (0.135) loss 3.2148 (2.7660) acc 75.0000 (83.7500) lr 1.5567e-04 eta 0:02:00
epoch [43/50] batch [10/49] time 0.177 (0.244) data 0.000 (0.068) loss 2.4082 (2.9234) acc 93.7500 (81.5625) lr 1.5567e-04 eta 0:01:33
epoch [43/50] batch [15/49] time 0.176 (0.222) data 0.000 (0.046) loss 2.7363 (2.8961) acc 93.7500 (82.0833) lr 1.5567e-04 eta 0:01:23
epoch [43/50] batch [20/49] time 0.176 (0.210) data 0.000 (0.034) loss 2.7793 (2.9088) acc 87.5000 (81.4062) lr 1.5567e-04 eta 0:01:18
epoch [43/50] batch [25/49] time 0.177 (0.204) data 0.000 (0.027) loss 3.1914 (2.9430) acc 78.1250 (80.8750) lr 1.5567e-04 eta 0:01:14
epoch [43/50] batch [30/49] time 0.177 (0.199) data 0.000 (0.023) loss 3.1562 (2.9706) acc 87.5000 (80.9375) lr 1.5567e-04 eta 0:01:12
epoch [43/50] batch [35/49] time 0.176 (0.196) data 0.000 (0.020) loss 3.3164 (2.9434) acc 78.1250 (81.1607) lr 1.5567e-04 eta 0:01:09
epoch [43/50] batch [40/49] time 0.176 (0.194) data 0.000 (0.017) loss 2.6895 (2.9468) acc 90.6250 (81.4844) lr 1.5567e-04 eta 0:01:08
epoch [43/50] batch [45/49] time 0.174 (0.191) data 0.000 (0.015) loss 3.6992 (2.9705) acc 75.0000 (81.0417) lr 1.5567e-04 eta 0:01:06
epoch [44/50] batch [5/49] time 0.175 (0.349) data 0.000 (0.173) loss 2.9902 (3.1629) acc 84.3750 (78.7500) lr 1.2369e-04 eta 0:01:57
epoch [44/50] batch [10/49] time 0.175 (0.262) data 0.000 (0.087) loss 3.7070 (3.2465) acc 71.8750 (75.3125) lr 1.2369e-04 eta 0:01:27
epoch [44/50] batch [15/49] time 0.180 (0.234) data 0.000 (0.058) loss 3.0762 (3.1924) acc 71.8750 (76.2500) lr 1.2369e-04 eta 0:01:16
epoch [44/50] batch [20/49] time 0.175 (0.219) data 0.000 (0.043) loss 2.5586 (3.1815) acc 84.3750 (76.0938) lr 1.2369e-04 eta 0:01:10
epoch [44/50] batch [25/49] time 0.176 (0.211) data 0.000 (0.035) loss 2.8867 (3.1470) acc 68.7500 (75.6250) lr 1.2369e-04 eta 0:01:06
epoch [44/50] batch [30/49] time 0.175 (0.205) data 0.000 (0.029) loss 3.6680 (3.1729) acc 65.6250 (75.3125) lr 1.2369e-04 eta 0:01:04
epoch [44/50] batch [35/49] time 0.174 (0.200) data 0.000 (0.025) loss 2.9805 (3.1815) acc 87.5000 (75.7143) lr 1.2369e-04 eta 0:01:01
epoch [44/50] batch [40/49] time 0.174 (0.197) data 0.000 (0.022) loss 2.9043 (3.1795) acc 78.1250 (76.0938) lr 1.2369e-04 eta 0:00:59
epoch [44/50] batch [45/49] time 0.175 (0.195) data 0.000 (0.019) loss 3.4961 (3.1516) acc 62.5000 (76.5972) lr 1.2369e-04 eta 0:00:58
epoch [45/50] batch [5/49] time 0.176 (0.321) data 0.000 (0.142) loss 2.6934 (2.7738) acc 75.0000 (78.7500) lr 9.5173e-05 eta 0:01:32
epoch [45/50] batch [10/49] time 0.177 (0.249) data 0.000 (0.071) loss 3.3125 (2.9602) acc 78.1250 (79.0625) lr 9.5173e-05 eta 0:01:10
epoch [45/50] batch [15/49] time 0.175 (0.225) data 0.000 (0.048) loss 3.9004 (3.0312) acc 65.6250 (78.3333) lr 9.5173e-05 eta 0:01:02
epoch [45/50] batch [20/49] time 0.175 (0.212) data 0.000 (0.036) loss 3.2246 (3.0064) acc 65.6250 (78.5938) lr 9.5173e-05 eta 0:00:58
epoch [45/50] batch [25/49] time 0.175 (0.205) data 0.000 (0.029) loss 3.5195 (3.0131) acc 71.8750 (79.3750) lr 9.5173e-05 eta 0:00:55
epoch [45/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.4258 (3.0113) acc 90.6250 (79.6875) lr 9.5173e-05 eta 0:00:52
epoch [45/50] batch [35/49] time 0.175 (0.196) data 0.000 (0.021) loss 2.9922 (3.0400) acc 75.0000 (79.4643) lr 9.5173e-05 eta 0:00:50
epoch [45/50] batch [40/49] time 0.176 (0.194) data 0.000 (0.018) loss 3.3301 (3.0514) acc 75.0000 (79.2188) lr 9.5173e-05 eta 0:00:49
epoch [45/50] batch [45/49] time 0.176 (0.192) data 0.000 (0.016) loss 2.7734 (3.0484) acc 81.2500 (78.9583) lr 9.5173e-05 eta 0:00:47
epoch [46/50] batch [5/49] time 0.176 (0.310) data 0.000 (0.133) loss 3.2188 (2.9977) acc 78.1250 (80.0000) lr 7.0224e-05 eta 0:01:14
epoch [46/50] batch [10/49] time 0.177 (0.243) data 0.000 (0.067) loss 2.9121 (2.9312) acc 81.2500 (81.8750) lr 7.0224e-05 eta 0:00:57
epoch [46/50] batch [15/49] time 0.175 (0.220) data 0.000 (0.045) loss 2.8105 (2.9358) acc 78.1250 (80.2083) lr 7.0224e-05 eta 0:00:50
epoch [46/50] batch [20/49] time 0.177 (0.209) data 0.000 (0.033) loss 3.1152 (2.9610) acc 78.1250 (80.4688) lr 7.0224e-05 eta 0:00:47
epoch [46/50] batch [25/49] time 0.176 (0.203) data 0.000 (0.027) loss 2.6777 (2.9458) acc 87.5000 (80.6250) lr 7.0224e-05 eta 0:00:44
epoch [46/50] batch [30/49] time 0.177 (0.198) data 0.000 (0.022) loss 3.8555 (3.0004) acc 56.2500 (79.1667) lr 7.0224e-05 eta 0:00:42
epoch [46/50] batch [35/49] time 0.175 (0.195) data 0.000 (0.019) loss 2.8906 (2.9703) acc 78.1250 (79.3750) lr 7.0224e-05 eta 0:00:40
epoch [46/50] batch [40/49] time 0.174 (0.193) data 0.000 (0.017) loss 2.4414 (2.9530) acc 84.3750 (79.6875) lr 7.0224e-05 eta 0:00:39
epoch [46/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.015) loss 3.7109 (2.9701) acc 71.8750 (79.6528) lr 7.0224e-05 eta 0:00:38
epoch [47/50] batch [5/49] time 0.176 (0.316) data 0.001 (0.139) loss 3.5293 (3.0285) acc 68.7500 (81.2500) lr 4.8943e-05 eta 0:01:00
epoch [47/50] batch [10/49] time 0.176 (0.246) data 0.000 (0.070) loss 3.1270 (3.0203) acc 81.2500 (81.5625) lr 4.8943e-05 eta 0:00:45
epoch [47/50] batch [15/49] time 0.176 (0.223) data 0.000 (0.047) loss 2.8438 (2.9951) acc 81.2500 (80.6250) lr 4.8943e-05 eta 0:00:40
epoch [47/50] batch [20/49] time 0.176 (0.211) data 0.000 (0.035) loss 3.1074 (3.0041) acc 78.1250 (79.3750) lr 4.8943e-05 eta 0:00:37
epoch [47/50] batch [25/49] time 0.175 (0.204) data 0.000 (0.028) loss 3.4746 (2.9814) acc 65.6250 (79.8750) lr 4.8943e-05 eta 0:00:34
epoch [47/50] batch [30/49] time 0.180 (0.200) data 0.000 (0.023) loss 2.8047 (2.9553) acc 84.3750 (80.3125) lr 4.8943e-05 eta 0:00:33
epoch [47/50] batch [35/49] time 0.175 (0.196) data 0.000 (0.020) loss 3.9668 (3.0208) acc 59.3750 (79.1964) lr 4.8943e-05 eta 0:00:31
epoch [47/50] batch [40/49] time 0.174 (0.194) data 0.000 (0.018) loss 3.0723 (3.0097) acc 84.3750 (79.6094) lr 4.8943e-05 eta 0:00:30
epoch [47/50] batch [45/49] time 0.175 (0.191) data 0.000 (0.016) loss 3.3574 (3.0432) acc 78.1250 (79.2361) lr 4.8943e-05 eta 0:00:28
epoch [48/50] batch [5/49] time 0.181 (0.306) data 0.003 (0.128) loss 2.3438 (2.8895) acc 87.5000 (80.0000) lr 3.1417e-05 eta 0:00:43
epoch [48/50] batch [10/49] time 0.176 (0.241) data 0.000 (0.064) loss 2.7266 (2.9391) acc 84.3750 (81.2500) lr 3.1417e-05 eta 0:00:33
epoch [48/50] batch [15/49] time 0.176 (0.220) data 0.000 (0.043) loss 2.8906 (2.8992) acc 81.2500 (81.8750) lr 3.1417e-05 eta 0:00:28
epoch [48/50] batch [20/49] time 0.176 (0.209) data 0.000 (0.032) loss 3.2891 (2.9689) acc 75.0000 (80.3125) lr 3.1417e-05 eta 0:00:26
epoch [48/50] batch [25/49] time 0.177 (0.202) data 0.000 (0.026) loss 3.1641 (3.0071) acc 75.0000 (79.1250) lr 3.1417e-05 eta 0:00:24
epoch [48/50] batch [30/49] time 0.177 (0.198) data 0.000 (0.022) loss 3.1523 (3.0849) acc 81.2500 (78.2292) lr 3.1417e-05 eta 0:00:23
epoch [48/50] batch [35/49] time 0.174 (0.195) data 0.000 (0.018) loss 2.8633 (3.0399) acc 78.1250 (78.7500) lr 3.1417e-05 eta 0:00:21
epoch [48/50] batch [40/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.0527 (3.0436) acc 75.0000 (78.2812) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [45/49] time 0.177 (0.190) data 0.000 (0.014) loss 3.2344 (3.0268) acc 68.7500 (78.4028) lr 3.1417e-05 eta 0:00:19
epoch [49/50] batch [5/49] time 0.175 (0.322) data 0.000 (0.144) loss 2.9980 (2.9516) acc 78.1250 (81.2500) lr 1.7713e-05 eta 0:00:29
epoch [49/50] batch [10/49] time 0.175 (0.249) data 0.000 (0.072) loss 2.8086 (2.9523) acc 78.1250 (80.6250) lr 1.7713e-05 eta 0:00:21
epoch [49/50] batch [15/49] time 0.176 (0.224) data 0.000 (0.048) loss 2.6953 (2.9033) acc 81.2500 (80.6250) lr 1.7713e-05 eta 0:00:18
epoch [49/50] batch [20/49] time 0.177 (0.212) data 0.000 (0.036) loss 3.2422 (2.9269) acc 68.7500 (80.4688) lr 1.7713e-05 eta 0:00:16
epoch [49/50] batch [25/49] time 0.176 (0.205) data 0.000 (0.029) loss 3.2383 (2.9636) acc 81.2500 (80.6250) lr 1.7713e-05 eta 0:00:14
epoch [49/50] batch [30/49] time 0.176 (0.200) data 0.000 (0.024) loss 2.4336 (2.9424) acc 87.5000 (81.0417) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [35/49] time 0.175 (0.197) data 0.000 (0.021) loss 2.2871 (2.9459) acc 90.6250 (81.1607) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [40/49] time 0.175 (0.194) data 0.000 (0.018) loss 2.6973 (2.9807) acc 87.5000 (80.5469) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [45/49] time 0.175 (0.192) data 0.000 (0.016) loss 3.1113 (3.0011) acc 68.7500 (79.7222) lr 1.7713e-05 eta 0:00:10
epoch [50/50] batch [5/49] time 0.180 (0.328) data 0.000 (0.150) loss 2.8672 (2.8984) acc 81.2500 (78.1250) lr 7.8853e-06 eta 0:00:14
epoch [50/50] batch [10/49] time 0.176 (0.252) data 0.000 (0.075) loss 3.1445 (2.9623) acc 81.2500 (78.4375) lr 7.8853e-06 eta 0:00:09
epoch [50/50] batch [15/49] time 0.176 (0.227) data 0.000 (0.050) loss 3.1973 (3.0044) acc 81.2500 (78.1250) lr 7.8853e-06 eta 0:00:07
epoch [50/50] batch [20/49] time 0.176 (0.214) data 0.000 (0.038) loss 2.6875 (2.9567) acc 84.3750 (79.2188) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [25/49] time 0.181 (0.207) data 0.000 (0.030) loss 3.3828 (2.9570) acc 75.0000 (80.0000) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [30/49] time 0.176 (0.202) data 0.000 (0.025) loss 2.7188 (2.9471) acc 87.5000 (80.3125) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [35/49] time 0.175 (0.198) data 0.000 (0.022) loss 2.8613 (2.9335) acc 78.1250 (80.3571) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [40/49] time 0.175 (0.195) data 0.000 (0.019) loss 2.8594 (2.9293) acc 84.3750 (80.7031) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [45/49] time 0.174 (0.193) data 0.000 (0.017) loss 3.3203 (2.9270) acc 75.0000 (80.7639) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:07<00:56,  7.12s/it] 22%|██▏       | 2/9 [00:08<00:25,  3.61s/it] 33%|███▎      | 3/9 [00:09<00:14,  2.48s/it] 44%|████▍     | 4/9 [00:10<00:09,  1.96s/it] 56%|█████▌    | 5/9 [00:11<00:06,  1.67s/it] 67%|██████▋   | 6/9 [00:12<00:04,  1.49s/it] 78%|███████▊  | 7/9 [00:14<00:02,  1.38s/it] 89%|████████▉ | 8/9 [00:15<00:01,  1.31s/it]100%|██████████| 9/9 [00:15<00:00,  1.71s/it]
=> result
* total: 4,002
* correct: 3,259
* accuracy: 81.4%
* error: 18.6%
* macro_f1: 81.1%
Elapsed: 0:08:07
Run this job and save the output to output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,039
---------  ------------
['2012 FIAT 500 Abarth', '2012 FIAT 500 Convertible', '2012 Ferrari FF Coupe', '2012 Ferrari California Convertible', '2012 Ferrari 458 Italia Convertible', '2012 Ferrari 458 Italia Coupe', '2012 Fisker Karma Sedan', '2012 Ford F-450 Super Duty Crew Cab', '2007 Ford Mustang Convertible', '2007 Ford Freestar Minivan', '2009 Ford Expedition EL SUV', '2012 Ford Edge SUV', '2011 Ford Ranger SuperCab', '2006 Ford GT Coupe', '2012 Ford F-150 Regular Cab', '2007 Ford F-150 Regular Cab', '2007 Ford Focus Sedan', '2012 Ford E-Series Wagon Van', '2012 Ford Fiesta Sedan', '2012 GMC Terrain SUV', '2012 GMC Savana Van', '2012 GMC Yukon Hybrid SUV', '2012 GMC Acadia SUV', '2012 GMC Canyon Extended Cab', '1993 Geo Metro Convertible', '2010 HUMMER H3T Crew Cab', '2009 HUMMER H2 SUT Crew Cab', '2012 Honda Odyssey Minivan', '2007 Honda Odyssey Minivan', '2012 Honda Accord Coupe', '2012 Honda Accord Sedan', '2012 Hyundai Veloster Hatchback', '2012 Hyundai Santa Fe SUV', '2012 Hyundai Tucson SUV', '2012 Hyundai Veracruz SUV', '2012 Hyundai Sonata Hybrid Sedan', '2007 Hyundai Elantra Sedan', '2012 Hyundai Accent Sedan', '2012 Hyundai Genesis Sedan', '2012 Hyundai Sonata Sedan', '2012 Hyundai Elantra Touring Hatchback', '2012 Hyundai Azera Sedan', '2012 Infiniti G Coupe IPL', '2011 Infiniti QX56 SUV', '2008 Isuzu Ascender SUV', '2012 Jaguar XK XKR', '2012 Jeep Patriot SUV', '2012 Jeep Wrangler SUV', '2012 Jeep Liberty SUV', '2012 Jeep Grand Cherokee SUV', '2012 Jeep Compass SUV', '2008 Lamborghini Reventon Coupe', '2012 Lamborghini Aventador Coupe', '2012 Lamborghini Gallardo LP 570-4 Superleggera', '2001 Lamborghini Diablo Coupe', '2012 Land Rover Range Rover SUV', '2012 Land Rover LR2 SUV', '2011 Lincoln Town Car Sedan', '2012 MINI Cooper Roadster Convertible', '2012 Maybach Landaulet Convertible', '2011 Mazda Tribute SUV', '2012 McLaren MP4-12C Coupe', '1993 Mercedes-Benz 300-Class Convertible', '2012 Mercedes-Benz C-Class Sedan', '2009 Mercedes-Benz SL-Class Coupe', '2012 Mercedes-Benz E-Class Sedan', '2012 Mercedes-Benz S-Class Sedan', '2012 Mercedes-Benz Sprinter Van', '2012 Mitsubishi Lancer Sedan', '2012 Nissan Leaf Hatchback', '2012 Nissan NV Passenger Van', '2012 Nissan Juke Hatchback', '1998 Nissan 240SX Coupe', '1999 Plymouth Neon Coupe', '2012 Porsche Panamera Sedan', '2012 Ram C/V Cargo Van Minivan', '2012 Rolls-Royce Phantom Drophead Coupe Convertible', '2012 Rolls-Royce Ghost Sedan', '2012 Rolls-Royce Phantom Sedan', '2012 Scion xD Hatchback', '2009 Spyker C8 Convertible', '2009 Spyker C8 Coupe', '2007 Suzuki Aerio Sedan', '2012 Suzuki Kizashi Sedan', '2012 Suzuki SX4 Hatchback', '2012 Suzuki SX4 Sedan', '2012 Tesla Model S Sedan', '2012 Toyota Sequoia SUV', '2012 Toyota Camry Sedan', '2012 Toyota Corolla Sedan', '2012 Toyota 4Runner SUV', '2012 Volkswagen Golf Hatchback', '1991 Volkswagen Golf Hatchback', '2012 Volkswagen Beetle Hatchback', '2012 Volvo C30 Hatchback', '1993 Volvo 240 Sedan', '2007 Volvo XC90 SUV', '2012 smart fortwo Convertible']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2012 FIAT 500 Abarth, a type of car', 'X X X X 2012 FIAT 500 Convertible, a type of car', 'X X X X 2012 Ferrari FF Coupe, a type of car', 'X X X X 2012 Ferrari California Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Coupe, a type of car', 'X X X X 2012 Fisker Karma Sedan, a type of car', 'X X X X 2012 Ford F-450 Super Duty Crew Cab, a type of car', 'X X X X 2007 Ford Mustang Convertible, a type of car', 'X X X X 2007 Ford Freestar Minivan, a type of car', 'X X X X 2009 Ford Expedition EL SUV, a type of car', 'X X X X 2012 Ford Edge SUV, a type of car', 'X X X X 2011 Ford Ranger SuperCab, a type of car', 'X X X X 2006 Ford GT Coupe, a type of car', 'X X X X 2012 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford Focus Sedan, a type of car', 'X X X X 2012 Ford E-Series Wagon Van, a type of car', 'X X X X 2012 Ford Fiesta Sedan, a type of car', 'X X X X 2012 GMC Terrain SUV, a type of car', 'X X X X 2012 GMC Savana Van, a type of car', 'X X X X 2012 GMC Yukon Hybrid SUV, a type of car', 'X X X X 2012 GMC Acadia SUV, a type of car', 'X X X X 2012 GMC Canyon Extended Cab, a type of car', 'X X X X 1993 Geo Metro Convertible, a type of car', 'X X X X 2010 HUMMER H3T Crew Cab, a type of car', 'X X X X 2009 HUMMER H2 SUT Crew Cab, a type of car', 'X X X X 2012 Honda Odyssey Minivan, a type of car', 'X X X X 2007 Honda Odyssey Minivan, a type of car', 'X X X X 2012 Honda Accord Coupe, a type of car', 'X X X X 2012 Honda Accord Sedan, a type of car', 'X X X X 2012 Hyundai Veloster Hatchback, a type of car', 'X X X X 2012 Hyundai Santa Fe SUV, a type of car', 'X X X X 2012 Hyundai Tucson SUV, a type of car', 'X X X X 2012 Hyundai Veracruz SUV, a type of car', 'X X X X 2012 Hyundai Sonata Hybrid Sedan, a type of car', 'X X X X 2007 Hyundai Elantra Sedan, a type of car', 'X X X X 2012 Hyundai Accent Sedan, a type of car', 'X X X X 2012 Hyundai Genesis Sedan, a type of car', 'X X X X 2012 Hyundai Sonata Sedan, a type of car', 'X X X X 2012 Hyundai Elantra Touring Hatchback, a type of car', 'X X X X 2012 Hyundai Azera Sedan, a type of car', 'X X X X 2012 Infiniti G Coupe IPL, a type of car', 'X X X X 2011 Infiniti QX56 SUV, a type of car', 'X X X X 2008 Isuzu Ascender SUV, a type of car', 'X X X X 2012 Jaguar XK XKR, a type of car', 'X X X X 2012 Jeep Patriot SUV, a type of car', 'X X X X 2012 Jeep Wrangler SUV, a type of car', 'X X X X 2012 Jeep Liberty SUV, a type of car', 'X X X X 2012 Jeep Grand Cherokee SUV, a type of car', 'X X X X 2012 Jeep Compass SUV, a type of car', 'X X X X 2008 Lamborghini Reventon Coupe, a type of car', 'X X X X 2012 Lamborghini Aventador Coupe, a type of car', 'X X X X 2012 Lamborghini Gallardo LP 570-4 Superleggera, a type of car', 'X X X X 2001 Lamborghini Diablo Coupe, a type of car', 'X X X X 2012 Land Rover Range Rover SUV, a type of car', 'X X X X 2012 Land Rover LR2 SUV, a type of car', 'X X X X 2011 Lincoln Town Car Sedan, a type of car', 'X X X X 2012 MINI Cooper Roadster Convertible, a type of car', 'X X X X 2012 Maybach Landaulet Convertible, a type of car', 'X X X X 2011 Mazda Tribute SUV, a type of car', 'X X X X 2012 McLaren MP4-12C Coupe, a type of car', 'X X X X 1993 Mercedes-Benz 300-Class Convertible, a type of car', 'X X X X 2012 Mercedes-Benz C-Class Sedan, a type of car', 'X X X X 2009 Mercedes-Benz SL-Class Coupe, a type of car', 'X X X X 2012 Mercedes-Benz E-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz S-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz Sprinter Van, a type of car', 'X X X X 2012 Mitsubishi Lancer Sedan, a type of car', 'X X X X 2012 Nissan Leaf Hatchback, a type of car', 'X X X X 2012 Nissan NV Passenger Van, a type of car', 'X X X X 2012 Nissan Juke Hatchback, a type of car', 'X X X X 1998 Nissan 240SX Coupe, a type of car', 'X X X X 1999 Plymouth Neon Coupe, a type of car', 'X X X X 2012 Porsche Panamera Sedan, a type of car', 'X X X X 2012 Ram C/V Cargo Van Minivan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Drophead Coupe Convertible, a type of car', 'X X X X 2012 Rolls-Royce Ghost Sedan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Sedan, a type of car', 'X X X X 2012 Scion xD Hatchback, a type of car', 'X X X X 2009 Spyker C8 Convertible, a type of car', 'X X X X 2009 Spyker C8 Coupe, a type of car', 'X X X X 2007 Suzuki Aerio Sedan, a type of car', 'X X X X 2012 Suzuki Kizashi Sedan, a type of car', 'X X X X 2012 Suzuki SX4 Hatchback, a type of car', 'X X X X 2012 Suzuki SX4 Sedan, a type of car', 'X X X X 2012 Tesla Model S Sedan, a type of car', 'X X X X 2012 Toyota Sequoia SUV, a type of car', 'X X X X 2012 Toyota Camry Sedan, a type of car', 'X X X X 2012 Toyota Corolla Sedan, a type of car', 'X X X X 2012 Toyota 4Runner SUV, a type of car', 'X X X X 2012 Volkswagen Golf Hatchback, a type of car', 'X X X X 1991 Volkswagen Golf Hatchback, a type of car', 'X X X X 2012 Volkswagen Beetle Hatchback, a type of car', 'X X X X 2012 Volvo C30 Hatchback, a type of car', 'X X X X 1993 Volvo 240 Sedan, a type of car', 'X X X X 2007 Volvo XC90 SUV, a type of car', 'X X X X 2012 smart fortwo Convertible, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([98, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:07<01:01,  7.67s/it] 22%|██▏       | 2/9 [00:08<00:26,  3.83s/it] 33%|███▎      | 3/9 [00:09<00:15,  2.60s/it] 44%|████▍     | 4/9 [00:11<00:10,  2.02s/it] 56%|█████▌    | 5/9 [00:12<00:06,  1.70s/it] 67%|██████▋   | 6/9 [00:13<00:04,  1.51s/it] 78%|███████▊  | 7/9 [00:14<00:02,  1.39s/it] 89%|████████▉ | 8/9 [00:15<00:01,  1.31s/it]100%|██████████| 9/9 [00:15<00:00,  1.06it/s]100%|██████████| 9/9 [00:15<00:00,  1.77s/it]
=> result
* total: 4,039
* correct: 3,027
* accuracy: 74.9%
* error: 25.1%
* macro_f1: 73.6%
Run this job and save the output to output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,039
---------  ------------
['2012 FIAT 500 Abarth', '2012 FIAT 500 Convertible', '2012 Ferrari FF Coupe', '2012 Ferrari California Convertible', '2012 Ferrari 458 Italia Convertible', '2012 Ferrari 458 Italia Coupe', '2012 Fisker Karma Sedan', '2012 Ford F-450 Super Duty Crew Cab', '2007 Ford Mustang Convertible', '2007 Ford Freestar Minivan', '2009 Ford Expedition EL SUV', '2012 Ford Edge SUV', '2011 Ford Ranger SuperCab', '2006 Ford GT Coupe', '2012 Ford F-150 Regular Cab', '2007 Ford F-150 Regular Cab', '2007 Ford Focus Sedan', '2012 Ford E-Series Wagon Van', '2012 Ford Fiesta Sedan', '2012 GMC Terrain SUV', '2012 GMC Savana Van', '2012 GMC Yukon Hybrid SUV', '2012 GMC Acadia SUV', '2012 GMC Canyon Extended Cab', '1993 Geo Metro Convertible', '2010 HUMMER H3T Crew Cab', '2009 HUMMER H2 SUT Crew Cab', '2012 Honda Odyssey Minivan', '2007 Honda Odyssey Minivan', '2012 Honda Accord Coupe', '2012 Honda Accord Sedan', '2012 Hyundai Veloster Hatchback', '2012 Hyundai Santa Fe SUV', '2012 Hyundai Tucson SUV', '2012 Hyundai Veracruz SUV', '2012 Hyundai Sonata Hybrid Sedan', '2007 Hyundai Elantra Sedan', '2012 Hyundai Accent Sedan', '2012 Hyundai Genesis Sedan', '2012 Hyundai Sonata Sedan', '2012 Hyundai Elantra Touring Hatchback', '2012 Hyundai Azera Sedan', '2012 Infiniti G Coupe IPL', '2011 Infiniti QX56 SUV', '2008 Isuzu Ascender SUV', '2012 Jaguar XK XKR', '2012 Jeep Patriot SUV', '2012 Jeep Wrangler SUV', '2012 Jeep Liberty SUV', '2012 Jeep Grand Cherokee SUV', '2012 Jeep Compass SUV', '2008 Lamborghini Reventon Coupe', '2012 Lamborghini Aventador Coupe', '2012 Lamborghini Gallardo LP 570-4 Superleggera', '2001 Lamborghini Diablo Coupe', '2012 Land Rover Range Rover SUV', '2012 Land Rover LR2 SUV', '2011 Lincoln Town Car Sedan', '2012 MINI Cooper Roadster Convertible', '2012 Maybach Landaulet Convertible', '2011 Mazda Tribute SUV', '2012 McLaren MP4-12C Coupe', '1993 Mercedes-Benz 300-Class Convertible', '2012 Mercedes-Benz C-Class Sedan', '2009 Mercedes-Benz SL-Class Coupe', '2012 Mercedes-Benz E-Class Sedan', '2012 Mercedes-Benz S-Class Sedan', '2012 Mercedes-Benz Sprinter Van', '2012 Mitsubishi Lancer Sedan', '2012 Nissan Leaf Hatchback', '2012 Nissan NV Passenger Van', '2012 Nissan Juke Hatchback', '1998 Nissan 240SX Coupe', '1999 Plymouth Neon Coupe', '2012 Porsche Panamera Sedan', '2012 Ram C/V Cargo Van Minivan', '2012 Rolls-Royce Phantom Drophead Coupe Convertible', '2012 Rolls-Royce Ghost Sedan', '2012 Rolls-Royce Phantom Sedan', '2012 Scion xD Hatchback', '2009 Spyker C8 Convertible', '2009 Spyker C8 Coupe', '2007 Suzuki Aerio Sedan', '2012 Suzuki Kizashi Sedan', '2012 Suzuki SX4 Hatchback', '2012 Suzuki SX4 Sedan', '2012 Tesla Model S Sedan', '2012 Toyota Sequoia SUV', '2012 Toyota Camry Sedan', '2012 Toyota Corolla Sedan', '2012 Toyota 4Runner SUV', '2012 Volkswagen Golf Hatchback', '1991 Volkswagen Golf Hatchback', '2012 Volkswagen Beetle Hatchback', '2012 Volvo C30 Hatchback', '1993 Volvo 240 Sedan', '2007 Volvo XC90 SUV', '2012 smart fortwo Convertible']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2012 FIAT 500 Abarth, a type of car', 'X X X X 2012 FIAT 500 Convertible, a type of car', 'X X X X 2012 Ferrari FF Coupe, a type of car', 'X X X X 2012 Ferrari California Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Coupe, a type of car', 'X X X X 2012 Fisker Karma Sedan, a type of car', 'X X X X 2012 Ford F-450 Super Duty Crew Cab, a type of car', 'X X X X 2007 Ford Mustang Convertible, a type of car', 'X X X X 2007 Ford Freestar Minivan, a type of car', 'X X X X 2009 Ford Expedition EL SUV, a type of car', 'X X X X 2012 Ford Edge SUV, a type of car', 'X X X X 2011 Ford Ranger SuperCab, a type of car', 'X X X X 2006 Ford GT Coupe, a type of car', 'X X X X 2012 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford Focus Sedan, a type of car', 'X X X X 2012 Ford E-Series Wagon Van, a type of car', 'X X X X 2012 Ford Fiesta Sedan, a type of car', 'X X X X 2012 GMC Terrain SUV, a type of car', 'X X X X 2012 GMC Savana Van, a type of car', 'X X X X 2012 GMC Yukon Hybrid SUV, a type of car', 'X X X X 2012 GMC Acadia SUV, a type of car', 'X X X X 2012 GMC Canyon Extended Cab, a type of car', 'X X X X 1993 Geo Metro Convertible, a type of car', 'X X X X 2010 HUMMER H3T Crew Cab, a type of car', 'X X X X 2009 HUMMER H2 SUT Crew Cab, a type of car', 'X X X X 2012 Honda Odyssey Minivan, a type of car', 'X X X X 2007 Honda Odyssey Minivan, a type of car', 'X X X X 2012 Honda Accord Coupe, a type of car', 'X X X X 2012 Honda Accord Sedan, a type of car', 'X X X X 2012 Hyundai Veloster Hatchback, a type of car', 'X X X X 2012 Hyundai Santa Fe SUV, a type of car', 'X X X X 2012 Hyundai Tucson SUV, a type of car', 'X X X X 2012 Hyundai Veracruz SUV, a type of car', 'X X X X 2012 Hyundai Sonata Hybrid Sedan, a type of car', 'X X X X 2007 Hyundai Elantra Sedan, a type of car', 'X X X X 2012 Hyundai Accent Sedan, a type of car', 'X X X X 2012 Hyundai Genesis Sedan, a type of car', 'X X X X 2012 Hyundai Sonata Sedan, a type of car', 'X X X X 2012 Hyundai Elantra Touring Hatchback, a type of car', 'X X X X 2012 Hyundai Azera Sedan, a type of car', 'X X X X 2012 Infiniti G Coupe IPL, a type of car', 'X X X X 2011 Infiniti QX56 SUV, a type of car', 'X X X X 2008 Isuzu Ascender SUV, a type of car', 'X X X X 2012 Jaguar XK XKR, a type of car', 'X X X X 2012 Jeep Patriot SUV, a type of car', 'X X X X 2012 Jeep Wrangler SUV, a type of car', 'X X X X 2012 Jeep Liberty SUV, a type of car', 'X X X X 2012 Jeep Grand Cherokee SUV, a type of car', 'X X X X 2012 Jeep Compass SUV, a type of car', 'X X X X 2008 Lamborghini Reventon Coupe, a type of car', 'X X X X 2012 Lamborghini Aventador Coupe, a type of car', 'X X X X 2012 Lamborghini Gallardo LP 570-4 Superleggera, a type of car', 'X X X X 2001 Lamborghini Diablo Coupe, a type of car', 'X X X X 2012 Land Rover Range Rover SUV, a type of car', 'X X X X 2012 Land Rover LR2 SUV, a type of car', 'X X X X 2011 Lincoln Town Car Sedan, a type of car', 'X X X X 2012 MINI Cooper Roadster Convertible, a type of car', 'X X X X 2012 Maybach Landaulet Convertible, a type of car', 'X X X X 2011 Mazda Tribute SUV, a type of car', 'X X X X 2012 McLaren MP4-12C Coupe, a type of car', 'X X X X 1993 Mercedes-Benz 300-Class Convertible, a type of car', 'X X X X 2012 Mercedes-Benz C-Class Sedan, a type of car', 'X X X X 2009 Mercedes-Benz SL-Class Coupe, a type of car', 'X X X X 2012 Mercedes-Benz E-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz S-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz Sprinter Van, a type of car', 'X X X X 2012 Mitsubishi Lancer Sedan, a type of car', 'X X X X 2012 Nissan Leaf Hatchback, a type of car', 'X X X X 2012 Nissan NV Passenger Van, a type of car', 'X X X X 2012 Nissan Juke Hatchback, a type of car', 'X X X X 1998 Nissan 240SX Coupe, a type of car', 'X X X X 1999 Plymouth Neon Coupe, a type of car', 'X X X X 2012 Porsche Panamera Sedan, a type of car', 'X X X X 2012 Ram C/V Cargo Van Minivan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Drophead Coupe Convertible, a type of car', 'X X X X 2012 Rolls-Royce Ghost Sedan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Sedan, a type of car', 'X X X X 2012 Scion xD Hatchback, a type of car', 'X X X X 2009 Spyker C8 Convertible, a type of car', 'X X X X 2009 Spyker C8 Coupe, a type of car', 'X X X X 2007 Suzuki Aerio Sedan, a type of car', 'X X X X 2012 Suzuki Kizashi Sedan, a type of car', 'X X X X 2012 Suzuki SX4 Hatchback, a type of car', 'X X X X 2012 Suzuki SX4 Sedan, a type of car', 'X X X X 2012 Tesla Model S Sedan, a type of car', 'X X X X 2012 Toyota Sequoia SUV, a type of car', 'X X X X 2012 Toyota Camry Sedan, a type of car', 'X X X X 2012 Toyota Corolla Sedan, a type of car', 'X X X X 2012 Toyota 4Runner SUV, a type of car', 'X X X X 2012 Volkswagen Golf Hatchback, a type of car', 'X X X X 1991 Volkswagen Golf Hatchback, a type of car', 'X X X X 2012 Volkswagen Beetle Hatchback, a type of car', 'X X X X 2012 Volvo C30 Hatchback, a type of car', 'X X X X 1993 Volvo 240 Sedan, a type of car', 'X X X X 2007 Volvo XC90 SUV, a type of car', 'X X X X 2012 smart fortwo Convertible, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([98, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:04,  8.06s/it] 22%|██▏       | 2/9 [00:09<00:27,  3.98s/it] 33%|███▎      | 3/9 [00:10<00:16,  2.68s/it] 44%|████▍     | 4/9 [00:11<00:10,  2.07s/it] 56%|█████▌    | 5/9 [00:12<00:06,  1.73s/it] 67%|██████▋   | 6/9 [00:13<00:04,  1.53s/it] 78%|███████▊  | 7/9 [00:14<00:02,  1.40s/it] 89%|████████▉ | 8/9 [00:15<00:01,  1.31s/it]100%|██████████| 9/9 [00:16<00:00,  1.05it/s]100%|██████████| 9/9 [00:16<00:00,  1.81s/it]
=> result
* total: 4,039
* correct: 3,045
* accuracy: 75.4%
* error: 24.6%
* macro_f1: 74.3%
Run this job and save the output to output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/stanford_cars.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: StanfordCars
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: StanfordCars
Reading split from /data/yht/data/cl/data/stanford_cars/split_zhou_StanfordCars.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/stanford_cars/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------------
Dataset    StanfordCars
# classes  98
# train_x  1,568
# val      392
# test     4,039
---------  ------------
['2012 FIAT 500 Abarth', '2012 FIAT 500 Convertible', '2012 Ferrari FF Coupe', '2012 Ferrari California Convertible', '2012 Ferrari 458 Italia Convertible', '2012 Ferrari 458 Italia Coupe', '2012 Fisker Karma Sedan', '2012 Ford F-450 Super Duty Crew Cab', '2007 Ford Mustang Convertible', '2007 Ford Freestar Minivan', '2009 Ford Expedition EL SUV', '2012 Ford Edge SUV', '2011 Ford Ranger SuperCab', '2006 Ford GT Coupe', '2012 Ford F-150 Regular Cab', '2007 Ford F-150 Regular Cab', '2007 Ford Focus Sedan', '2012 Ford E-Series Wagon Van', '2012 Ford Fiesta Sedan', '2012 GMC Terrain SUV', '2012 GMC Savana Van', '2012 GMC Yukon Hybrid SUV', '2012 GMC Acadia SUV', '2012 GMC Canyon Extended Cab', '1993 Geo Metro Convertible', '2010 HUMMER H3T Crew Cab', '2009 HUMMER H2 SUT Crew Cab', '2012 Honda Odyssey Minivan', '2007 Honda Odyssey Minivan', '2012 Honda Accord Coupe', '2012 Honda Accord Sedan', '2012 Hyundai Veloster Hatchback', '2012 Hyundai Santa Fe SUV', '2012 Hyundai Tucson SUV', '2012 Hyundai Veracruz SUV', '2012 Hyundai Sonata Hybrid Sedan', '2007 Hyundai Elantra Sedan', '2012 Hyundai Accent Sedan', '2012 Hyundai Genesis Sedan', '2012 Hyundai Sonata Sedan', '2012 Hyundai Elantra Touring Hatchback', '2012 Hyundai Azera Sedan', '2012 Infiniti G Coupe IPL', '2011 Infiniti QX56 SUV', '2008 Isuzu Ascender SUV', '2012 Jaguar XK XKR', '2012 Jeep Patriot SUV', '2012 Jeep Wrangler SUV', '2012 Jeep Liberty SUV', '2012 Jeep Grand Cherokee SUV', '2012 Jeep Compass SUV', '2008 Lamborghini Reventon Coupe', '2012 Lamborghini Aventador Coupe', '2012 Lamborghini Gallardo LP 570-4 Superleggera', '2001 Lamborghini Diablo Coupe', '2012 Land Rover Range Rover SUV', '2012 Land Rover LR2 SUV', '2011 Lincoln Town Car Sedan', '2012 MINI Cooper Roadster Convertible', '2012 Maybach Landaulet Convertible', '2011 Mazda Tribute SUV', '2012 McLaren MP4-12C Coupe', '1993 Mercedes-Benz 300-Class Convertible', '2012 Mercedes-Benz C-Class Sedan', '2009 Mercedes-Benz SL-Class Coupe', '2012 Mercedes-Benz E-Class Sedan', '2012 Mercedes-Benz S-Class Sedan', '2012 Mercedes-Benz Sprinter Van', '2012 Mitsubishi Lancer Sedan', '2012 Nissan Leaf Hatchback', '2012 Nissan NV Passenger Van', '2012 Nissan Juke Hatchback', '1998 Nissan 240SX Coupe', '1999 Plymouth Neon Coupe', '2012 Porsche Panamera Sedan', '2012 Ram C/V Cargo Van Minivan', '2012 Rolls-Royce Phantom Drophead Coupe Convertible', '2012 Rolls-Royce Ghost Sedan', '2012 Rolls-Royce Phantom Sedan', '2012 Scion xD Hatchback', '2009 Spyker C8 Convertible', '2009 Spyker C8 Coupe', '2007 Suzuki Aerio Sedan', '2012 Suzuki Kizashi Sedan', '2012 Suzuki SX4 Hatchback', '2012 Suzuki SX4 Sedan', '2012 Tesla Model S Sedan', '2012 Toyota Sequoia SUV', '2012 Toyota Camry Sedan', '2012 Toyota Corolla Sedan', '2012 Toyota 4Runner SUV', '2012 Volkswagen Golf Hatchback', '1991 Volkswagen Golf Hatchback', '2012 Volkswagen Beetle Hatchback', '2012 Volvo C30 Hatchback', '1993 Volvo 240 Sedan', '2007 Volvo XC90 SUV', '2012 smart fortwo Convertible']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X 2012 FIAT 500 Abarth, a type of car', 'X X X X 2012 FIAT 500 Convertible, a type of car', 'X X X X 2012 Ferrari FF Coupe, a type of car', 'X X X X 2012 Ferrari California Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Convertible, a type of car', 'X X X X 2012 Ferrari 458 Italia Coupe, a type of car', 'X X X X 2012 Fisker Karma Sedan, a type of car', 'X X X X 2012 Ford F-450 Super Duty Crew Cab, a type of car', 'X X X X 2007 Ford Mustang Convertible, a type of car', 'X X X X 2007 Ford Freestar Minivan, a type of car', 'X X X X 2009 Ford Expedition EL SUV, a type of car', 'X X X X 2012 Ford Edge SUV, a type of car', 'X X X X 2011 Ford Ranger SuperCab, a type of car', 'X X X X 2006 Ford GT Coupe, a type of car', 'X X X X 2012 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford F-150 Regular Cab, a type of car', 'X X X X 2007 Ford Focus Sedan, a type of car', 'X X X X 2012 Ford E-Series Wagon Van, a type of car', 'X X X X 2012 Ford Fiesta Sedan, a type of car', 'X X X X 2012 GMC Terrain SUV, a type of car', 'X X X X 2012 GMC Savana Van, a type of car', 'X X X X 2012 GMC Yukon Hybrid SUV, a type of car', 'X X X X 2012 GMC Acadia SUV, a type of car', 'X X X X 2012 GMC Canyon Extended Cab, a type of car', 'X X X X 1993 Geo Metro Convertible, a type of car', 'X X X X 2010 HUMMER H3T Crew Cab, a type of car', 'X X X X 2009 HUMMER H2 SUT Crew Cab, a type of car', 'X X X X 2012 Honda Odyssey Minivan, a type of car', 'X X X X 2007 Honda Odyssey Minivan, a type of car', 'X X X X 2012 Honda Accord Coupe, a type of car', 'X X X X 2012 Honda Accord Sedan, a type of car', 'X X X X 2012 Hyundai Veloster Hatchback, a type of car', 'X X X X 2012 Hyundai Santa Fe SUV, a type of car', 'X X X X 2012 Hyundai Tucson SUV, a type of car', 'X X X X 2012 Hyundai Veracruz SUV, a type of car', 'X X X X 2012 Hyundai Sonata Hybrid Sedan, a type of car', 'X X X X 2007 Hyundai Elantra Sedan, a type of car', 'X X X X 2012 Hyundai Accent Sedan, a type of car', 'X X X X 2012 Hyundai Genesis Sedan, a type of car', 'X X X X 2012 Hyundai Sonata Sedan, a type of car', 'X X X X 2012 Hyundai Elantra Touring Hatchback, a type of car', 'X X X X 2012 Hyundai Azera Sedan, a type of car', 'X X X X 2012 Infiniti G Coupe IPL, a type of car', 'X X X X 2011 Infiniti QX56 SUV, a type of car', 'X X X X 2008 Isuzu Ascender SUV, a type of car', 'X X X X 2012 Jaguar XK XKR, a type of car', 'X X X X 2012 Jeep Patriot SUV, a type of car', 'X X X X 2012 Jeep Wrangler SUV, a type of car', 'X X X X 2012 Jeep Liberty SUV, a type of car', 'X X X X 2012 Jeep Grand Cherokee SUV, a type of car', 'X X X X 2012 Jeep Compass SUV, a type of car', 'X X X X 2008 Lamborghini Reventon Coupe, a type of car', 'X X X X 2012 Lamborghini Aventador Coupe, a type of car', 'X X X X 2012 Lamborghini Gallardo LP 570-4 Superleggera, a type of car', 'X X X X 2001 Lamborghini Diablo Coupe, a type of car', 'X X X X 2012 Land Rover Range Rover SUV, a type of car', 'X X X X 2012 Land Rover LR2 SUV, a type of car', 'X X X X 2011 Lincoln Town Car Sedan, a type of car', 'X X X X 2012 MINI Cooper Roadster Convertible, a type of car', 'X X X X 2012 Maybach Landaulet Convertible, a type of car', 'X X X X 2011 Mazda Tribute SUV, a type of car', 'X X X X 2012 McLaren MP4-12C Coupe, a type of car', 'X X X X 1993 Mercedes-Benz 300-Class Convertible, a type of car', 'X X X X 2012 Mercedes-Benz C-Class Sedan, a type of car', 'X X X X 2009 Mercedes-Benz SL-Class Coupe, a type of car', 'X X X X 2012 Mercedes-Benz E-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz S-Class Sedan, a type of car', 'X X X X 2012 Mercedes-Benz Sprinter Van, a type of car', 'X X X X 2012 Mitsubishi Lancer Sedan, a type of car', 'X X X X 2012 Nissan Leaf Hatchback, a type of car', 'X X X X 2012 Nissan NV Passenger Van, a type of car', 'X X X X 2012 Nissan Juke Hatchback, a type of car', 'X X X X 1998 Nissan 240SX Coupe, a type of car', 'X X X X 1999 Plymouth Neon Coupe, a type of car', 'X X X X 2012 Porsche Panamera Sedan, a type of car', 'X X X X 2012 Ram C/V Cargo Van Minivan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Drophead Coupe Convertible, a type of car', 'X X X X 2012 Rolls-Royce Ghost Sedan, a type of car', 'X X X X 2012 Rolls-Royce Phantom Sedan, a type of car', 'X X X X 2012 Scion xD Hatchback, a type of car', 'X X X X 2009 Spyker C8 Convertible, a type of car', 'X X X X 2009 Spyker C8 Coupe, a type of car', 'X X X X 2007 Suzuki Aerio Sedan, a type of car', 'X X X X 2012 Suzuki Kizashi Sedan, a type of car', 'X X X X 2012 Suzuki SX4 Hatchback, a type of car', 'X X X X 2012 Suzuki SX4 Sedan, a type of car', 'X X X X 2012 Tesla Model S Sedan, a type of car', 'X X X X 2012 Toyota Sequoia SUV, a type of car', 'X X X X 2012 Toyota Camry Sedan, a type of car', 'X X X X 2012 Toyota Corolla Sedan, a type of car', 'X X X X 2012 Toyota 4Runner SUV, a type of car', 'X X X X 2012 Volkswagen Golf Hatchback, a type of car', 'X X X X 1991 Volkswagen Golf Hatchback, a type of car', 'X X X X 2012 Volkswagen Beetle Hatchback, a type of car', 'X X X X 2012 Volvo C30 Hatchback, a type of car', 'X X X X 1993 Volvo 240 Sedan, a type of car', 'X X X X 2007 Volvo XC90 SUV, a type of car', 'X X X X 2012 smart fortwo Convertible, a type of car']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([98, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/stanford_cars/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/9 [00:00<?, ?it/s] 11%|█         | 1/9 [00:08<01:04,  8.10s/it] 22%|██▏       | 2/9 [00:09<00:28,  4.01s/it] 33%|███▎      | 3/9 [00:10<00:16,  2.70s/it] 44%|████▍     | 4/9 [00:11<00:10,  2.08s/it] 56%|█████▌    | 5/9 [00:12<00:06,  1.74s/it] 67%|██████▋   | 6/9 [00:13<00:04,  1.53s/it] 78%|███████▊  | 7/9 [00:14<00:02,  1.40s/it] 89%|████████▉ | 8/9 [00:16<00:01,  1.32s/it]100%|██████████| 9/9 [00:16<00:00,  1.05it/s]100%|██████████| 9/9 [00:16<00:00,  1.82s/it]
=> result
* total: 4,039
* correct: 2,986
* accuracy: 73.9%
* error: 26.1%
* macro_f1: 72.6%
Run this job and save the output to output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  51
# train_x  816
# val      204
# test     1,934
---------  ------
['Apply_Eye_Makeup', 'Apply_Lipstick', 'Archery', 'Baby_Crawling', 'Balance_Beam', 'Band_Marching', 'Baseball_Pitch', 'Basketball', 'Basketball_Dunk', 'Bench_Press', 'Biking', 'Billiards', 'Blow_Dry_Hair', 'Blowing_Candles', 'Body_Weight_Squats', 'Bowling', 'Boxing_Punching_Bag', 'Boxing_Speed_Bag', 'Breast_Stroke', 'Brushing_Teeth', 'Clean_And_Jerk', 'Cliff_Diving', 'Cricket_Bowling', 'Cricket_Shot', 'Cutting_In_Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field_Hockey_Penalty', 'Floor_Gymnastics', 'Frisbee_Catch', 'Front_Crawl', 'Golf_Swing', 'Haircut', 'Hammering', 'Hammer_Throw', 'Handstand_Pushups', 'Handstand_Walking', 'Head_Massage', 'High_Jump', 'Horse_Race', 'Horse_Riding', 'Hula_Hoop', 'Ice_Dancing', 'Javelin_Throw', 'Juggling_Balls', 'Jumping_Jack', 'Jump_Rope', 'Kayaking', 'Knitting', 'Long_Jump']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Apply Eye Makeup.', 'a photo of a person doing Apply Lipstick.', 'a photo of a person doing Archery.', 'a photo of a person doing Baby Crawling.', 'a photo of a person doing Balance Beam.', 'a photo of a person doing Band Marching.', 'a photo of a person doing Baseball Pitch.', 'a photo of a person doing Basketball.', 'a photo of a person doing Basketball Dunk.', 'a photo of a person doing Bench Press.', 'a photo of a person doing Biking.', 'a photo of a person doing Billiards.', 'a photo of a person doing Blow Dry Hair.', 'a photo of a person doing Blowing Candles.', 'a photo of a person doing Body Weight Squats.', 'a photo of a person doing Bowling.', 'a photo of a person doing Boxing Punching Bag.', 'a photo of a person doing Boxing Speed Bag.', 'a photo of a person doing Breast Stroke.', 'a photo of a person doing Brushing Teeth.', 'a photo of a person doing Clean And Jerk.', 'a photo of a person doing Cliff Diving.', 'a photo of a person doing Cricket Bowling.', 'a photo of a person doing Cricket Shot.', 'a photo of a person doing Cutting In Kitchen.', 'a photo of a person doing Diving.', 'a photo of a person doing Drumming.', 'a photo of a person doing Fencing.', 'a photo of a person doing Field Hockey Penalty.', 'a photo of a person doing Floor Gymnastics.', 'a photo of a person doing Frisbee Catch.', 'a photo of a person doing Front Crawl.', 'a photo of a person doing Golf Swing.', 'a photo of a person doing Haircut.', 'a photo of a person doing Hammering.', 'a photo of a person doing Hammer Throw.', 'a photo of a person doing Handstand Pushups.', 'a photo of a person doing Handstand Walking.', 'a photo of a person doing Head Massage.', 'a photo of a person doing High Jump.', 'a photo of a person doing Horse Race.', 'a photo of a person doing Horse Riding.', 'a photo of a person doing Hula Hoop.', 'a photo of a person doing Ice Dancing.', 'a photo of a person doing Javelin Throw.', 'a photo of a person doing Juggling Balls.', 'a photo of a person doing Jumping Jack.', 'a photo of a person doing Jump Rope.', 'a photo of a person doing Kayaking.', 'a photo of a person doing Knitting.', 'a photo of a person doing Long Jump.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/26] time 0.153 (0.297) data 0.000 (0.130) loss 6.0703 (5.8172) acc 50.0000 (53.7500) lr 1.0000e-05 eta 0:06:24
epoch [1/50] batch [10/26] time 0.152 (0.225) data 0.000 (0.065) loss 5.7031 (5.7918) acc 56.2500 (55.0000) lr 1.0000e-05 eta 0:04:50
epoch [1/50] batch [15/26] time 0.154 (0.201) data 0.000 (0.043) loss 5.9883 (5.7878) acc 50.0000 (53.5417) lr 1.0000e-05 eta 0:04:18
epoch [1/50] batch [20/26] time 0.157 (0.190) data 0.000 (0.033) loss 5.6367 (5.7426) acc 56.2500 (54.2188) lr 1.0000e-05 eta 0:04:03
epoch [1/50] batch [25/26] time 0.158 (0.183) data 0.000 (0.026) loss 5.6328 (5.6633) acc 62.5000 (54.8750) lr 1.0000e-05 eta 0:03:53
epoch [2/50] batch [5/26] time 0.155 (0.293) data 0.000 (0.136) loss 4.0547 (4.6160) acc 65.6250 (57.5000) lr 2.0000e-03 eta 0:06:12
epoch [2/50] batch [10/26] time 0.152 (0.223) data 0.000 (0.068) loss 3.4219 (4.2664) acc 71.8750 (61.5625) lr 2.0000e-03 eta 0:04:42
epoch [2/50] batch [15/26] time 0.152 (0.200) data 0.000 (0.046) loss 3.5977 (4.1637) acc 62.5000 (60.4167) lr 2.0000e-03 eta 0:04:11
epoch [2/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.034) loss 3.4688 (4.0273) acc 75.0000 (61.5625) lr 2.0000e-03 eta 0:03:55
epoch [2/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.027) loss 3.4141 (3.9222) acc 62.5000 (61.2500) lr 2.0000e-03 eta 0:03:45
epoch [3/50] batch [5/26] time 0.154 (0.291) data 0.000 (0.137) loss 2.9336 (3.3555) acc 62.5000 (64.3750) lr 1.9980e-03 eta 0:06:01
epoch [3/50] batch [10/26] time 0.154 (0.222) data 0.000 (0.069) loss 2.9492 (3.1586) acc 68.7500 (65.9375) lr 1.9980e-03 eta 0:04:35
epoch [3/50] batch [15/26] time 0.155 (0.199) data 0.000 (0.046) loss 2.8496 (3.1352) acc 71.8750 (65.4167) lr 1.9980e-03 eta 0:04:05
epoch [3/50] batch [20/26] time 0.152 (0.187) data 0.000 (0.034) loss 2.6914 (3.1070) acc 71.8750 (65.3125) lr 1.9980e-03 eta 0:03:50
epoch [3/50] batch [25/26] time 0.152 (0.180) data 0.000 (0.028) loss 2.7852 (3.0954) acc 71.8750 (65.6250) lr 1.9980e-03 eta 0:03:40
epoch [4/50] batch [5/26] time 0.152 (0.273) data 0.000 (0.119) loss 2.7852 (3.1102) acc 68.7500 (61.8750) lr 1.9921e-03 eta 0:05:32
epoch [4/50] batch [10/26] time 0.157 (0.213) data 0.000 (0.059) loss 2.9375 (2.9486) acc 65.6250 (66.8750) lr 1.9921e-03 eta 0:04:18
epoch [4/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.040) loss 2.5820 (2.8464) acc 75.0000 (68.7500) lr 1.9921e-03 eta 0:03:53
epoch [4/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.030) loss 2.9512 (2.8109) acc 62.5000 (69.0625) lr 1.9921e-03 eta 0:03:40
epoch [4/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 2.7578 (2.8247) acc 78.1250 (69.0000) lr 1.9921e-03 eta 0:03:32
epoch [5/50] batch [5/26] time 0.153 (0.274) data 0.000 (0.121) loss 2.5977 (2.6379) acc 75.0000 (72.5000) lr 1.9823e-03 eta 0:05:26
epoch [5/50] batch [10/26] time 0.152 (0.213) data 0.000 (0.061) loss 2.5488 (2.6936) acc 75.0000 (71.5625) lr 1.9823e-03 eta 0:04:13
epoch [5/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.040) loss 2.7383 (2.7465) acc 62.5000 (70.2083) lr 1.9823e-03 eta 0:03:47
epoch [5/50] batch [20/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.8574 (2.7026) acc 68.7500 (70.0000) lr 1.9823e-03 eta 0:03:34
epoch [5/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.024) loss 1.9277 (2.6619) acc 87.5000 (70.6250) lr 1.9823e-03 eta 0:03:26
epoch [6/50] batch [5/26] time 0.154 (0.327) data 0.000 (0.172) loss 2.5566 (2.4285) acc 75.0000 (73.7500) lr 1.9686e-03 eta 0:06:21
epoch [6/50] batch [10/26] time 0.153 (0.240) data 0.000 (0.086) loss 2.8438 (2.5014) acc 75.0000 (74.0625) lr 1.9686e-03 eta 0:04:38
epoch [6/50] batch [15/26] time 0.152 (0.211) data 0.000 (0.057) loss 2.1641 (2.5180) acc 84.3750 (74.3750) lr 1.9686e-03 eta 0:04:03
epoch [6/50] batch [20/26] time 0.152 (0.196) data 0.000 (0.043) loss 2.2129 (2.5275) acc 75.0000 (73.2812) lr 1.9686e-03 eta 0:03:45
epoch [6/50] batch [25/26] time 0.152 (0.188) data 0.000 (0.035) loss 2.5645 (2.5252) acc 81.2500 (73.3750) lr 1.9686e-03 eta 0:03:34
epoch [7/50] batch [5/26] time 0.151 (0.275) data 0.000 (0.122) loss 2.5723 (2.4230) acc 65.6250 (76.2500) lr 1.9511e-03 eta 0:05:12
epoch [7/50] batch [10/26] time 0.152 (0.214) data 0.000 (0.061) loss 2.8027 (2.5021) acc 68.7500 (74.3750) lr 1.9511e-03 eta 0:04:02
epoch [7/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.041) loss 2.4375 (2.4943) acc 65.6250 (73.1250) lr 1.9511e-03 eta 0:03:37
epoch [7/50] batch [20/26] time 0.154 (0.183) data 0.000 (0.031) loss 2.6660 (2.5026) acc 81.2500 (73.9062) lr 1.9511e-03 eta 0:03:25
epoch [7/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.025) loss 2.5156 (2.4859) acc 81.2500 (75.0000) lr 1.9511e-03 eta 0:03:18
epoch [8/50] batch [5/26] time 0.153 (0.279) data 0.000 (0.126) loss 2.5020 (2.3727) acc 78.1250 (79.3750) lr 1.9298e-03 eta 0:05:10
epoch [8/50] batch [10/26] time 0.153 (0.216) data 0.000 (0.063) loss 2.7891 (2.4318) acc 62.5000 (76.5625) lr 1.9298e-03 eta 0:03:59
epoch [8/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.042) loss 2.5977 (2.4254) acc 68.7500 (75.8333) lr 1.9298e-03 eta 0:03:35
epoch [8/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.032) loss 2.0977 (2.4187) acc 84.3750 (76.0938) lr 1.9298e-03 eta 0:03:22
epoch [8/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.025) loss 2.5195 (2.4018) acc 68.7500 (76.3750) lr 1.9298e-03 eta 0:03:15
epoch [9/50] batch [5/26] time 0.153 (0.293) data 0.000 (0.139) loss 2.8477 (2.3330) acc 71.8750 (75.0000) lr 1.9048e-03 eta 0:05:18
epoch [9/50] batch [10/26] time 0.153 (0.223) data 0.000 (0.070) loss 2.0859 (2.2920) acc 78.1250 (76.2500) lr 1.9048e-03 eta 0:04:01
epoch [9/50] batch [15/26] time 0.152 (0.199) data 0.000 (0.047) loss 3.0000 (2.4128) acc 62.5000 (75.2083) lr 1.9048e-03 eta 0:03:34
epoch [9/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.035) loss 1.7617 (2.3402) acc 84.3750 (76.8750) lr 1.9048e-03 eta 0:03:21
epoch [9/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.028) loss 2.3086 (2.3347) acc 81.2500 (77.2500) lr 1.9048e-03 eta 0:03:12
epoch [10/50] batch [5/26] time 0.153 (0.289) data 0.000 (0.134) loss 2.1113 (2.4047) acc 78.1250 (73.7500) lr 1.8763e-03 eta 0:05:06
epoch [10/50] batch [10/26] time 0.153 (0.221) data 0.000 (0.067) loss 2.1035 (2.3428) acc 81.2500 (75.9375) lr 1.8763e-03 eta 0:03:53
epoch [10/50] batch [15/26] time 0.152 (0.199) data 0.000 (0.045) loss 3.0781 (2.3555) acc 68.7500 (77.5000) lr 1.8763e-03 eta 0:03:28
epoch [10/50] batch [20/26] time 0.154 (0.187) data 0.000 (0.034) loss 2.8867 (2.3450) acc 75.0000 (78.9062) lr 1.8763e-03 eta 0:03:15
epoch [10/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.027) loss 2.0977 (2.3068) acc 75.0000 (78.7500) lr 1.8763e-03 eta 0:03:07
epoch [11/50] batch [5/26] time 0.154 (0.278) data 0.000 (0.124) loss 2.6211 (2.4117) acc 75.0000 (80.0000) lr 1.8443e-03 eta 0:04:47
epoch [11/50] batch [10/26] time 0.154 (0.216) data 0.000 (0.062) loss 2.5586 (2.2977) acc 68.7500 (78.7500) lr 1.8443e-03 eta 0:03:42
epoch [11/50] batch [15/26] time 0.152 (0.195) data 0.000 (0.042) loss 2.2734 (2.2486) acc 78.1250 (79.7917) lr 1.8443e-03 eta 0:03:19
epoch [11/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.031) loss 2.4395 (2.2329) acc 84.3750 (80.4688) lr 1.8443e-03 eta 0:03:07
epoch [11/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.025) loss 2.3477 (2.2243) acc 78.1250 (80.6250) lr 1.8443e-03 eta 0:03:00
epoch [12/50] batch [5/26] time 0.154 (0.313) data 0.000 (0.159) loss 1.8301 (2.0156) acc 87.5000 (85.0000) lr 1.8090e-03 eta 0:05:15
epoch [12/50] batch [10/26] time 0.155 (0.234) data 0.000 (0.079) loss 2.4453 (2.1770) acc 71.8750 (82.1875) lr 1.8090e-03 eta 0:03:54
epoch [12/50] batch [15/26] time 0.154 (0.208) data 0.000 (0.053) loss 3.1172 (2.1826) acc 65.6250 (81.4583) lr 1.8090e-03 eta 0:03:27
epoch [12/50] batch [20/26] time 0.153 (0.200) data 0.000 (0.040) loss 2.3477 (2.2168) acc 75.0000 (80.3125) lr 1.8090e-03 eta 0:03:18
epoch [12/50] batch [25/26] time 0.153 (0.190) data 0.000 (0.032) loss 2.9570 (2.2820) acc 65.6250 (78.8750) lr 1.8090e-03 eta 0:03:08
epoch [13/50] batch [5/26] time 0.154 (0.288) data 0.000 (0.133) loss 2.1133 (2.0758) acc 78.1250 (83.7500) lr 1.7705e-03 eta 0:04:43
epoch [13/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.067) loss 2.1621 (2.1465) acc 84.3750 (82.1875) lr 1.7705e-03 eta 0:03:35
epoch [13/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.044) loss 2.4043 (2.1217) acc 78.1250 (81.4583) lr 1.7705e-03 eta 0:03:12
epoch [13/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.033) loss 1.9648 (2.1005) acc 87.5000 (81.4062) lr 1.7705e-03 eta 0:03:00
epoch [13/50] batch [25/26] time 0.155 (0.180) data 0.000 (0.027) loss 2.1523 (2.1064) acc 81.2500 (81.5000) lr 1.7705e-03 eta 0:02:53
epoch [14/50] batch [5/26] time 0.157 (0.298) data 0.000 (0.140) loss 2.3574 (2.0852) acc 71.8750 (82.5000) lr 1.7290e-03 eta 0:04:45
epoch [14/50] batch [10/26] time 0.157 (0.228) data 0.000 (0.070) loss 2.0352 (2.0514) acc 81.2500 (82.5000) lr 1.7290e-03 eta 0:03:36
epoch [14/50] batch [15/26] time 0.156 (0.204) data 0.000 (0.047) loss 2.1719 (2.0786) acc 84.3750 (82.9167) lr 1.7290e-03 eta 0:03:12
epoch [14/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.035) loss 2.1016 (2.1249) acc 87.5000 (81.5625) lr 1.7290e-03 eta 0:03:00
epoch [14/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.028) loss 1.7754 (2.1282) acc 84.3750 (82.2500) lr 1.7290e-03 eta 0:02:51
epoch [15/50] batch [5/26] time 0.154 (0.283) data 0.000 (0.127) loss 1.5771 (2.0877) acc 90.6250 (83.7500) lr 1.6845e-03 eta 0:04:23
epoch [15/50] batch [10/26] time 0.157 (0.218) data 0.000 (0.064) loss 2.1660 (2.0991) acc 78.1250 (83.1250) lr 1.6845e-03 eta 0:03:22
epoch [15/50] batch [15/26] time 0.154 (0.197) data 0.000 (0.043) loss 2.0977 (2.1792) acc 81.2500 (81.2500) lr 1.6845e-03 eta 0:03:01
epoch [15/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.032) loss 2.1152 (2.1145) acc 81.2500 (82.1875) lr 1.6845e-03 eta 0:02:50
epoch [15/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.8203 (2.1127) acc 90.6250 (82.6250) lr 1.6845e-03 eta 0:02:43
epoch [16/50] batch [5/26] time 0.155 (0.302) data 0.000 (0.147) loss 1.9990 (1.9854) acc 81.2500 (85.6250) lr 1.6374e-03 eta 0:04:32
epoch [16/50] batch [10/26] time 0.156 (0.228) data 0.000 (0.074) loss 1.4609 (2.0043) acc 93.7500 (82.8125) lr 1.6374e-03 eta 0:03:25
epoch [16/50] batch [15/26] time 0.154 (0.204) data 0.000 (0.049) loss 2.3242 (2.0367) acc 78.1250 (81.4583) lr 1.6374e-03 eta 0:03:02
epoch [16/50] batch [20/26] time 0.154 (0.191) data 0.000 (0.037) loss 1.7451 (2.0237) acc 90.6250 (82.6562) lr 1.6374e-03 eta 0:02:50
epoch [16/50] batch [25/26] time 0.153 (0.184) data 0.000 (0.030) loss 1.9971 (2.0752) acc 84.3750 (82.6250) lr 1.6374e-03 eta 0:02:42
epoch [17/50] batch [5/26] time 0.155 (0.314) data 0.000 (0.159) loss 2.1250 (1.8051) acc 81.2500 (86.8750) lr 1.5878e-03 eta 0:04:36
epoch [17/50] batch [10/26] time 0.156 (0.236) data 0.000 (0.080) loss 1.7598 (1.9031) acc 90.6250 (84.0625) lr 1.5878e-03 eta 0:03:25
epoch [17/50] batch [15/26] time 0.154 (0.208) data 0.000 (0.053) loss 1.9922 (1.9486) acc 81.2500 (84.1667) lr 1.5878e-03 eta 0:03:01
epoch [17/50] batch [20/26] time 0.155 (0.195) data 0.001 (0.040) loss 2.0469 (2.0131) acc 71.8750 (82.8125) lr 1.5878e-03 eta 0:02:48
epoch [17/50] batch [25/26] time 0.153 (0.187) data 0.000 (0.032) loss 1.5830 (1.9842) acc 96.8750 (84.5000) lr 1.5878e-03 eta 0:02:40
epoch [18/50] batch [5/26] time 0.154 (0.314) data 0.000 (0.159) loss 2.2422 (1.9670) acc 81.2500 (82.5000) lr 1.5358e-03 eta 0:04:27
epoch [18/50] batch [10/26] time 0.155 (0.234) data 0.000 (0.079) loss 2.0879 (2.0106) acc 84.3750 (83.4375) lr 1.5358e-03 eta 0:03:18
epoch [18/50] batch [15/26] time 0.156 (0.208) data 0.000 (0.053) loss 1.8438 (2.0473) acc 87.5000 (82.9167) lr 1.5358e-03 eta 0:02:55
epoch [18/50] batch [20/26] time 0.154 (0.194) data 0.000 (0.040) loss 2.2441 (2.0654) acc 78.1250 (82.5000) lr 1.5358e-03 eta 0:02:42
epoch [18/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.032) loss 2.3633 (2.0840) acc 78.1250 (82.2500) lr 1.5358e-03 eta 0:02:35
epoch [19/50] batch [5/26] time 0.154 (0.315) data 0.000 (0.160) loss 1.8574 (1.9563) acc 87.5000 (86.8750) lr 1.4818e-03 eta 0:04:20
epoch [19/50] batch [10/26] time 0.154 (0.235) data 0.000 (0.080) loss 1.5771 (1.9710) acc 87.5000 (85.9375) lr 1.4818e-03 eta 0:03:13
epoch [19/50] batch [15/26] time 0.154 (0.208) data 0.000 (0.054) loss 1.9023 (1.9332) acc 84.3750 (86.4583) lr 1.4818e-03 eta 0:02:50
epoch [19/50] batch [20/26] time 0.158 (0.195) data 0.000 (0.040) loss 2.0176 (2.0282) acc 84.3750 (85.1562) lr 1.4818e-03 eta 0:02:38
epoch [19/50] batch [25/26] time 0.154 (0.187) data 0.000 (0.032) loss 2.2188 (2.0394) acc 78.1250 (84.3750) lr 1.4818e-03 eta 0:02:30
epoch [20/50] batch [5/26] time 0.155 (0.304) data 0.000 (0.148) loss 2.0371 (2.0551) acc 84.3750 (84.3750) lr 1.4258e-03 eta 0:04:03
epoch [20/50] batch [10/26] time 0.154 (0.229) data 0.000 (0.074) loss 2.0508 (1.9247) acc 78.1250 (85.6250) lr 1.4258e-03 eta 0:03:02
epoch [20/50] batch [15/26] time 0.153 (0.204) data 0.000 (0.050) loss 1.4668 (1.9490) acc 96.8750 (86.0417) lr 1.4258e-03 eta 0:02:41
epoch [20/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 2.0234 (2.0180) acc 81.2500 (84.6875) lr 1.4258e-03 eta 0:02:30
epoch [20/50] batch [25/26] time 0.154 (0.184) data 0.000 (0.030) loss 2.5508 (2.0492) acc 78.1250 (84.6250) lr 1.4258e-03 eta 0:02:23
epoch [21/50] batch [5/26] time 0.154 (0.310) data 0.000 (0.155) loss 1.7510 (2.0100) acc 90.6250 (83.7500) lr 1.3681e-03 eta 0:04:00
epoch [21/50] batch [10/26] time 0.155 (0.232) data 0.000 (0.077) loss 1.5684 (1.9290) acc 90.6250 (86.5625) lr 1.3681e-03 eta 0:02:58
epoch [21/50] batch [15/26] time 0.154 (0.206) data 0.000 (0.052) loss 1.9209 (1.9717) acc 84.3750 (85.6250) lr 1.3681e-03 eta 0:02:37
epoch [21/50] batch [20/26] time 0.154 (0.193) data 0.000 (0.039) loss 2.2910 (2.0381) acc 84.3750 (85.3125) lr 1.3681e-03 eta 0:02:26
epoch [21/50] batch [25/26] time 0.154 (0.185) data 0.000 (0.031) loss 2.3379 (2.0552) acc 75.0000 (84.5000) lr 1.3681e-03 eta 0:02:19
epoch [22/50] batch [5/26] time 0.155 (0.277) data 0.000 (0.120) loss 2.0117 (1.8725) acc 81.2500 (85.6250) lr 1.3090e-03 eta 0:03:27
epoch [22/50] batch [10/26] time 0.156 (0.216) data 0.000 (0.060) loss 1.7148 (1.7818) acc 90.6250 (87.1875) lr 1.3090e-03 eta 0:02:40
epoch [22/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.040) loss 1.6992 (1.8645) acc 84.3750 (86.2500) lr 1.3090e-03 eta 0:02:24
epoch [22/50] batch [20/26] time 0.154 (0.185) data 0.000 (0.030) loss 2.2539 (1.9240) acc 81.2500 (86.2500) lr 1.3090e-03 eta 0:02:15
epoch [22/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.024) loss 1.4941 (1.8869) acc 90.6250 (87.2500) lr 1.3090e-03 eta 0:02:10
epoch [23/50] batch [5/26] time 0.155 (0.287) data 0.000 (0.131) loss 2.4297 (1.8719) acc 78.1250 (88.7500) lr 1.2487e-03 eta 0:03:27
epoch [23/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.066) loss 2.1484 (2.0056) acc 87.5000 (84.6875) lr 1.2487e-03 eta 0:02:38
epoch [23/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.044) loss 2.2324 (2.0134) acc 84.3750 (84.7917) lr 1.2487e-03 eta 0:02:21
epoch [23/50] batch [20/26] time 0.154 (0.187) data 0.000 (0.033) loss 1.9971 (1.9876) acc 84.3750 (85.6250) lr 1.2487e-03 eta 0:02:12
epoch [23/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.026) loss 1.6465 (2.0129) acc 90.6250 (85.0000) lr 1.2487e-03 eta 0:02:06
epoch [24/50] batch [5/26] time 0.155 (0.280) data 0.000 (0.125) loss 1.9180 (2.0691) acc 81.2500 (82.5000) lr 1.1874e-03 eta 0:03:14
epoch [24/50] batch [10/26] time 0.154 (0.217) data 0.000 (0.063) loss 1.7090 (1.9813) acc 84.3750 (84.3750) lr 1.1874e-03 eta 0:02:30
epoch [24/50] batch [15/26] time 0.154 (0.196) data 0.000 (0.042) loss 1.8369 (1.9867) acc 81.2500 (84.3750) lr 1.1874e-03 eta 0:02:14
epoch [24/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.4346 (1.9111) acc 96.8750 (85.7812) lr 1.1874e-03 eta 0:02:06
epoch [24/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 1.9023 (1.9444) acc 90.6250 (85.5000) lr 1.1874e-03 eta 0:02:01
epoch [25/50] batch [5/26] time 0.153 (0.306) data 0.000 (0.151) loss 2.1035 (2.1613) acc 81.2500 (82.5000) lr 1.1253e-03 eta 0:03:25
epoch [25/50] batch [10/26] time 0.155 (0.230) data 0.000 (0.076) loss 2.0781 (2.1240) acc 84.3750 (81.5625) lr 1.1253e-03 eta 0:02:33
epoch [25/50] batch [15/26] time 0.154 (0.204) data 0.000 (0.051) loss 2.5527 (2.0995) acc 81.2500 (83.9583) lr 1.1253e-03 eta 0:02:15
epoch [25/50] batch [20/26] time 0.154 (0.192) data 0.000 (0.038) loss 1.6250 (2.0038) acc 93.7500 (84.6875) lr 1.1253e-03 eta 0:02:05
epoch [25/50] batch [25/26] time 0.154 (0.184) data 0.000 (0.030) loss 1.7139 (1.9864) acc 90.6250 (85.5000) lr 1.1253e-03 eta 0:01:59
epoch [26/50] batch [5/26] time 0.156 (0.292) data 0.000 (0.136) loss 2.1523 (1.9291) acc 84.3750 (88.7500) lr 1.0628e-03 eta 0:03:08
epoch [26/50] batch [10/26] time 0.155 (0.223) data 0.000 (0.068) loss 2.0586 (1.8642) acc 96.8750 (89.3750) lr 1.0628e-03 eta 0:02:22
epoch [26/50] batch [15/26] time 0.154 (0.200) data 0.000 (0.046) loss 2.2578 (1.9903) acc 87.5000 (87.2917) lr 1.0628e-03 eta 0:02:07
epoch [26/50] batch [20/26] time 0.154 (0.189) data 0.000 (0.034) loss 1.5225 (1.9556) acc 93.7500 (88.1250) lr 1.0628e-03 eta 0:01:58
epoch [26/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.027) loss 2.5859 (1.9849) acc 75.0000 (87.1250) lr 1.0628e-03 eta 0:01:53
epoch [27/50] batch [5/26] time 0.154 (0.287) data 0.000 (0.132) loss 1.5273 (1.7082) acc 87.5000 (90.0000) lr 1.0000e-03 eta 0:02:57
epoch [27/50] batch [10/26] time 0.154 (0.221) data 0.000 (0.066) loss 2.1777 (1.7989) acc 84.3750 (89.0625) lr 1.0000e-03 eta 0:02:15
epoch [27/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.044) loss 1.5840 (1.7570) acc 90.6250 (88.9583) lr 1.0000e-03 eta 0:02:00
epoch [27/50] batch [20/26] time 0.154 (0.187) data 0.000 (0.033) loss 1.9629 (1.8423) acc 90.6250 (87.8125) lr 1.0000e-03 eta 0:01:52
epoch [27/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.027) loss 2.0781 (1.9005) acc 84.3750 (87.1250) lr 1.0000e-03 eta 0:01:47
epoch [28/50] batch [5/26] time 0.154 (0.302) data 0.000 (0.147) loss 1.8975 (1.9855) acc 90.6250 (86.2500) lr 9.3721e-04 eta 0:02:59
epoch [28/50] batch [10/26] time 0.154 (0.229) data 0.001 (0.074) loss 1.6602 (1.8447) acc 93.7500 (88.7500) lr 9.3721e-04 eta 0:02:14
epoch [28/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 1.8262 (1.8175) acc 93.7500 (90.0000) lr 9.3721e-04 eta 0:01:58
epoch [28/50] batch [20/26] time 0.154 (0.191) data 0.000 (0.037) loss 2.2910 (1.8936) acc 81.2500 (88.4375) lr 9.3721e-04 eta 0:01:50
epoch [28/50] batch [25/26] time 0.155 (0.183) data 0.000 (0.030) loss 1.8848 (1.8688) acc 90.6250 (89.1250) lr 9.3721e-04 eta 0:01:45
epoch [29/50] batch [5/26] time 0.154 (0.291) data 0.000 (0.136) loss 1.5508 (1.9688) acc 93.7500 (87.5000) lr 8.7467e-04 eta 0:02:45
epoch [29/50] batch [10/26] time 0.156 (0.223) data 0.000 (0.068) loss 1.6191 (1.9084) acc 90.6250 (88.1250) lr 8.7467e-04 eta 0:02:05
epoch [29/50] batch [15/26] time 0.156 (0.201) data 0.000 (0.045) loss 1.8008 (1.8842) acc 84.3750 (88.3333) lr 8.7467e-04 eta 0:01:51
epoch [29/50] batch [20/26] time 0.155 (0.189) data 0.000 (0.034) loss 1.6719 (1.8831) acc 87.5000 (87.9688) lr 8.7467e-04 eta 0:01:44
epoch [29/50] batch [25/26] time 0.155 (0.182) data 0.000 (0.027) loss 1.6074 (1.8967) acc 100.0000 (87.2500) lr 8.7467e-04 eta 0:01:39
epoch [30/50] batch [5/26] time 0.154 (0.300) data 0.000 (0.145) loss 2.1543 (1.9996) acc 84.3750 (85.6250) lr 8.1262e-04 eta 0:02:42
epoch [30/50] batch [10/26] time 0.153 (0.227) data 0.000 (0.073) loss 1.8682 (1.9763) acc 84.3750 (85.0000) lr 8.1262e-04 eta 0:02:01
epoch [30/50] batch [15/26] time 0.154 (0.203) data 0.000 (0.049) loss 1.9258 (1.9311) acc 87.5000 (86.6667) lr 8.1262e-04 eta 0:01:47
epoch [30/50] batch [20/26] time 0.154 (0.191) data 0.000 (0.036) loss 2.0117 (1.9582) acc 81.2500 (86.0938) lr 8.1262e-04 eta 0:01:40
epoch [30/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.4492 (1.9299) acc 93.7500 (86.2500) lr 8.1262e-04 eta 0:01:35
epoch [31/50] batch [5/26] time 0.155 (0.281) data 0.000 (0.125) loss 1.3867 (1.7115) acc 100.0000 (92.5000) lr 7.5131e-04 eta 0:02:24
epoch [31/50] batch [10/26] time 0.154 (0.217) data 0.000 (0.063) loss 2.0156 (1.7032) acc 81.2500 (91.2500) lr 7.5131e-04 eta 0:01:50
epoch [31/50] batch [15/26] time 0.153 (0.196) data 0.000 (0.042) loss 2.0566 (1.7630) acc 87.5000 (90.6250) lr 7.5131e-04 eta 0:01:38
epoch [31/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.8613 (1.7695) acc 90.6250 (90.4688) lr 7.5131e-04 eta 0:01:32
epoch [31/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 1.9834 (1.8095) acc 87.5000 (89.8750) lr 7.5131e-04 eta 0:01:28
epoch [32/50] batch [5/26] time 0.153 (0.295) data 0.000 (0.141) loss 2.1211 (1.9160) acc 84.3750 (88.1250) lr 6.9098e-04 eta 0:02:24
epoch [32/50] batch [10/26] time 0.155 (0.225) data 0.001 (0.071) loss 1.8574 (1.7924) acc 90.6250 (91.5625) lr 6.9098e-04 eta 0:01:48
epoch [32/50] batch [15/26] time 0.154 (0.201) data 0.000 (0.047) loss 2.0566 (1.8559) acc 87.5000 (90.2083) lr 6.9098e-04 eta 0:01:36
epoch [32/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.035) loss 2.0957 (1.8636) acc 87.5000 (89.8438) lr 6.9098e-04 eta 0:01:29
epoch [32/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.7324 (1.8656) acc 87.5000 (88.6250) lr 6.9098e-04 eta 0:01:25
epoch [33/50] batch [5/26] time 0.154 (0.268) data 0.000 (0.112) loss 1.9043 (1.9512) acc 93.7500 (90.0000) lr 6.3188e-04 eta 0:02:04
epoch [33/50] batch [10/26] time 0.156 (0.212) data 0.000 (0.056) loss 1.9639 (2.0045) acc 93.7500 (89.6875) lr 6.3188e-04 eta 0:01:36
epoch [33/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.037) loss 1.8672 (1.8919) acc 87.5000 (90.4167) lr 6.3188e-04 eta 0:01:27
epoch [33/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.028) loss 2.4102 (1.9377) acc 78.1250 (89.6875) lr 6.3188e-04 eta 0:01:21
epoch [33/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.022) loss 1.6016 (1.8979) acc 93.7500 (89.6250) lr 6.3188e-04 eta 0:01:18
epoch [34/50] batch [5/26] time 0.155 (0.297) data 0.000 (0.141) loss 1.4707 (1.6248) acc 96.8750 (94.3750) lr 5.7422e-04 eta 0:02:09
epoch [34/50] batch [10/26] time 0.155 (0.226) data 0.000 (0.071) loss 1.4717 (1.6714) acc 87.5000 (91.8750) lr 5.7422e-04 eta 0:01:37
epoch [34/50] batch [15/26] time 0.155 (0.202) data 0.000 (0.047) loss 1.5273 (1.7191) acc 93.7500 (90.2083) lr 5.7422e-04 eta 0:01:26
epoch [34/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.036) loss 2.4043 (1.7563) acc 78.1250 (89.3750) lr 5.7422e-04 eta 0:01:20
epoch [34/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.028) loss 1.9346 (1.8009) acc 93.7500 (89.1250) lr 5.7422e-04 eta 0:01:16
epoch [35/50] batch [5/26] time 0.153 (0.273) data 0.000 (0.119) loss 1.5391 (1.8705) acc 90.6250 (87.5000) lr 5.1825e-04 eta 0:01:52
epoch [35/50] batch [10/26] time 0.153 (0.213) data 0.000 (0.060) loss 1.8008 (1.7985) acc 96.8750 (90.0000) lr 5.1825e-04 eta 0:01:26
epoch [35/50] batch [15/26] time 0.154 (0.193) data 0.000 (0.040) loss 2.3398 (1.8122) acc 81.2500 (90.0000) lr 5.1825e-04 eta 0:01:17
epoch [35/50] batch [20/26] time 0.154 (0.184) data 0.000 (0.030) loss 1.5488 (1.8359) acc 87.5000 (89.3750) lr 5.1825e-04 eta 0:01:12
epoch [35/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.024) loss 1.7461 (1.8512) acc 87.5000 (88.6250) lr 5.1825e-04 eta 0:01:09
epoch [36/50] batch [5/26] time 0.154 (0.276) data 0.000 (0.121) loss 1.6875 (1.7793) acc 96.8750 (89.3750) lr 4.6417e-04 eta 0:01:46
epoch [36/50] batch [10/26] time 0.156 (0.216) data 0.000 (0.061) loss 1.6182 (1.7625) acc 93.7500 (89.3750) lr 4.6417e-04 eta 0:01:21
epoch [36/50] batch [15/26] time 0.154 (0.195) data 0.000 (0.041) loss 1.9111 (1.7776) acc 93.7500 (90.2083) lr 4.6417e-04 eta 0:01:13
epoch [36/50] batch [20/26] time 0.154 (0.185) data 0.000 (0.030) loss 1.8672 (1.7870) acc 87.5000 (89.6875) lr 4.6417e-04 eta 0:01:08
epoch [36/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 1.7959 (1.7934) acc 87.5000 (89.3750) lr 4.6417e-04 eta 0:01:05
epoch [37/50] batch [5/26] time 0.155 (0.267) data 0.000 (0.112) loss 2.1250 (1.7932) acc 81.2500 (88.7500) lr 4.1221e-04 eta 0:01:35
epoch [37/50] batch [10/26] time 0.155 (0.211) data 0.000 (0.056) loss 1.7500 (1.8334) acc 90.6250 (88.1250) lr 4.1221e-04 eta 0:01:14
epoch [37/50] batch [15/26] time 0.155 (0.192) data 0.000 (0.037) loss 2.6016 (1.9174) acc 75.0000 (87.2917) lr 4.1221e-04 eta 0:01:06
epoch [37/50] batch [20/26] time 0.155 (0.183) data 0.000 (0.028) loss 2.1230 (1.8863) acc 90.6250 (87.9688) lr 4.1221e-04 eta 0:01:02
epoch [37/50] batch [25/26] time 0.154 (0.177) data 0.000 (0.023) loss 1.4629 (1.8686) acc 93.7500 (88.2500) lr 4.1221e-04 eta 0:00:59
epoch [38/50] batch [5/26] time 0.155 (0.279) data 0.000 (0.124) loss 2.0195 (1.7982) acc 84.3750 (88.1250) lr 3.6258e-04 eta 0:01:33
epoch [38/50] batch [10/26] time 0.154 (0.216) data 0.000 (0.062) loss 1.7842 (1.7777) acc 90.6250 (89.3750) lr 3.6258e-04 eta 0:01:10
epoch [38/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.041) loss 1.5039 (1.7462) acc 96.8750 (89.5833) lr 3.6258e-04 eta 0:01:03
epoch [38/50] batch [20/26] time 0.154 (0.184) data 0.000 (0.031) loss 1.9893 (1.8864) acc 93.7500 (87.5000) lr 3.6258e-04 eta 0:00:58
epoch [38/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.025) loss 1.6553 (1.8294) acc 90.6250 (88.3750) lr 3.6258e-04 eta 0:00:55
epoch [39/50] batch [5/26] time 0.154 (0.263) data 0.000 (0.108) loss 2.0020 (2.0168) acc 90.6250 (88.7500) lr 3.1545e-04 eta 0:01:20
epoch [39/50] batch [10/26] time 0.154 (0.209) data 0.000 (0.054) loss 1.8105 (1.9091) acc 90.6250 (88.1250) lr 3.1545e-04 eta 0:01:03
epoch [39/50] batch [15/26] time 0.154 (0.191) data 0.000 (0.036) loss 1.7871 (1.9272) acc 87.5000 (87.7083) lr 3.1545e-04 eta 0:00:56
epoch [39/50] batch [20/26] time 0.153 (0.181) data 0.000 (0.027) loss 2.0664 (1.9307) acc 81.2500 (87.6562) lr 3.1545e-04 eta 0:00:52
epoch [39/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.022) loss 1.5898 (1.8638) acc 100.0000 (88.5000) lr 3.1545e-04 eta 0:00:50
epoch [40/50] batch [5/26] time 0.153 (0.288) data 0.000 (0.134) loss 1.5781 (1.7437) acc 96.8750 (91.2500) lr 2.7103e-04 eta 0:01:20
epoch [40/50] batch [10/26] time 0.156 (0.221) data 0.000 (0.067) loss 1.6182 (1.8017) acc 87.5000 (88.4375) lr 2.7103e-04 eta 0:01:01
epoch [40/50] batch [15/26] time 0.156 (0.200) data 0.000 (0.045) loss 2.2773 (1.9048) acc 84.3750 (86.6667) lr 2.7103e-04 eta 0:00:54
epoch [40/50] batch [20/26] time 0.157 (0.189) data 0.001 (0.034) loss 2.2305 (1.8809) acc 78.1250 (87.0312) lr 2.7103e-04 eta 0:00:50
epoch [40/50] batch [25/26] time 0.155 (0.182) data 0.000 (0.027) loss 1.6699 (1.8682) acc 93.7500 (87.5000) lr 2.7103e-04 eta 0:00:47
epoch [41/50] batch [5/26] time 0.155 (0.263) data 0.000 (0.108) loss 1.5352 (1.6734) acc 90.6250 (93.1250) lr 2.2949e-04 eta 0:01:06
epoch [41/50] batch [10/26] time 0.155 (0.208) data 0.000 (0.054) loss 2.0898 (1.7521) acc 87.5000 (91.5625) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [15/26] time 0.154 (0.190) data 0.000 (0.036) loss 1.6709 (1.7678) acc 90.6250 (90.4167) lr 2.2949e-04 eta 0:00:46
epoch [41/50] batch [20/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.6406 (1.7829) acc 90.6250 (89.2188) lr 2.2949e-04 eta 0:00:43
epoch [41/50] batch [25/26] time 0.155 (0.176) data 0.000 (0.022) loss 2.2109 (1.8188) acc 78.1250 (88.5000) lr 2.2949e-04 eta 0:00:41
epoch [42/50] batch [5/26] time 0.154 (0.315) data 0.000 (0.159) loss 1.9248 (1.8865) acc 90.6250 (90.6250) lr 1.9098e-04 eta 0:01:12
epoch [42/50] batch [10/26] time 0.154 (0.235) data 0.000 (0.080) loss 1.9199 (1.8742) acc 84.3750 (88.4375) lr 1.9098e-04 eta 0:00:52
epoch [42/50] batch [15/26] time 0.154 (0.208) data 0.000 (0.053) loss 1.8525 (1.8617) acc 90.6250 (88.7500) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [20/26] time 0.154 (0.195) data 0.000 (0.040) loss 1.5898 (1.8105) acc 90.6250 (89.3750) lr 1.9098e-04 eta 0:00:41
epoch [42/50] batch [25/26] time 0.154 (0.186) data 0.000 (0.032) loss 1.7969 (1.8262) acc 96.8750 (89.5000) lr 1.9098e-04 eta 0:00:38
epoch [43/50] batch [5/26] time 0.154 (0.293) data 0.000 (0.138) loss 1.7793 (1.7977) acc 90.6250 (89.3750) lr 1.5567e-04 eta 0:00:59
epoch [43/50] batch [10/26] time 0.156 (0.224) data 0.000 (0.069) loss 1.6572 (1.8622) acc 84.3750 (87.5000) lr 1.5567e-04 eta 0:00:44
epoch [43/50] batch [15/26] time 0.155 (0.201) data 0.000 (0.046) loss 1.4824 (1.8049) acc 90.6250 (88.9583) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [20/26] time 0.156 (0.190) data 0.000 (0.035) loss 2.2109 (1.8206) acc 84.3750 (89.2188) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/26] time 0.155 (0.183) data 0.000 (0.028) loss 1.7363 (1.8192) acc 87.5000 (89.0000) lr 1.5567e-04 eta 0:00:33
epoch [44/50] batch [5/26] time 0.154 (0.278) data 0.000 (0.122) loss 1.4512 (1.5684) acc 96.8750 (95.0000) lr 1.2369e-04 eta 0:00:49
epoch [44/50] batch [10/26] time 0.154 (0.216) data 0.000 (0.061) loss 2.2930 (1.7116) acc 87.5000 (91.8750) lr 1.2369e-04 eta 0:00:37
epoch [44/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.041) loss 2.0293 (1.8104) acc 87.5000 (90.6250) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [20/26] time 0.155 (0.184) data 0.000 (0.031) loss 1.6680 (1.7431) acc 93.7500 (91.4062) lr 1.2369e-04 eta 0:00:29
epoch [44/50] batch [25/26] time 0.156 (0.179) data 0.000 (0.025) loss 1.5820 (1.7448) acc 90.6250 (91.0000) lr 1.2369e-04 eta 0:00:28
epoch [45/50] batch [5/26] time 0.154 (0.276) data 0.000 (0.121) loss 1.6641 (1.7029) acc 90.6250 (91.8750) lr 9.5173e-05 eta 0:00:41
epoch [45/50] batch [10/26] time 0.154 (0.215) data 0.000 (0.061) loss 1.7773 (1.7650) acc 84.3750 (90.0000) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [15/26] time 0.154 (0.195) data 0.000 (0.040) loss 1.3867 (1.7690) acc 93.7500 (89.3750) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [20/26] time 0.154 (0.185) data 0.000 (0.030) loss 1.9375 (1.8158) acc 87.5000 (88.2812) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 1.8057 (1.8095) acc 90.6250 (89.2500) lr 9.5173e-05 eta 0:00:23
epoch [46/50] batch [5/26] time 0.154 (0.271) data 0.000 (0.115) loss 1.6328 (1.7154) acc 93.7500 (90.6250) lr 7.0224e-05 eta 0:00:33
epoch [46/50] batch [10/26] time 0.155 (0.213) data 0.000 (0.058) loss 1.7109 (1.7561) acc 93.7500 (90.0000) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [15/26] time 0.154 (0.194) data 0.000 (0.039) loss 1.6504 (1.7186) acc 93.7500 (90.8333) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.157 (0.184) data 0.000 (0.029) loss 1.3662 (1.6903) acc 96.8750 (91.7188) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.023) loss 2.1055 (1.7411) acc 87.5000 (91.2500) lr 7.0224e-05 eta 0:00:18
epoch [47/50] batch [5/26] time 0.154 (0.286) data 0.000 (0.130) loss 1.8594 (2.2586) acc 90.6250 (84.3750) lr 4.8943e-05 eta 0:00:28
epoch [47/50] batch [10/26] time 0.153 (0.220) data 0.000 (0.065) loss 1.7090 (2.0780) acc 96.8750 (86.5625) lr 4.8943e-05 eta 0:00:20
epoch [47/50] batch [15/26] time 0.153 (0.197) data 0.000 (0.043) loss 1.5977 (1.9718) acc 96.8750 (87.7083) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [20/26] time 0.156 (0.186) data 0.000 (0.033) loss 1.5635 (1.9148) acc 90.6250 (88.5938) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.154 (0.180) data 0.000 (0.026) loss 1.5303 (1.8762) acc 96.8750 (88.8750) lr 4.8943e-05 eta 0:00:14
epoch [48/50] batch [5/26] time 0.155 (0.302) data 0.000 (0.146) loss 2.5840 (1.7742) acc 78.1250 (88.7500) lr 3.1417e-05 eta 0:00:22
epoch [48/50] batch [10/26] time 0.154 (0.228) data 0.000 (0.073) loss 1.7666 (1.8075) acc 93.7500 (88.4375) lr 3.1417e-05 eta 0:00:15
epoch [48/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 2.4590 (1.8561) acc 75.0000 (88.1250) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/26] time 0.155 (0.191) data 0.000 (0.037) loss 2.2188 (1.8595) acc 84.3750 (88.4375) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 2.0410 (1.8672) acc 90.6250 (88.7500) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.157 (0.293) data 0.001 (0.136) loss 2.4219 (1.8521) acc 81.2500 (90.0000) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [10/26] time 0.154 (0.224) data 0.000 (0.068) loss 1.9434 (1.7930) acc 87.5000 (89.6875) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/26] time 0.155 (0.201) data 0.000 (0.045) loss 1.9971 (1.9131) acc 90.6250 (88.1250) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.155 (0.189) data 0.000 (0.034) loss 2.1094 (1.8925) acc 87.5000 (88.2812) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [25/26] time 0.156 (0.183) data 0.000 (0.027) loss 1.5332 (1.9155) acc 93.7500 (87.6250) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.154 (0.274) data 0.000 (0.118) loss 1.9629 (1.9244) acc 87.5000 (86.8750) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/26] time 0.154 (0.214) data 0.000 (0.059) loss 2.4531 (1.9699) acc 78.1250 (85.9375) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.039) loss 1.5879 (1.8537) acc 90.6250 (88.3333) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.154 (0.184) data 0.000 (0.030) loss 2.1699 (1.8803) acc 90.6250 (88.5938) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.154 (0.178) data 0.000 (0.024) loss 1.9336 (1.8563) acc 84.3750 (89.0000) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.72s/it] 50%|█████     | 2/4 [00:04<00:04,  2.20s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.72s/it]100%|██████████| 4/4 [00:07<00:00,  1.46s/it]100%|██████████| 4/4 [00:07<00:00,  1.80s/it]
=> result
* total: 1,934
* correct: 1,684
* accuracy: 87.1%
* error: 12.9%
* macro_f1: 86.3%
Elapsed: 0:04:04
Run this job and save the output to output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  51
# train_x  816
# val      204
# test     1,934
---------  ------
['Apply_Eye_Makeup', 'Apply_Lipstick', 'Archery', 'Baby_Crawling', 'Balance_Beam', 'Band_Marching', 'Baseball_Pitch', 'Basketball', 'Basketball_Dunk', 'Bench_Press', 'Biking', 'Billiards', 'Blow_Dry_Hair', 'Blowing_Candles', 'Body_Weight_Squats', 'Bowling', 'Boxing_Punching_Bag', 'Boxing_Speed_Bag', 'Breast_Stroke', 'Brushing_Teeth', 'Clean_And_Jerk', 'Cliff_Diving', 'Cricket_Bowling', 'Cricket_Shot', 'Cutting_In_Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field_Hockey_Penalty', 'Floor_Gymnastics', 'Frisbee_Catch', 'Front_Crawl', 'Golf_Swing', 'Haircut', 'Hammering', 'Hammer_Throw', 'Handstand_Pushups', 'Handstand_Walking', 'Head_Massage', 'High_Jump', 'Horse_Race', 'Horse_Riding', 'Hula_Hoop', 'Ice_Dancing', 'Javelin_Throw', 'Juggling_Balls', 'Jumping_Jack', 'Jump_Rope', 'Kayaking', 'Knitting', 'Long_Jump']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Apply Eye Makeup.', 'a photo of a person doing Apply Lipstick.', 'a photo of a person doing Archery.', 'a photo of a person doing Baby Crawling.', 'a photo of a person doing Balance Beam.', 'a photo of a person doing Band Marching.', 'a photo of a person doing Baseball Pitch.', 'a photo of a person doing Basketball.', 'a photo of a person doing Basketball Dunk.', 'a photo of a person doing Bench Press.', 'a photo of a person doing Biking.', 'a photo of a person doing Billiards.', 'a photo of a person doing Blow Dry Hair.', 'a photo of a person doing Blowing Candles.', 'a photo of a person doing Body Weight Squats.', 'a photo of a person doing Bowling.', 'a photo of a person doing Boxing Punching Bag.', 'a photo of a person doing Boxing Speed Bag.', 'a photo of a person doing Breast Stroke.', 'a photo of a person doing Brushing Teeth.', 'a photo of a person doing Clean And Jerk.', 'a photo of a person doing Cliff Diving.', 'a photo of a person doing Cricket Bowling.', 'a photo of a person doing Cricket Shot.', 'a photo of a person doing Cutting In Kitchen.', 'a photo of a person doing Diving.', 'a photo of a person doing Drumming.', 'a photo of a person doing Fencing.', 'a photo of a person doing Field Hockey Penalty.', 'a photo of a person doing Floor Gymnastics.', 'a photo of a person doing Frisbee Catch.', 'a photo of a person doing Front Crawl.', 'a photo of a person doing Golf Swing.', 'a photo of a person doing Haircut.', 'a photo of a person doing Hammering.', 'a photo of a person doing Hammer Throw.', 'a photo of a person doing Handstand Pushups.', 'a photo of a person doing Handstand Walking.', 'a photo of a person doing Head Massage.', 'a photo of a person doing High Jump.', 'a photo of a person doing Horse Race.', 'a photo of a person doing Horse Riding.', 'a photo of a person doing Hula Hoop.', 'a photo of a person doing Ice Dancing.', 'a photo of a person doing Javelin Throw.', 'a photo of a person doing Juggling Balls.', 'a photo of a person doing Jumping Jack.', 'a photo of a person doing Jump Rope.', 'a photo of a person doing Kayaking.', 'a photo of a person doing Knitting.', 'a photo of a person doing Long Jump.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/26] time 0.154 (0.314) data 0.000 (0.151) loss 5.7031 (5.5531) acc 53.1250 (54.3750) lr 1.0000e-05 eta 0:06:47
epoch [1/50] batch [10/26] time 0.153 (0.234) data 0.000 (0.076) loss 5.4453 (5.4016) acc 43.7500 (55.3125) lr 1.0000e-05 eta 0:05:01
epoch [1/50] batch [15/26] time 0.152 (0.207) data 0.000 (0.051) loss 5.3164 (5.4544) acc 59.3750 (55.0000) lr 1.0000e-05 eta 0:04:25
epoch [1/50] batch [20/26] time 0.152 (0.193) data 0.000 (0.038) loss 5.5039 (5.4057) acc 59.3750 (56.4062) lr 1.0000e-05 eta 0:04:07
epoch [1/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.030) loss 5.2969 (5.3537) acc 71.8750 (58.2500) lr 1.0000e-05 eta 0:03:55
epoch [2/50] batch [5/26] time 0.153 (0.322) data 0.000 (0.169) loss 3.8281 (4.5641) acc 65.6250 (60.6250) lr 2.0000e-03 eta 0:06:48
epoch [2/50] batch [10/26] time 0.153 (0.237) data 0.000 (0.084) loss 3.7695 (4.2283) acc 65.6250 (61.5625) lr 2.0000e-03 eta 0:05:00
epoch [2/50] batch [15/26] time 0.153 (0.209) data 0.000 (0.056) loss 3.1621 (4.0159) acc 78.1250 (64.3750) lr 2.0000e-03 eta 0:04:23
epoch [2/50] batch [20/26] time 0.153 (0.195) data 0.000 (0.042) loss 3.5371 (3.8775) acc 50.0000 (63.7500) lr 2.0000e-03 eta 0:04:04
epoch [2/50] batch [25/26] time 0.153 (0.187) data 0.000 (0.034) loss 2.8184 (3.7879) acc 78.1250 (64.2500) lr 2.0000e-03 eta 0:03:53
epoch [3/50] batch [5/26] time 0.153 (0.289) data 0.000 (0.134) loss 3.5312 (3.3328) acc 65.6250 (66.2500) lr 1.9980e-03 eta 0:05:59
epoch [3/50] batch [10/26] time 0.154 (0.221) data 0.000 (0.067) loss 2.7598 (3.1516) acc 75.0000 (65.9375) lr 1.9980e-03 eta 0:04:33
epoch [3/50] batch [15/26] time 0.155 (0.199) data 0.000 (0.045) loss 3.0645 (3.0551) acc 62.5000 (68.5417) lr 1.9980e-03 eta 0:04:05
epoch [3/50] batch [20/26] time 0.155 (0.188) data 0.000 (0.034) loss 3.4102 (3.0435) acc 50.0000 (67.8125) lr 1.9980e-03 eta 0:03:50
epoch [3/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 3.5684 (3.0243) acc 65.6250 (68.6250) lr 1.9980e-03 eta 0:03:41
epoch [4/50] batch [5/26] time 0.154 (0.315) data 0.000 (0.161) loss 2.3262 (2.7391) acc 71.8750 (68.7500) lr 1.9921e-03 eta 0:06:23
epoch [4/50] batch [10/26] time 0.154 (0.234) data 0.000 (0.081) loss 2.6289 (2.6869) acc 75.0000 (71.2500) lr 1.9921e-03 eta 0:04:43
epoch [4/50] batch [15/26] time 0.153 (0.207) data 0.000 (0.054) loss 2.7129 (2.7915) acc 71.8750 (69.5833) lr 1.9921e-03 eta 0:04:10
epoch [4/50] batch [20/26] time 0.153 (0.194) data 0.000 (0.040) loss 2.9492 (2.7904) acc 78.1250 (69.8438) lr 1.9921e-03 eta 0:03:52
epoch [4/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.032) loss 2.6348 (2.7966) acc 75.0000 (69.6250) lr 1.9921e-03 eta 0:03:42
epoch [5/50] batch [5/26] time 0.155 (0.319) data 0.000 (0.163) loss 2.9277 (3.0031) acc 75.0000 (70.0000) lr 1.9823e-03 eta 0:06:19
epoch [5/50] batch [10/26] time 0.153 (0.236) data 0.000 (0.082) loss 3.0312 (2.8436) acc 62.5000 (70.9375) lr 1.9823e-03 eta 0:04:40
epoch [5/50] batch [15/26] time 0.153 (0.208) data 0.000 (0.054) loss 3.0547 (2.7816) acc 62.5000 (71.2500) lr 1.9823e-03 eta 0:04:06
epoch [5/50] batch [20/26] time 0.153 (0.195) data 0.000 (0.041) loss 2.6094 (2.7511) acc 68.7500 (71.0938) lr 1.9823e-03 eta 0:03:48
epoch [5/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.033) loss 2.7930 (2.7017) acc 71.8750 (72.8750) lr 1.9823e-03 eta 0:03:38
epoch [6/50] batch [5/26] time 0.152 (0.298) data 0.000 (0.145) loss 2.9922 (2.4316) acc 71.8750 (76.8750) lr 1.9686e-03 eta 0:05:46
epoch [6/50] batch [10/26] time 0.152 (0.226) data 0.000 (0.073) loss 2.6055 (2.5244) acc 81.2500 (77.1875) lr 1.9686e-03 eta 0:04:22
epoch [6/50] batch [15/26] time 0.156 (0.202) data 0.000 (0.048) loss 3.1367 (2.5997) acc 59.3750 (73.7500) lr 1.9686e-03 eta 0:03:53
epoch [6/50] batch [20/26] time 0.156 (0.191) data 0.001 (0.036) loss 2.7930 (2.5771) acc 59.3750 (73.9062) lr 1.9686e-03 eta 0:03:39
epoch [6/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 2.5352 (2.5694) acc 68.7500 (73.7500) lr 1.9686e-03 eta 0:03:29
epoch [7/50] batch [5/26] time 0.153 (0.293) data 0.000 (0.139) loss 3.7500 (2.4488) acc 50.0000 (73.1250) lr 1.9511e-03 eta 0:05:33
epoch [7/50] batch [10/26] time 0.152 (0.223) data 0.000 (0.069) loss 2.6523 (2.4346) acc 65.6250 (74.3750) lr 1.9511e-03 eta 0:04:12
epoch [7/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 2.0527 (2.4258) acc 81.2500 (75.4167) lr 1.9511e-03 eta 0:03:45
epoch [7/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.035) loss 2.5547 (2.4403) acc 71.8750 (75.1562) lr 1.9511e-03 eta 0:03:31
epoch [7/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.028) loss 2.6230 (2.4710) acc 68.7500 (74.8750) lr 1.9511e-03 eta 0:03:22
epoch [8/50] batch [5/26] time 0.154 (0.333) data 0.000 (0.178) loss 3.0703 (2.6344) acc 75.0000 (73.1250) lr 1.9298e-03 eta 0:06:10
epoch [8/50] batch [10/26] time 0.155 (0.244) data 0.000 (0.089) loss 2.1855 (2.4383) acc 78.1250 (76.5625) lr 1.9298e-03 eta 0:04:30
epoch [8/50] batch [15/26] time 0.153 (0.214) data 0.000 (0.060) loss 3.1211 (2.4836) acc 62.5000 (75.4167) lr 1.9298e-03 eta 0:03:55
epoch [8/50] batch [20/26] time 0.153 (0.199) data 0.000 (0.045) loss 1.9844 (2.4426) acc 71.8750 (74.6875) lr 1.9298e-03 eta 0:03:38
epoch [8/50] batch [25/26] time 0.153 (0.190) data 0.000 (0.036) loss 2.9141 (2.3884) acc 65.6250 (75.2500) lr 1.9298e-03 eta 0:03:27
epoch [9/50] batch [5/26] time 0.155 (0.300) data 0.000 (0.146) loss 2.8711 (2.3758) acc 68.7500 (79.3750) lr 1.9048e-03 eta 0:05:26
epoch [9/50] batch [10/26] time 0.154 (0.227) data 0.000 (0.073) loss 2.4434 (2.2918) acc 78.1250 (80.0000) lr 1.9048e-03 eta 0:04:05
epoch [9/50] batch [15/26] time 0.154 (0.203) data 0.000 (0.049) loss 1.9785 (2.3690) acc 84.3750 (77.2917) lr 1.9048e-03 eta 0:03:38
epoch [9/50] batch [20/26] time 0.153 (0.191) data 0.000 (0.037) loss 2.5469 (2.3732) acc 68.7500 (76.5625) lr 1.9048e-03 eta 0:03:24
epoch [9/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.1504 (2.3433) acc 84.3750 (77.2500) lr 1.9048e-03 eta 0:03:15
epoch [10/50] batch [5/26] time 0.156 (0.295) data 0.000 (0.140) loss 2.4492 (2.2170) acc 71.8750 (76.8750) lr 1.8763e-03 eta 0:05:13
epoch [10/50] batch [10/26] time 0.155 (0.226) data 0.000 (0.070) loss 2.8125 (2.3575) acc 75.0000 (77.5000) lr 1.8763e-03 eta 0:03:58
epoch [10/50] batch [15/26] time 0.154 (0.202) data 0.000 (0.047) loss 2.2461 (2.3071) acc 75.0000 (77.0833) lr 1.8763e-03 eta 0:03:32
epoch [10/50] batch [20/26] time 0.155 (0.190) data 0.000 (0.035) loss 2.5859 (2.2843) acc 71.8750 (78.5938) lr 1.8763e-03 eta 0:03:18
epoch [10/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.028) loss 1.9297 (2.2832) acc 84.3750 (79.2500) lr 1.8763e-03 eta 0:03:10
epoch [11/50] batch [5/26] time 0.153 (0.293) data 0.000 (0.138) loss 2.2227 (2.0521) acc 84.3750 (85.0000) lr 1.8443e-03 eta 0:05:02
epoch [11/50] batch [10/26] time 0.155 (0.223) data 0.000 (0.069) loss 2.0273 (2.0403) acc 81.2500 (83.4375) lr 1.8443e-03 eta 0:03:50
epoch [11/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 2.6055 (2.1883) acc 78.1250 (80.8333) lr 1.8443e-03 eta 0:03:25
epoch [11/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.035) loss 2.2324 (2.1860) acc 87.5000 (81.2500) lr 1.8443e-03 eta 0:03:12
epoch [11/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.028) loss 1.9922 (2.2013) acc 81.2500 (81.1250) lr 1.8443e-03 eta 0:03:04
epoch [12/50] batch [5/26] time 0.154 (0.316) data 0.000 (0.160) loss 2.2188 (2.2297) acc 87.5000 (81.2500) lr 1.8090e-03 eta 0:05:18
epoch [12/50] batch [10/26] time 0.155 (0.236) data 0.000 (0.080) loss 1.8418 (2.1192) acc 87.5000 (82.1875) lr 1.8090e-03 eta 0:03:56
epoch [12/50] batch [15/26] time 0.155 (0.209) data 0.000 (0.054) loss 2.6445 (2.1502) acc 78.1250 (81.0417) lr 1.8090e-03 eta 0:03:28
epoch [12/50] batch [20/26] time 0.153 (0.195) data 0.000 (0.040) loss 2.3066 (2.1837) acc 81.2500 (80.1562) lr 1.8090e-03 eta 0:03:14
epoch [12/50] batch [25/26] time 0.153 (0.187) data 0.000 (0.032) loss 2.7637 (2.2126) acc 68.7500 (79.5000) lr 1.8090e-03 eta 0:03:04
epoch [13/50] batch [5/26] time 0.153 (0.307) data 0.000 (0.153) loss 2.3164 (2.3281) acc 75.0000 (76.8750) lr 1.7705e-03 eta 0:05:01
epoch [13/50] batch [10/26] time 0.153 (0.230) data 0.000 (0.076) loss 2.2363 (2.2317) acc 78.1250 (78.7500) lr 1.7705e-03 eta 0:03:44
epoch [13/50] batch [15/26] time 0.155 (0.204) data 0.000 (0.051) loss 2.2051 (2.1898) acc 78.1250 (79.1667) lr 1.7705e-03 eta 0:03:18
epoch [13/50] batch [20/26] time 0.156 (0.192) data 0.000 (0.038) loss 2.3223 (2.2492) acc 78.1250 (77.8125) lr 1.7705e-03 eta 0:03:06
epoch [13/50] batch [25/26] time 0.155 (0.185) data 0.000 (0.031) loss 2.1152 (2.2805) acc 90.6250 (77.6250) lr 1.7705e-03 eta 0:02:58
epoch [14/50] batch [5/26] time 0.154 (0.312) data 0.000 (0.156) loss 2.0859 (2.0166) acc 81.2500 (85.0000) lr 1.7290e-03 eta 0:04:58
epoch [14/50] batch [10/26] time 0.154 (0.233) data 0.000 (0.078) loss 1.8672 (1.9874) acc 84.3750 (83.4375) lr 1.7290e-03 eta 0:03:42
epoch [14/50] batch [15/26] time 0.154 (0.207) data 0.000 (0.052) loss 2.6934 (2.0443) acc 71.8750 (82.5000) lr 1.7290e-03 eta 0:03:15
epoch [14/50] batch [20/26] time 0.153 (0.193) data 0.000 (0.039) loss 2.3125 (2.0738) acc 71.8750 (81.0938) lr 1.7290e-03 eta 0:03:02
epoch [14/50] batch [25/26] time 0.153 (0.186) data 0.000 (0.031) loss 2.1582 (2.0841) acc 81.2500 (81.5000) lr 1.7290e-03 eta 0:02:53
epoch [15/50] batch [5/26] time 0.155 (0.319) data 0.000 (0.163) loss 2.8477 (2.3482) acc 71.8750 (80.0000) lr 1.6845e-03 eta 0:04:56
epoch [15/50] batch [10/26] time 0.154 (0.237) data 0.000 (0.082) loss 1.7793 (2.1436) acc 90.6250 (83.1250) lr 1.6845e-03 eta 0:03:39
epoch [15/50] batch [15/26] time 0.155 (0.209) data 0.000 (0.055) loss 1.8818 (2.1071) acc 81.2500 (82.7083) lr 1.6845e-03 eta 0:03:12
epoch [15/50] batch [20/26] time 0.153 (0.196) data 0.000 (0.041) loss 1.8877 (2.1021) acc 84.3750 (82.0312) lr 1.6845e-03 eta 0:02:59
epoch [15/50] batch [25/26] time 0.154 (0.187) data 0.000 (0.033) loss 1.9023 (2.0764) acc 87.5000 (82.6250) lr 1.6845e-03 eta 0:02:50
epoch [16/50] batch [5/26] time 0.154 (0.297) data 0.000 (0.142) loss 2.0801 (1.8531) acc 84.3750 (87.5000) lr 1.6374e-03 eta 0:04:29
epoch [16/50] batch [10/26] time 0.154 (0.226) data 0.000 (0.071) loss 2.5938 (2.1541) acc 71.8750 (81.2500) lr 1.6374e-03 eta 0:03:23
epoch [16/50] batch [15/26] time 0.155 (0.202) data 0.000 (0.048) loss 2.1328 (2.1426) acc 84.3750 (81.4583) lr 1.6374e-03 eta 0:03:00
epoch [16/50] batch [20/26] time 0.155 (0.190) data 0.000 (0.036) loss 2.2773 (2.1835) acc 68.7500 (79.5312) lr 1.6374e-03 eta 0:02:49
epoch [16/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 2.4609 (2.1732) acc 68.7500 (79.6250) lr 1.6374e-03 eta 0:02:42
epoch [17/50] batch [5/26] time 0.155 (0.302) data 0.000 (0.147) loss 2.3711 (2.1203) acc 81.2500 (81.2500) lr 1.5878e-03 eta 0:04:25
epoch [17/50] batch [10/26] time 0.155 (0.228) data 0.000 (0.074) loss 1.9014 (2.0927) acc 84.3750 (80.6250) lr 1.5878e-03 eta 0:03:19
epoch [17/50] batch [15/26] time 0.153 (0.203) data 0.000 (0.049) loss 2.0801 (1.9768) acc 78.1250 (82.5000) lr 1.5878e-03 eta 0:02:56
epoch [17/50] batch [20/26] time 0.156 (0.191) data 0.000 (0.037) loss 2.6484 (2.1066) acc 65.6250 (80.7812) lr 1.5878e-03 eta 0:02:44
epoch [17/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.030) loss 2.0293 (2.0521) acc 84.3750 (81.8750) lr 1.5878e-03 eta 0:02:37
epoch [18/50] batch [5/26] time 0.155 (0.339) data 0.000 (0.184) loss 2.0547 (1.9387) acc 81.2500 (88.1250) lr 1.5358e-03 eta 0:04:49
epoch [18/50] batch [10/26] time 0.154 (0.247) data 0.000 (0.092) loss 2.4453 (2.1277) acc 68.7500 (81.5625) lr 1.5358e-03 eta 0:03:29
epoch [18/50] batch [15/26] time 0.154 (0.216) data 0.000 (0.062) loss 1.6758 (2.0451) acc 87.5000 (83.5417) lr 1.5358e-03 eta 0:03:01
epoch [18/50] batch [20/26] time 0.154 (0.200) data 0.000 (0.046) loss 2.2734 (2.0927) acc 81.2500 (82.9688) lr 1.5358e-03 eta 0:02:47
epoch [18/50] batch [25/26] time 0.154 (0.191) data 0.000 (0.037) loss 1.8945 (2.0799) acc 90.6250 (83.3750) lr 1.5358e-03 eta 0:02:38
epoch [19/50] batch [5/26] time 0.154 (0.302) data 0.000 (0.147) loss 1.7139 (1.8518) acc 87.5000 (86.2500) lr 1.4818e-03 eta 0:04:10
epoch [19/50] batch [10/26] time 0.154 (0.229) data 0.000 (0.074) loss 2.1562 (2.0754) acc 84.3750 (81.8750) lr 1.4818e-03 eta 0:03:07
epoch [19/50] batch [15/26] time 0.155 (0.204) data 0.000 (0.049) loss 1.7178 (2.0548) acc 84.3750 (82.9167) lr 1.4818e-03 eta 0:02:46
epoch [19/50] batch [20/26] time 0.154 (0.192) data 0.000 (0.037) loss 2.8477 (2.0849) acc 65.6250 (82.9688) lr 1.4818e-03 eta 0:02:35
epoch [19/50] batch [25/26] time 0.155 (0.184) data 0.000 (0.030) loss 2.0039 (2.0779) acc 93.7500 (83.1250) lr 1.4818e-03 eta 0:02:28
epoch [20/50] batch [5/26] time 0.152 (0.312) data 0.000 (0.157) loss 2.0820 (1.7182) acc 78.1250 (89.3750) lr 1.4258e-03 eta 0:04:10
epoch [20/50] batch [10/26] time 0.155 (0.233) data 0.000 (0.079) loss 1.8438 (1.7701) acc 93.7500 (88.1250) lr 1.4258e-03 eta 0:03:05
epoch [20/50] batch [15/26] time 0.153 (0.206) data 0.000 (0.053) loss 2.1074 (1.9285) acc 81.2500 (86.0417) lr 1.4258e-03 eta 0:02:43
epoch [20/50] batch [20/26] time 0.154 (0.193) data 0.000 (0.040) loss 1.4648 (1.9874) acc 90.6250 (85.1562) lr 1.4258e-03 eta 0:02:31
epoch [20/50] batch [25/26] time 0.155 (0.185) data 0.000 (0.032) loss 1.9277 (2.0270) acc 87.5000 (85.0000) lr 1.4258e-03 eta 0:02:24
epoch [21/50] batch [5/26] time 0.155 (0.297) data 0.000 (0.143) loss 1.9893 (1.9740) acc 84.3750 (84.3750) lr 1.3681e-03 eta 0:03:50
epoch [21/50] batch [10/26] time 0.155 (0.226) data 0.000 (0.072) loss 1.7871 (2.0845) acc 90.6250 (83.7500) lr 1.3681e-03 eta 0:02:54
epoch [21/50] batch [15/26] time 0.153 (0.202) data 0.000 (0.048) loss 1.9668 (2.0630) acc 78.1250 (83.9583) lr 1.3681e-03 eta 0:02:34
epoch [21/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.036) loss 1.6338 (2.0239) acc 93.7500 (84.2188) lr 1.3681e-03 eta 0:02:24
epoch [21/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.1797 (2.0921) acc 87.5000 (82.5000) lr 1.3681e-03 eta 0:02:17
epoch [22/50] batch [5/26] time 0.155 (0.297) data 0.000 (0.142) loss 1.8008 (2.1641) acc 90.6250 (87.5000) lr 1.3090e-03 eta 0:03:42
epoch [22/50] batch [10/26] time 0.156 (0.226) data 0.001 (0.071) loss 2.3320 (2.2467) acc 78.1250 (83.4375) lr 1.3090e-03 eta 0:02:47
epoch [22/50] batch [15/26] time 0.155 (0.202) data 0.000 (0.047) loss 2.3945 (2.1949) acc 78.1250 (83.1250) lr 1.3090e-03 eta 0:02:29
epoch [22/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.036) loss 1.7793 (2.1813) acc 93.7500 (83.4375) lr 1.3090e-03 eta 0:02:19
epoch [22/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 1.6172 (2.1364) acc 87.5000 (83.0000) lr 1.3090e-03 eta 0:02:13
epoch [23/50] batch [5/26] time 0.154 (0.290) data 0.000 (0.135) loss 1.7021 (1.7871) acc 87.5000 (86.8750) lr 1.2487e-03 eta 0:03:29
epoch [23/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.068) loss 1.7344 (1.8349) acc 90.6250 (87.1875) lr 1.2487e-03 eta 0:02:39
epoch [23/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.045) loss 1.9844 (1.8947) acc 81.2500 (86.8750) lr 1.2487e-03 eta 0:02:22
epoch [23/50] batch [20/26] time 0.155 (0.188) data 0.001 (0.034) loss 1.6621 (1.9159) acc 93.7500 (87.0312) lr 1.2487e-03 eta 0:02:13
epoch [23/50] batch [25/26] time 0.155 (0.181) data 0.000 (0.027) loss 2.1816 (1.9200) acc 78.1250 (87.0000) lr 1.2487e-03 eta 0:02:07
epoch [24/50] batch [5/26] time 0.159 (0.307) data 0.000 (0.148) loss 2.2324 (1.9789) acc 81.2500 (83.1250) lr 1.1874e-03 eta 0:03:34
epoch [24/50] batch [10/26] time 0.156 (0.233) data 0.000 (0.074) loss 2.5664 (2.0414) acc 68.7500 (82.1875) lr 1.1874e-03 eta 0:02:41
epoch [24/50] batch [15/26] time 0.153 (0.206) data 0.000 (0.050) loss 2.0703 (2.0105) acc 81.2500 (82.2917) lr 1.1874e-03 eta 0:02:21
epoch [24/50] batch [20/26] time 0.154 (0.193) data 0.000 (0.037) loss 2.2227 (2.0407) acc 78.1250 (82.6562) lr 1.1874e-03 eta 0:02:11
epoch [24/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.030) loss 1.9688 (2.0345) acc 78.1250 (82.8750) lr 1.1874e-03 eta 0:02:05
epoch [25/50] batch [5/26] time 0.154 (0.296) data 0.000 (0.138) loss 2.3281 (1.9559) acc 71.8750 (85.0000) lr 1.1253e-03 eta 0:03:18
epoch [25/50] batch [10/26] time 0.156 (0.226) data 0.000 (0.069) loss 1.5781 (1.8800) acc 93.7500 (86.5625) lr 1.1253e-03 eta 0:02:30
epoch [25/50] batch [15/26] time 0.154 (0.202) data 0.000 (0.046) loss 1.7686 (1.8764) acc 90.6250 (86.8750) lr 1.1253e-03 eta 0:02:13
epoch [25/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.035) loss 1.6641 (1.8428) acc 90.6250 (87.8125) lr 1.1253e-03 eta 0:02:04
epoch [25/50] batch [25/26] time 0.155 (0.183) data 0.000 (0.028) loss 2.2500 (1.8852) acc 81.2500 (86.6250) lr 1.1253e-03 eta 0:01:59
epoch [26/50] batch [5/26] time 0.154 (0.328) data 0.000 (0.173) loss 2.3594 (1.9297) acc 81.2500 (85.0000) lr 1.0628e-03 eta 0:03:31
epoch [26/50] batch [10/26] time 0.154 (0.241) data 0.000 (0.087) loss 1.5498 (1.8866) acc 90.6250 (86.5625) lr 1.0628e-03 eta 0:02:34
epoch [26/50] batch [15/26] time 0.156 (0.212) data 0.000 (0.058) loss 2.7852 (2.0148) acc 78.1250 (85.2083) lr 1.0628e-03 eta 0:02:14
epoch [26/50] batch [20/26] time 0.156 (0.198) data 0.000 (0.043) loss 2.3418 (2.0415) acc 81.2500 (85.0000) lr 1.0628e-03 eta 0:02:04
epoch [26/50] batch [25/26] time 0.155 (0.190) data 0.000 (0.035) loss 2.0742 (2.0211) acc 84.3750 (85.1250) lr 1.0628e-03 eta 0:01:58
epoch [27/50] batch [5/26] time 0.153 (0.290) data 0.000 (0.134) loss 1.9941 (1.9893) acc 84.3750 (85.0000) lr 1.0000e-03 eta 0:02:59
epoch [27/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.067) loss 1.5547 (1.9056) acc 100.0000 (88.1250) lr 1.0000e-03 eta 0:02:16
epoch [27/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.045) loss 1.6846 (1.9348) acc 93.7500 (87.5000) lr 1.0000e-03 eta 0:02:01
epoch [27/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.034) loss 2.7891 (1.9814) acc 78.1250 (86.5625) lr 1.0000e-03 eta 0:01:53
epoch [27/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 1.7783 (1.9433) acc 87.5000 (87.1250) lr 1.0000e-03 eta 0:01:48
epoch [28/50] batch [5/26] time 0.156 (0.292) data 0.000 (0.136) loss 2.0723 (2.0844) acc 87.5000 (87.5000) lr 9.3721e-04 eta 0:02:52
epoch [28/50] batch [10/26] time 0.157 (0.224) data 0.000 (0.068) loss 2.2559 (2.0549) acc 84.3750 (86.5625) lr 9.3721e-04 eta 0:02:11
epoch [28/50] batch [15/26] time 0.154 (0.201) data 0.000 (0.046) loss 1.4854 (1.9382) acc 93.7500 (87.9167) lr 9.3721e-04 eta 0:01:57
epoch [28/50] batch [20/26] time 0.155 (0.189) data 0.001 (0.034) loss 1.7578 (1.9590) acc 87.5000 (87.5000) lr 9.3721e-04 eta 0:01:49
epoch [28/50] batch [25/26] time 0.153 (0.182) data 0.000 (0.027) loss 1.6152 (1.9495) acc 93.7500 (87.3750) lr 9.3721e-04 eta 0:01:44
epoch [29/50] batch [5/26] time 0.155 (0.297) data 0.000 (0.143) loss 2.5039 (2.0658) acc 81.2500 (85.0000) lr 8.7467e-04 eta 0:02:48
epoch [29/50] batch [10/26] time 0.153 (0.225) data 0.000 (0.072) loss 1.7744 (1.9057) acc 81.2500 (88.1250) lr 8.7467e-04 eta 0:02:06
epoch [29/50] batch [15/26] time 0.155 (0.201) data 0.000 (0.048) loss 1.6816 (1.8768) acc 90.6250 (89.1667) lr 8.7467e-04 eta 0:01:52
epoch [29/50] batch [20/26] time 0.156 (0.190) data 0.000 (0.036) loss 1.8965 (1.8513) acc 90.6250 (89.2188) lr 8.7467e-04 eta 0:01:44
epoch [29/50] batch [25/26] time 0.156 (0.183) data 0.000 (0.029) loss 1.8916 (1.8677) acc 84.3750 (89.0000) lr 8.7467e-04 eta 0:01:40
epoch [30/50] batch [5/26] time 0.156 (0.284) data 0.000 (0.125) loss 2.0566 (2.0086) acc 81.2500 (82.5000) lr 8.1262e-04 eta 0:02:33
epoch [30/50] batch [10/26] time 0.153 (0.219) data 0.000 (0.063) loss 2.1582 (1.8544) acc 75.0000 (85.9375) lr 8.1262e-04 eta 0:01:57
epoch [30/50] batch [15/26] time 0.152 (0.197) data 0.000 (0.042) loss 2.1777 (1.8878) acc 81.2500 (85.8333) lr 8.1262e-04 eta 0:01:44
epoch [30/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.031) loss 1.5996 (1.9137) acc 90.6250 (85.6250) lr 8.1262e-04 eta 0:01:37
epoch [30/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 2.3418 (1.9364) acc 87.5000 (85.8750) lr 8.1262e-04 eta 0:01:33
epoch [31/50] batch [5/26] time 0.157 (0.306) data 0.000 (0.148) loss 2.2656 (1.8672) acc 78.1250 (86.2500) lr 7.5131e-04 eta 0:02:37
epoch [31/50] batch [10/26] time 0.155 (0.230) data 0.000 (0.074) loss 1.7910 (1.8619) acc 84.3750 (86.5625) lr 7.5131e-04 eta 0:01:57
epoch [31/50] batch [15/26] time 0.155 (0.205) data 0.000 (0.050) loss 1.2578 (1.8186) acc 96.8750 (87.7083) lr 7.5131e-04 eta 0:01:43
epoch [31/50] batch [20/26] time 0.154 (0.192) data 0.000 (0.037) loss 1.6016 (1.8146) acc 90.6250 (88.2812) lr 7.5131e-04 eta 0:01:36
epoch [31/50] batch [25/26] time 0.153 (0.185) data 0.000 (0.030) loss 1.8467 (1.8596) acc 90.6250 (88.1250) lr 7.5131e-04 eta 0:01:31
epoch [32/50] batch [5/26] time 0.155 (0.284) data 0.001 (0.128) loss 1.8457 (2.0180) acc 87.5000 (85.6250) lr 6.9098e-04 eta 0:02:18
epoch [32/50] batch [10/26] time 0.154 (0.219) data 0.000 (0.064) loss 1.9912 (1.9497) acc 87.5000 (86.8750) lr 6.9098e-04 eta 0:01:46
epoch [32/50] batch [15/26] time 0.155 (0.198) data 0.000 (0.043) loss 1.6797 (1.8864) acc 93.7500 (87.9167) lr 6.9098e-04 eta 0:01:34
epoch [32/50] batch [20/26] time 0.155 (0.187) data 0.000 (0.032) loss 2.2734 (1.8948) acc 78.1250 (87.3438) lr 6.9098e-04 eta 0:01:28
epoch [32/50] batch [25/26] time 0.155 (0.181) data 0.000 (0.026) loss 1.7842 (1.8687) acc 84.3750 (87.3750) lr 6.9098e-04 eta 0:01:24
epoch [33/50] batch [5/26] time 0.155 (0.289) data 0.000 (0.132) loss 1.8545 (1.9178) acc 90.6250 (88.7500) lr 6.3188e-04 eta 0:02:13
epoch [33/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.066) loss 1.4648 (1.8969) acc 93.7500 (89.3750) lr 6.3188e-04 eta 0:01:41
epoch [33/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.044) loss 1.8066 (1.8962) acc 90.6250 (88.9583) lr 6.3188e-04 eta 0:01:30
epoch [33/50] batch [20/26] time 0.155 (0.188) data 0.000 (0.033) loss 2.1738 (1.9236) acc 87.5000 (88.5938) lr 6.3188e-04 eta 0:01:24
epoch [33/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 1.7754 (1.8864) acc 87.5000 (88.7500) lr 6.3188e-04 eta 0:01:20
epoch [34/50] batch [5/26] time 0.154 (0.298) data 0.000 (0.142) loss 1.7490 (1.8291) acc 90.6250 (88.1250) lr 5.7422e-04 eta 0:02:10
epoch [34/50] batch [10/26] time 0.156 (0.227) data 0.000 (0.071) loss 1.7305 (1.8316) acc 93.7500 (89.3750) lr 5.7422e-04 eta 0:01:37
epoch [34/50] batch [15/26] time 0.154 (0.203) data 0.000 (0.048) loss 1.7588 (1.8512) acc 90.6250 (88.9583) lr 5.7422e-04 eta 0:01:26
epoch [34/50] batch [20/26] time 0.153 (0.190) data 0.000 (0.036) loss 1.9316 (1.8366) acc 81.2500 (89.2188) lr 5.7422e-04 eta 0:01:20
epoch [34/50] batch [25/26] time 0.153 (0.183) data 0.000 (0.029) loss 1.6426 (1.7971) acc 90.6250 (90.0000) lr 5.7422e-04 eta 0:01:16
epoch [35/50] batch [5/26] time 0.155 (0.294) data 0.000 (0.139) loss 2.1387 (1.9777) acc 84.3750 (86.2500) lr 5.1825e-04 eta 0:02:00
epoch [35/50] batch [10/26] time 0.155 (0.225) data 0.000 (0.069) loss 1.4111 (1.9047) acc 96.8750 (87.8125) lr 5.1825e-04 eta 0:01:31
epoch [35/50] batch [15/26] time 0.155 (0.202) data 0.000 (0.046) loss 1.9541 (1.8589) acc 84.3750 (88.3333) lr 5.1825e-04 eta 0:01:20
epoch [35/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.035) loss 1.9658 (1.8419) acc 84.3750 (87.6562) lr 5.1825e-04 eta 0:01:15
epoch [35/50] batch [25/26] time 0.155 (0.183) data 0.001 (0.028) loss 1.9033 (1.8470) acc 90.6250 (87.6250) lr 5.1825e-04 eta 0:01:11
epoch [36/50] batch [5/26] time 0.154 (0.275) data 0.000 (0.119) loss 1.8867 (1.7555) acc 90.6250 (88.7500) lr 4.6417e-04 eta 0:01:45
epoch [36/50] batch [10/26] time 0.155 (0.215) data 0.000 (0.060) loss 2.2598 (1.9092) acc 87.5000 (88.1250) lr 4.6417e-04 eta 0:01:21
epoch [36/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.040) loss 1.7617 (1.9361) acc 87.5000 (87.7083) lr 4.6417e-04 eta 0:01:12
epoch [36/50] batch [20/26] time 0.154 (0.184) data 0.000 (0.030) loss 1.9258 (1.9099) acc 84.3750 (87.9688) lr 4.6417e-04 eta 0:01:08
epoch [36/50] batch [25/26] time 0.155 (0.178) data 0.000 (0.024) loss 1.6328 (1.8946) acc 90.6250 (88.2500) lr 4.6417e-04 eta 0:01:05
epoch [37/50] batch [5/26] time 0.155 (0.285) data 0.000 (0.129) loss 1.5410 (1.9080) acc 93.7500 (89.3750) lr 4.1221e-04 eta 0:01:42
epoch [37/50] batch [10/26] time 0.156 (0.220) data 0.000 (0.065) loss 1.8076 (1.9222) acc 90.6250 (87.5000) lr 4.1221e-04 eta 0:01:18
epoch [37/50] batch [15/26] time 0.155 (0.199) data 0.000 (0.043) loss 2.1953 (1.8878) acc 87.5000 (89.1667) lr 4.1221e-04 eta 0:01:09
epoch [37/50] batch [20/26] time 0.158 (0.188) data 0.001 (0.032) loss 1.6025 (1.8704) acc 93.7500 (88.9062) lr 4.1221e-04 eta 0:01:04
epoch [37/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.026) loss 1.8965 (1.8714) acc 87.5000 (89.1250) lr 4.1221e-04 eta 0:01:01
epoch [38/50] batch [5/26] time 0.154 (0.273) data 0.000 (0.118) loss 1.8486 (1.7447) acc 84.3750 (90.6250) lr 3.6258e-04 eta 0:01:30
epoch [38/50] batch [10/26] time 0.155 (0.214) data 0.000 (0.059) loss 2.0039 (1.8152) acc 81.2500 (89.6875) lr 3.6258e-04 eta 0:01:10
epoch [38/50] batch [15/26] time 0.156 (0.195) data 0.000 (0.040) loss 2.4238 (1.8579) acc 81.2500 (89.3750) lr 3.6258e-04 eta 0:01:02
epoch [38/50] batch [20/26] time 0.154 (0.185) data 0.001 (0.030) loss 1.8955 (1.8714) acc 84.3750 (89.2188) lr 3.6258e-04 eta 0:00:58
epoch [38/50] batch [25/26] time 0.159 (0.179) data 0.000 (0.024) loss 2.2207 (1.8634) acc 87.5000 (89.2500) lr 3.6258e-04 eta 0:00:55
epoch [39/50] batch [5/26] time 0.155 (0.286) data 0.000 (0.131) loss 2.0684 (2.0457) acc 90.6250 (85.0000) lr 3.1545e-04 eta 0:01:27
epoch [39/50] batch [10/26] time 0.155 (0.220) data 0.000 (0.066) loss 1.8496 (1.9086) acc 84.3750 (87.1875) lr 3.1545e-04 eta 0:01:06
epoch [39/50] batch [15/26] time 0.154 (0.198) data 0.000 (0.044) loss 1.6191 (1.8555) acc 93.7500 (88.9583) lr 3.1545e-04 eta 0:00:58
epoch [39/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.033) loss 1.5371 (1.8654) acc 90.6250 (88.9062) lr 3.1545e-04 eta 0:00:54
epoch [39/50] batch [25/26] time 0.156 (0.180) data 0.000 (0.026) loss 1.9160 (1.8629) acc 87.5000 (88.1250) lr 3.1545e-04 eta 0:00:51
epoch [40/50] batch [5/26] time 0.155 (0.288) data 0.000 (0.131) loss 1.6016 (1.7494) acc 90.6250 (89.3750) lr 2.7103e-04 eta 0:01:20
epoch [40/50] batch [10/26] time 0.155 (0.221) data 0.000 (0.066) loss 1.6426 (1.8072) acc 93.7500 (88.4375) lr 2.7103e-04 eta 0:01:01
epoch [40/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.044) loss 1.7734 (1.8337) acc 87.5000 (87.7083) lr 2.7103e-04 eta 0:00:53
epoch [40/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.033) loss 1.7090 (1.8427) acc 93.7500 (88.1250) lr 2.7103e-04 eta 0:00:49
epoch [40/50] batch [25/26] time 0.155 (0.181) data 0.000 (0.026) loss 1.5703 (1.8287) acc 90.6250 (88.5000) lr 2.7103e-04 eta 0:00:47
epoch [41/50] batch [5/26] time 0.155 (0.281) data 0.000 (0.126) loss 1.5459 (1.7309) acc 96.8750 (95.0000) lr 2.2949e-04 eta 0:01:11
epoch [41/50] batch [10/26] time 0.157 (0.218) data 0.000 (0.063) loss 2.0820 (1.6724) acc 78.1250 (93.4375) lr 2.2949e-04 eta 0:00:54
epoch [41/50] batch [15/26] time 0.156 (0.198) data 0.000 (0.042) loss 2.4766 (1.7592) acc 75.0000 (91.2500) lr 2.2949e-04 eta 0:00:48
epoch [41/50] batch [20/26] time 0.155 (0.187) data 0.000 (0.032) loss 2.2031 (1.7578) acc 84.3750 (90.7812) lr 2.2949e-04 eta 0:00:44
epoch [41/50] batch [25/26] time 0.156 (0.181) data 0.000 (0.026) loss 2.1641 (1.7923) acc 87.5000 (90.2500) lr 2.2949e-04 eta 0:00:42
epoch [42/50] batch [5/26] time 0.154 (0.287) data 0.000 (0.132) loss 1.8896 (1.6863) acc 90.6250 (93.1250) lr 1.9098e-04 eta 0:01:05
epoch [42/50] batch [10/26] time 0.153 (0.220) data 0.000 (0.066) loss 1.4512 (1.6873) acc 90.6250 (92.1875) lr 1.9098e-04 eta 0:00:49
epoch [42/50] batch [15/26] time 0.154 (0.198) data 0.000 (0.044) loss 1.8389 (1.7801) acc 90.6250 (90.8333) lr 1.9098e-04 eta 0:00:43
epoch [42/50] batch [20/26] time 0.155 (0.188) data 0.000 (0.033) loss 1.5176 (1.7710) acc 100.0000 (90.4688) lr 1.9098e-04 eta 0:00:40
epoch [42/50] batch [25/26] time 0.156 (0.181) data 0.000 (0.027) loss 1.8574 (1.7789) acc 93.7500 (90.0000) lr 1.9098e-04 eta 0:00:37
epoch [43/50] batch [5/26] time 0.154 (0.290) data 0.000 (0.135) loss 2.0273 (1.9438) acc 93.7500 (89.3750) lr 1.5567e-04 eta 0:00:58
epoch [43/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.067) loss 2.1973 (1.8848) acc 81.2500 (88.4375) lr 1.5567e-04 eta 0:00:44
epoch [43/50] batch [15/26] time 0.154 (0.200) data 0.000 (0.045) loss 1.3535 (1.8123) acc 96.8750 (89.7917) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.034) loss 2.0391 (1.8590) acc 78.1250 (89.0625) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 1.8896 (1.8342) acc 90.6250 (89.2500) lr 1.5567e-04 eta 0:00:33
epoch [44/50] batch [5/26] time 0.155 (0.291) data 0.000 (0.135) loss 1.7656 (1.8783) acc 93.7500 (90.0000) lr 1.2369e-04 eta 0:00:51
epoch [44/50] batch [10/26] time 0.154 (0.223) data 0.000 (0.068) loss 2.3027 (1.8745) acc 75.0000 (89.3750) lr 1.2369e-04 eta 0:00:38
epoch [44/50] batch [15/26] time 0.157 (0.201) data 0.000 (0.045) loss 2.2754 (1.9355) acc 84.3750 (88.7500) lr 1.2369e-04 eta 0:00:33
epoch [44/50] batch [20/26] time 0.156 (0.190) data 0.000 (0.034) loss 1.7285 (1.9107) acc 93.7500 (88.9062) lr 1.2369e-04 eta 0:00:30
epoch [44/50] batch [25/26] time 0.155 (0.183) data 0.000 (0.027) loss 1.5381 (1.8820) acc 90.6250 (89.3750) lr 1.2369e-04 eta 0:00:28
epoch [45/50] batch [5/26] time 0.155 (0.298) data 0.000 (0.142) loss 2.0391 (2.0900) acc 84.3750 (87.5000) lr 9.5173e-05 eta 0:00:45
epoch [45/50] batch [10/26] time 0.154 (0.227) data 0.000 (0.071) loss 1.7051 (1.8981) acc 93.7500 (88.4375) lr 9.5173e-05 eta 0:00:33
epoch [45/50] batch [15/26] time 0.155 (0.203) data 0.000 (0.048) loss 1.4902 (1.8051) acc 90.6250 (89.5833) lr 9.5173e-05 eta 0:00:28
epoch [45/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.036) loss 1.7422 (1.8246) acc 93.7500 (89.6875) lr 9.5173e-05 eta 0:00:25
epoch [45/50] batch [25/26] time 0.154 (0.183) data 0.000 (0.029) loss 2.4062 (1.8066) acc 81.2500 (89.8750) lr 9.5173e-05 eta 0:00:24
epoch [46/50] batch [5/26] time 0.155 (0.280) data 0.000 (0.123) loss 2.3223 (1.9734) acc 84.3750 (86.8750) lr 7.0224e-05 eta 0:00:34
epoch [46/50] batch [10/26] time 0.155 (0.217) data 0.000 (0.062) loss 1.7129 (1.8646) acc 90.6250 (88.4375) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [15/26] time 0.153 (0.196) data 0.000 (0.041) loss 2.2500 (1.8821) acc 78.1250 (88.1250) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.153 (0.186) data 0.000 (0.031) loss 1.6133 (1.8467) acc 93.7500 (89.3750) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.154 (0.179) data 0.000 (0.025) loss 1.7461 (1.8028) acc 90.6250 (89.8750) lr 7.0224e-05 eta 0:00:18
epoch [47/50] batch [5/26] time 0.155 (0.294) data 0.000 (0.138) loss 1.7188 (1.6611) acc 90.6250 (92.5000) lr 4.8943e-05 eta 0:00:29
epoch [47/50] batch [10/26] time 0.153 (0.224) data 0.000 (0.069) loss 1.8945 (1.7613) acc 90.6250 (90.9375) lr 4.8943e-05 eta 0:00:21
epoch [47/50] batch [15/26] time 0.156 (0.201) data 0.000 (0.046) loss 1.4307 (1.7146) acc 93.7500 (92.5000) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [20/26] time 0.153 (0.189) data 0.000 (0.035) loss 2.0840 (1.7462) acc 75.0000 (91.2500) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.6006 (1.7275) acc 96.8750 (91.6250) lr 4.8943e-05 eta 0:00:14
epoch [48/50] batch [5/26] time 0.154 (0.279) data 0.000 (0.123) loss 1.4951 (1.8279) acc 96.8750 (91.2500) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [10/26] time 0.154 (0.217) data 0.000 (0.061) loss 2.2363 (1.8565) acc 87.5000 (91.5625) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [15/26] time 0.153 (0.196) data 0.000 (0.041) loss 1.6465 (1.8700) acc 90.6250 (90.0000) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.9619 (1.9103) acc 81.2500 (89.0625) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.025) loss 1.9053 (1.8912) acc 90.6250 (89.2500) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.155 (0.288) data 0.000 (0.133) loss 2.1250 (1.8742) acc 87.5000 (88.1250) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [10/26] time 0.155 (0.222) data 0.000 (0.066) loss 1.8984 (1.8136) acc 87.5000 (88.7500) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [15/26] time 0.154 (0.199) data 0.000 (0.044) loss 1.6035 (1.8846) acc 90.6250 (88.3333) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.154 (0.188) data 0.000 (0.033) loss 1.4766 (1.8575) acc 90.6250 (88.9062) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [25/26] time 0.154 (0.181) data 0.000 (0.027) loss 2.1406 (1.8484) acc 87.5000 (89.2500) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.157 (0.289) data 0.000 (0.131) loss 1.8203 (1.7189) acc 87.5000 (92.5000) lr 7.8853e-06 eta 0:00:06
epoch [50/50] batch [10/26] time 0.157 (0.223) data 0.000 (0.066) loss 1.2803 (1.6646) acc 96.8750 (93.7500) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.155 (0.201) data 0.000 (0.044) loss 1.9785 (1.7831) acc 90.6250 (92.2917) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.156 (0.189) data 0.000 (0.033) loss 1.7021 (1.7727) acc 93.7500 (92.1875) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.156 (0.183) data 0.000 (0.026) loss 1.6133 (1.7435) acc 96.8750 (92.6250) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:06<00:18,  6.05s/it] 50%|█████     | 2/4 [00:07<00:06,  3.16s/it] 75%|███████▌  | 3/4 [00:08<00:02,  2.23s/it]100%|██████████| 4/4 [00:09<00:00,  1.77s/it]100%|██████████| 4/4 [00:09<00:00,  2.38s/it]
=> result
* total: 1,934
* correct: 1,701
* accuracy: 88.0%
* error: 12.0%
* macro_f1: 87.3%
Elapsed: 0:04:09
Run this job and save the output to output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  51
# train_x  816
# val      204
# test     1,934
---------  ------
['Apply_Eye_Makeup', 'Apply_Lipstick', 'Archery', 'Baby_Crawling', 'Balance_Beam', 'Band_Marching', 'Baseball_Pitch', 'Basketball', 'Basketball_Dunk', 'Bench_Press', 'Biking', 'Billiards', 'Blow_Dry_Hair', 'Blowing_Candles', 'Body_Weight_Squats', 'Bowling', 'Boxing_Punching_Bag', 'Boxing_Speed_Bag', 'Breast_Stroke', 'Brushing_Teeth', 'Clean_And_Jerk', 'Cliff_Diving', 'Cricket_Bowling', 'Cricket_Shot', 'Cutting_In_Kitchen', 'Diving', 'Drumming', 'Fencing', 'Field_Hockey_Penalty', 'Floor_Gymnastics', 'Frisbee_Catch', 'Front_Crawl', 'Golf_Swing', 'Haircut', 'Hammering', 'Hammer_Throw', 'Handstand_Pushups', 'Handstand_Walking', 'Head_Massage', 'High_Jump', 'Horse_Race', 'Horse_Riding', 'Hula_Hoop', 'Ice_Dancing', 'Javelin_Throw', 'Juggling_Balls', 'Jumping_Jack', 'Jump_Rope', 'Kayaking', 'Knitting', 'Long_Jump']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Apply Eye Makeup.', 'a photo of a person doing Apply Lipstick.', 'a photo of a person doing Archery.', 'a photo of a person doing Baby Crawling.', 'a photo of a person doing Balance Beam.', 'a photo of a person doing Band Marching.', 'a photo of a person doing Baseball Pitch.', 'a photo of a person doing Basketball.', 'a photo of a person doing Basketball Dunk.', 'a photo of a person doing Bench Press.', 'a photo of a person doing Biking.', 'a photo of a person doing Billiards.', 'a photo of a person doing Blow Dry Hair.', 'a photo of a person doing Blowing Candles.', 'a photo of a person doing Body Weight Squats.', 'a photo of a person doing Bowling.', 'a photo of a person doing Boxing Punching Bag.', 'a photo of a person doing Boxing Speed Bag.', 'a photo of a person doing Breast Stroke.', 'a photo of a person doing Brushing Teeth.', 'a photo of a person doing Clean And Jerk.', 'a photo of a person doing Cliff Diving.', 'a photo of a person doing Cricket Bowling.', 'a photo of a person doing Cricket Shot.', 'a photo of a person doing Cutting In Kitchen.', 'a photo of a person doing Diving.', 'a photo of a person doing Drumming.', 'a photo of a person doing Fencing.', 'a photo of a person doing Field Hockey Penalty.', 'a photo of a person doing Floor Gymnastics.', 'a photo of a person doing Frisbee Catch.', 'a photo of a person doing Front Crawl.', 'a photo of a person doing Golf Swing.', 'a photo of a person doing Haircut.', 'a photo of a person doing Hammering.', 'a photo of a person doing Hammer Throw.', 'a photo of a person doing Handstand Pushups.', 'a photo of a person doing Handstand Walking.', 'a photo of a person doing Head Massage.', 'a photo of a person doing High Jump.', 'a photo of a person doing Horse Race.', 'a photo of a person doing Horse Riding.', 'a photo of a person doing Hula Hoop.', 'a photo of a person doing Ice Dancing.', 'a photo of a person doing Javelin Throw.', 'a photo of a person doing Juggling Balls.', 'a photo of a person doing Jumping Jack.', 'a photo of a person doing Jump Rope.', 'a photo of a person doing Kayaking.', 'a photo of a person doing Knitting.', 'a photo of a person doing Long Jump.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([51, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/26] time 0.153 (0.293) data 0.000 (0.127) loss 5.2031 (5.2141) acc 59.3750 (64.3750) lr 1.0000e-05 eta 0:06:19
epoch [1/50] batch [10/26] time 0.152 (0.223) data 0.000 (0.064) loss 5.4258 (5.2219) acc 53.1250 (61.8750) lr 1.0000e-05 eta 0:04:47
epoch [1/50] batch [15/26] time 0.152 (0.199) data 0.000 (0.042) loss 4.6055 (5.1602) acc 75.0000 (62.9167) lr 1.0000e-05 eta 0:04:16
epoch [1/50] batch [20/26] time 0.152 (0.188) data 0.000 (0.032) loss 5.7500 (5.2432) acc 46.8750 (60.9375) lr 1.0000e-05 eta 0:04:00
epoch [1/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.026) loss 5.6016 (5.2200) acc 53.1250 (60.7500) lr 1.0000e-05 eta 0:03:50
epoch [2/50] batch [5/26] time 0.153 (0.278) data 0.000 (0.124) loss 3.9883 (4.4664) acc 50.0000 (58.1250) lr 2.0000e-03 eta 0:05:52
epoch [2/50] batch [10/26] time 0.153 (0.215) data 0.000 (0.062) loss 3.6914 (4.1186) acc 65.6250 (61.5625) lr 2.0000e-03 eta 0:04:32
epoch [2/50] batch [15/26] time 0.151 (0.194) data 0.000 (0.042) loss 3.2090 (3.9574) acc 68.7500 (63.1250) lr 2.0000e-03 eta 0:04:04
epoch [2/50] batch [20/26] time 0.151 (0.183) data 0.000 (0.031) loss 3.3164 (3.8312) acc 56.2500 (62.6562) lr 2.0000e-03 eta 0:03:50
epoch [2/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.025) loss 3.5781 (3.7131) acc 62.5000 (64.3750) lr 2.0000e-03 eta 0:03:41
epoch [3/50] batch [5/26] time 0.153 (0.275) data 0.000 (0.122) loss 3.2734 (3.1625) acc 65.6250 (65.6250) lr 1.9980e-03 eta 0:05:42
epoch [3/50] batch [10/26] time 0.153 (0.214) data 0.000 (0.061) loss 3.0742 (3.0096) acc 62.5000 (67.5000) lr 1.9980e-03 eta 0:04:24
epoch [3/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.041) loss 2.9922 (2.9936) acc 65.6250 (67.2917) lr 1.9980e-03 eta 0:03:58
epoch [3/50] batch [20/26] time 0.151 (0.183) data 0.000 (0.031) loss 2.7988 (2.9865) acc 71.8750 (67.9688) lr 1.9980e-03 eta 0:03:44
epoch [3/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.024) loss 3.2852 (3.0218) acc 68.7500 (68.2500) lr 1.9980e-03 eta 0:03:36
epoch [4/50] batch [5/26] time 0.153 (0.277) data 0.000 (0.122) loss 2.4023 (2.9965) acc 71.8750 (65.6250) lr 1.9921e-03 eta 0:05:37
epoch [4/50] batch [10/26] time 0.154 (0.215) data 0.000 (0.061) loss 2.8555 (2.9400) acc 65.6250 (68.1250) lr 1.9921e-03 eta 0:04:20
epoch [4/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.041) loss 2.8477 (2.9115) acc 65.6250 (68.1250) lr 1.9921e-03 eta 0:03:55
epoch [4/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.031) loss 2.3086 (2.8264) acc 75.0000 (69.0625) lr 1.9921e-03 eta 0:03:41
epoch [4/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.025) loss 2.9258 (2.8410) acc 75.0000 (69.7500) lr 1.9921e-03 eta 0:03:32
epoch [5/50] batch [5/26] time 0.152 (0.288) data 0.000 (0.134) loss 2.7266 (2.7691) acc 62.5000 (68.1250) lr 1.9823e-03 eta 0:05:42
epoch [5/50] batch [10/26] time 0.154 (0.221) data 0.000 (0.067) loss 2.5195 (2.6451) acc 78.1250 (71.8750) lr 1.9823e-03 eta 0:04:21
epoch [5/50] batch [15/26] time 0.154 (0.198) data 0.000 (0.045) loss 2.3027 (2.7165) acc 90.6250 (72.0833) lr 1.9823e-03 eta 0:03:53
epoch [5/50] batch [20/26] time 0.152 (0.187) data 0.000 (0.034) loss 2.8320 (2.6640) acc 68.7500 (72.9688) lr 1.9823e-03 eta 0:03:39
epoch [5/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.027) loss 2.8594 (2.6647) acc 68.7500 (73.0000) lr 1.9823e-03 eta 0:03:30
epoch [6/50] batch [5/26] time 0.153 (0.274) data 0.000 (0.120) loss 1.9707 (2.4715) acc 84.3750 (78.7500) lr 1.9686e-03 eta 0:05:19
epoch [6/50] batch [10/26] time 0.154 (0.214) data 0.000 (0.060) loss 2.8008 (2.5680) acc 62.5000 (75.6250) lr 1.9686e-03 eta 0:04:08
epoch [6/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.040) loss 2.9062 (2.5518) acc 71.8750 (75.4167) lr 1.9686e-03 eta 0:03:43
epoch [6/50] batch [20/26] time 0.152 (0.183) data 0.000 (0.030) loss 2.5312 (2.5338) acc 78.1250 (75.6250) lr 1.9686e-03 eta 0:03:30
epoch [6/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.024) loss 2.2988 (2.5324) acc 78.1250 (75.6250) lr 1.9686e-03 eta 0:03:22
epoch [7/50] batch [5/26] time 0.153 (0.281) data 0.000 (0.127) loss 2.4570 (2.6418) acc 78.1250 (72.5000) lr 1.9511e-03 eta 0:05:20
epoch [7/50] batch [10/26] time 0.152 (0.217) data 0.000 (0.064) loss 2.1660 (2.4826) acc 75.0000 (74.6875) lr 1.9511e-03 eta 0:04:06
epoch [7/50] batch [15/26] time 0.152 (0.195) data 0.000 (0.043) loss 2.4492 (2.4126) acc 75.0000 (75.8333) lr 1.9511e-03 eta 0:03:40
epoch [7/50] batch [20/26] time 0.152 (0.185) data 0.000 (0.032) loss 2.7441 (2.4275) acc 71.8750 (75.4688) lr 1.9511e-03 eta 0:03:27
epoch [7/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.026) loss 1.8008 (2.4396) acc 84.3750 (75.3750) lr 1.9511e-03 eta 0:03:19
epoch [8/50] batch [5/26] time 0.157 (0.273) data 0.000 (0.117) loss 2.1348 (2.4404) acc 75.0000 (75.0000) lr 1.9298e-03 eta 0:05:03
epoch [8/50] batch [10/26] time 0.153 (0.213) data 0.000 (0.058) loss 3.1211 (2.4440) acc 62.5000 (75.0000) lr 1.9298e-03 eta 0:03:55
epoch [8/50] batch [15/26] time 0.155 (0.193) data 0.000 (0.039) loss 2.2676 (2.4834) acc 87.5000 (75.2083) lr 1.9298e-03 eta 0:03:32
epoch [8/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.2617 (2.4632) acc 81.2500 (76.5625) lr 1.9298e-03 eta 0:03:20
epoch [8/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.023) loss 2.9844 (2.4280) acc 65.6250 (77.6250) lr 1.9298e-03 eta 0:03:13
epoch [9/50] batch [5/26] time 0.152 (0.273) data 0.000 (0.118) loss 1.9775 (2.3049) acc 93.7500 (81.8750) lr 1.9048e-03 eta 0:04:56
epoch [9/50] batch [10/26] time 0.153 (0.213) data 0.000 (0.059) loss 1.6641 (2.1192) acc 90.6250 (81.8750) lr 1.9048e-03 eta 0:03:50
epoch [9/50] batch [15/26] time 0.152 (0.193) data 0.000 (0.040) loss 2.1738 (2.1799) acc 75.0000 (80.6250) lr 1.9048e-03 eta 0:03:27
epoch [9/50] batch [20/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.8896 (2.1966) acc 90.6250 (80.7812) lr 1.9048e-03 eta 0:03:15
epoch [9/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.024) loss 2.3516 (2.2462) acc 81.2500 (80.1250) lr 1.9048e-03 eta 0:03:08
epoch [10/50] batch [5/26] time 0.156 (0.297) data 0.000 (0.141) loss 1.8984 (2.0645) acc 78.1250 (82.5000) lr 1.8763e-03 eta 0:05:14
epoch [10/50] batch [10/26] time 0.153 (0.225) data 0.000 (0.071) loss 2.1914 (2.0946) acc 81.2500 (82.5000) lr 1.8763e-03 eta 0:03:58
epoch [10/50] batch [15/26] time 0.152 (0.201) data 0.000 (0.047) loss 2.7812 (2.2495) acc 71.8750 (80.4167) lr 1.8763e-03 eta 0:03:31
epoch [10/50] batch [20/26] time 0.152 (0.189) data 0.000 (0.035) loss 2.4629 (2.2870) acc 78.1250 (79.5312) lr 1.8763e-03 eta 0:03:17
epoch [10/50] batch [25/26] time 0.152 (0.181) data 0.000 (0.028) loss 2.5391 (2.2627) acc 78.1250 (80.1250) lr 1.8763e-03 eta 0:03:08
epoch [11/50] batch [5/26] time 0.152 (0.272) data 0.000 (0.118) loss 1.9258 (2.2062) acc 87.5000 (81.8750) lr 1.8443e-03 eta 0:04:41
epoch [11/50] batch [10/26] time 0.153 (0.212) data 0.000 (0.059) loss 2.2266 (2.2100) acc 81.2500 (82.1875) lr 1.8443e-03 eta 0:03:38
epoch [11/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.040) loss 2.3691 (2.2174) acc 71.8750 (80.8333) lr 1.8443e-03 eta 0:03:17
epoch [11/50] batch [20/26] time 0.152 (0.182) data 0.000 (0.030) loss 2.0684 (2.2066) acc 78.1250 (80.4688) lr 1.8443e-03 eta 0:03:06
epoch [11/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.024) loss 2.9023 (2.2106) acc 71.8750 (81.0000) lr 1.8443e-03 eta 0:02:59
epoch [12/50] batch [5/26] time 0.152 (0.280) data 0.000 (0.126) loss 2.1836 (2.1461) acc 78.1250 (81.8750) lr 1.8090e-03 eta 0:04:42
epoch [12/50] batch [10/26] time 0.154 (0.216) data 0.001 (0.063) loss 2.4258 (2.0985) acc 68.7500 (82.1875) lr 1.8090e-03 eta 0:03:37
epoch [12/50] batch [15/26] time 0.152 (0.195) data 0.000 (0.042) loss 2.5488 (2.1463) acc 71.8750 (81.6667) lr 1.8090e-03 eta 0:03:14
epoch [12/50] batch [20/26] time 0.152 (0.184) data 0.000 (0.032) loss 2.1816 (2.1592) acc 81.2500 (80.7812) lr 1.8090e-03 eta 0:03:03
epoch [12/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.025) loss 2.0508 (2.1481) acc 84.3750 (81.3750) lr 1.8090e-03 eta 0:02:55
epoch [13/50] batch [5/26] time 0.155 (0.295) data 0.000 (0.140) loss 1.7334 (1.8912) acc 87.5000 (85.0000) lr 1.7705e-03 eta 0:04:49
epoch [13/50] batch [10/26] time 0.155 (0.225) data 0.001 (0.070) loss 2.0430 (2.0544) acc 84.3750 (80.6250) lr 1.7705e-03 eta 0:03:40
epoch [13/50] batch [15/26] time 0.153 (0.202) data 0.000 (0.047) loss 1.8643 (2.0835) acc 84.3750 (81.6667) lr 1.7705e-03 eta 0:03:16
epoch [13/50] batch [20/26] time 0.154 (0.190) data 0.000 (0.035) loss 2.6641 (2.0960) acc 71.8750 (81.5625) lr 1.7705e-03 eta 0:03:03
epoch [13/50] batch [25/26] time 0.152 (0.182) data 0.000 (0.028) loss 1.8721 (2.0977) acc 87.5000 (81.5000) lr 1.7705e-03 eta 0:02:55
epoch [14/50] batch [5/26] time 0.152 (0.278) data 0.000 (0.123) loss 2.0898 (1.9848) acc 87.5000 (85.0000) lr 1.7290e-03 eta 0:04:25
epoch [14/50] batch [10/26] time 0.153 (0.215) data 0.000 (0.062) loss 2.3496 (2.0949) acc 71.8750 (82.5000) lr 1.7290e-03 eta 0:03:25
epoch [14/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.041) loss 2.4375 (2.1686) acc 65.6250 (81.0417) lr 1.7290e-03 eta 0:03:04
epoch [14/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.031) loss 1.6357 (2.1415) acc 90.6250 (81.8750) lr 1.7290e-03 eta 0:02:53
epoch [14/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.025) loss 2.3711 (2.1545) acc 84.3750 (81.6250) lr 1.7290e-03 eta 0:02:46
epoch [15/50] batch [5/26] time 0.153 (0.265) data 0.000 (0.112) loss 2.1348 (2.3254) acc 81.2500 (81.2500) lr 1.6845e-03 eta 0:04:07
epoch [15/50] batch [10/26] time 0.153 (0.209) data 0.000 (0.056) loss 2.8418 (2.3185) acc 71.8750 (79.3750) lr 1.6845e-03 eta 0:03:13
epoch [15/50] batch [15/26] time 0.152 (0.190) data 0.000 (0.037) loss 1.7910 (2.2207) acc 87.5000 (81.0417) lr 1.6845e-03 eta 0:02:55
epoch [15/50] batch [20/26] time 0.152 (0.181) data 0.000 (0.028) loss 2.1582 (2.1505) acc 81.2500 (82.1875) lr 1.6845e-03 eta 0:02:45
epoch [15/50] batch [25/26] time 0.152 (0.175) data 0.000 (0.022) loss 2.1660 (2.1121) acc 81.2500 (83.0000) lr 1.6845e-03 eta 0:02:39
epoch [16/50] batch [5/26] time 0.153 (0.277) data 0.000 (0.123) loss 1.4404 (2.0658) acc 96.8750 (85.6250) lr 1.6374e-03 eta 0:04:10
epoch [16/50] batch [10/26] time 0.153 (0.215) data 0.000 (0.062) loss 1.6895 (2.0169) acc 87.5000 (85.0000) lr 1.6374e-03 eta 0:03:13
epoch [16/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.041) loss 2.2910 (2.0350) acc 87.5000 (85.6250) lr 1.6374e-03 eta 0:02:53
epoch [16/50] batch [20/26] time 0.153 (0.184) data 0.000 (0.031) loss 1.6523 (2.0924) acc 90.6250 (83.7500) lr 1.6374e-03 eta 0:02:43
epoch [16/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.025) loss 1.7871 (2.0713) acc 87.5000 (83.8750) lr 1.6374e-03 eta 0:02:37
epoch [17/50] batch [5/26] time 0.154 (0.271) data 0.000 (0.117) loss 1.6738 (2.2379) acc 90.6250 (81.8750) lr 1.5878e-03 eta 0:03:58
epoch [17/50] batch [10/26] time 0.154 (0.212) data 0.000 (0.058) loss 1.8486 (2.1562) acc 90.6250 (81.5625) lr 1.5878e-03 eta 0:03:05
epoch [17/50] batch [15/26] time 0.153 (0.193) data 0.000 (0.039) loss 2.3516 (2.0977) acc 75.0000 (82.2917) lr 1.5878e-03 eta 0:02:47
epoch [17/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.5586 (2.0772) acc 75.0000 (83.2812) lr 1.5878e-03 eta 0:02:37
epoch [17/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.023) loss 1.8906 (2.1051) acc 90.6250 (82.7500) lr 1.5878e-03 eta 0:02:31
epoch [18/50] batch [5/26] time 0.156 (0.268) data 0.000 (0.113) loss 1.6875 (1.7643) acc 90.6250 (86.8750) lr 1.5358e-03 eta 0:03:48
epoch [18/50] batch [10/26] time 0.154 (0.211) data 0.000 (0.056) loss 2.0391 (1.8876) acc 87.5000 (85.6250) lr 1.5358e-03 eta 0:02:58
epoch [18/50] batch [15/26] time 0.152 (0.192) data 0.000 (0.038) loss 1.8750 (1.9604) acc 87.5000 (84.3750) lr 1.5358e-03 eta 0:02:41
epoch [18/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.028) loss 2.2871 (1.9958) acc 75.0000 (83.9062) lr 1.5358e-03 eta 0:02:32
epoch [18/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.023) loss 2.4355 (2.0447) acc 75.0000 (83.5000) lr 1.5358e-03 eta 0:02:26
epoch [19/50] batch [5/26] time 0.155 (0.278) data 0.000 (0.121) loss 2.1621 (2.0201) acc 71.8750 (80.6250) lr 1.4818e-03 eta 0:03:49
epoch [19/50] batch [10/26] time 0.154 (0.216) data 0.001 (0.061) loss 2.3672 (2.1273) acc 81.2500 (80.9375) lr 1.4818e-03 eta 0:02:57
epoch [19/50] batch [15/26] time 0.152 (0.195) data 0.000 (0.041) loss 1.6143 (2.0637) acc 100.0000 (83.7500) lr 1.4818e-03 eta 0:02:39
epoch [19/50] batch [20/26] time 0.152 (0.184) data 0.000 (0.031) loss 1.8848 (2.0902) acc 87.5000 (83.2812) lr 1.4818e-03 eta 0:02:29
epoch [19/50] batch [25/26] time 0.152 (0.178) data 0.000 (0.024) loss 1.4570 (2.0634) acc 96.8750 (83.7500) lr 1.4818e-03 eta 0:02:23
epoch [20/50] batch [5/26] time 0.153 (0.281) data 0.000 (0.128) loss 1.6230 (1.8201) acc 93.7500 (88.7500) lr 1.4258e-03 eta 0:03:45
epoch [20/50] batch [10/26] time 0.153 (0.218) data 0.000 (0.064) loss 2.3652 (1.9591) acc 78.1250 (85.0000) lr 1.4258e-03 eta 0:02:53
epoch [20/50] batch [15/26] time 0.152 (0.196) data 0.000 (0.043) loss 1.5605 (1.9460) acc 87.5000 (85.8333) lr 1.4258e-03 eta 0:02:35
epoch [20/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.032) loss 2.1309 (1.9433) acc 78.1250 (86.8750) lr 1.4258e-03 eta 0:02:25
epoch [20/50] batch [25/26] time 0.153 (0.179) data 0.000 (0.026) loss 1.9941 (1.9156) acc 93.7500 (87.3750) lr 1.4258e-03 eta 0:02:19
epoch [21/50] batch [5/26] time 0.153 (0.263) data 0.000 (0.108) loss 1.2666 (1.8039) acc 93.7500 (89.3750) lr 1.3681e-03 eta 0:03:23
epoch [21/50] batch [10/26] time 0.153 (0.208) data 0.000 (0.054) loss 1.8545 (1.7945) acc 84.3750 (88.7500) lr 1.3681e-03 eta 0:02:39
epoch [21/50] batch [15/26] time 0.153 (0.189) data 0.000 (0.036) loss 1.5596 (1.8397) acc 93.7500 (88.9583) lr 1.3681e-03 eta 0:02:24
epoch [21/50] batch [20/26] time 0.152 (0.180) data 0.000 (0.027) loss 2.0742 (1.8810) acc 78.1250 (88.1250) lr 1.3681e-03 eta 0:02:16
epoch [21/50] batch [25/26] time 0.152 (0.174) data 0.000 (0.022) loss 2.4609 (1.9322) acc 78.1250 (87.2500) lr 1.3681e-03 eta 0:02:11
epoch [22/50] batch [5/26] time 0.152 (0.280) data 0.000 (0.127) loss 1.7168 (1.7902) acc 87.5000 (86.8750) lr 1.3090e-03 eta 0:03:29
epoch [22/50] batch [10/26] time 0.154 (0.217) data 0.000 (0.063) loss 2.4258 (1.8365) acc 68.7500 (86.8750) lr 1.3090e-03 eta 0:02:41
epoch [22/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.042) loss 2.1250 (1.9896) acc 87.5000 (85.2083) lr 1.3090e-03 eta 0:02:24
epoch [22/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.032) loss 1.3770 (1.9396) acc 96.8750 (86.2500) lr 1.3090e-03 eta 0:02:15
epoch [22/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.025) loss 1.8037 (1.9279) acc 87.5000 (86.1250) lr 1.3090e-03 eta 0:02:09
epoch [23/50] batch [5/26] time 0.152 (0.292) data 0.000 (0.137) loss 1.7871 (1.7168) acc 87.5000 (93.7500) lr 1.2487e-03 eta 0:03:30
epoch [23/50] batch [10/26] time 0.154 (0.223) data 0.000 (0.069) loss 2.3789 (1.9398) acc 84.3750 (89.3750) lr 1.2487e-03 eta 0:02:40
epoch [23/50] batch [15/26] time 0.153 (0.200) data 0.000 (0.046) loss 2.0586 (1.9094) acc 84.3750 (88.7500) lr 1.2487e-03 eta 0:02:22
epoch [23/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.034) loss 1.6289 (1.8817) acc 90.6250 (87.6562) lr 1.2487e-03 eta 0:02:13
epoch [23/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.028) loss 1.9766 (1.8678) acc 90.6250 (88.3750) lr 1.2487e-03 eta 0:02:07
epoch [24/50] batch [5/26] time 0.153 (0.272) data 0.000 (0.117) loss 1.7529 (1.8037) acc 84.3750 (86.8750) lr 1.1874e-03 eta 0:03:09
epoch [24/50] batch [10/26] time 0.154 (0.213) data 0.000 (0.059) loss 1.9355 (1.7959) acc 84.3750 (86.8750) lr 1.1874e-03 eta 0:02:27
epoch [24/50] batch [15/26] time 0.153 (0.193) data 0.000 (0.039) loss 1.9424 (1.8212) acc 84.3750 (86.6667) lr 1.1874e-03 eta 0:02:12
epoch [24/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 1.9814 (1.8802) acc 90.6250 (86.8750) lr 1.1874e-03 eta 0:02:04
epoch [24/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.024) loss 1.6641 (1.9013) acc 96.8750 (86.3750) lr 1.1874e-03 eta 0:01:59
epoch [25/50] batch [5/26] time 0.153 (0.274) data 0.000 (0.119) loss 2.4180 (2.1779) acc 75.0000 (82.5000) lr 1.1253e-03 eta 0:03:03
epoch [25/50] batch [10/26] time 0.154 (0.214) data 0.000 (0.060) loss 1.4131 (1.8596) acc 96.8750 (86.5625) lr 1.1253e-03 eta 0:02:22
epoch [25/50] batch [15/26] time 0.153 (0.194) data 0.000 (0.040) loss 2.2090 (1.9204) acc 81.2500 (85.8333) lr 1.1253e-03 eta 0:02:07
epoch [25/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.030) loss 1.8438 (1.9316) acc 93.7500 (85.7812) lr 1.1253e-03 eta 0:02:00
epoch [25/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.024) loss 2.5645 (1.9448) acc 78.1250 (85.7500) lr 1.1253e-03 eta 0:01:55
epoch [26/50] batch [5/26] time 0.153 (0.279) data 0.000 (0.125) loss 1.9766 (1.9125) acc 87.5000 (86.8750) lr 1.0628e-03 eta 0:02:59
epoch [26/50] batch [10/26] time 0.153 (0.216) data 0.000 (0.063) loss 1.8242 (1.9197) acc 81.2500 (87.8125) lr 1.0628e-03 eta 0:02:18
epoch [26/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.042) loss 1.6758 (1.9156) acc 93.7500 (88.7500) lr 1.0628e-03 eta 0:02:03
epoch [26/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.031) loss 1.9824 (1.9562) acc 81.2500 (87.1875) lr 1.0628e-03 eta 0:01:56
epoch [26/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.025) loss 2.4062 (1.9873) acc 75.0000 (87.0000) lr 1.0628e-03 eta 0:01:51
epoch [27/50] batch [5/26] time 0.153 (0.272) data 0.000 (0.117) loss 1.7090 (1.8900) acc 90.6250 (88.1250) lr 1.0000e-03 eta 0:02:48
epoch [27/50] batch [10/26] time 0.154 (0.213) data 0.000 (0.058) loss 1.7812 (1.9221) acc 93.7500 (88.4375) lr 1.0000e-03 eta 0:02:10
epoch [27/50] batch [15/26] time 0.153 (0.193) data 0.000 (0.039) loss 2.1582 (1.9957) acc 90.6250 (88.3333) lr 1.0000e-03 eta 0:01:57
epoch [27/50] batch [20/26] time 0.153 (0.183) data 0.000 (0.029) loss 2.3789 (2.0047) acc 75.0000 (87.1875) lr 1.0000e-03 eta 0:01:50
epoch [27/50] batch [25/26] time 0.153 (0.177) data 0.000 (0.023) loss 1.5703 (1.9917) acc 96.8750 (87.2500) lr 1.0000e-03 eta 0:01:45
epoch [28/50] batch [5/26] time 0.154 (0.276) data 0.000 (0.121) loss 2.2402 (1.9471) acc 87.5000 (87.5000) lr 9.3721e-04 eta 0:02:43
epoch [28/50] batch [10/26] time 0.158 (0.216) data 0.000 (0.061) loss 2.0996 (1.9183) acc 84.3750 (87.8125) lr 9.3721e-04 eta 0:02:06
epoch [28/50] batch [15/26] time 0.156 (0.196) data 0.000 (0.040) loss 1.7324 (1.8911) acc 100.0000 (88.7500) lr 9.3721e-04 eta 0:01:54
epoch [28/50] batch [20/26] time 0.156 (0.186) data 0.000 (0.030) loss 1.9863 (1.9126) acc 84.3750 (87.5000) lr 9.3721e-04 eta 0:01:47
epoch [28/50] batch [25/26] time 0.155 (0.180) data 0.000 (0.024) loss 1.7627 (1.8934) acc 90.6250 (88.1250) lr 9.3721e-04 eta 0:01:42
epoch [29/50] batch [5/26] time 0.157 (0.275) data 0.000 (0.116) loss 1.6973 (1.9152) acc 90.6250 (88.1250) lr 8.7467e-04 eta 0:02:35
epoch [29/50] batch [10/26] time 0.154 (0.216) data 0.000 (0.058) loss 1.7324 (1.8967) acc 90.6250 (86.8750) lr 8.7467e-04 eta 0:02:01
epoch [29/50] batch [15/26] time 0.153 (0.195) data 0.000 (0.039) loss 2.0820 (1.8674) acc 84.3750 (88.3333) lr 8.7467e-04 eta 0:01:48
epoch [29/50] batch [20/26] time 0.153 (0.185) data 0.000 (0.029) loss 2.1016 (1.9068) acc 87.5000 (87.5000) lr 8.7467e-04 eta 0:01:41
epoch [29/50] batch [25/26] time 0.153 (0.178) data 0.000 (0.023) loss 2.0020 (1.8682) acc 84.3750 (88.1250) lr 8.7467e-04 eta 0:01:37
epoch [30/50] batch [5/26] time 0.153 (0.260) data 0.000 (0.105) loss 2.1836 (2.0305) acc 84.3750 (87.5000) lr 8.1262e-04 eta 0:02:20
epoch [30/50] batch [10/26] time 0.155 (0.207) data 0.000 (0.053) loss 1.8457 (1.8745) acc 87.5000 (89.0625) lr 8.1262e-04 eta 0:01:50
epoch [30/50] batch [15/26] time 0.153 (0.189) data 0.000 (0.035) loss 2.1445 (1.9436) acc 84.3750 (86.8750) lr 8.1262e-04 eta 0:01:40
epoch [30/50] batch [20/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.8682 (1.8875) acc 84.3750 (87.6562) lr 8.1262e-04 eta 0:01:34
epoch [30/50] batch [25/26] time 0.152 (0.174) data 0.000 (0.021) loss 2.0117 (1.8540) acc 84.3750 (88.1250) lr 8.1262e-04 eta 0:01:30
epoch [31/50] batch [5/26] time 0.153 (0.261) data 0.000 (0.107) loss 1.5312 (1.8979) acc 90.6250 (85.6250) lr 7.5131e-04 eta 0:02:14
epoch [31/50] batch [10/26] time 0.153 (0.207) data 0.000 (0.054) loss 1.9180 (1.9237) acc 87.5000 (85.0000) lr 7.5131e-04 eta 0:01:45
epoch [31/50] batch [15/26] time 0.152 (0.189) data 0.000 (0.036) loss 1.8750 (1.9328) acc 93.7500 (86.0417) lr 7.5131e-04 eta 0:01:35
epoch [31/50] batch [20/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.4355 (1.8735) acc 96.8750 (87.6562) lr 7.5131e-04 eta 0:01:29
epoch [31/50] batch [25/26] time 0.152 (0.174) data 0.000 (0.022) loss 1.5781 (1.8880) acc 93.7500 (87.8750) lr 7.5131e-04 eta 0:01:26
epoch [32/50] batch [5/26] time 0.153 (0.261) data 0.000 (0.107) loss 1.7734 (1.8654) acc 93.7500 (89.3750) lr 6.9098e-04 eta 0:02:07
epoch [32/50] batch [10/26] time 0.154 (0.207) data 0.000 (0.053) loss 1.7451 (1.8115) acc 87.5000 (89.3750) lr 6.9098e-04 eta 0:01:40
epoch [32/50] batch [15/26] time 0.153 (0.189) data 0.000 (0.036) loss 1.5674 (1.8726) acc 84.3750 (87.9167) lr 6.9098e-04 eta 0:01:30
epoch [32/50] batch [20/26] time 0.152 (0.180) data 0.000 (0.027) loss 1.5664 (1.8389) acc 90.6250 (88.5938) lr 6.9098e-04 eta 0:01:25
epoch [32/50] batch [25/26] time 0.153 (0.175) data 0.000 (0.021) loss 1.7109 (1.8184) acc 90.6250 (88.7500) lr 6.9098e-04 eta 0:01:21
epoch [33/50] batch [5/26] time 0.153 (0.257) data 0.000 (0.104) loss 1.5830 (1.6701) acc 90.6250 (90.6250) lr 6.3188e-04 eta 0:01:59
epoch [33/50] batch [10/26] time 0.153 (0.205) data 0.000 (0.052) loss 2.0879 (1.9406) acc 87.5000 (87.1875) lr 6.3188e-04 eta 0:01:33
epoch [33/50] batch [15/26] time 0.152 (0.187) data 0.000 (0.035) loss 2.0059 (1.8759) acc 87.5000 (87.9167) lr 6.3188e-04 eta 0:01:24
epoch [33/50] batch [20/26] time 0.152 (0.179) data 0.000 (0.026) loss 1.9980 (1.8325) acc 87.5000 (88.7500) lr 6.3188e-04 eta 0:01:20
epoch [33/50] batch [25/26] time 0.154 (0.173) data 0.000 (0.021) loss 2.2070 (1.8306) acc 84.3750 (88.5000) lr 6.3188e-04 eta 0:01:16
epoch [34/50] batch [5/26] time 0.154 (0.272) data 0.000 (0.118) loss 1.4473 (1.8510) acc 100.0000 (88.7500) lr 5.7422e-04 eta 0:01:58
epoch [34/50] batch [10/26] time 0.153 (0.213) data 0.000 (0.059) loss 1.5664 (1.7816) acc 96.8750 (89.6875) lr 5.7422e-04 eta 0:01:31
epoch [34/50] batch [15/26] time 0.153 (0.193) data 0.000 (0.039) loss 1.9814 (1.7882) acc 87.5000 (90.0000) lr 5.7422e-04 eta 0:01:22
epoch [34/50] batch [20/26] time 0.152 (0.183) data 0.000 (0.030) loss 1.7305 (1.8021) acc 90.6250 (89.0625) lr 5.7422e-04 eta 0:01:17
epoch [34/50] batch [25/26] time 0.152 (0.177) data 0.000 (0.024) loss 2.1602 (1.8207) acc 81.2500 (88.3750) lr 5.7422e-04 eta 0:01:13
epoch [35/50] batch [5/26] time 0.154 (0.257) data 0.000 (0.102) loss 1.3193 (1.5773) acc 96.8750 (92.5000) lr 5.1825e-04 eta 0:01:45
epoch [35/50] batch [10/26] time 0.154 (0.205) data 0.000 (0.051) loss 1.4961 (1.6621) acc 96.8750 (91.8750) lr 5.1825e-04 eta 0:01:23
epoch [35/50] batch [15/26] time 0.154 (0.188) data 0.000 (0.034) loss 1.8398 (1.7881) acc 87.5000 (90.2083) lr 5.1825e-04 eta 0:01:15
epoch [35/50] batch [20/26] time 0.153 (0.179) data 0.000 (0.026) loss 2.2285 (1.7584) acc 84.3750 (90.7812) lr 5.1825e-04 eta 0:01:10
epoch [35/50] batch [25/26] time 0.153 (0.174) data 0.000 (0.021) loss 2.0547 (1.8202) acc 90.6250 (89.2500) lr 5.1825e-04 eta 0:01:07
epoch [36/50] batch [5/26] time 0.154 (0.269) data 0.000 (0.115) loss 1.8721 (1.9070) acc 90.6250 (88.1250) lr 4.6417e-04 eta 0:01:43
epoch [36/50] batch [10/26] time 0.154 (0.211) data 0.000 (0.057) loss 2.1309 (1.9225) acc 84.3750 (88.4375) lr 4.6417e-04 eta 0:01:20
epoch [36/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.038) loss 1.9482 (1.9113) acc 87.5000 (87.9167) lr 4.6417e-04 eta 0:01:11
epoch [36/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.029) loss 1.7354 (1.8643) acc 93.7500 (88.9062) lr 4.6417e-04 eta 0:01:07
epoch [36/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.023) loss 2.1465 (1.8352) acc 84.3750 (89.2500) lr 4.6417e-04 eta 0:01:04
epoch [37/50] batch [5/26] time 0.156 (0.292) data 0.000 (0.136) loss 1.5625 (1.9154) acc 96.8750 (90.0000) lr 4.1221e-04 eta 0:01:44
epoch [37/50] batch [10/26] time 0.154 (0.223) data 0.000 (0.068) loss 1.3174 (1.8359) acc 96.8750 (91.5625) lr 4.1221e-04 eta 0:01:18
epoch [37/50] batch [15/26] time 0.154 (0.200) data 0.000 (0.046) loss 1.7783 (1.7921) acc 93.7500 (91.6667) lr 4.1221e-04 eta 0:01:09
epoch [37/50] batch [20/26] time 0.153 (0.188) data 0.000 (0.034) loss 1.6328 (1.7724) acc 96.8750 (92.1875) lr 4.1221e-04 eta 0:01:04
epoch [37/50] batch [25/26] time 0.153 (0.181) data 0.000 (0.027) loss 1.8857 (1.7886) acc 90.6250 (91.3750) lr 4.1221e-04 eta 0:01:01
epoch [38/50] batch [5/26] time 0.154 (0.263) data 0.000 (0.109) loss 1.5098 (1.7602) acc 93.7500 (91.2500) lr 3.6258e-04 eta 0:01:27
epoch [38/50] batch [10/26] time 0.155 (0.209) data 0.000 (0.054) loss 1.7480 (1.8426) acc 93.7500 (89.6875) lr 3.6258e-04 eta 0:01:08
epoch [38/50] batch [15/26] time 0.158 (0.191) data 0.000 (0.036) loss 1.9043 (1.8544) acc 84.3750 (88.5417) lr 3.6258e-04 eta 0:01:01
epoch [38/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.027) loss 2.1641 (1.8759) acc 84.3750 (88.1250) lr 3.6258e-04 eta 0:00:57
epoch [38/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.022) loss 1.8750 (1.8418) acc 87.5000 (88.2500) lr 3.6258e-04 eta 0:00:55
epoch [39/50] batch [5/26] time 0.153 (0.260) data 0.000 (0.105) loss 2.0918 (1.8846) acc 84.3750 (87.5000) lr 3.1545e-04 eta 0:01:19
epoch [39/50] batch [10/26] time 0.154 (0.206) data 0.000 (0.053) loss 1.5303 (1.7661) acc 96.8750 (90.3125) lr 3.1545e-04 eta 0:01:02
epoch [39/50] batch [15/26] time 0.153 (0.188) data 0.000 (0.035) loss 1.7480 (1.7779) acc 93.7500 (89.7917) lr 3.1545e-04 eta 0:00:55
epoch [39/50] batch [20/26] time 0.153 (0.180) data 0.000 (0.026) loss 2.0684 (1.7675) acc 90.6250 (90.7812) lr 3.1545e-04 eta 0:00:52
epoch [39/50] batch [25/26] time 0.154 (0.174) data 0.000 (0.021) loss 1.3369 (1.7625) acc 96.8750 (90.8750) lr 3.1545e-04 eta 0:00:49
epoch [40/50] batch [5/26] time 0.155 (0.257) data 0.000 (0.101) loss 2.2852 (1.8744) acc 84.3750 (89.3750) lr 2.7103e-04 eta 0:01:12
epoch [40/50] batch [10/26] time 0.154 (0.205) data 0.000 (0.050) loss 1.4580 (1.7843) acc 93.7500 (90.9375) lr 2.7103e-04 eta 0:00:56
epoch [40/50] batch [15/26] time 0.153 (0.188) data 0.000 (0.034) loss 1.5723 (1.8042) acc 87.5000 (90.2083) lr 2.7103e-04 eta 0:00:50
epoch [40/50] batch [20/26] time 0.152 (0.179) data 0.000 (0.025) loss 1.6318 (1.7727) acc 84.3750 (90.1562) lr 2.7103e-04 eta 0:00:47
epoch [40/50] batch [25/26] time 0.152 (0.174) data 0.000 (0.020) loss 1.6211 (1.7858) acc 93.7500 (89.7500) lr 2.7103e-04 eta 0:00:45
epoch [41/50] batch [5/26] time 0.154 (0.267) data 0.000 (0.112) loss 1.6055 (1.7281) acc 93.7500 (89.3750) lr 2.2949e-04 eta 0:01:08
epoch [41/50] batch [10/26] time 0.155 (0.211) data 0.000 (0.056) loss 1.7021 (1.6528) acc 90.6250 (90.6250) lr 2.2949e-04 eta 0:00:52
epoch [41/50] batch [15/26] time 0.155 (0.192) data 0.000 (0.038) loss 2.1680 (1.7533) acc 84.3750 (90.2083) lr 2.2949e-04 eta 0:00:47
epoch [41/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.028) loss 1.4434 (1.7230) acc 96.8750 (91.2500) lr 2.2949e-04 eta 0:00:43
epoch [41/50] batch [25/26] time 0.152 (0.176) data 0.000 (0.023) loss 1.4893 (1.7573) acc 96.8750 (90.8750) lr 2.2949e-04 eta 0:00:41
epoch [42/50] batch [5/26] time 0.153 (0.264) data 0.000 (0.110) loss 1.9160 (1.7533) acc 90.6250 (90.6250) lr 1.9098e-04 eta 0:01:00
epoch [42/50] batch [10/26] time 0.153 (0.209) data 0.000 (0.055) loss 1.9023 (1.7338) acc 87.5000 (91.2500) lr 1.9098e-04 eta 0:00:46
epoch [42/50] batch [15/26] time 0.153 (0.190) data 0.000 (0.037) loss 2.0918 (1.7352) acc 81.2500 (90.8333) lr 1.9098e-04 eta 0:00:41
epoch [42/50] batch [20/26] time 0.152 (0.181) data 0.000 (0.028) loss 2.0000 (1.7635) acc 87.5000 (90.3125) lr 1.9098e-04 eta 0:00:38
epoch [42/50] batch [25/26] time 0.152 (0.175) data 0.000 (0.022) loss 2.1348 (1.7476) acc 87.5000 (90.8750) lr 1.9098e-04 eta 0:00:36
epoch [43/50] batch [5/26] time 0.158 (0.289) data 0.000 (0.134) loss 1.7197 (1.5846) acc 87.5000 (91.2500) lr 1.5567e-04 eta 0:00:58
epoch [43/50] batch [10/26] time 0.153 (0.222) data 0.000 (0.067) loss 2.0742 (1.6733) acc 84.3750 (90.6250) lr 1.5567e-04 eta 0:00:43
epoch [43/50] batch [15/26] time 0.153 (0.199) data 0.000 (0.045) loss 2.1016 (1.7596) acc 87.5000 (89.3750) lr 1.5567e-04 eta 0:00:38
epoch [43/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.034) loss 1.8252 (1.7819) acc 81.2500 (88.9062) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.027) loss 1.2715 (1.7966) acc 96.8750 (88.7500) lr 1.5567e-04 eta 0:00:33
epoch [44/50] batch [5/26] time 0.153 (0.267) data 0.000 (0.113) loss 2.4668 (1.9094) acc 81.2500 (90.0000) lr 1.2369e-04 eta 0:00:47
epoch [44/50] batch [10/26] time 0.154 (0.210) data 0.000 (0.057) loss 1.7910 (1.8174) acc 96.8750 (91.8750) lr 1.2369e-04 eta 0:00:36
epoch [44/50] batch [15/26] time 0.154 (0.191) data 0.000 (0.038) loss 2.1328 (1.8125) acc 84.3750 (90.8333) lr 1.2369e-04 eta 0:00:31
epoch [44/50] batch [20/26] time 0.154 (0.182) data 0.000 (0.028) loss 1.6211 (1.7766) acc 93.7500 (91.0938) lr 1.2369e-04 eta 0:00:29
epoch [44/50] batch [25/26] time 0.154 (0.176) data 0.000 (0.023) loss 2.2773 (1.8001) acc 84.3750 (90.3750) lr 1.2369e-04 eta 0:00:27
epoch [45/50] batch [5/26] time 0.154 (0.265) data 0.000 (0.111) loss 2.3398 (2.0531) acc 75.0000 (85.0000) lr 9.5173e-05 eta 0:00:39
epoch [45/50] batch [10/26] time 0.154 (0.209) data 0.000 (0.055) loss 1.6934 (1.9297) acc 90.6250 (87.1875) lr 9.5173e-05 eta 0:00:30
epoch [45/50] batch [15/26] time 0.153 (0.190) data 0.000 (0.037) loss 1.3320 (1.8279) acc 93.7500 (88.7500) lr 9.5173e-05 eta 0:00:26
epoch [45/50] batch [20/26] time 0.153 (0.181) data 0.000 (0.028) loss 1.6719 (1.8158) acc 90.6250 (89.3750) lr 9.5173e-05 eta 0:00:24
epoch [45/50] batch [25/26] time 0.154 (0.175) data 0.000 (0.022) loss 1.8867 (1.7838) acc 87.5000 (89.8750) lr 9.5173e-05 eta 0:00:22
epoch [46/50] batch [5/26] time 0.157 (0.286) data 0.000 (0.129) loss 2.0078 (1.8404) acc 84.3750 (88.1250) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [10/26] time 0.155 (0.220) data 0.000 (0.065) loss 1.4355 (1.8326) acc 93.7500 (88.4375) lr 7.0224e-05 eta 0:00:26
epoch [46/50] batch [15/26] time 0.153 (0.198) data 0.000 (0.043) loss 1.4707 (1.8237) acc 90.6250 (87.5000) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [20/26] time 0.153 (0.187) data 0.000 (0.032) loss 1.9932 (1.8207) acc 84.3750 (87.8125) lr 7.0224e-05 eta 0:00:20
epoch [46/50] batch [25/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.5342 (1.7812) acc 96.8750 (88.8750) lr 7.0224e-05 eta 0:00:18
epoch [47/50] batch [5/26] time 0.154 (0.260) data 0.000 (0.106) loss 2.1758 (1.8932) acc 90.6250 (90.6250) lr 4.8943e-05 eta 0:00:25
epoch [47/50] batch [10/26] time 0.155 (0.207) data 0.000 (0.053) loss 1.8145 (1.8716) acc 90.6250 (89.3750) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [15/26] time 0.155 (0.189) data 0.000 (0.035) loss 1.9668 (1.8234) acc 90.6250 (90.0000) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [20/26] time 0.153 (0.180) data 0.000 (0.027) loss 1.7168 (1.7688) acc 90.6250 (90.7812) lr 4.8943e-05 eta 0:00:15
epoch [47/50] batch [25/26] time 0.153 (0.175) data 0.000 (0.021) loss 1.7939 (1.7683) acc 90.6250 (90.6250) lr 4.8943e-05 eta 0:00:13
epoch [48/50] batch [5/26] time 0.153 (0.269) data 0.000 (0.114) loss 1.8496 (1.7189) acc 84.3750 (89.3750) lr 3.1417e-05 eta 0:00:19
epoch [48/50] batch [10/26] time 0.154 (0.211) data 0.000 (0.057) loss 1.7510 (1.7214) acc 84.3750 (90.3125) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.038) loss 2.0742 (1.7411) acc 84.3750 (89.7917) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.029) loss 2.0508 (1.7524) acc 84.3750 (89.6875) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.023) loss 1.5264 (1.7248) acc 90.6250 (90.2500) lr 3.1417e-05 eta 0:00:09
epoch [49/50] batch [5/26] time 0.154 (0.267) data 0.000 (0.112) loss 1.7656 (1.8939) acc 84.3750 (87.5000) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [10/26] time 0.155 (0.211) data 0.000 (0.056) loss 1.9424 (1.8064) acc 90.6250 (89.6875) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [15/26] time 0.153 (0.192) data 0.000 (0.037) loss 1.7822 (1.8195) acc 87.5000 (89.1667) lr 1.7713e-05 eta 0:00:07
epoch [49/50] batch [20/26] time 0.153 (0.182) data 0.000 (0.028) loss 1.7783 (1.7954) acc 90.6250 (89.8438) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/26] time 0.153 (0.176) data 0.000 (0.023) loss 1.9160 (1.8110) acc 87.5000 (89.6250) lr 1.7713e-05 eta 0:00:04
epoch [50/50] batch [5/26] time 0.155 (0.258) data 0.000 (0.102) loss 1.6055 (1.8607) acc 90.6250 (88.7500) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/26] time 0.154 (0.206) data 0.000 (0.051) loss 1.5703 (1.8238) acc 93.7500 (88.7500) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/26] time 0.155 (0.189) data 0.000 (0.034) loss 2.2969 (1.8416) acc 81.2500 (89.5833) lr 7.8853e-06 eta 0:00:02
epoch [50/50] batch [20/26] time 0.153 (0.180) data 0.000 (0.026) loss 1.6016 (1.8334) acc 96.8750 (89.8438) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [25/26] time 0.153 (0.175) data 0.000 (0.020) loss 1.7930 (1.8292) acc 93.7500 (90.0000) lr 7.8853e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.71s/it] 50%|█████     | 2/4 [00:04<00:04,  2.20s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.71s/it]100%|██████████| 4/4 [00:07<00:00,  1.46s/it]100%|██████████| 4/4 [00:07<00:00,  1.79s/it]
=> result
* total: 1,934
* correct: 1,690
* accuracy: 87.4%
* error: 12.6%
* macro_f1: 86.6%
Elapsed: 0:03:59
Run this job and save the output to output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  50
# train_x  800
# val      200
# test     1,849
---------  ------
['Lunges', 'Military_Parade', 'Mixing', 'Mopping_Floor', 'Nunchucks', 'Parallel_Bars', 'Pizza_Tossing', 'Playing_Cello', 'Playing_Daf', 'Playing_Dhol', 'Playing_Flute', 'Playing_Guitar', 'Playing_Piano', 'Playing_Sitar', 'Playing_Tabla', 'Playing_Violin', 'Pole_Vault', 'Pommel_Horse', 'Pull_Ups', 'Punch', 'Push_Ups', 'Rafting', 'Rock_Climbing_Indoor', 'Rope_Climbing', 'Rowing', 'Salsa_Spin', 'Shaving_Beard', 'Shotput', 'Skate_Boarding', 'Skiing', 'Skijet', 'Sky_Diving', 'Soccer_Juggling', 'Soccer_Penalty', 'Still_Rings', 'Sumo_Wrestling', 'Surfing', 'Swing', 'Table_Tennis_Shot', 'Tai_Chi', 'Tennis_Swing', 'Throw_Discus', 'Trampoline_Jumping', 'Typing', 'Uneven_Bars', 'Volleyball_Spiking', 'Walking_With_Dog', 'Wall_Pushups', 'Writing_On_Board', 'Yo_Yo']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Lunges.', 'a photo of a person doing Military Parade.', 'a photo of a person doing Mixing.', 'a photo of a person doing Mopping Floor.', 'a photo of a person doing Nunchucks.', 'a photo of a person doing Parallel Bars.', 'a photo of a person doing Pizza Tossing.', 'a photo of a person doing Playing Cello.', 'a photo of a person doing Playing Daf.', 'a photo of a person doing Playing Dhol.', 'a photo of a person doing Playing Flute.', 'a photo of a person doing Playing Guitar.', 'a photo of a person doing Playing Piano.', 'a photo of a person doing Playing Sitar.', 'a photo of a person doing Playing Tabla.', 'a photo of a person doing Playing Violin.', 'a photo of a person doing Pole Vault.', 'a photo of a person doing Pommel Horse.', 'a photo of a person doing Pull Ups.', 'a photo of a person doing Punch.', 'a photo of a person doing Push Ups.', 'a photo of a person doing Rafting.', 'a photo of a person doing Rock Climbing Indoor.', 'a photo of a person doing Rope Climbing.', 'a photo of a person doing Rowing.', 'a photo of a person doing Salsa Spin.', 'a photo of a person doing Shaving Beard.', 'a photo of a person doing Shotput.', 'a photo of a person doing Skate Boarding.', 'a photo of a person doing Skiing.', 'a photo of a person doing Skijet.', 'a photo of a person doing Sky Diving.', 'a photo of a person doing Soccer Juggling.', 'a photo of a person doing Soccer Penalty.', 'a photo of a person doing Still Rings.', 'a photo of a person doing Sumo Wrestling.', 'a photo of a person doing Surfing.', 'a photo of a person doing Swing.', 'a photo of a person doing Table Tennis Shot.', 'a photo of a person doing Tai Chi.', 'a photo of a person doing Tennis Swing.', 'a photo of a person doing Throw Discus.', 'a photo of a person doing Trampoline Jumping.', 'a photo of a person doing Typing.', 'a photo of a person doing Uneven Bars.', 'a photo of a person doing Volleyball Spiking.', 'a photo of a person doing Walking With Dog.', 'a photo of a person doing Wall Pushups.', 'a photo of a person doing Writing On Board.', 'a photo of a person doing Yo Yo.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.84s/it] 50%|█████     | 2/4 [00:04<00:04,  2.24s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.73s/it]100%|██████████| 4/4 [00:06<00:00,  1.39s/it]100%|██████████| 4/4 [00:07<00:00,  1.77s/it]
=> result
* total: 1,849
* correct: 1,485
* accuracy: 80.3%
* error: 19.7%
* macro_f1: 78.9%
Run this job and save the output to output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  50
# train_x  800
# val      200
# test     1,849
---------  ------
['Lunges', 'Military_Parade', 'Mixing', 'Mopping_Floor', 'Nunchucks', 'Parallel_Bars', 'Pizza_Tossing', 'Playing_Cello', 'Playing_Daf', 'Playing_Dhol', 'Playing_Flute', 'Playing_Guitar', 'Playing_Piano', 'Playing_Sitar', 'Playing_Tabla', 'Playing_Violin', 'Pole_Vault', 'Pommel_Horse', 'Pull_Ups', 'Punch', 'Push_Ups', 'Rafting', 'Rock_Climbing_Indoor', 'Rope_Climbing', 'Rowing', 'Salsa_Spin', 'Shaving_Beard', 'Shotput', 'Skate_Boarding', 'Skiing', 'Skijet', 'Sky_Diving', 'Soccer_Juggling', 'Soccer_Penalty', 'Still_Rings', 'Sumo_Wrestling', 'Surfing', 'Swing', 'Table_Tennis_Shot', 'Tai_Chi', 'Tennis_Swing', 'Throw_Discus', 'Trampoline_Jumping', 'Typing', 'Uneven_Bars', 'Volleyball_Spiking', 'Walking_With_Dog', 'Wall_Pushups', 'Writing_On_Board', 'Yo_Yo']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Lunges.', 'a photo of a person doing Military Parade.', 'a photo of a person doing Mixing.', 'a photo of a person doing Mopping Floor.', 'a photo of a person doing Nunchucks.', 'a photo of a person doing Parallel Bars.', 'a photo of a person doing Pizza Tossing.', 'a photo of a person doing Playing Cello.', 'a photo of a person doing Playing Daf.', 'a photo of a person doing Playing Dhol.', 'a photo of a person doing Playing Flute.', 'a photo of a person doing Playing Guitar.', 'a photo of a person doing Playing Piano.', 'a photo of a person doing Playing Sitar.', 'a photo of a person doing Playing Tabla.', 'a photo of a person doing Playing Violin.', 'a photo of a person doing Pole Vault.', 'a photo of a person doing Pommel Horse.', 'a photo of a person doing Pull Ups.', 'a photo of a person doing Punch.', 'a photo of a person doing Push Ups.', 'a photo of a person doing Rafting.', 'a photo of a person doing Rock Climbing Indoor.', 'a photo of a person doing Rope Climbing.', 'a photo of a person doing Rowing.', 'a photo of a person doing Salsa Spin.', 'a photo of a person doing Shaving Beard.', 'a photo of a person doing Shotput.', 'a photo of a person doing Skate Boarding.', 'a photo of a person doing Skiing.', 'a photo of a person doing Skijet.', 'a photo of a person doing Sky Diving.', 'a photo of a person doing Soccer Juggling.', 'a photo of a person doing Soccer Penalty.', 'a photo of a person doing Still Rings.', 'a photo of a person doing Sumo Wrestling.', 'a photo of a person doing Surfing.', 'a photo of a person doing Swing.', 'a photo of a person doing Table Tennis Shot.', 'a photo of a person doing Tai Chi.', 'a photo of a person doing Tennis Swing.', 'a photo of a person doing Throw Discus.', 'a photo of a person doing Trampoline Jumping.', 'a photo of a person doing Typing.', 'a photo of a person doing Uneven Bars.', 'a photo of a person doing Volleyball Spiking.', 'a photo of a person doing Walking With Dog.', 'a photo of a person doing Wall Pushups.', 'a photo of a person doing Writing On Board.', 'a photo of a person doing Yo Yo.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.95s/it] 50%|█████     | 2/4 [00:05<00:04,  2.29s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.76s/it]100%|██████████| 4/4 [00:07<00:00,  1.40s/it]100%|██████████| 4/4 [00:07<00:00,  1.79s/it]
=> result
* total: 1,849
* correct: 1,476
* accuracy: 79.8%
* error: 20.2%
* macro_f1: 78.2%
Run this job and save the output to output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/ucf101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: UCF101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: UCF101
Reading split from /data/yht/data/cl/data/ucf101/split_zhou_UCF101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/ucf101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ------
Dataset    UCF101
# classes  50
# train_x  800
# val      200
# test     1,849
---------  ------
['Lunges', 'Military_Parade', 'Mixing', 'Mopping_Floor', 'Nunchucks', 'Parallel_Bars', 'Pizza_Tossing', 'Playing_Cello', 'Playing_Daf', 'Playing_Dhol', 'Playing_Flute', 'Playing_Guitar', 'Playing_Piano', 'Playing_Sitar', 'Playing_Tabla', 'Playing_Violin', 'Pole_Vault', 'Pommel_Horse', 'Pull_Ups', 'Punch', 'Push_Ups', 'Rafting', 'Rock_Climbing_Indoor', 'Rope_Climbing', 'Rowing', 'Salsa_Spin', 'Shaving_Beard', 'Shotput', 'Skate_Boarding', 'Skiing', 'Skijet', 'Sky_Diving', 'Soccer_Juggling', 'Soccer_Penalty', 'Still_Rings', 'Sumo_Wrestling', 'Surfing', 'Swing', 'Table_Tennis_Shot', 'Tai_Chi', 'Tennis_Swing', 'Throw_Discus', 'Trampoline_Jumping', 'Typing', 'Uneven_Bars', 'Volleyball_Spiking', 'Walking_With_Dog', 'Wall_Pushups', 'Writing_On_Board', 'Yo_Yo']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['a photo of a person doing Lunges.', 'a photo of a person doing Military Parade.', 'a photo of a person doing Mixing.', 'a photo of a person doing Mopping Floor.', 'a photo of a person doing Nunchucks.', 'a photo of a person doing Parallel Bars.', 'a photo of a person doing Pizza Tossing.', 'a photo of a person doing Playing Cello.', 'a photo of a person doing Playing Daf.', 'a photo of a person doing Playing Dhol.', 'a photo of a person doing Playing Flute.', 'a photo of a person doing Playing Guitar.', 'a photo of a person doing Playing Piano.', 'a photo of a person doing Playing Sitar.', 'a photo of a person doing Playing Tabla.', 'a photo of a person doing Playing Violin.', 'a photo of a person doing Pole Vault.', 'a photo of a person doing Pommel Horse.', 'a photo of a person doing Pull Ups.', 'a photo of a person doing Punch.', 'a photo of a person doing Push Ups.', 'a photo of a person doing Rafting.', 'a photo of a person doing Rock Climbing Indoor.', 'a photo of a person doing Rope Climbing.', 'a photo of a person doing Rowing.', 'a photo of a person doing Salsa Spin.', 'a photo of a person doing Shaving Beard.', 'a photo of a person doing Shotput.', 'a photo of a person doing Skate Boarding.', 'a photo of a person doing Skiing.', 'a photo of a person doing Skijet.', 'a photo of a person doing Sky Diving.', 'a photo of a person doing Soccer Juggling.', 'a photo of a person doing Soccer Penalty.', 'a photo of a person doing Still Rings.', 'a photo of a person doing Sumo Wrestling.', 'a photo of a person doing Surfing.', 'a photo of a person doing Swing.', 'a photo of a person doing Table Tennis Shot.', 'a photo of a person doing Tai Chi.', 'a photo of a person doing Tennis Swing.', 'a photo of a person doing Throw Discus.', 'a photo of a person doing Trampoline Jumping.', 'a photo of a person doing Typing.', 'a photo of a person doing Uneven Bars.', 'a photo of a person doing Volleyball Spiking.', 'a photo of a person doing Walking With Dog.', 'a photo of a person doing Wall Pushups.', 'a photo of a person doing Writing On Board.', 'a photo of a person doing Yo Yo.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/ucf101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:10,  3.66s/it] 50%|█████     | 2/4 [00:04<00:04,  2.17s/it] 75%|███████▌  | 3/4 [00:05<00:01,  1.69s/it]100%|██████████| 4/4 [00:06<00:00,  1.36s/it]100%|██████████| 4/4 [00:06<00:00,  1.73s/it]
=> result
* total: 1,849
* correct: 1,489
* accuracy: 80.5%
* error: 19.5%
* macro_f1: 79.0%
Run this job and save the output to output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     1,549
---------  ----------
['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X face.', 'X X X X leopard.', 'X X X X motorbike.', 'X X X X accordion.', 'X X X X airplane.', 'X X X X anchor.', 'X X X X ant.', 'X X X X barrel.', 'X X X X bass.', 'X X X X beaver.', 'X X X X binocular.', 'X X X X bonsai.', 'X X X X brain.', 'X X X X brontosaurus.', 'X X X X buddha.', 'X X X X butterfly.', 'X X X X camera.', 'X X X X cannon.', 'X X X X car side.', 'X X X X ceiling fan.', 'X X X X cellphone.', 'X X X X chair.', 'X X X X chandelier.', 'X X X X cougar body.', 'X X X X cougar face.', 'X X X X crab.', 'X X X X crayfish.', 'X X X X crocodile.', 'X X X X crocodile head.', 'X X X X cup.', 'X X X X dalmatian.', 'X X X X dollar bill.', 'X X X X dolphin.', 'X X X X dragonfly.', 'X X X X electric guitar.', 'X X X X elephant.', 'X X X X emu.', 'X X X X euphonium.', 'X X X X ewer.', 'X X X X ferry.', 'X X X X flamingo.', 'X X X X flamingo head.', 'X X X X garfield.', 'X X X X gerenuk.', 'X X X X gramophone.', 'X X X X grand piano.', 'X X X X hawksbill.', 'X X X X headphone.', 'X X X X hedgehog.', 'X X X X helicopter.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/tensorboard)
epoch [1/50] batch [5/25] time 0.152 (0.312) data 0.000 (0.149) loss 5.1836 (5.5234) acc 71.8750 (65.6250) lr 1.0000e-05 eta 0:06:28
epoch [1/50] batch [10/25] time 0.151 (0.232) data 0.000 (0.075) loss 4.7578 (5.2934) acc 81.2500 (70.6250) lr 1.0000e-05 eta 0:04:47
epoch [1/50] batch [15/25] time 0.152 (0.205) data 0.000 (0.050) loss 4.7656 (5.2826) acc 81.2500 (70.0000) lr 1.0000e-05 eta 0:04:13
epoch [1/50] batch [20/25] time 0.151 (0.192) data 0.000 (0.037) loss 4.6016 (5.2283) acc 78.1250 (70.1562) lr 1.0000e-05 eta 0:03:55
epoch [1/50] batch [25/25] time 0.151 (0.184) data 0.000 (0.030) loss 5.2539 (5.2088) acc 71.8750 (70.7500) lr 2.0000e-03 eta 0:03:45
epoch [2/50] batch [5/25] time 0.151 (0.280) data 0.000 (0.127) loss 3.4004 (4.1996) acc 84.3750 (77.5000) lr 2.0000e-03 eta 0:05:41
epoch [2/50] batch [10/25] time 0.151 (0.216) data 0.000 (0.064) loss 3.0352 (3.7447) acc 87.5000 (80.3125) lr 2.0000e-03 eta 0:04:22
epoch [2/50] batch [15/25] time 0.151 (0.194) data 0.000 (0.043) loss 3.1172 (3.4138) acc 81.2500 (82.9167) lr 2.0000e-03 eta 0:03:54
epoch [2/50] batch [20/25] time 0.151 (0.183) data 0.000 (0.032) loss 3.1641 (3.3329) acc 75.0000 (81.2500) lr 2.0000e-03 eta 0:03:40
epoch [2/50] batch [25/25] time 0.161 (0.177) data 0.000 (0.026) loss 2.7969 (3.1854) acc 81.2500 (81.8750) lr 1.9980e-03 eta 0:03:32
epoch [3/50] batch [5/25] time 0.153 (0.276) data 0.000 (0.121) loss 2.9941 (2.5891) acc 78.1250 (82.5000) lr 1.9980e-03 eta 0:05:29
epoch [3/50] batch [10/25] time 0.152 (0.214) data 0.000 (0.061) loss 2.6758 (2.5426) acc 84.3750 (82.1875) lr 1.9980e-03 eta 0:04:15
epoch [3/50] batch [15/25] time 0.156 (0.194) data 0.000 (0.041) loss 2.3848 (2.4525) acc 81.2500 (83.5417) lr 1.9980e-03 eta 0:03:49
epoch [3/50] batch [20/25] time 0.154 (0.184) data 0.000 (0.030) loss 2.3711 (2.3855) acc 81.2500 (84.3750) lr 1.9980e-03 eta 0:03:37
epoch [3/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.024) loss 2.5781 (2.3823) acc 81.2500 (84.1250) lr 1.9921e-03 eta 0:03:29
epoch [4/50] batch [5/25] time 0.152 (0.281) data 0.000 (0.128) loss 2.1211 (1.7875) acc 90.6250 (91.2500) lr 1.9921e-03 eta 0:05:28
epoch [4/50] batch [10/25] time 0.152 (0.217) data 0.000 (0.064) loss 2.1484 (1.9463) acc 81.2500 (88.1250) lr 1.9921e-03 eta 0:04:12
epoch [4/50] batch [15/25] time 0.154 (0.195) data 0.000 (0.043) loss 2.1836 (2.0031) acc 84.3750 (86.0417) lr 1.9921e-03 eta 0:03:46
epoch [4/50] batch [20/25] time 0.152 (0.185) data 0.000 (0.032) loss 2.2988 (1.9832) acc 84.3750 (86.7188) lr 1.9921e-03 eta 0:03:33
epoch [4/50] batch [25/25] time 0.152 (0.178) data 0.000 (0.026) loss 1.5742 (2.0099) acc 93.7500 (86.5000) lr 1.9823e-03 eta 0:03:24
epoch [5/50] batch [5/25] time 0.153 (0.272) data 0.000 (0.118) loss 2.0645 (1.8758) acc 84.3750 (88.1250) lr 1.9823e-03 eta 0:05:11
epoch [5/50] batch [10/25] time 0.152 (0.212) data 0.000 (0.059) loss 1.5977 (1.8550) acc 90.6250 (87.5000) lr 1.9823e-03 eta 0:04:02
epoch [5/50] batch [15/25] time 0.152 (0.192) data 0.000 (0.039) loss 2.4141 (1.8697) acc 81.2500 (86.6667) lr 1.9823e-03 eta 0:03:38
epoch [5/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.030) loss 1.9277 (1.8648) acc 87.5000 (87.1875) lr 1.9823e-03 eta 0:03:25
epoch [5/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.024) loss 1.5615 (1.8858) acc 90.6250 (87.0000) lr 1.9686e-03 eta 0:03:18
epoch [6/50] batch [5/25] time 0.156 (0.279) data 0.000 (0.122) loss 1.8262 (1.6869) acc 84.3750 (90.0000) lr 1.9686e-03 eta 0:05:12
epoch [6/50] batch [10/25] time 0.154 (0.216) data 0.000 (0.061) loss 1.6699 (1.7538) acc 93.7500 (88.1250) lr 1.9686e-03 eta 0:04:01
epoch [6/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.041) loss 2.3711 (1.7889) acc 75.0000 (87.2917) lr 1.9686e-03 eta 0:03:36
epoch [6/50] batch [20/25] time 0.152 (0.184) data 0.000 (0.031) loss 1.4219 (1.7924) acc 93.7500 (88.1250) lr 1.9686e-03 eta 0:03:23
epoch [6/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.025) loss 1.8789 (1.8067) acc 81.2500 (87.7500) lr 1.9511e-03 eta 0:03:15
epoch [7/50] batch [5/25] time 0.153 (0.329) data 0.000 (0.175) loss 1.9902 (1.7678) acc 84.3750 (88.1250) lr 1.9511e-03 eta 0:06:00
epoch [7/50] batch [10/25] time 0.152 (0.241) data 0.000 (0.088) loss 2.5957 (1.7958) acc 78.1250 (88.1250) lr 1.9511e-03 eta 0:04:22
epoch [7/50] batch [15/25] time 0.151 (0.211) data 0.000 (0.059) loss 1.5430 (1.7979) acc 84.3750 (86.6667) lr 1.9511e-03 eta 0:03:49
epoch [7/50] batch [20/25] time 0.151 (0.196) data 0.000 (0.044) loss 1.6387 (1.7805) acc 90.6250 (86.5625) lr 1.9511e-03 eta 0:03:31
epoch [7/50] batch [25/25] time 0.152 (0.187) data 0.000 (0.035) loss 1.7607 (1.7741) acc 87.5000 (86.6250) lr 1.9298e-03 eta 0:03:21
epoch [8/50] batch [5/25] time 0.153 (0.289) data 0.000 (0.134) loss 1.3643 (1.8777) acc 96.8750 (85.0000) lr 1.9298e-03 eta 0:05:08
epoch [8/50] batch [10/25] time 0.153 (0.221) data 0.000 (0.067) loss 1.8730 (1.8162) acc 84.3750 (86.8750) lr 1.9298e-03 eta 0:03:55
epoch [8/50] batch [15/25] time 0.152 (0.198) data 0.000 (0.045) loss 1.4004 (1.7042) acc 96.8750 (88.9583) lr 1.9298e-03 eta 0:03:29
epoch [8/50] batch [20/25] time 0.152 (0.187) data 0.000 (0.034) loss 1.8291 (1.7313) acc 81.2500 (88.2812) lr 1.9298e-03 eta 0:03:16
epoch [8/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.5635 (1.7419) acc 84.3750 (88.0000) lr 1.9048e-03 eta 0:03:08
epoch [9/50] batch [5/25] time 0.153 (0.310) data 0.000 (0.156) loss 1.9141 (1.7037) acc 90.6250 (88.7500) lr 1.9048e-03 eta 0:05:23
epoch [9/50] batch [10/25] time 0.152 (0.231) data 0.000 (0.078) loss 1.9238 (1.6740) acc 87.5000 (89.3750) lr 1.9048e-03 eta 0:04:00
epoch [9/50] batch [15/25] time 0.152 (0.205) data 0.000 (0.052) loss 1.9463 (1.7268) acc 84.3750 (87.5000) lr 1.9048e-03 eta 0:03:32
epoch [9/50] batch [20/25] time 0.153 (0.192) data 0.000 (0.039) loss 2.0059 (1.7592) acc 87.5000 (87.3438) lr 1.9048e-03 eta 0:03:17
epoch [9/50] batch [25/25] time 0.153 (0.184) data 0.000 (0.031) loss 1.8857 (1.7625) acc 84.3750 (87.3750) lr 1.8763e-03 eta 0:03:08
epoch [10/50] batch [5/25] time 0.154 (0.279) data 0.000 (0.124) loss 1.6357 (1.7480) acc 87.5000 (86.2500) lr 1.8763e-03 eta 0:04:44
epoch [10/50] batch [10/25] time 0.153 (0.216) data 0.000 (0.062) loss 1.8125 (1.6644) acc 84.3750 (86.8750) lr 1.8763e-03 eta 0:03:39
epoch [10/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.042) loss 1.8926 (1.7415) acc 96.8750 (87.2917) lr 1.8763e-03 eta 0:03:16
epoch [10/50] batch [20/25] time 0.155 (0.185) data 0.000 (0.031) loss 1.4570 (1.7540) acc 90.6250 (86.7188) lr 1.8763e-03 eta 0:03:05
epoch [10/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.025) loss 1.4453 (1.7274) acc 93.7500 (87.3750) lr 1.8443e-03 eta 0:02:58
epoch [11/50] batch [5/25] time 0.154 (0.271) data 0.000 (0.117) loss 1.2422 (1.6670) acc 90.6250 (86.2500) lr 1.8443e-03 eta 0:04:29
epoch [11/50] batch [10/25] time 0.153 (0.212) data 0.000 (0.058) loss 1.4121 (1.7283) acc 90.6250 (85.6250) lr 1.8443e-03 eta 0:03:29
epoch [11/50] batch [15/25] time 0.154 (0.193) data 0.000 (0.039) loss 2.0703 (1.7357) acc 87.5000 (86.2500) lr 1.8443e-03 eta 0:03:10
epoch [11/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.029) loss 1.6826 (1.6977) acc 90.6250 (87.9688) lr 1.8443e-03 eta 0:02:59
epoch [11/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.023) loss 1.7852 (1.7139) acc 87.5000 (87.7500) lr 1.8090e-03 eta 0:02:52
epoch [12/50] batch [5/25] time 0.155 (0.278) data 0.000 (0.123) loss 2.1602 (1.7580) acc 75.0000 (85.0000) lr 1.8090e-03 eta 0:04:30
epoch [12/50] batch [10/25] time 0.152 (0.216) data 0.000 (0.062) loss 1.7012 (1.6555) acc 87.5000 (88.1250) lr 1.8090e-03 eta 0:03:28
epoch [12/50] batch [15/25] time 0.154 (0.195) data 0.000 (0.041) loss 1.2295 (1.6361) acc 96.8750 (88.9583) lr 1.8090e-03 eta 0:03:06
epoch [12/50] batch [20/25] time 0.154 (0.184) data 0.000 (0.031) loss 1.7832 (1.6219) acc 90.6250 (89.2188) lr 1.8090e-03 eta 0:02:56
epoch [12/50] batch [25/25] time 0.155 (0.178) data 0.000 (0.025) loss 1.4766 (1.6389) acc 90.6250 (89.1250) lr 1.7705e-03 eta 0:02:49
epoch [13/50] batch [5/25] time 0.154 (0.297) data 0.000 (0.142) loss 1.9268 (1.7611) acc 81.2500 (85.6250) lr 1.7705e-03 eta 0:04:40
epoch [13/50] batch [10/25] time 0.153 (0.225) data 0.000 (0.071) loss 1.5508 (1.7932) acc 87.5000 (85.9375) lr 1.7705e-03 eta 0:03:31
epoch [13/50] batch [15/25] time 0.154 (0.201) data 0.000 (0.048) loss 1.3887 (1.7207) acc 93.7500 (88.1250) lr 1.7705e-03 eta 0:03:08
epoch [13/50] batch [20/25] time 0.153 (0.189) data 0.000 (0.036) loss 1.5322 (1.6862) acc 90.6250 (88.1250) lr 1.7705e-03 eta 0:02:56
epoch [13/50] batch [25/25] time 0.153 (0.182) data 0.000 (0.029) loss 1.5801 (1.6442) acc 90.6250 (88.7500) lr 1.7290e-03 eta 0:02:48
epoch [14/50] batch [5/25] time 0.153 (0.310) data 0.000 (0.156) loss 1.1582 (1.5447) acc 100.0000 (90.6250) lr 1.7290e-03 eta 0:04:45
epoch [14/50] batch [10/25] time 0.153 (0.232) data 0.000 (0.078) loss 1.3516 (1.5241) acc 87.5000 (89.3750) lr 1.7290e-03 eta 0:03:32
epoch [14/50] batch [15/25] time 0.155 (0.206) data 0.000 (0.052) loss 1.5322 (1.5587) acc 90.6250 (90.0000) lr 1.7290e-03 eta 0:03:07
epoch [14/50] batch [20/25] time 0.154 (0.193) data 0.001 (0.039) loss 1.0186 (1.5822) acc 100.0000 (89.2188) lr 1.7290e-03 eta 0:02:54
epoch [14/50] batch [25/25] time 0.154 (0.185) data 0.000 (0.031) loss 1.2891 (1.5605) acc 93.7500 (89.2500) lr 1.6845e-03 eta 0:02:46
epoch [15/50] batch [5/25] time 0.152 (0.304) data 0.000 (0.150) loss 1.9492 (1.8271) acc 81.2500 (86.2500) lr 1.6845e-03 eta 0:04:31
epoch [15/50] batch [10/25] time 0.152 (0.228) data 0.000 (0.075) loss 1.6162 (1.7791) acc 81.2500 (85.3125) lr 1.6845e-03 eta 0:03:23
epoch [15/50] batch [15/25] time 0.156 (0.204) data 0.000 (0.050) loss 1.9189 (1.6930) acc 87.5000 (86.6667) lr 1.6845e-03 eta 0:03:00
epoch [15/50] batch [20/25] time 0.154 (0.191) data 0.000 (0.038) loss 1.7529 (1.6867) acc 87.5000 (87.1875) lr 1.6845e-03 eta 0:02:48
epoch [15/50] batch [25/25] time 0.153 (0.184) data 0.000 (0.030) loss 1.3623 (1.6629) acc 87.5000 (87.1250) lr 1.6374e-03 eta 0:02:40
epoch [16/50] batch [5/25] time 0.153 (0.282) data 0.000 (0.128) loss 1.3291 (1.5211) acc 90.6250 (89.3750) lr 1.6374e-03 eta 0:04:05
epoch [16/50] batch [10/25] time 0.154 (0.218) data 0.000 (0.064) loss 1.7080 (1.5332) acc 90.6250 (90.0000) lr 1.6374e-03 eta 0:03:08
epoch [16/50] batch [15/25] time 0.154 (0.197) data 0.000 (0.043) loss 1.1846 (1.5378) acc 93.7500 (90.4167) lr 1.6374e-03 eta 0:02:49
epoch [16/50] batch [20/25] time 0.153 (0.186) data 0.000 (0.032) loss 1.8164 (1.6183) acc 84.3750 (89.0625) lr 1.6374e-03 eta 0:02:39
epoch [16/50] batch [25/25] time 0.154 (0.179) data 0.000 (0.026) loss 1.6992 (1.6225) acc 87.5000 (88.6250) lr 1.5878e-03 eta 0:02:32
epoch [17/50] batch [5/25] time 0.155 (0.282) data 0.000 (0.127) loss 1.4824 (1.7059) acc 87.5000 (85.6250) lr 1.5878e-03 eta 0:03:58
epoch [17/50] batch [10/25] time 0.155 (0.218) data 0.000 (0.064) loss 1.3203 (1.5999) acc 93.7500 (87.1875) lr 1.5878e-03 eta 0:03:03
epoch [17/50] batch [15/25] time 0.156 (0.197) data 0.000 (0.043) loss 1.8848 (1.6157) acc 87.5000 (87.7083) lr 1.5878e-03 eta 0:02:44
epoch [17/50] batch [20/25] time 0.154 (0.186) data 0.000 (0.032) loss 1.6133 (1.5394) acc 87.5000 (89.0625) lr 1.5878e-03 eta 0:02:34
epoch [17/50] batch [25/25] time 0.153 (0.180) data 0.000 (0.026) loss 1.6084 (1.5231) acc 90.6250 (89.1250) lr 1.5358e-03 eta 0:02:28
epoch [18/50] batch [5/25] time 0.153 (0.284) data 0.000 (0.130) loss 1.7051 (1.7162) acc 87.5000 (83.7500) lr 1.5358e-03 eta 0:03:52
epoch [18/50] batch [10/25] time 0.153 (0.218) data 0.000 (0.065) loss 1.6084 (1.6336) acc 87.5000 (85.3125) lr 1.5358e-03 eta 0:02:57
epoch [18/50] batch [15/25] time 0.153 (0.197) data 0.000 (0.043) loss 1.3418 (1.5326) acc 93.7500 (87.9167) lr 1.5358e-03 eta 0:02:39
epoch [18/50] batch [20/25] time 0.153 (0.186) data 0.000 (0.033) loss 2.0645 (1.5875) acc 84.3750 (87.0312) lr 1.5358e-03 eta 0:02:29
epoch [18/50] batch [25/25] time 0.153 (0.179) data 0.000 (0.026) loss 1.2803 (1.6062) acc 96.8750 (87.7500) lr 1.4818e-03 eta 0:02:23
epoch [19/50] batch [5/25] time 0.154 (0.307) data 0.000 (0.153) loss 1.5020 (1.5600) acc 90.6250 (89.3750) lr 1.4818e-03 eta 0:04:04
epoch [19/50] batch [10/25] time 0.157 (0.231) data 0.000 (0.076) loss 2.1484 (1.8167) acc 81.2500 (85.9375) lr 1.4818e-03 eta 0:03:02
epoch [19/50] batch [15/25] time 0.154 (0.205) data 0.000 (0.051) loss 1.9922 (1.8056) acc 90.6250 (86.2500) lr 1.4818e-03 eta 0:02:41
epoch [19/50] batch [20/25] time 0.154 (0.193) data 0.000 (0.038) loss 1.3379 (1.7008) acc 87.5000 (87.5000) lr 1.4818e-03 eta 0:02:30
epoch [19/50] batch [25/25] time 0.153 (0.185) data 0.000 (0.031) loss 1.8135 (1.6954) acc 87.5000 (87.3750) lr 1.4258e-03 eta 0:02:23
epoch [20/50] batch [5/25] time 0.153 (0.300) data 0.000 (0.146) loss 1.9150 (1.5285) acc 87.5000 (89.3750) lr 1.4258e-03 eta 0:03:51
epoch [20/50] batch [10/25] time 0.154 (0.227) data 0.000 (0.073) loss 2.1738 (1.5083) acc 84.3750 (89.3750) lr 1.4258e-03 eta 0:02:53
epoch [20/50] batch [15/25] time 0.153 (0.203) data 0.000 (0.049) loss 1.3047 (1.5138) acc 90.6250 (89.1667) lr 1.4258e-03 eta 0:02:34
epoch [20/50] batch [20/25] time 0.155 (0.191) data 0.000 (0.037) loss 2.0430 (1.5790) acc 78.1250 (87.9688) lr 1.4258e-03 eta 0:02:23
epoch [20/50] batch [25/25] time 0.153 (0.183) data 0.000 (0.029) loss 1.5254 (1.5866) acc 90.6250 (87.6250) lr 1.3681e-03 eta 0:02:17
epoch [21/50] batch [5/25] time 0.154 (0.304) data 0.000 (0.150) loss 1.1992 (1.2518) acc 100.0000 (94.3750) lr 1.3681e-03 eta 0:03:46
epoch [21/50] batch [10/25] time 0.153 (0.229) data 0.000 (0.075) loss 1.8301 (1.5553) acc 81.2500 (89.3750) lr 1.3681e-03 eta 0:02:49
epoch [21/50] batch [15/25] time 0.153 (0.204) data 0.000 (0.050) loss 2.0938 (1.5984) acc 81.2500 (89.1667) lr 1.3681e-03 eta 0:02:29
epoch [21/50] batch [20/25] time 0.153 (0.191) data 0.000 (0.038) loss 1.4141 (1.5693) acc 90.6250 (89.6875) lr 1.3681e-03 eta 0:02:19
epoch [21/50] batch [25/25] time 0.153 (0.184) data 0.000 (0.030) loss 1.5059 (1.5657) acc 87.5000 (89.1250) lr 1.3090e-03 eta 0:02:13
epoch [22/50] batch [5/25] time 0.154 (0.286) data 0.000 (0.131) loss 1.5156 (1.4162) acc 93.7500 (90.0000) lr 1.3090e-03 eta 0:03:25
epoch [22/50] batch [10/25] time 0.155 (0.220) data 0.000 (0.066) loss 1.2773 (1.4967) acc 96.8750 (89.3750) lr 1.3090e-03 eta 0:02:37
epoch [22/50] batch [15/25] time 0.154 (0.198) data 0.000 (0.044) loss 1.4541 (1.4831) acc 93.7500 (90.8333) lr 1.3090e-03 eta 0:02:20
epoch [22/50] batch [20/25] time 0.153 (0.187) data 0.000 (0.033) loss 1.3340 (1.4406) acc 93.7500 (91.7188) lr 1.3090e-03 eta 0:02:11
epoch [22/50] batch [25/25] time 0.153 (0.180) data 0.000 (0.026) loss 2.3164 (1.5009) acc 78.1250 (90.7500) lr 1.2487e-03 eta 0:02:06
epoch [23/50] batch [5/25] time 0.154 (0.284) data 0.000 (0.129) loss 2.3066 (1.6643) acc 81.2500 (87.5000) lr 1.2487e-03 eta 0:03:17
epoch [23/50] batch [10/25] time 0.154 (0.219) data 0.000 (0.065) loss 1.4580 (1.6127) acc 87.5000 (87.8125) lr 1.2487e-03 eta 0:02:31
epoch [23/50] batch [15/25] time 0.154 (0.198) data 0.000 (0.043) loss 1.4131 (1.5600) acc 87.5000 (87.7083) lr 1.2487e-03 eta 0:02:15
epoch [23/50] batch [20/25] time 0.155 (0.187) data 0.000 (0.033) loss 2.0449 (1.5466) acc 87.5000 (88.2812) lr 1.2487e-03 eta 0:02:07
epoch [23/50] batch [25/25] time 0.154 (0.180) data 0.000 (0.026) loss 1.5293 (1.5155) acc 90.6250 (88.6250) lr 1.1874e-03 eta 0:02:01
epoch [24/50] batch [5/25] time 0.154 (0.321) data 0.000 (0.166) loss 1.3252 (1.6260) acc 87.5000 (89.3750) lr 1.1874e-03 eta 0:03:35
epoch [24/50] batch [10/25] time 0.154 (0.238) data 0.000 (0.083) loss 1.5684 (1.4850) acc 87.5000 (90.3125) lr 1.1874e-03 eta 0:02:37
epoch [24/50] batch [15/25] time 0.158 (0.210) data 0.000 (0.055) loss 1.3916 (1.5393) acc 87.5000 (89.3750) lr 1.1874e-03 eta 0:02:18
epoch [24/50] batch [20/25] time 0.154 (0.196) data 0.000 (0.042) loss 1.9277 (1.5498) acc 78.1250 (89.0625) lr 1.1874e-03 eta 0:02:08
epoch [24/50] batch [25/25] time 0.153 (0.187) data 0.000 (0.033) loss 1.3848 (1.5371) acc 96.8750 (89.0000) lr 1.1253e-03 eta 0:02:01
epoch [25/50] batch [5/25] time 0.154 (0.321) data 0.000 (0.167) loss 1.4297 (1.4762) acc 93.7500 (90.6250) lr 1.1253e-03 eta 0:03:27
epoch [25/50] batch [10/25] time 0.153 (0.238) data 0.000 (0.084) loss 1.5879 (1.4901) acc 93.7500 (90.9375) lr 1.1253e-03 eta 0:02:32
epoch [25/50] batch [15/25] time 0.153 (0.210) data 0.000 (0.056) loss 1.4570 (1.5036) acc 90.6250 (91.0417) lr 1.1253e-03 eta 0:02:13
epoch [25/50] batch [20/25] time 0.155 (0.196) data 0.000 (0.042) loss 1.6299 (1.4812) acc 93.7500 (91.4062) lr 1.1253e-03 eta 0:02:03
epoch [25/50] batch [25/25] time 0.152 (0.187) data 0.000 (0.034) loss 1.1035 (1.4490) acc 90.6250 (91.2500) lr 1.0628e-03 eta 0:01:57
epoch [26/50] batch [5/25] time 0.153 (0.306) data 0.000 (0.152) loss 1.3574 (1.3465) acc 93.7500 (93.1250) lr 1.0628e-03 eta 0:03:09
epoch [26/50] batch [10/25] time 0.153 (0.230) data 0.000 (0.076) loss 1.4297 (1.4701) acc 87.5000 (91.5625) lr 1.0628e-03 eta 0:02:21
epoch [26/50] batch [15/25] time 0.153 (0.205) data 0.000 (0.051) loss 1.8486 (1.5204) acc 87.5000 (90.0000) lr 1.0628e-03 eta 0:02:04
epoch [26/50] batch [20/25] time 0.155 (0.192) data 0.000 (0.038) loss 1.9297 (1.5098) acc 78.1250 (89.3750) lr 1.0628e-03 eta 0:01:56
epoch [26/50] batch [25/25] time 0.153 (0.185) data 0.000 (0.031) loss 0.9941 (1.5028) acc 96.8750 (89.3750) lr 1.0000e-03 eta 0:01:50
epoch [27/50] batch [5/25] time 0.154 (0.312) data 0.000 (0.158) loss 1.3145 (1.1805) acc 96.8750 (96.2500) lr 1.0000e-03 eta 0:03:05
epoch [27/50] batch [10/25] time 0.154 (0.233) data 0.000 (0.079) loss 1.1816 (1.2227) acc 93.7500 (94.3750) lr 1.0000e-03 eta 0:02:17
epoch [27/50] batch [15/25] time 0.153 (0.206) data 0.000 (0.053) loss 1.8438 (1.3592) acc 84.3750 (92.2917) lr 1.0000e-03 eta 0:02:00
epoch [27/50] batch [20/25] time 0.154 (0.193) data 0.000 (0.040) loss 1.3262 (1.3990) acc 93.7500 (91.7188) lr 1.0000e-03 eta 0:01:52
epoch [27/50] batch [25/25] time 0.152 (0.185) data 0.000 (0.032) loss 1.6572 (1.4573) acc 87.5000 (90.5000) lr 9.3721e-04 eta 0:01:46
epoch [28/50] batch [5/25] time 0.153 (0.297) data 0.000 (0.140) loss 1.2139 (1.2225) acc 93.7500 (93.1250) lr 9.3721e-04 eta 0:02:49
epoch [28/50] batch [10/25] time 0.155 (0.226) data 0.000 (0.070) loss 1.6338 (1.3961) acc 87.5000 (91.2500) lr 9.3721e-04 eta 0:02:07
epoch [28/50] batch [15/25] time 0.154 (0.202) data 0.000 (0.047) loss 1.7666 (1.4611) acc 90.6250 (90.8333) lr 9.3721e-04 eta 0:01:53
epoch [28/50] batch [20/25] time 0.153 (0.190) data 0.000 (0.035) loss 1.7842 (1.5161) acc 81.2500 (89.5312) lr 9.3721e-04 eta 0:01:45
epoch [28/50] batch [25/25] time 0.152 (0.183) data 0.000 (0.028) loss 1.4785 (1.4854) acc 90.6250 (90.0000) lr 8.7467e-04 eta 0:01:40
epoch [29/50] batch [5/25] time 0.152 (0.287) data 0.000 (0.134) loss 1.5127 (1.4791) acc 90.6250 (90.0000) lr 8.7467e-04 eta 0:02:36
epoch [29/50] batch [10/25] time 0.153 (0.220) data 0.000 (0.067) loss 1.2148 (1.4956) acc 93.7500 (88.4375) lr 8.7467e-04 eta 0:01:58
epoch [29/50] batch [15/25] time 0.152 (0.197) data 0.000 (0.045) loss 1.6260 (1.5113) acc 87.5000 (88.5417) lr 8.7467e-04 eta 0:01:45
epoch [29/50] batch [20/25] time 0.155 (0.187) data 0.000 (0.034) loss 1.9404 (1.5379) acc 87.5000 (89.0625) lr 8.7467e-04 eta 0:01:38
epoch [29/50] batch [25/25] time 0.153 (0.180) data 0.000 (0.027) loss 1.2695 (1.5096) acc 93.7500 (89.7500) lr 8.1262e-04 eta 0:01:34
epoch [30/50] batch [5/25] time 0.153 (0.265) data 0.000 (0.111) loss 1.5098 (1.3183) acc 90.6250 (93.1250) lr 8.1262e-04 eta 0:02:17
epoch [30/50] batch [10/25] time 0.153 (0.209) data 0.000 (0.056) loss 1.0088 (1.2428) acc 100.0000 (95.0000) lr 8.1262e-04 eta 0:01:47
epoch [30/50] batch [15/25] time 0.152 (0.190) data 0.000 (0.037) loss 1.4287 (1.3326) acc 93.7500 (93.1250) lr 8.1262e-04 eta 0:01:36
epoch [30/50] batch [20/25] time 0.152 (0.181) data 0.000 (0.028) loss 2.0527 (1.3996) acc 81.2500 (91.5625) lr 8.1262e-04 eta 0:01:31
epoch [30/50] batch [25/25] time 0.152 (0.175) data 0.000 (0.022) loss 1.8516 (1.4430) acc 87.5000 (90.8750) lr 7.5131e-04 eta 0:01:27
epoch [31/50] batch [5/25] time 0.153 (0.288) data 0.000 (0.134) loss 1.3184 (1.5203) acc 96.8750 (90.0000) lr 7.5131e-04 eta 0:02:22
epoch [31/50] batch [10/25] time 0.153 (0.221) data 0.000 (0.067) loss 1.9297 (1.5179) acc 84.3750 (88.7500) lr 7.5131e-04 eta 0:01:48
epoch [31/50] batch [15/25] time 0.152 (0.198) data 0.000 (0.045) loss 1.6406 (1.4120) acc 81.2500 (90.4167) lr 7.5131e-04 eta 0:01:36
epoch [31/50] batch [20/25] time 0.153 (0.187) data 0.000 (0.034) loss 1.5625 (1.4052) acc 87.5000 (91.0938) lr 7.5131e-04 eta 0:01:29
epoch [31/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.5674 (1.4172) acc 90.6250 (91.2500) lr 6.9098e-04 eta 0:01:25
epoch [32/50] batch [5/25] time 0.152 (0.289) data 0.000 (0.137) loss 1.9004 (1.6650) acc 78.1250 (86.2500) lr 6.9098e-04 eta 0:02:15
epoch [32/50] batch [10/25] time 0.152 (0.221) data 0.000 (0.068) loss 1.3691 (1.5082) acc 93.7500 (88.4375) lr 6.9098e-04 eta 0:01:42
epoch [32/50] batch [15/25] time 0.152 (0.198) data 0.000 (0.046) loss 1.5342 (1.5189) acc 93.7500 (88.3333) lr 6.9098e-04 eta 0:01:30
epoch [32/50] batch [20/25] time 0.154 (0.186) data 0.000 (0.034) loss 1.9814 (1.5692) acc 78.1250 (87.8125) lr 6.9098e-04 eta 0:01:24
epoch [32/50] batch [25/25] time 0.154 (0.180) data 0.000 (0.027) loss 1.5625 (1.5346) acc 90.6250 (88.7500) lr 6.3188e-04 eta 0:01:20
epoch [33/50] batch [5/25] time 0.155 (0.297) data 0.000 (0.142) loss 1.7988 (1.4482) acc 87.5000 (92.5000) lr 6.3188e-04 eta 0:02:11
epoch [33/50] batch [10/25] time 0.153 (0.225) data 0.000 (0.071) loss 2.0352 (1.5586) acc 84.3750 (89.6875) lr 6.3188e-04 eta 0:01:38
epoch [33/50] batch [15/25] time 0.152 (0.201) data 0.000 (0.047) loss 1.3418 (1.5160) acc 93.7500 (90.0000) lr 6.3188e-04 eta 0:01:27
epoch [33/50] batch [20/25] time 0.152 (0.188) data 0.000 (0.036) loss 2.1641 (1.5918) acc 78.1250 (88.4375) lr 6.3188e-04 eta 0:01:21
epoch [33/50] batch [25/25] time 0.152 (0.181) data 0.000 (0.028) loss 1.0332 (1.5970) acc 93.7500 (88.1250) lr 5.7422e-04 eta 0:01:17
epoch [34/50] batch [5/25] time 0.152 (0.291) data 0.000 (0.138) loss 1.2910 (1.2920) acc 87.5000 (89.3750) lr 5.7422e-04 eta 0:02:02
epoch [34/50] batch [10/25] time 0.152 (0.222) data 0.000 (0.069) loss 1.2412 (1.4405) acc 87.5000 (89.3750) lr 5.7422e-04 eta 0:01:32
epoch [34/50] batch [15/25] time 0.152 (0.199) data 0.000 (0.046) loss 1.3789 (1.4456) acc 96.8750 (90.8333) lr 5.7422e-04 eta 0:01:21
epoch [34/50] batch [20/25] time 0.152 (0.187) data 0.000 (0.035) loss 1.1211 (1.4066) acc 96.8750 (90.9375) lr 5.7422e-04 eta 0:01:15
epoch [34/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.028) loss 1.2314 (1.4089) acc 93.7500 (91.3750) lr 5.1825e-04 eta 0:01:12
epoch [35/50] batch [5/25] time 0.153 (0.287) data 0.000 (0.134) loss 1.4219 (1.3408) acc 93.7500 (93.1250) lr 5.1825e-04 eta 0:01:53
epoch [35/50] batch [10/25] time 0.154 (0.220) data 0.000 (0.067) loss 1.4521 (1.3561) acc 93.7500 (92.8125) lr 5.1825e-04 eta 0:01:25
epoch [35/50] batch [15/25] time 0.152 (0.197) data 0.000 (0.045) loss 1.0967 (1.3232) acc 96.8750 (92.7083) lr 5.1825e-04 eta 0:01:15
epoch [35/50] batch [20/25] time 0.152 (0.186) data 0.000 (0.034) loss 1.5381 (1.3520) acc 93.7500 (92.8125) lr 5.1825e-04 eta 0:01:10
epoch [35/50] batch [25/25] time 0.153 (0.179) data 0.000 (0.027) loss 2.3184 (1.3722) acc 75.0000 (92.8750) lr 4.6417e-04 eta 0:01:07
epoch [36/50] batch [5/25] time 0.153 (0.298) data 0.000 (0.144) loss 1.2559 (1.4000) acc 93.7500 (89.3750) lr 4.6417e-04 eta 0:01:50
epoch [36/50] batch [10/25] time 0.152 (0.225) data 0.000 (0.072) loss 1.3506 (1.3889) acc 90.6250 (90.3125) lr 4.6417e-04 eta 0:01:22
epoch [36/50] batch [15/25] time 0.152 (0.201) data 0.000 (0.048) loss 1.4277 (1.4579) acc 93.7500 (89.3750) lr 4.6417e-04 eta 0:01:12
epoch [36/50] batch [20/25] time 0.152 (0.189) data 0.000 (0.036) loss 1.1787 (1.3990) acc 96.8750 (90.9375) lr 4.6417e-04 eta 0:01:06
epoch [36/50] batch [25/25] time 0.152 (0.181) data 0.000 (0.029) loss 1.3125 (1.4016) acc 87.5000 (90.8750) lr 4.1221e-04 eta 0:01:03
epoch [37/50] batch [5/25] time 0.154 (0.288) data 0.000 (0.133) loss 1.2676 (1.4514) acc 96.8750 (91.8750) lr 4.1221e-04 eta 0:01:39
epoch [37/50] batch [10/25] time 0.153 (0.221) data 0.000 (0.067) loss 1.4902 (1.4843) acc 87.5000 (90.3125) lr 4.1221e-04 eta 0:01:15
epoch [37/50] batch [15/25] time 0.153 (0.198) data 0.000 (0.044) loss 1.0586 (1.4298) acc 96.8750 (90.6250) lr 4.1221e-04 eta 0:01:06
epoch [37/50] batch [20/25] time 0.153 (0.187) data 0.000 (0.033) loss 1.2148 (1.4592) acc 90.6250 (90.3125) lr 4.1221e-04 eta 0:01:01
epoch [37/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.3555 (1.4121) acc 87.5000 (91.2500) lr 3.6258e-04 eta 0:00:58
epoch [38/50] batch [5/25] time 0.153 (0.297) data 0.000 (0.142) loss 1.8066 (1.6842) acc 81.2500 (86.2500) lr 3.6258e-04 eta 0:01:34
epoch [38/50] batch [10/25] time 0.152 (0.224) data 0.000 (0.071) loss 1.6904 (1.4798) acc 87.5000 (89.3750) lr 3.6258e-04 eta 0:01:10
epoch [38/50] batch [15/25] time 0.152 (0.200) data 0.000 (0.047) loss 1.3242 (1.4552) acc 93.7500 (89.5833) lr 3.6258e-04 eta 0:01:02
epoch [38/50] batch [20/25] time 0.152 (0.188) data 0.000 (0.036) loss 1.4570 (1.4734) acc 93.7500 (89.6875) lr 3.6258e-04 eta 0:00:57
epoch [38/50] batch [25/25] time 0.153 (0.181) data 0.000 (0.029) loss 1.1230 (1.4549) acc 90.6250 (90.0000) lr 3.1545e-04 eta 0:00:54
epoch [39/50] batch [5/25] time 0.153 (0.304) data 0.000 (0.149) loss 1.0869 (1.5244) acc 93.7500 (90.6250) lr 3.1545e-04 eta 0:01:29
epoch [39/50] batch [10/25] time 0.153 (0.228) data 0.000 (0.075) loss 1.6523 (1.4875) acc 90.6250 (91.2500) lr 3.1545e-04 eta 0:01:06
epoch [39/50] batch [15/25] time 0.152 (0.203) data 0.000 (0.050) loss 1.4912 (1.4745) acc 90.6250 (91.2500) lr 3.1545e-04 eta 0:00:57
epoch [39/50] batch [20/25] time 0.152 (0.190) data 0.000 (0.037) loss 1.7139 (1.4896) acc 84.3750 (90.1562) lr 3.1545e-04 eta 0:00:53
epoch [39/50] batch [25/25] time 0.152 (0.183) data 0.000 (0.030) loss 1.4619 (1.4653) acc 93.7500 (91.0000) lr 2.7103e-04 eta 0:00:50
epoch [40/50] batch [5/25] time 0.152 (0.290) data 0.000 (0.135) loss 1.2598 (1.2488) acc 96.8750 (94.3750) lr 2.7103e-04 eta 0:01:18
epoch [40/50] batch [10/25] time 0.153 (0.221) data 0.000 (0.067) loss 1.5771 (1.3693) acc 81.2500 (91.8750) lr 2.7103e-04 eta 0:00:58
epoch [40/50] batch [15/25] time 0.153 (0.199) data 0.000 (0.045) loss 1.4668 (1.4044) acc 90.6250 (91.4583) lr 2.7103e-04 eta 0:00:51
epoch [40/50] batch [20/25] time 0.153 (0.187) data 0.000 (0.034) loss 1.8633 (1.4509) acc 93.7500 (91.5625) lr 2.7103e-04 eta 0:00:47
epoch [40/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.5918 (1.4602) acc 84.3750 (90.8750) lr 2.2949e-04 eta 0:00:45
epoch [41/50] batch [5/25] time 0.154 (0.269) data 0.000 (0.114) loss 1.3682 (1.6471) acc 93.7500 (87.5000) lr 2.2949e-04 eta 0:01:05
epoch [41/50] batch [10/25] time 0.152 (0.211) data 0.000 (0.057) loss 1.5469 (1.5990) acc 87.5000 (87.8125) lr 2.2949e-04 eta 0:00:50
epoch [41/50] batch [15/25] time 0.152 (0.191) data 0.000 (0.038) loss 1.1016 (1.5261) acc 96.8750 (88.9583) lr 2.2949e-04 eta 0:00:44
epoch [41/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.5137 (1.5474) acc 93.7500 (88.5938) lr 2.2949e-04 eta 0:00:41
epoch [41/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.0977 (1.5263) acc 93.7500 (89.0000) lr 1.9098e-04 eta 0:00:39
epoch [42/50] batch [5/25] time 0.154 (0.293) data 0.000 (0.139) loss 1.0918 (1.4119) acc 93.7500 (91.8750) lr 1.9098e-04 eta 0:01:04
epoch [42/50] batch [10/25] time 0.152 (0.223) data 0.000 (0.070) loss 0.7837 (1.3302) acc 100.0000 (92.1875) lr 1.9098e-04 eta 0:00:47
epoch [42/50] batch [15/25] time 0.153 (0.199) data 0.000 (0.046) loss 1.2051 (1.3563) acc 96.8750 (91.8750) lr 1.9098e-04 eta 0:00:41
epoch [42/50] batch [20/25] time 0.152 (0.188) data 0.000 (0.035) loss 1.5576 (1.3682) acc 93.7500 (91.8750) lr 1.9098e-04 eta 0:00:38
epoch [42/50] batch [25/25] time 0.153 (0.181) data 0.000 (0.028) loss 1.1885 (1.3716) acc 93.7500 (91.7500) lr 1.5567e-04 eta 0:00:36
epoch [43/50] batch [5/25] time 0.153 (0.295) data 0.000 (0.142) loss 1.2432 (1.3303) acc 93.7500 (91.8750) lr 1.5567e-04 eta 0:00:57
epoch [43/50] batch [10/25] time 0.153 (0.224) data 0.000 (0.071) loss 1.3906 (1.4151) acc 87.5000 (90.3125) lr 1.5567e-04 eta 0:00:42
epoch [43/50] batch [15/25] time 0.153 (0.200) data 0.000 (0.047) loss 1.5615 (1.4314) acc 87.5000 (90.0000) lr 1.5567e-04 eta 0:00:37
epoch [43/50] batch [20/25] time 0.153 (0.189) data 0.000 (0.036) loss 1.7041 (1.4578) acc 87.5000 (89.6875) lr 1.5567e-04 eta 0:00:33
epoch [43/50] batch [25/25] time 0.153 (0.181) data 0.000 (0.029) loss 1.2109 (1.4189) acc 90.6250 (90.3750) lr 1.2369e-04 eta 0:00:31
epoch [44/50] batch [5/25] time 0.153 (0.296) data 0.000 (0.143) loss 1.5312 (1.4803) acc 90.6250 (91.2500) lr 1.2369e-04 eta 0:00:50
epoch [44/50] batch [10/25] time 0.152 (0.225) data 0.000 (0.071) loss 1.1973 (1.5708) acc 96.8750 (88.4375) lr 1.2369e-04 eta 0:00:37
epoch [44/50] batch [15/25] time 0.153 (0.201) data 0.000 (0.048) loss 1.4434 (1.4836) acc 90.6250 (90.6250) lr 1.2369e-04 eta 0:00:32
epoch [44/50] batch [20/25] time 0.152 (0.188) data 0.000 (0.036) loss 0.9824 (1.4599) acc 100.0000 (91.4062) lr 1.2369e-04 eta 0:00:29
epoch [44/50] batch [25/25] time 0.153 (0.181) data 0.000 (0.029) loss 1.4160 (1.4609) acc 90.6250 (90.7500) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [5/25] time 0.155 (0.311) data 0.000 (0.156) loss 1.3486 (1.3373) acc 93.7500 (91.8750) lr 9.5173e-05 eta 0:00:45
epoch [45/50] batch [10/25] time 0.152 (0.232) data 0.000 (0.078) loss 1.3701 (1.4056) acc 90.6250 (91.8750) lr 9.5173e-05 eta 0:00:32
epoch [45/50] batch [15/25] time 0.153 (0.205) data 0.000 (0.052) loss 1.9023 (1.4072) acc 84.3750 (91.8750) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [20/25] time 0.152 (0.192) data 0.000 (0.039) loss 1.4902 (1.4407) acc 90.6250 (91.7188) lr 9.5173e-05 eta 0:00:24
epoch [45/50] batch [25/25] time 0.152 (0.184) data 0.000 (0.031) loss 1.4043 (1.4721) acc 87.5000 (91.1250) lr 7.0224e-05 eta 0:00:23
epoch [46/50] batch [5/25] time 0.153 (0.292) data 0.000 (0.138) loss 0.8887 (1.2164) acc 100.0000 (94.3750) lr 7.0224e-05 eta 0:00:35
epoch [46/50] batch [10/25] time 0.153 (0.223) data 0.000 (0.069) loss 1.4502 (1.3584) acc 87.5000 (91.5625) lr 7.0224e-05 eta 0:00:25
epoch [46/50] batch [15/25] time 0.153 (0.199) data 0.000 (0.046) loss 1.4707 (1.3235) acc 90.6250 (92.2917) lr 7.0224e-05 eta 0:00:21
epoch [46/50] batch [20/25] time 0.153 (0.188) data 0.000 (0.035) loss 1.4834 (1.3991) acc 90.6250 (91.4062) lr 7.0224e-05 eta 0:00:19
epoch [46/50] batch [25/25] time 0.152 (0.181) data 0.000 (0.028) loss 1.1084 (1.4267) acc 96.8750 (91.1250) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [5/25] time 0.153 (0.284) data 0.000 (0.131) loss 1.4268 (1.3562) acc 87.5000 (91.2500) lr 4.8943e-05 eta 0:00:27
epoch [47/50] batch [10/25] time 0.153 (0.219) data 0.000 (0.065) loss 1.7900 (1.4226) acc 87.5000 (91.5625) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [15/25] time 0.153 (0.197) data 0.000 (0.044) loss 1.1748 (1.4124) acc 93.7500 (91.2500) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [20/25] time 0.153 (0.186) data 0.000 (0.033) loss 1.5273 (1.4085) acc 90.6250 (91.2500) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [25/25] time 0.153 (0.179) data 0.000 (0.026) loss 1.5293 (1.3865) acc 90.6250 (91.8750) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [5/25] time 0.154 (0.299) data 0.000 (0.145) loss 1.3047 (1.4066) acc 90.6250 (93.1250) lr 3.1417e-05 eta 0:00:20
epoch [48/50] batch [10/25] time 0.152 (0.226) data 0.000 (0.073) loss 1.4541 (1.4510) acc 96.8750 (92.5000) lr 3.1417e-05 eta 0:00:14
epoch [48/50] batch [15/25] time 0.152 (0.201) data 0.000 (0.049) loss 0.9102 (1.4230) acc 96.8750 (92.9167) lr 3.1417e-05 eta 0:00:12
epoch [48/50] batch [20/25] time 0.152 (0.189) data 0.000 (0.036) loss 1.2939 (1.4456) acc 93.7500 (91.8750) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.4141 (1.4393) acc 93.7500 (92.2500) lr 1.7713e-05 eta 0:00:09
epoch [49/50] batch [5/25] time 0.152 (0.287) data 0.000 (0.134) loss 1.3223 (1.3955) acc 93.7500 (93.1250) lr 1.7713e-05 eta 0:00:12
epoch [49/50] batch [10/25] time 0.153 (0.220) data 0.000 (0.067) loss 1.2471 (1.5591) acc 96.8750 (91.2500) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [15/25] time 0.153 (0.197) data 0.000 (0.045) loss 1.2686 (1.5280) acc 87.5000 (90.4167) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [20/25] time 0.154 (0.186) data 0.000 (0.034) loss 1.2461 (1.4569) acc 93.7500 (91.4062) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/25] time 0.151 (0.179) data 0.000 (0.027) loss 1.8691 (1.4361) acc 87.5000 (91.8750) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [5/25] time 0.153 (0.290) data 0.000 (0.136) loss 0.9336 (1.3102) acc 100.0000 (93.7500) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/25] time 0.154 (0.222) data 0.000 (0.068) loss 1.3711 (1.2779) acc 90.6250 (93.1250) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/25] time 0.152 (0.199) data 0.000 (0.046) loss 1.0410 (1.3283) acc 93.7500 (92.7083) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.152 (0.187) data 0.000 (0.034) loss 1.1680 (1.4010) acc 93.7500 (91.4062) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.1777 (1.4007) acc 93.7500 (91.6250) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.94s/it] 50%|█████     | 2/4 [00:05<00:04,  2.29s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.77s/it]100%|██████████| 4/4 [00:06<00:00,  1.13s/it]100%|██████████| 4/4 [00:06<00:00,  1.63s/it]
=> result
* total: 1,549
* correct: 1,519
* accuracy: 98.1%
* error: 1.9%
* macro_f1: 96.0%
Elapsed: 0:03:59
Run this job and save the output to output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     1,549
---------  ----------
['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X face.', 'X X X X leopard.', 'X X X X motorbike.', 'X X X X accordion.', 'X X X X airplane.', 'X X X X anchor.', 'X X X X ant.', 'X X X X barrel.', 'X X X X bass.', 'X X X X beaver.', 'X X X X binocular.', 'X X X X bonsai.', 'X X X X brain.', 'X X X X brontosaurus.', 'X X X X buddha.', 'X X X X butterfly.', 'X X X X camera.', 'X X X X cannon.', 'X X X X car side.', 'X X X X ceiling fan.', 'X X X X cellphone.', 'X X X X chair.', 'X X X X chandelier.', 'X X X X cougar body.', 'X X X X cougar face.', 'X X X X crab.', 'X X X X crayfish.', 'X X X X crocodile.', 'X X X X crocodile head.', 'X X X X cup.', 'X X X X dalmatian.', 'X X X X dollar bill.', 'X X X X dolphin.', 'X X X X dragonfly.', 'X X X X electric guitar.', 'X X X X elephant.', 'X X X X emu.', 'X X X X euphonium.', 'X X X X ewer.', 'X X X X ferry.', 'X X X X flamingo.', 'X X X X flamingo head.', 'X X X X garfield.', 'X X X X gerenuk.', 'X X X X gramophone.', 'X X X X grand piano.', 'X X X X hawksbill.', 'X X X X headphone.', 'X X X X hedgehog.', 'X X X X helicopter.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/tensorboard)
epoch [1/50] batch [5/25] time 0.153 (0.313) data 0.000 (0.149) loss 4.8516 (5.0531) acc 71.8750 (71.8750) lr 1.0000e-05 eta 0:06:29
epoch [1/50] batch [10/25] time 0.152 (0.233) data 0.000 (0.074) loss 4.4023 (4.8094) acc 81.2500 (76.8750) lr 1.0000e-05 eta 0:04:48
epoch [1/50] batch [15/25] time 0.151 (0.206) data 0.000 (0.050) loss 4.8359 (4.9109) acc 71.8750 (73.3333) lr 1.0000e-05 eta 0:04:13
epoch [1/50] batch [20/25] time 0.151 (0.192) data 0.000 (0.037) loss 4.4492 (4.8578) acc 71.8750 (74.0625) lr 1.0000e-05 eta 0:03:56
epoch [1/50] batch [25/25] time 0.152 (0.184) data 0.000 (0.030) loss 4.7969 (4.8255) acc 71.8750 (74.7500) lr 2.0000e-03 eta 0:03:45
epoch [2/50] batch [5/25] time 0.155 (0.274) data 0.000 (0.119) loss 3.5664 (3.8867) acc 81.2500 (80.0000) lr 2.0000e-03 eta 0:05:34
epoch [2/50] batch [10/25] time 0.151 (0.214) data 0.000 (0.060) loss 2.8965 (3.6010) acc 87.5000 (79.6875) lr 2.0000e-03 eta 0:04:20
epoch [2/50] batch [15/25] time 0.153 (0.194) data 0.000 (0.040) loss 2.6016 (3.3887) acc 84.3750 (81.0417) lr 2.0000e-03 eta 0:03:54
epoch [2/50] batch [20/25] time 0.151 (0.183) data 0.000 (0.030) loss 2.7383 (3.2804) acc 81.2500 (81.2500) lr 2.0000e-03 eta 0:03:40
epoch [2/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.024) loss 2.5352 (3.1517) acc 71.8750 (81.3750) lr 1.9980e-03 eta 0:03:32
epoch [3/50] batch [5/25] time 0.151 (0.271) data 0.000 (0.118) loss 2.3086 (2.4793) acc 87.5000 (83.7500) lr 1.9980e-03 eta 0:05:23
epoch [3/50] batch [10/25] time 0.151 (0.211) data 0.000 (0.059) loss 2.1934 (2.3750) acc 87.5000 (85.9375) lr 1.9980e-03 eta 0:04:11
epoch [3/50] batch [15/25] time 0.152 (0.191) data 0.000 (0.039) loss 2.2812 (2.3270) acc 87.5000 (86.6667) lr 1.9980e-03 eta 0:03:46
epoch [3/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.030) loss 2.0664 (2.3108) acc 87.5000 (85.7812) lr 1.9980e-03 eta 0:03:34
epoch [3/50] batch [25/25] time 0.151 (0.176) data 0.000 (0.024) loss 1.6641 (2.2671) acc 100.0000 (86.2500) lr 1.9921e-03 eta 0:03:26
epoch [4/50] batch [5/25] time 0.152 (0.282) data 0.000 (0.128) loss 1.9336 (2.0678) acc 87.5000 (90.6250) lr 1.9921e-03 eta 0:05:29
epoch [4/50] batch [10/25] time 0.152 (0.217) data 0.000 (0.064) loss 2.0781 (2.2106) acc 90.6250 (87.8125) lr 1.9921e-03 eta 0:04:12
epoch [4/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.043) loss 2.4844 (2.2285) acc 87.5000 (87.0833) lr 1.9921e-03 eta 0:03:46
epoch [4/50] batch [20/25] time 0.151 (0.185) data 0.000 (0.032) loss 1.7002 (2.1581) acc 96.8750 (88.1250) lr 1.9921e-03 eta 0:03:33
epoch [4/50] batch [25/25] time 0.152 (0.178) data 0.000 (0.026) loss 2.3379 (2.1237) acc 75.0000 (88.1250) lr 1.9823e-03 eta 0:03:24
epoch [5/50] batch [5/25] time 0.153 (0.277) data 0.000 (0.123) loss 1.6387 (1.9646) acc 93.7500 (86.2500) lr 1.9823e-03 eta 0:05:16
epoch [5/50] batch [10/25] time 0.152 (0.214) data 0.000 (0.061) loss 2.1211 (1.9625) acc 84.3750 (85.0000) lr 1.9823e-03 eta 0:04:04
epoch [5/50] batch [15/25] time 0.152 (0.194) data 0.000 (0.041) loss 1.8389 (2.0078) acc 87.5000 (85.2083) lr 1.9823e-03 eta 0:03:39
epoch [5/50] batch [20/25] time 0.152 (0.183) data 0.000 (0.031) loss 1.9893 (2.0283) acc 87.5000 (85.0000) lr 1.9823e-03 eta 0:03:27
epoch [5/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.025) loss 2.5703 (2.0492) acc 78.1250 (84.5000) lr 1.9686e-03 eta 0:03:19
epoch [6/50] batch [5/25] time 0.152 (0.275) data 0.000 (0.122) loss 1.7451 (1.9461) acc 87.5000 (88.7500) lr 1.9686e-03 eta 0:05:08
epoch [6/50] batch [10/25] time 0.152 (0.214) data 0.000 (0.061) loss 2.0195 (1.9064) acc 81.2500 (85.9375) lr 1.9686e-03 eta 0:03:58
epoch [6/50] batch [15/25] time 0.152 (0.193) data 0.000 (0.041) loss 1.7666 (1.8658) acc 90.6250 (85.6250) lr 1.9686e-03 eta 0:03:34
epoch [6/50] batch [20/25] time 0.152 (0.183) data 0.000 (0.031) loss 1.9639 (1.8866) acc 87.5000 (85.4688) lr 1.9686e-03 eta 0:03:21
epoch [6/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.024) loss 2.2715 (1.9177) acc 81.2500 (85.1250) lr 1.9511e-03 eta 0:03:14
epoch [7/50] batch [5/25] time 0.153 (0.306) data 0.000 (0.151) loss 2.0488 (1.9414) acc 78.1250 (84.3750) lr 1.9511e-03 eta 0:05:34
epoch [7/50] batch [10/25] time 0.152 (0.230) data 0.000 (0.076) loss 2.1641 (1.9717) acc 71.8750 (83.1250) lr 1.9511e-03 eta 0:04:10
epoch [7/50] batch [15/25] time 0.152 (0.204) data 0.000 (0.051) loss 1.4355 (1.8853) acc 93.7500 (85.6250) lr 1.9511e-03 eta 0:03:41
epoch [7/50] batch [20/25] time 0.152 (0.191) data 0.000 (0.038) loss 1.9307 (1.8753) acc 90.6250 (86.4062) lr 1.9511e-03 eta 0:03:26
epoch [7/50] batch [25/25] time 0.153 (0.183) data 0.000 (0.030) loss 2.2031 (1.8937) acc 75.0000 (85.6250) lr 1.9298e-03 eta 0:03:17
epoch [8/50] batch [5/25] time 0.153 (0.277) data 0.000 (0.123) loss 1.5840 (1.8250) acc 93.7500 (87.5000) lr 1.9298e-03 eta 0:04:56
epoch [8/50] batch [10/25] time 0.152 (0.215) data 0.000 (0.061) loss 2.1875 (2.0031) acc 81.2500 (85.0000) lr 1.9298e-03 eta 0:03:49
epoch [8/50] batch [15/25] time 0.151 (0.194) data 0.000 (0.041) loss 1.9980 (1.9639) acc 84.3750 (85.6250) lr 1.9298e-03 eta 0:03:25
epoch [8/50] batch [20/25] time 0.153 (0.184) data 0.000 (0.031) loss 2.1562 (1.9105) acc 81.2500 (86.0938) lr 1.9298e-03 eta 0:03:13
epoch [8/50] batch [25/25] time 0.152 (0.178) data 0.000 (0.025) loss 1.4404 (1.8649) acc 93.7500 (86.5000) lr 1.9048e-03 eta 0:03:06
epoch [9/50] batch [5/25] time 0.152 (0.292) data 0.000 (0.140) loss 1.6270 (1.6998) acc 87.5000 (89.3750) lr 1.9048e-03 eta 0:05:05
epoch [9/50] batch [10/25] time 0.152 (0.222) data 0.000 (0.070) loss 1.7734 (1.8096) acc 84.3750 (85.3125) lr 1.9048e-03 eta 0:03:51
epoch [9/50] batch [15/25] time 0.152 (0.199) data 0.000 (0.047) loss 1.8867 (1.8320) acc 87.5000 (85.8333) lr 1.9048e-03 eta 0:03:25
epoch [9/50] batch [20/25] time 0.152 (0.187) data 0.000 (0.035) loss 1.6113 (1.8474) acc 90.6250 (85.9375) lr 1.9048e-03 eta 0:03:12
epoch [9/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.028) loss 1.8672 (1.8741) acc 87.5000 (85.5000) lr 1.8763e-03 eta 0:03:04
epoch [10/50] batch [5/25] time 0.152 (0.271) data 0.000 (0.116) loss 1.7461 (1.9674) acc 84.3750 (85.0000) lr 1.8763e-03 eta 0:04:36
epoch [10/50] batch [10/25] time 0.152 (0.212) data 0.000 (0.058) loss 1.4961 (1.6567) acc 93.7500 (90.6250) lr 1.8763e-03 eta 0:03:35
epoch [10/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.039) loss 1.3086 (1.6844) acc 96.8750 (89.5833) lr 1.8763e-03 eta 0:03:13
epoch [10/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.7861 (1.6968) acc 90.6250 (89.0625) lr 1.8763e-03 eta 0:03:02
epoch [10/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.5312 (1.7237) acc 90.6250 (88.5000) lr 1.8443e-03 eta 0:02:56
epoch [11/50] batch [5/25] time 0.152 (0.277) data 0.000 (0.124) loss 1.3438 (1.7402) acc 93.7500 (88.7500) lr 1.8443e-03 eta 0:04:35
epoch [11/50] batch [10/25] time 0.152 (0.214) data 0.000 (0.062) loss 1.7998 (1.7436) acc 87.5000 (87.8125) lr 1.8443e-03 eta 0:03:32
epoch [11/50] batch [15/25] time 0.151 (0.194) data 0.000 (0.041) loss 1.7588 (1.7783) acc 84.3750 (86.8750) lr 1.8443e-03 eta 0:03:10
epoch [11/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.031) loss 1.9014 (1.7868) acc 87.5000 (86.5625) lr 1.8443e-03 eta 0:02:59
epoch [11/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.025) loss 1.3594 (1.7544) acc 90.6250 (86.6250) lr 1.8090e-03 eta 0:02:52
epoch [12/50] batch [5/25] time 0.153 (0.278) data 0.000 (0.124) loss 1.6055 (1.7779) acc 84.3750 (84.3750) lr 1.8090e-03 eta 0:04:30
epoch [12/50] batch [10/25] time 0.152 (0.215) data 0.000 (0.062) loss 1.7500 (1.7214) acc 87.5000 (84.6875) lr 1.8090e-03 eta 0:03:27
epoch [12/50] batch [15/25] time 0.152 (0.194) data 0.000 (0.042) loss 2.4004 (1.7153) acc 68.7500 (84.3750) lr 1.8090e-03 eta 0:03:06
epoch [12/50] batch [20/25] time 0.152 (0.184) data 0.000 (0.031) loss 2.0957 (1.8008) acc 81.2500 (83.7500) lr 1.8090e-03 eta 0:02:55
epoch [12/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.025) loss 1.9883 (1.8081) acc 78.1250 (83.7500) lr 1.7705e-03 eta 0:02:48
epoch [13/50] batch [5/25] time 0.154 (0.285) data 0.000 (0.130) loss 1.7715 (1.6162) acc 87.5000 (91.2500) lr 1.7705e-03 eta 0:04:29
epoch [13/50] batch [10/25] time 0.153 (0.220) data 0.000 (0.065) loss 1.9834 (1.6080) acc 84.3750 (90.3125) lr 1.7705e-03 eta 0:03:26
epoch [13/50] batch [15/25] time 0.153 (0.197) data 0.000 (0.044) loss 1.5645 (1.6120) acc 87.5000 (89.5833) lr 1.7705e-03 eta 0:03:04
epoch [13/50] batch [20/25] time 0.153 (0.186) data 0.000 (0.033) loss 2.0293 (1.6661) acc 78.1250 (89.0625) lr 1.7705e-03 eta 0:02:53
epoch [13/50] batch [25/25] time 0.152 (0.179) data 0.000 (0.026) loss 2.3418 (1.6391) acc 87.5000 (90.1250) lr 1.7290e-03 eta 0:02:45
epoch [14/50] batch [5/25] time 0.152 (0.271) data 0.000 (0.118) loss 1.3760 (1.6219) acc 90.6250 (88.1250) lr 1.7290e-03 eta 0:04:09
epoch [14/50] batch [10/25] time 0.152 (0.212) data 0.000 (0.059) loss 1.7568 (1.6181) acc 81.2500 (88.7500) lr 1.7290e-03 eta 0:03:13
epoch [14/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.039) loss 1.3555 (1.5443) acc 93.7500 (89.7917) lr 1.7290e-03 eta 0:02:54
epoch [14/50] batch [20/25] time 0.153 (0.182) data 0.000 (0.030) loss 1.7480 (1.5768) acc 87.5000 (89.6875) lr 1.7290e-03 eta 0:02:44
epoch [14/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.024) loss 2.2109 (1.6228) acc 81.2500 (88.5000) lr 1.6845e-03 eta 0:02:38
epoch [15/50] batch [5/25] time 0.154 (0.291) data 0.000 (0.137) loss 1.8379 (1.5498) acc 87.5000 (88.1250) lr 1.6845e-03 eta 0:04:20
epoch [15/50] batch [10/25] time 0.152 (0.222) data 0.000 (0.069) loss 1.8838 (1.7252) acc 81.2500 (85.6250) lr 1.6845e-03 eta 0:03:17
epoch [15/50] batch [15/25] time 0.152 (0.199) data 0.000 (0.046) loss 1.5977 (1.6616) acc 84.3750 (86.8750) lr 1.6845e-03 eta 0:02:55
epoch [15/50] batch [20/25] time 0.152 (0.187) data 0.000 (0.034) loss 1.9746 (1.6858) acc 81.2500 (86.0938) lr 1.6845e-03 eta 0:02:44
epoch [15/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.028) loss 2.1523 (1.6923) acc 75.0000 (86.2500) lr 1.6374e-03 eta 0:02:37
epoch [16/50] batch [5/25] time 0.152 (0.273) data 0.000 (0.118) loss 1.4668 (1.7354) acc 96.8750 (87.5000) lr 1.6374e-03 eta 0:03:57
epoch [16/50] batch [10/25] time 0.153 (0.213) data 0.000 (0.059) loss 1.5645 (1.7255) acc 93.7500 (88.1250) lr 1.6374e-03 eta 0:03:04
epoch [16/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.040) loss 1.5879 (1.7324) acc 96.8750 (87.7083) lr 1.6374e-03 eta 0:02:46
epoch [16/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.030) loss 1.7500 (1.7171) acc 90.6250 (88.2812) lr 1.6374e-03 eta 0:02:36
epoch [16/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.024) loss 2.2031 (1.7294) acc 75.0000 (88.0000) lr 1.5878e-03 eta 0:02:30
epoch [17/50] batch [5/25] time 0.152 (0.272) data 0.000 (0.118) loss 1.5098 (1.6213) acc 96.8750 (89.3750) lr 1.5878e-03 eta 0:03:49
epoch [17/50] batch [10/25] time 0.152 (0.212) data 0.000 (0.059) loss 1.5020 (1.6053) acc 87.5000 (88.7500) lr 1.5878e-03 eta 0:02:58
epoch [17/50] batch [15/25] time 0.152 (0.192) data 0.000 (0.039) loss 1.5020 (1.5965) acc 87.5000 (88.1250) lr 1.5878e-03 eta 0:02:40
epoch [17/50] batch [20/25] time 0.153 (0.182) data 0.000 (0.030) loss 1.2881 (1.5616) acc 90.6250 (88.9062) lr 1.5878e-03 eta 0:02:31
epoch [17/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.024) loss 1.5908 (1.5464) acc 90.6250 (89.0000) lr 1.5358e-03 eta 0:02:25
epoch [18/50] batch [5/25] time 0.152 (0.289) data 0.000 (0.135) loss 2.5879 (1.7248) acc 75.0000 (86.8750) lr 1.5358e-03 eta 0:03:57
epoch [18/50] batch [10/25] time 0.152 (0.221) data 0.000 (0.068) loss 1.3457 (1.7001) acc 96.8750 (86.5625) lr 1.5358e-03 eta 0:03:00
epoch [18/50] batch [15/25] time 0.153 (0.198) data 0.000 (0.045) loss 1.4434 (1.7371) acc 90.6250 (86.4583) lr 1.5358e-03 eta 0:02:40
epoch [18/50] batch [20/25] time 0.152 (0.187) data 0.000 (0.034) loss 1.9512 (1.6724) acc 84.3750 (87.1875) lr 1.5358e-03 eta 0:02:30
epoch [18/50] batch [25/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.6455 (1.6795) acc 84.3750 (86.8750) lr 1.4818e-03 eta 0:02:23
epoch [19/50] batch [5/25] time 0.153 (0.268) data 0.000 (0.114) loss 2.2266 (1.7066) acc 84.3750 (89.3750) lr 1.4818e-03 eta 0:03:32
epoch [19/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.057) loss 1.7041 (1.6417) acc 84.3750 (89.6875) lr 1.4818e-03 eta 0:02:46
epoch [19/50] batch [15/25] time 0.152 (0.191) data 0.000 (0.038) loss 1.7031 (1.6105) acc 90.6250 (89.7917) lr 1.4818e-03 eta 0:02:30
epoch [19/50] batch [20/25] time 0.152 (0.181) data 0.000 (0.029) loss 1.7832 (1.6125) acc 81.2500 (88.5938) lr 1.4818e-03 eta 0:02:21
epoch [19/50] batch [25/25] time 0.152 (0.176) data 0.000 (0.023) loss 1.4248 (1.6654) acc 87.5000 (87.3750) lr 1.4258e-03 eta 0:02:16
epoch [20/50] batch [5/25] time 0.152 (0.269) data 0.000 (0.115) loss 1.6699 (1.5795) acc 81.2500 (89.3750) lr 1.4258e-03 eta 0:03:26
epoch [20/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.058) loss 2.0293 (1.6123) acc 78.1250 (89.3750) lr 1.4258e-03 eta 0:02:41
epoch [20/50] batch [15/25] time 0.152 (0.191) data 0.000 (0.038) loss 1.7764 (1.5906) acc 81.2500 (88.9583) lr 1.4258e-03 eta 0:02:25
epoch [20/50] batch [20/25] time 0.152 (0.181) data 0.000 (0.029) loss 1.8730 (1.6463) acc 90.6250 (88.7500) lr 1.4258e-03 eta 0:02:16
epoch [20/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.1387 (1.6104) acc 93.7500 (89.1250) lr 1.3681e-03 eta 0:02:11
epoch [21/50] batch [5/25] time 0.152 (0.270) data 0.000 (0.114) loss 1.1934 (1.4645) acc 93.7500 (89.3750) lr 1.3681e-03 eta 0:03:20
epoch [21/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.057) loss 2.0488 (1.5913) acc 78.1250 (88.1250) lr 1.3681e-03 eta 0:02:36
epoch [21/50] batch [15/25] time 0.152 (0.192) data 0.000 (0.038) loss 2.2109 (1.6109) acc 78.1250 (87.9167) lr 1.3681e-03 eta 0:02:20
epoch [21/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.3271 (1.6056) acc 93.7500 (88.2812) lr 1.3681e-03 eta 0:02:12
epoch [21/50] batch [25/25] time 0.152 (0.176) data 0.000 (0.023) loss 1.2070 (1.5626) acc 96.8750 (88.8750) lr 1.3090e-03 eta 0:02:07
epoch [22/50] batch [5/25] time 0.152 (0.270) data 0.000 (0.116) loss 1.4600 (1.3514) acc 90.6250 (90.6250) lr 1.3090e-03 eta 0:03:14
epoch [22/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.058) loss 1.1660 (1.4234) acc 96.8750 (90.3125) lr 1.3090e-03 eta 0:02:31
epoch [22/50] batch [15/25] time 0.152 (0.192) data 0.000 (0.039) loss 1.5342 (1.5135) acc 90.6250 (88.9583) lr 1.3090e-03 eta 0:02:16
epoch [22/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.029) loss 2.3633 (1.6089) acc 65.6250 (87.8125) lr 1.3090e-03 eta 0:02:08
epoch [22/50] batch [25/25] time 0.152 (0.176) data 0.000 (0.023) loss 1.6982 (1.6170) acc 90.6250 (87.8750) lr 1.2487e-03 eta 0:02:03
epoch [23/50] batch [5/25] time 0.152 (0.270) data 0.000 (0.116) loss 1.7344 (1.6383) acc 93.7500 (88.7500) lr 1.2487e-03 eta 0:03:07
epoch [23/50] batch [10/25] time 0.152 (0.211) data 0.000 (0.058) loss 1.9160 (1.6646) acc 84.3750 (88.1250) lr 1.2487e-03 eta 0:02:25
epoch [23/50] batch [15/25] time 0.152 (0.192) data 0.000 (0.039) loss 2.3066 (1.6891) acc 75.0000 (87.9167) lr 1.2487e-03 eta 0:02:11
epoch [23/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.6074 (1.6321) acc 90.6250 (88.7500) lr 1.2487e-03 eta 0:02:03
epoch [23/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.7275 (1.6135) acc 87.5000 (88.5000) lr 1.1874e-03 eta 0:01:58
epoch [24/50] batch [5/25] time 0.152 (0.295) data 0.000 (0.141) loss 1.2676 (1.3514) acc 93.7500 (95.0000) lr 1.1874e-03 eta 0:03:17
epoch [24/50] batch [10/25] time 0.153 (0.225) data 0.000 (0.071) loss 1.5488 (1.5446) acc 87.5000 (90.6250) lr 1.1874e-03 eta 0:02:29
epoch [24/50] batch [15/25] time 0.152 (0.201) data 0.000 (0.047) loss 1.7207 (1.5212) acc 87.5000 (90.6250) lr 1.1874e-03 eta 0:02:12
epoch [24/50] batch [20/25] time 0.152 (0.188) data 0.000 (0.035) loss 1.4121 (1.5246) acc 87.5000 (89.0625) lr 1.1874e-03 eta 0:02:03
epoch [24/50] batch [25/25] time 0.152 (0.181) data 0.000 (0.028) loss 1.2363 (1.5161) acc 96.8750 (89.2500) lr 1.1253e-03 eta 0:01:57
epoch [25/50] batch [5/25] time 0.154 (0.300) data 0.000 (0.145) loss 1.9727 (1.6576) acc 81.2500 (87.5000) lr 1.1253e-03 eta 0:03:13
epoch [25/50] batch [10/25] time 0.153 (0.227) data 0.000 (0.073) loss 1.6436 (1.5624) acc 84.3750 (88.1250) lr 1.1253e-03 eta 0:02:25
epoch [25/50] batch [15/25] time 0.153 (0.202) data 0.000 (0.048) loss 2.1172 (1.6066) acc 81.2500 (87.9167) lr 1.1253e-03 eta 0:02:08
epoch [25/50] batch [20/25] time 0.152 (0.190) data 0.000 (0.036) loss 1.7637 (1.5934) acc 90.6250 (88.5938) lr 1.1253e-03 eta 0:01:59
epoch [25/50] batch [25/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.3867 (1.6041) acc 96.8750 (88.3750) lr 1.0628e-03 eta 0:01:53
epoch [26/50] batch [5/25] time 0.153 (0.278) data 0.000 (0.125) loss 1.0898 (1.4096) acc 93.7500 (91.2500) lr 1.0628e-03 eta 0:02:52
epoch [26/50] batch [10/25] time 0.152 (0.215) data 0.000 (0.062) loss 1.7295 (1.4979) acc 87.5000 (90.6250) lr 1.0628e-03 eta 0:02:12
epoch [26/50] batch [15/25] time 0.153 (0.194) data 0.000 (0.042) loss 1.4844 (1.5030) acc 87.5000 (90.2083) lr 1.0628e-03 eta 0:01:58
epoch [26/50] batch [20/25] time 0.152 (0.184) data 0.000 (0.031) loss 1.7754 (1.4938) acc 81.2500 (90.3125) lr 1.0628e-03 eta 0:01:51
epoch [26/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.025) loss 1.6660 (1.4935) acc 87.5000 (90.3750) lr 1.0000e-03 eta 0:01:46
epoch [27/50] batch [5/25] time 0.151 (0.277) data 0.000 (0.125) loss 1.9395 (1.4156) acc 84.3750 (93.1250) lr 1.0000e-03 eta 0:02:44
epoch [27/50] batch [10/25] time 0.151 (0.214) data 0.000 (0.062) loss 1.5879 (1.4969) acc 87.5000 (90.3125) lr 1.0000e-03 eta 0:02:06
epoch [27/50] batch [15/25] time 0.151 (0.193) data 0.000 (0.042) loss 1.7598 (1.5340) acc 84.3750 (89.7917) lr 1.0000e-03 eta 0:01:52
epoch [27/50] batch [20/25] time 0.151 (0.182) data 0.000 (0.031) loss 1.2471 (1.5620) acc 90.6250 (89.0625) lr 1.0000e-03 eta 0:01:45
epoch [27/50] batch [25/25] time 0.151 (0.176) data 0.000 (0.025) loss 1.8750 (1.5602) acc 87.5000 (89.2500) lr 9.3721e-04 eta 0:01:41
epoch [28/50] batch [5/25] time 0.153 (0.265) data 0.000 (0.111) loss 1.3711 (1.3453) acc 90.6250 (90.6250) lr 9.3721e-04 eta 0:02:30
epoch [28/50] batch [10/25] time 0.153 (0.209) data 0.000 (0.056) loss 1.4561 (1.4536) acc 93.7500 (90.9375) lr 9.3721e-04 eta 0:01:58
epoch [28/50] batch [15/25] time 0.154 (0.191) data 0.000 (0.037) loss 1.3242 (1.4668) acc 96.8750 (91.6667) lr 9.3721e-04 eta 0:01:46
epoch [28/50] batch [20/25] time 0.153 (0.181) data 0.000 (0.028) loss 1.5430 (1.4965) acc 81.2500 (90.4688) lr 9.3721e-04 eta 0:01:40
epoch [28/50] batch [25/25] time 0.153 (0.175) data 0.000 (0.022) loss 1.2773 (1.5045) acc 90.6250 (90.2500) lr 8.7467e-04 eta 0:01:36
epoch [29/50] batch [5/25] time 0.152 (0.265) data 0.000 (0.111) loss 1.3145 (1.4693) acc 90.6250 (89.3750) lr 8.7467e-04 eta 0:02:24
epoch [29/50] batch [10/25] time 0.152 (0.208) data 0.000 (0.056) loss 1.2910 (1.4157) acc 96.8750 (90.3125) lr 8.7467e-04 eta 0:01:52
epoch [29/50] batch [15/25] time 0.153 (0.190) data 0.000 (0.037) loss 1.2451 (1.3934) acc 100.0000 (90.6250) lr 8.7467e-04 eta 0:01:41
epoch [29/50] batch [20/25] time 0.152 (0.180) data 0.000 (0.028) loss 1.6621 (1.4620) acc 90.6250 (90.0000) lr 8.7467e-04 eta 0:01:35
epoch [29/50] batch [25/25] time 0.152 (0.175) data 0.000 (0.022) loss 1.9463 (1.4816) acc 84.3750 (89.7500) lr 8.1262e-04 eta 0:01:31
epoch [30/50] batch [5/25] time 0.153 (0.261) data 0.000 (0.106) loss 1.2441 (1.4969) acc 93.7500 (89.3750) lr 8.1262e-04 eta 0:02:15
epoch [30/50] batch [10/25] time 0.153 (0.207) data 0.000 (0.053) loss 1.6816 (1.6342) acc 87.5000 (87.5000) lr 8.1262e-04 eta 0:01:46
epoch [30/50] batch [15/25] time 0.154 (0.189) data 0.000 (0.036) loss 1.1914 (1.5689) acc 93.7500 (88.1250) lr 8.1262e-04 eta 0:01:36
epoch [30/50] batch [20/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.6396 (1.5987) acc 93.7500 (88.5938) lr 8.1262e-04 eta 0:01:30
epoch [30/50] batch [25/25] time 0.154 (0.175) data 0.000 (0.021) loss 1.1328 (1.5312) acc 100.0000 (89.7500) lr 7.5131e-04 eta 0:01:27
epoch [31/50] batch [5/25] time 0.152 (0.274) data 0.000 (0.120) loss 1.3457 (1.4293) acc 87.5000 (92.5000) lr 7.5131e-04 eta 0:02:15
epoch [31/50] batch [10/25] time 0.152 (0.214) data 0.000 (0.060) loss 1.7119 (1.4921) acc 84.3750 (90.9375) lr 7.5131e-04 eta 0:01:44
epoch [31/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.040) loss 1.8652 (1.5832) acc 87.5000 (89.7917) lr 7.5131e-04 eta 0:01:33
epoch [31/50] batch [20/25] time 0.152 (0.183) data 0.000 (0.030) loss 1.1533 (1.5325) acc 93.7500 (89.5312) lr 7.5131e-04 eta 0:01:27
epoch [31/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.024) loss 2.0547 (1.5547) acc 71.8750 (89.0000) lr 6.9098e-04 eta 0:01:23
epoch [32/50] batch [5/25] time 0.152 (0.269) data 0.000 (0.114) loss 1.5400 (1.4516) acc 93.7500 (91.8750) lr 6.9098e-04 eta 0:02:06
epoch [32/50] batch [10/25] time 0.154 (0.211) data 0.000 (0.057) loss 1.7500 (1.5324) acc 81.2500 (89.0625) lr 6.9098e-04 eta 0:01:38
epoch [32/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.038) loss 1.7461 (1.5336) acc 87.5000 (89.5833) lr 6.9098e-04 eta 0:01:28
epoch [32/50] batch [20/25] time 0.153 (0.186) data 0.000 (0.029) loss 1.7129 (1.5832) acc 90.6250 (89.0625) lr 6.9098e-04 eta 0:01:24
epoch [32/50] batch [25/25] time 0.154 (0.180) data 0.000 (0.023) loss 1.0547 (1.5967) acc 96.8750 (88.3750) lr 6.3188e-04 eta 0:01:20
epoch [33/50] batch [5/25] time 0.155 (0.271) data 0.000 (0.117) loss 1.3311 (1.5652) acc 90.6250 (91.2500) lr 6.3188e-04 eta 0:02:00
epoch [33/50] batch [10/25] time 0.153 (0.213) data 0.000 (0.058) loss 2.0586 (1.4310) acc 78.1250 (90.9375) lr 6.3188e-04 eta 0:01:33
epoch [33/50] batch [15/25] time 0.152 (0.193) data 0.000 (0.039) loss 1.6279 (1.5077) acc 87.5000 (89.3750) lr 6.3188e-04 eta 0:01:23
epoch [33/50] batch [20/25] time 0.152 (0.183) data 0.000 (0.029) loss 1.1553 (1.4776) acc 93.7500 (90.4688) lr 6.3188e-04 eta 0:01:18
epoch [33/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.023) loss 1.1240 (1.4802) acc 100.0000 (90.5000) lr 5.7422e-04 eta 0:01:15
epoch [34/50] batch [5/25] time 0.153 (0.280) data 0.000 (0.125) loss 1.2305 (1.4564) acc 93.7500 (88.7500) lr 5.7422e-04 eta 0:01:57
epoch [34/50] batch [10/25] time 0.153 (0.217) data 0.000 (0.062) loss 2.0352 (1.5150) acc 84.3750 (88.4375) lr 5.7422e-04 eta 0:01:30
epoch [34/50] batch [15/25] time 0.152 (0.195) data 0.000 (0.042) loss 1.5166 (1.5150) acc 96.8750 (88.5417) lr 5.7422e-04 eta 0:01:20
epoch [34/50] batch [20/25] time 0.152 (0.185) data 0.000 (0.031) loss 1.4941 (1.5262) acc 90.6250 (88.5938) lr 5.7422e-04 eta 0:01:14
epoch [34/50] batch [25/25] time 0.152 (0.178) data 0.000 (0.025) loss 1.4023 (1.5267) acc 87.5000 (89.0000) lr 5.1825e-04 eta 0:01:11
epoch [35/50] batch [5/25] time 0.154 (0.309) data 0.000 (0.155) loss 1.8340 (1.4266) acc 81.2500 (91.8750) lr 5.1825e-04 eta 0:02:01
epoch [35/50] batch [10/25] time 0.154 (0.231) data 0.000 (0.077) loss 1.4717 (1.5139) acc 90.6250 (91.5625) lr 5.1825e-04 eta 0:01:30
epoch [35/50] batch [15/25] time 0.152 (0.205) data 0.000 (0.052) loss 1.1094 (1.5025) acc 96.8750 (91.2500) lr 5.1825e-04 eta 0:01:18
epoch [35/50] batch [20/25] time 0.153 (0.192) data 0.000 (0.039) loss 1.2256 (1.4591) acc 93.7500 (92.0312) lr 5.1825e-04 eta 0:01:12
epoch [35/50] batch [25/25] time 0.152 (0.184) data 0.000 (0.031) loss 1.6426 (1.4802) acc 90.6250 (91.3750) lr 4.6417e-04 eta 0:01:08
epoch [36/50] batch [5/25] time 0.154 (0.297) data 0.000 (0.142) loss 1.3750 (1.4508) acc 93.7500 (91.8750) lr 4.6417e-04 eta 0:01:50
epoch [36/50] batch [10/25] time 0.154 (0.226) data 0.000 (0.071) loss 1.2471 (1.5731) acc 87.5000 (87.5000) lr 4.6417e-04 eta 0:01:22
epoch [36/50] batch [15/25] time 0.153 (0.202) data 0.000 (0.047) loss 1.2793 (1.5047) acc 93.7500 (89.1667) lr 4.6417e-04 eta 0:01:12
epoch [36/50] batch [20/25] time 0.153 (0.190) data 0.000 (0.036) loss 1.4199 (1.5403) acc 90.6250 (88.9062) lr 4.6417e-04 eta 0:01:07
epoch [36/50] batch [25/25] time 0.153 (0.182) data 0.000 (0.029) loss 1.8594 (1.5900) acc 84.3750 (88.6250) lr 4.1221e-04 eta 0:01:03
epoch [37/50] batch [5/25] time 0.155 (0.283) data 0.000 (0.127) loss 1.3145 (1.5205) acc 93.7500 (88.7500) lr 4.1221e-04 eta 0:01:37
epoch [37/50] batch [10/25] time 0.152 (0.218) data 0.000 (0.064) loss 1.5420 (1.4531) acc 90.6250 (90.0000) lr 4.1221e-04 eta 0:01:14
epoch [37/50] batch [15/25] time 0.152 (0.196) data 0.000 (0.042) loss 1.7539 (1.4798) acc 87.5000 (88.9583) lr 4.1221e-04 eta 0:01:05
epoch [37/50] batch [20/25] time 0.154 (0.185) data 0.000 (0.032) loss 1.9258 (1.5209) acc 84.3750 (88.4375) lr 4.1221e-04 eta 0:01:01
epoch [37/50] batch [25/25] time 0.152 (0.179) data 0.000 (0.026) loss 1.1348 (1.5540) acc 96.8750 (88.5000) lr 3.6258e-04 eta 0:00:58
epoch [38/50] batch [5/25] time 0.154 (0.270) data 0.000 (0.116) loss 1.1895 (1.4887) acc 100.0000 (95.0000) lr 3.6258e-04 eta 0:01:26
epoch [38/50] batch [10/25] time 0.152 (0.211) data 0.000 (0.058) loss 1.3613 (1.4759) acc 93.7500 (93.1250) lr 3.6258e-04 eta 0:01:06
epoch [38/50] batch [15/25] time 0.152 (0.192) data 0.000 (0.039) loss 2.0254 (1.5623) acc 81.2500 (90.6250) lr 3.6258e-04 eta 0:00:59
epoch [38/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.8164 (1.5774) acc 81.2500 (90.0000) lr 3.6258e-04 eta 0:00:55
epoch [38/50] batch [25/25] time 0.152 (0.176) data 0.000 (0.023) loss 1.3291 (1.5681) acc 87.5000 (89.5000) lr 3.1545e-04 eta 0:00:52
epoch [39/50] batch [5/25] time 0.153 (0.287) data 0.000 (0.132) loss 1.1357 (1.4502) acc 96.8750 (92.5000) lr 3.1545e-04 eta 0:01:24
epoch [39/50] batch [10/25] time 0.153 (0.220) data 0.000 (0.066) loss 1.2432 (1.4393) acc 93.7500 (92.5000) lr 3.1545e-04 eta 0:01:03
epoch [39/50] batch [15/25] time 0.151 (0.198) data 0.000 (0.044) loss 1.6016 (1.4364) acc 87.5000 (91.6667) lr 3.1545e-04 eta 0:00:56
epoch [39/50] batch [20/25] time 0.156 (0.187) data 0.000 (0.033) loss 1.2588 (1.4252) acc 93.7500 (91.8750) lr 3.1545e-04 eta 0:00:52
epoch [39/50] batch [25/25] time 0.154 (0.180) data 0.000 (0.027) loss 1.4326 (1.4586) acc 90.6250 (91.3750) lr 2.7103e-04 eta 0:00:49
epoch [40/50] batch [5/25] time 0.154 (0.265) data 0.000 (0.111) loss 1.2910 (1.4229) acc 87.5000 (90.6250) lr 2.7103e-04 eta 0:01:11
epoch [40/50] batch [10/25] time 0.154 (0.209) data 0.000 (0.056) loss 1.1318 (1.4303) acc 93.7500 (90.0000) lr 2.7103e-04 eta 0:00:55
epoch [40/50] batch [15/25] time 0.152 (0.191) data 0.000 (0.037) loss 1.1543 (1.4854) acc 96.8750 (89.7917) lr 2.7103e-04 eta 0:00:49
epoch [40/50] batch [20/25] time 0.152 (0.181) data 0.000 (0.028) loss 1.6748 (1.5060) acc 90.6250 (90.0000) lr 2.7103e-04 eta 0:00:46
epoch [40/50] batch [25/25] time 0.153 (0.175) data 0.000 (0.022) loss 1.9004 (1.5363) acc 81.2500 (89.8750) lr 2.2949e-04 eta 0:00:43
epoch [41/50] batch [5/25] time 0.153 (0.260) data 0.000 (0.105) loss 1.7676 (1.4748) acc 90.6250 (91.8750) lr 2.2949e-04 eta 0:01:03
epoch [41/50] batch [10/25] time 0.153 (0.207) data 0.000 (0.053) loss 1.5508 (1.4837) acc 87.5000 (90.9375) lr 2.2949e-04 eta 0:00:49
epoch [41/50] batch [15/25] time 0.153 (0.189) data 0.000 (0.035) loss 1.5283 (1.5332) acc 93.7500 (90.4167) lr 2.2949e-04 eta 0:00:44
epoch [41/50] batch [20/25] time 0.153 (0.180) data 0.000 (0.026) loss 1.7070 (1.5202) acc 87.5000 (90.7812) lr 2.2949e-04 eta 0:00:41
epoch [41/50] batch [25/25] time 0.153 (0.174) data 0.000 (0.021) loss 1.3750 (1.4804) acc 90.6250 (91.2500) lr 1.9098e-04 eta 0:00:39
epoch [42/50] batch [5/25] time 0.154 (0.263) data 0.000 (0.109) loss 1.4531 (1.6959) acc 90.6250 (87.5000) lr 1.9098e-04 eta 0:00:57
epoch [42/50] batch [10/25] time 0.152 (0.209) data 0.000 (0.055) loss 1.4473 (1.6023) acc 96.8750 (90.0000) lr 1.9098e-04 eta 0:00:44
epoch [42/50] batch [15/25] time 0.152 (0.190) data 0.000 (0.037) loss 1.2852 (1.5383) acc 93.7500 (90.4167) lr 1.9098e-04 eta 0:00:39
epoch [42/50] batch [20/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.3555 (1.5492) acc 93.7500 (90.7812) lr 1.9098e-04 eta 0:00:36
epoch [42/50] batch [25/25] time 0.153 (0.175) data 0.000 (0.022) loss 0.9692 (1.5510) acc 100.0000 (90.2500) lr 1.5567e-04 eta 0:00:34
epoch [43/50] batch [5/25] time 0.154 (0.270) data 0.000 (0.116) loss 1.5068 (1.3915) acc 84.3750 (89.3750) lr 1.5567e-04 eta 0:00:52
epoch [43/50] batch [10/25] time 0.152 (0.213) data 0.000 (0.058) loss 1.2695 (1.4143) acc 96.8750 (91.5625) lr 1.5567e-04 eta 0:00:40
epoch [43/50] batch [15/25] time 0.154 (0.193) data 0.000 (0.039) loss 1.6152 (1.4924) acc 87.5000 (90.8333) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.029) loss 1.1689 (1.4616) acc 96.8750 (91.0938) lr 1.5567e-04 eta 0:00:32
epoch [43/50] batch [25/25] time 0.152 (0.177) data 0.000 (0.023) loss 1.2725 (1.5133) acc 96.8750 (90.1250) lr 1.2369e-04 eta 0:00:30
epoch [44/50] batch [5/25] time 0.155 (0.270) data 0.000 (0.115) loss 1.2910 (1.3398) acc 96.8750 (95.0000) lr 1.2369e-04 eta 0:00:45
epoch [44/50] batch [10/25] time 0.152 (0.212) data 0.000 (0.058) loss 1.5908 (1.4346) acc 87.5000 (91.8750) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.039) loss 1.3477 (1.4056) acc 87.5000 (91.6667) lr 1.2369e-04 eta 0:00:30
epoch [44/50] batch [20/25] time 0.153 (0.182) data 0.000 (0.029) loss 1.6836 (1.4327) acc 93.7500 (91.4062) lr 1.2369e-04 eta 0:00:28
epoch [44/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 2.0137 (1.4911) acc 90.6250 (90.6250) lr 9.5173e-05 eta 0:00:26
epoch [45/50] batch [5/25] time 0.154 (0.295) data 0.000 (0.140) loss 2.0527 (1.5773) acc 78.1250 (87.5000) lr 9.5173e-05 eta 0:00:42
epoch [45/50] batch [10/25] time 0.153 (0.224) data 0.000 (0.070) loss 1.2207 (1.5005) acc 96.8750 (90.3125) lr 9.5173e-05 eta 0:00:31
epoch [45/50] batch [15/25] time 0.153 (0.200) data 0.000 (0.047) loss 1.0352 (1.4607) acc 96.8750 (90.2083) lr 9.5173e-05 eta 0:00:27
epoch [45/50] batch [20/25] time 0.152 (0.188) data 0.000 (0.035) loss 1.2490 (1.4620) acc 93.7500 (90.6250) lr 9.5173e-05 eta 0:00:24
epoch [45/50] batch [25/25] time 0.152 (0.181) data 0.000 (0.028) loss 1.4277 (1.4341) acc 90.6250 (90.8750) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [5/25] time 0.154 (0.259) data 0.000 (0.104) loss 1.0752 (1.4549) acc 96.8750 (90.6250) lr 7.0224e-05 eta 0:00:31
epoch [46/50] batch [10/25] time 0.226 (0.214) data 0.000 (0.052) loss 1.2705 (1.4390) acc 93.7500 (91.5625) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.035) loss 1.9092 (1.4650) acc 81.2500 (90.4167) lr 7.0224e-05 eta 0:00:21
epoch [46/50] batch [20/25] time 0.153 (0.185) data 0.000 (0.026) loss 1.6084 (1.5203) acc 87.5000 (89.5312) lr 7.0224e-05 eta 0:00:19
epoch [46/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.021) loss 1.7344 (1.5369) acc 78.1250 (89.0000) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [5/25] time 0.153 (0.273) data 0.000 (0.118) loss 1.5342 (1.5908) acc 87.5000 (87.5000) lr 4.8943e-05 eta 0:00:25
epoch [47/50] batch [10/25] time 0.154 (0.213) data 0.000 (0.059) loss 0.8975 (1.4675) acc 96.8750 (88.7500) lr 4.8943e-05 eta 0:00:19
epoch [47/50] batch [15/25] time 0.154 (0.193) data 0.000 (0.040) loss 1.6533 (1.4710) acc 90.6250 (89.5833) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.030) loss 1.3672 (1.4513) acc 87.5000 (90.0000) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.024) loss 1.0781 (1.4655) acc 96.8750 (90.1250) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [5/25] time 0.153 (0.261) data 0.000 (0.106) loss 1.3027 (1.3592) acc 90.6250 (92.5000) lr 3.1417e-05 eta 0:00:18
epoch [48/50] batch [10/25] time 0.153 (0.207) data 0.000 (0.053) loss 1.6289 (1.4557) acc 93.7500 (92.1875) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [15/25] time 0.152 (0.189) data 0.000 (0.035) loss 1.6348 (1.4898) acc 90.6250 (90.4167) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [20/25] time 0.152 (0.180) data 0.000 (0.027) loss 1.3379 (1.5041) acc 93.7500 (90.3125) lr 3.1417e-05 eta 0:00:09
epoch [48/50] batch [25/25] time 0.153 (0.174) data 0.000 (0.021) loss 1.5664 (1.4567) acc 93.7500 (90.8750) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [5/25] time 0.154 (0.265) data 0.000 (0.109) loss 1.2988 (1.6051) acc 96.8750 (90.6250) lr 1.7713e-05 eta 0:00:11
epoch [49/50] batch [10/25] time 0.152 (0.209) data 0.000 (0.055) loss 1.4404 (1.5116) acc 90.6250 (91.8750) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [15/25] time 0.154 (0.191) data 0.000 (0.037) loss 1.0469 (1.4683) acc 93.7500 (91.8750) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [20/25] time 0.153 (0.181) data 0.000 (0.027) loss 2.1016 (1.5192) acc 81.2500 (90.9375) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/25] time 0.154 (0.176) data 0.000 (0.022) loss 1.3232 (1.5357) acc 96.8750 (90.7500) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [5/25] time 0.153 (0.269) data 0.000 (0.115) loss 1.5557 (1.6123) acc 90.6250 (88.1250) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.058) loss 1.7363 (1.5627) acc 84.3750 (89.6875) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/25] time 0.152 (0.192) data 0.000 (0.039) loss 1.8037 (1.5375) acc 87.5000 (90.2083) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.153 (0.182) data 0.000 (0.029) loss 1.1660 (1.5182) acc 93.7500 (90.1562) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.7744 (1.5390) acc 84.3750 (89.8750) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.87s/it] 50%|█████     | 2/4 [00:05<00:04,  2.26s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.75s/it]100%|██████████| 4/4 [00:06<00:00,  1.13s/it]100%|██████████| 4/4 [00:06<00:00,  1.61s/it]
=> result
* total: 1,549
* correct: 1,526
* accuracy: 98.5%
* error: 1.5%
* macro_f1: 97.1%
Elapsed: 0:03:54
Run this job and save the output to output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
Setting fixed seed: 3
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: False
head: 
load_epoch: None
model_dir: 
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.W', '1.0', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'base']
output_dir: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
resume: 
root: /data/yht/data/cl/data/
seed: 3
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: base
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
RESUME: 
SEED: 3
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_3.pkl
SUBSAMPLE BASE CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     1,549
---------  ----------
['face', 'leopard', 'motorbike', 'accordion', 'airplane', 'anchor', 'ant', 'barrel', 'bass', 'beaver', 'binocular', 'bonsai', 'brain', 'brontosaurus', 'buddha', 'butterfly', 'camera', 'cannon', 'car_side', 'ceiling_fan', 'cellphone', 'chair', 'chandelier', 'cougar_body', 'cougar_face', 'crab', 'crayfish', 'crocodile', 'crocodile_head', 'cup', 'dalmatian', 'dollar_bill', 'dolphin', 'dragonfly', 'electric_guitar', 'elephant', 'emu', 'euphonium', 'ewer', 'ferry', 'flamingo', 'flamingo_head', 'garfield', 'gerenuk', 'gramophone', 'grand_piano', 'hawksbill', 'headphone', 'hedgehog', 'helicopter']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X face.', 'X X X X leopard.', 'X X X X motorbike.', 'X X X X accordion.', 'X X X X airplane.', 'X X X X anchor.', 'X X X X ant.', 'X X X X barrel.', 'X X X X bass.', 'X X X X beaver.', 'X X X X binocular.', 'X X X X bonsai.', 'X X X X brain.', 'X X X X brontosaurus.', 'X X X X buddha.', 'X X X X butterfly.', 'X X X X camera.', 'X X X X cannon.', 'X X X X car side.', 'X X X X ceiling fan.', 'X X X X cellphone.', 'X X X X chair.', 'X X X X chandelier.', 'X X X X cougar body.', 'X X X X cougar face.', 'X X X X crab.', 'X X X X crayfish.', 'X X X X crocodile.', 'X X X X crocodile head.', 'X X X X cup.', 'X X X X dalmatian.', 'X X X X dollar bill.', 'X X X X dolphin.', 'X X X X dragonfly.', 'X X X X electric guitar.', 'X X X X elephant.', 'X X X X emu.', 'X X X X euphonium.', 'X X X X ewer.', 'X X X X ferry.', 'X X X X flamingo.', 'X X X X flamingo head.', 'X X X X garfield.', 'X X X X gerenuk.', 'X X X X gramophone.', 'X X X X grand piano.', 'X X X X hawksbill.', 'X X X X headphone.', 'X X X X hedgehog.', 'X X X X helicopter.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
Note that load_model() is skipped as no pretrained model is given
No checkpoint found, train from scratch
Initialize tensorboard (log_dir=output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/tensorboard)
epoch [1/50] batch [5/25] time 0.152 (0.278) data 0.000 (0.116) loss 5.1172 (5.1844) acc 84.3750 (75.6250) lr 1.0000e-05 eta 0:05:46
epoch [1/50] batch [10/25] time 0.151 (0.215) data 0.000 (0.058) loss 4.6719 (5.1578) acc 87.5000 (76.5625) lr 1.0000e-05 eta 0:04:26
epoch [1/50] batch [15/25] time 0.158 (0.194) data 0.000 (0.039) loss 4.7734 (5.1516) acc 75.0000 (75.6250) lr 1.0000e-05 eta 0:04:00
epoch [1/50] batch [20/25] time 0.155 (0.185) data 0.000 (0.029) loss 5.1484 (5.1588) acc 81.2500 (74.6875) lr 1.0000e-05 eta 0:03:47
epoch [1/50] batch [25/25] time 0.154 (0.179) data 0.000 (0.023) loss 5.4844 (5.1752) acc 59.3750 (73.8750) lr 2.0000e-03 eta 0:03:39
epoch [2/50] batch [5/25] time 0.155 (0.293) data 0.000 (0.138) loss 3.6387 (4.1367) acc 78.1250 (79.3750) lr 2.0000e-03 eta 0:05:57
epoch [2/50] batch [10/25] time 0.152 (0.224) data 0.000 (0.069) loss 3.3379 (3.8258) acc 84.3750 (80.6250) lr 2.0000e-03 eta 0:04:31
epoch [2/50] batch [15/25] time 0.152 (0.200) data 0.000 (0.046) loss 2.6270 (3.6086) acc 90.6250 (81.6667) lr 2.0000e-03 eta 0:04:01
epoch [2/50] batch [20/25] time 0.152 (0.188) data 0.000 (0.035) loss 2.6211 (3.4034) acc 84.3750 (81.7188) lr 2.0000e-03 eta 0:03:46
epoch [2/50] batch [25/25] time 0.153 (0.181) data 0.000 (0.028) loss 2.2598 (3.2404) acc 84.3750 (82.3750) lr 1.9980e-03 eta 0:03:36
epoch [3/50] batch [5/25] time 0.153 (0.282) data 0.000 (0.130) loss 2.4727 (2.5555) acc 78.1250 (80.0000) lr 1.9980e-03 eta 0:05:37
epoch [3/50] batch [10/25] time 0.153 (0.218) data 0.000 (0.065) loss 2.2773 (2.4340) acc 87.5000 (83.1250) lr 1.9980e-03 eta 0:04:18
epoch [3/50] batch [15/25] time 0.152 (0.196) data 0.000 (0.043) loss 1.8438 (2.3529) acc 93.7500 (85.0000) lr 1.9980e-03 eta 0:03:52
epoch [3/50] batch [20/25] time 0.153 (0.185) data 0.000 (0.033) loss 2.3633 (2.3281) acc 78.1250 (84.3750) lr 1.9980e-03 eta 0:03:38
epoch [3/50] batch [25/25] time 0.152 (0.179) data 0.000 (0.026) loss 2.0566 (2.3005) acc 81.2500 (84.5000) lr 1.9921e-03 eta 0:03:29
epoch [4/50] batch [5/25] time 0.153 (0.279) data 0.000 (0.126) loss 1.7188 (2.0072) acc 93.7500 (85.6250) lr 1.9921e-03 eta 0:05:26
epoch [4/50] batch [10/25] time 0.153 (0.216) data 0.000 (0.063) loss 1.8330 (1.9887) acc 84.3750 (84.6875) lr 1.9921e-03 eta 0:04:11
epoch [4/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.042) loss 2.1094 (2.0323) acc 84.3750 (84.3750) lr 1.9921e-03 eta 0:03:46
epoch [4/50] batch [20/25] time 0.153 (0.185) data 0.000 (0.032) loss 2.4414 (2.0171) acc 81.2500 (85.4688) lr 1.9921e-03 eta 0:03:33
epoch [4/50] batch [25/25] time 0.154 (0.178) data 0.000 (0.025) loss 1.8906 (2.0113) acc 87.5000 (85.6250) lr 1.9823e-03 eta 0:03:25
epoch [5/50] batch [5/25] time 0.153 (0.281) data 0.000 (0.128) loss 1.9229 (2.0547) acc 87.5000 (87.5000) lr 1.9823e-03 eta 0:05:21
epoch [5/50] batch [10/25] time 0.152 (0.217) data 0.000 (0.064) loss 2.2285 (1.9652) acc 81.2500 (87.5000) lr 1.9823e-03 eta 0:04:07
epoch [5/50] batch [15/25] time 0.152 (0.195) data 0.000 (0.043) loss 1.9443 (1.9654) acc 87.5000 (87.7083) lr 1.9823e-03 eta 0:03:41
epoch [5/50] batch [20/25] time 0.153 (0.185) data 0.000 (0.032) loss 1.5605 (1.8954) acc 93.7500 (88.1250) lr 1.9823e-03 eta 0:03:28
epoch [5/50] batch [25/25] time 0.152 (0.178) data 0.000 (0.026) loss 2.3867 (1.9102) acc 75.0000 (87.6250) lr 1.9686e-03 eta 0:03:20
epoch [6/50] batch [5/25] time 0.153 (0.272) data 0.000 (0.119) loss 1.8828 (1.8479) acc 90.6250 (88.7500) lr 1.9686e-03 eta 0:05:04
epoch [6/50] batch [10/25] time 0.153 (0.213) data 0.000 (0.059) loss 2.2402 (1.8891) acc 84.3750 (88.4375) lr 1.9686e-03 eta 0:03:57
epoch [6/50] batch [15/25] time 0.152 (0.193) data 0.000 (0.040) loss 1.4619 (1.8850) acc 96.8750 (88.1250) lr 1.9686e-03 eta 0:03:33
epoch [6/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.030) loss 1.5186 (1.8908) acc 90.6250 (87.5000) lr 1.9686e-03 eta 0:03:21
epoch [6/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.024) loss 1.7197 (1.8664) acc 87.5000 (87.8750) lr 1.9511e-03 eta 0:03:14
epoch [7/50] batch [5/25] time 0.153 (0.279) data 0.000 (0.125) loss 1.6348 (1.7635) acc 84.3750 (86.2500) lr 1.9511e-03 eta 0:05:06
epoch [7/50] batch [10/25] time 0.153 (0.216) data 0.000 (0.063) loss 2.2891 (1.8069) acc 78.1250 (85.3125) lr 1.9511e-03 eta 0:03:55
epoch [7/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.042) loss 1.6426 (1.8845) acc 90.6250 (85.4167) lr 1.9511e-03 eta 0:03:31
epoch [7/50] batch [20/25] time 0.154 (0.185) data 0.000 (0.031) loss 2.0840 (1.8854) acc 84.3750 (85.7812) lr 1.9511e-03 eta 0:03:19
epoch [7/50] batch [25/25] time 0.154 (0.178) data 0.000 (0.025) loss 1.7734 (1.8648) acc 87.5000 (86.2500) lr 1.9298e-03 eta 0:03:11
epoch [8/50] batch [5/25] time 0.153 (0.274) data 0.000 (0.120) loss 2.2051 (1.7084) acc 78.1250 (87.5000) lr 1.9298e-03 eta 0:04:53
epoch [8/50] batch [10/25] time 0.155 (0.214) data 0.000 (0.060) loss 2.2695 (1.7983) acc 78.1250 (85.9375) lr 1.9298e-03 eta 0:03:47
epoch [8/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.040) loss 1.9482 (1.8039) acc 90.6250 (86.2500) lr 1.9298e-03 eta 0:03:25
epoch [8/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.030) loss 1.6680 (1.7471) acc 93.7500 (87.8125) lr 1.9298e-03 eta 0:03:13
epoch [8/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.024) loss 1.7744 (1.7806) acc 84.3750 (87.0000) lr 1.9048e-03 eta 0:03:06
epoch [9/50] batch [5/25] time 0.154 (0.270) data 0.000 (0.116) loss 1.4600 (1.6262) acc 100.0000 (93.1250) lr 1.9048e-03 eta 0:04:42
epoch [9/50] batch [10/25] time 0.153 (0.212) data 0.000 (0.058) loss 1.3633 (1.6899) acc 90.6250 (90.3125) lr 1.9048e-03 eta 0:03:40
epoch [9/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.039) loss 1.2598 (1.6391) acc 93.7500 (91.0417) lr 1.9048e-03 eta 0:03:18
epoch [9/50] batch [20/25] time 0.153 (0.182) data 0.000 (0.029) loss 1.9961 (1.6881) acc 84.3750 (90.1562) lr 1.9048e-03 eta 0:03:07
epoch [9/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.023) loss 1.5010 (1.6846) acc 93.7500 (89.2500) lr 1.8763e-03 eta 0:03:00
epoch [10/50] batch [5/25] time 0.154 (0.273) data 0.000 (0.118) loss 1.3652 (1.7301) acc 96.8750 (88.7500) lr 1.8763e-03 eta 0:04:38
epoch [10/50] batch [10/25] time 0.153 (0.213) data 0.000 (0.059) loss 1.8779 (1.7466) acc 84.3750 (87.1875) lr 1.8763e-03 eta 0:03:36
epoch [10/50] batch [15/25] time 0.152 (0.193) data 0.000 (0.040) loss 1.5801 (1.6841) acc 87.5000 (87.9167) lr 1.8763e-03 eta 0:03:14
epoch [10/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.030) loss 1.2842 (1.6423) acc 90.6250 (88.5938) lr 1.8763e-03 eta 0:03:03
epoch [10/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.024) loss 1.8770 (1.6676) acc 84.3750 (88.2500) lr 1.8443e-03 eta 0:02:57
epoch [11/50] batch [5/25] time 0.153 (0.274) data 0.000 (0.120) loss 1.9336 (1.7455) acc 75.0000 (85.6250) lr 1.8443e-03 eta 0:04:32
epoch [11/50] batch [10/25] time 0.153 (0.214) data 0.000 (0.060) loss 1.6504 (1.7043) acc 90.6250 (86.8750) lr 1.8443e-03 eta 0:03:31
epoch [11/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.040) loss 1.5547 (1.6580) acc 90.6250 (87.5000) lr 1.8443e-03 eta 0:03:10
epoch [11/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.030) loss 1.6953 (1.6923) acc 90.6250 (86.8750) lr 1.8443e-03 eta 0:02:59
epoch [11/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.024) loss 1.6289 (1.6662) acc 90.6250 (87.2500) lr 1.8090e-03 eta 0:02:52
epoch [12/50] batch [5/25] time 0.154 (0.277) data 0.000 (0.123) loss 1.5391 (1.3619) acc 90.6250 (92.5000) lr 1.8090e-03 eta 0:04:29
epoch [12/50] batch [10/25] time 0.153 (0.216) data 0.000 (0.062) loss 1.4434 (1.5289) acc 96.8750 (90.9375) lr 1.8090e-03 eta 0:03:28
epoch [12/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.041) loss 1.6074 (1.5210) acc 90.6250 (91.0417) lr 1.8090e-03 eta 0:03:06
epoch [12/50] batch [20/25] time 0.153 (0.184) data 0.000 (0.031) loss 1.1670 (1.5510) acc 96.8750 (90.7812) lr 1.8090e-03 eta 0:02:55
epoch [12/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.025) loss 1.3838 (1.6072) acc 87.5000 (89.3750) lr 1.7705e-03 eta 0:02:49
epoch [13/50] batch [5/25] time 0.153 (0.274) data 0.000 (0.120) loss 1.9482 (1.7535) acc 81.2500 (85.0000) lr 1.7705e-03 eta 0:04:18
epoch [13/50] batch [10/25] time 0.153 (0.213) data 0.000 (0.060) loss 2.0410 (1.8357) acc 90.6250 (85.9375) lr 1.7705e-03 eta 0:03:20
epoch [13/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.040) loss 1.4434 (1.7952) acc 96.8750 (86.8750) lr 1.7705e-03 eta 0:03:00
epoch [13/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.030) loss 1.9004 (1.7781) acc 81.2500 (86.7188) lr 1.7705e-03 eta 0:02:50
epoch [13/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.024) loss 1.2764 (1.7087) acc 100.0000 (88.3750) lr 1.7290e-03 eta 0:02:44
epoch [14/50] batch [5/25] time 0.154 (0.273) data 0.000 (0.118) loss 1.4893 (1.5674) acc 84.3750 (87.5000) lr 1.7290e-03 eta 0:04:10
epoch [14/50] batch [10/25] time 0.153 (0.213) data 0.000 (0.059) loss 1.8516 (1.8245) acc 84.3750 (85.9375) lr 1.7290e-03 eta 0:03:15
epoch [14/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.040) loss 1.4023 (1.6988) acc 90.6250 (87.0833) lr 1.7290e-03 eta 0:02:55
epoch [14/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.030) loss 1.4111 (1.7021) acc 93.7500 (86.8750) lr 1.7290e-03 eta 0:02:45
epoch [14/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.024) loss 1.2051 (1.6347) acc 96.8750 (87.8750) lr 1.6845e-03 eta 0:02:39
epoch [15/50] batch [5/25] time 0.153 (0.272) data 0.000 (0.118) loss 1.4326 (1.4426) acc 90.6250 (91.8750) lr 1.6845e-03 eta 0:04:03
epoch [15/50] batch [10/25] time 0.154 (0.213) data 0.000 (0.059) loss 1.5078 (1.5043) acc 87.5000 (91.2500) lr 1.6845e-03 eta 0:03:09
epoch [15/50] batch [15/25] time 0.154 (0.193) data 0.000 (0.040) loss 1.1748 (1.5471) acc 96.8750 (90.2083) lr 1.6845e-03 eta 0:02:51
epoch [15/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.030) loss 1.8809 (1.6142) acc 81.2500 (89.0625) lr 1.6845e-03 eta 0:02:41
epoch [15/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.024) loss 2.0879 (1.6575) acc 84.3750 (88.1250) lr 1.6374e-03 eta 0:02:35
epoch [16/50] batch [5/25] time 0.154 (0.276) data 0.000 (0.121) loss 1.5273 (1.4074) acc 87.5000 (93.1250) lr 1.6374e-03 eta 0:03:59
epoch [16/50] batch [10/25] time 0.153 (0.214) data 0.000 (0.061) loss 1.5029 (1.4461) acc 90.6250 (91.5625) lr 1.6374e-03 eta 0:03:05
epoch [16/50] batch [15/25] time 0.153 (0.194) data 0.000 (0.041) loss 1.3545 (1.4618) acc 93.7500 (91.0417) lr 1.6374e-03 eta 0:02:46
epoch [16/50] batch [20/25] time 0.153 (0.184) data 0.000 (0.030) loss 1.5146 (1.4974) acc 87.5000 (90.7812) lr 1.6374e-03 eta 0:02:37
epoch [16/50] batch [25/25] time 0.154 (0.178) data 0.000 (0.024) loss 1.3330 (1.5184) acc 100.0000 (90.3750) lr 1.5878e-03 eta 0:02:31
epoch [17/50] batch [5/25] time 0.154 (0.308) data 0.000 (0.154) loss 1.9902 (1.5928) acc 78.1250 (90.0000) lr 1.5878e-03 eta 0:04:20
epoch [17/50] batch [10/25] time 0.154 (0.231) data 0.000 (0.077) loss 1.9502 (1.5688) acc 84.3750 (89.6875) lr 1.5878e-03 eta 0:03:14
epoch [17/50] batch [15/25] time 0.156 (0.206) data 0.000 (0.052) loss 1.2246 (1.4964) acc 93.7500 (90.8333) lr 1.5878e-03 eta 0:02:51
epoch [17/50] batch [20/25] time 0.154 (0.193) data 0.000 (0.039) loss 1.4854 (1.4896) acc 90.6250 (91.0938) lr 1.5878e-03 eta 0:02:40
epoch [17/50] batch [25/25] time 0.154 (0.185) data 0.000 (0.031) loss 1.4180 (1.5130) acc 90.6250 (89.6250) lr 1.5358e-03 eta 0:02:32
epoch [18/50] batch [5/25] time 0.154 (0.270) data 0.000 (0.116) loss 2.0195 (1.6131) acc 78.1250 (86.8750) lr 1.5358e-03 eta 0:03:41
epoch [18/50] batch [10/25] time 0.154 (0.212) data 0.000 (0.058) loss 1.8301 (1.6374) acc 90.6250 (87.1875) lr 1.5358e-03 eta 0:02:52
epoch [18/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.039) loss 1.2656 (1.5600) acc 90.6250 (88.7500) lr 1.5358e-03 eta 0:02:36
epoch [18/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.029) loss 1.1621 (1.5188) acc 93.7500 (89.5312) lr 1.5358e-03 eta 0:02:27
epoch [18/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.023) loss 1.7148 (1.5160) acc 87.5000 (89.8750) lr 1.4818e-03 eta 0:02:21
epoch [19/50] batch [5/25] time 0.154 (0.268) data 0.000 (0.114) loss 1.6641 (1.8076) acc 87.5000 (86.2500) lr 1.4818e-03 eta 0:03:33
epoch [19/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.057) loss 1.2676 (1.6046) acc 93.7500 (88.4375) lr 1.4818e-03 eta 0:02:46
epoch [19/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.038) loss 1.4629 (1.6146) acc 93.7500 (89.5833) lr 1.4818e-03 eta 0:02:30
epoch [19/50] batch [20/25] time 0.154 (0.182) data 0.000 (0.029) loss 1.5889 (1.5547) acc 87.5000 (90.0000) lr 1.4818e-03 eta 0:02:21
epoch [19/50] batch [25/25] time 0.154 (0.176) data 0.000 (0.023) loss 1.3945 (1.5277) acc 87.5000 (90.1250) lr 1.4258e-03 eta 0:02:16
epoch [20/50] batch [5/25] time 0.155 (0.288) data 0.000 (0.133) loss 1.9355 (1.4787) acc 84.3750 (88.7500) lr 1.4258e-03 eta 0:03:41
epoch [20/50] batch [10/25] time 0.154 (0.221) data 0.000 (0.066) loss 1.9160 (1.6653) acc 84.3750 (88.1250) lr 1.4258e-03 eta 0:02:49
epoch [20/50] batch [15/25] time 0.154 (0.199) data 0.000 (0.044) loss 1.3506 (1.6322) acc 87.5000 (88.7500) lr 1.4258e-03 eta 0:02:31
epoch [20/50] batch [20/25] time 0.154 (0.188) data 0.000 (0.033) loss 1.2617 (1.6083) acc 90.6250 (88.5938) lr 1.4258e-03 eta 0:02:21
epoch [20/50] batch [25/25] time 0.154 (0.181) data 0.000 (0.027) loss 2.3125 (1.5952) acc 68.7500 (88.0000) lr 1.3681e-03 eta 0:02:15
epoch [21/50] batch [5/25] time 0.154 (0.273) data 0.000 (0.119) loss 1.8887 (1.5344) acc 87.5000 (90.6250) lr 1.3681e-03 eta 0:03:23
epoch [21/50] batch [10/25] time 0.153 (0.214) data 0.000 (0.059) loss 1.1152 (1.4056) acc 93.7500 (92.5000) lr 1.3681e-03 eta 0:02:37
epoch [21/50] batch [15/25] time 0.154 (0.193) data 0.000 (0.040) loss 1.6855 (1.4169) acc 90.6250 (91.8750) lr 1.3681e-03 eta 0:02:22
epoch [21/50] batch [20/25] time 0.154 (0.184) data 0.000 (0.030) loss 1.2676 (1.4380) acc 90.6250 (91.7188) lr 1.3681e-03 eta 0:02:13
epoch [21/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.024) loss 1.4521 (1.4337) acc 90.6250 (91.7500) lr 1.3090e-03 eta 0:02:08
epoch [22/50] batch [5/25] time 0.153 (0.279) data 0.000 (0.124) loss 1.9375 (1.8252) acc 87.5000 (88.7500) lr 1.3090e-03 eta 0:03:20
epoch [22/50] batch [10/25] time 0.154 (0.216) data 0.000 (0.062) loss 1.4688 (1.6707) acc 90.6250 (88.4375) lr 1.3090e-03 eta 0:02:34
epoch [22/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.041) loss 2.2207 (1.7007) acc 75.0000 (87.5000) lr 1.3090e-03 eta 0:02:18
epoch [22/50] batch [20/25] time 0.153 (0.185) data 0.000 (0.031) loss 1.2520 (1.6216) acc 90.6250 (88.7500) lr 1.3090e-03 eta 0:02:10
epoch [22/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.025) loss 1.1562 (1.5765) acc 96.8750 (89.3750) lr 1.2487e-03 eta 0:02:04
epoch [23/50] batch [5/25] time 0.155 (0.274) data 0.000 (0.119) loss 2.6094 (1.6779) acc 78.1250 (89.3750) lr 1.2487e-03 eta 0:03:10
epoch [23/50] batch [10/25] time 0.153 (0.214) data 0.000 (0.060) loss 1.8135 (1.6232) acc 93.7500 (90.0000) lr 1.2487e-03 eta 0:02:27
epoch [23/50] batch [15/25] time 0.153 (0.194) data 0.000 (0.040) loss 0.9673 (1.5405) acc 96.8750 (90.6250) lr 1.2487e-03 eta 0:02:12
epoch [23/50] batch [20/25] time 0.153 (0.184) data 0.000 (0.030) loss 1.1172 (1.4989) acc 93.7500 (91.2500) lr 1.2487e-03 eta 0:02:04
epoch [23/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.024) loss 1.3105 (1.4757) acc 93.7500 (91.1250) lr 1.1874e-03 eta 0:01:59
epoch [24/50] batch [5/25] time 0.153 (0.277) data 0.000 (0.123) loss 1.6514 (1.6346) acc 87.5000 (88.1250) lr 1.1874e-03 eta 0:03:05
epoch [24/50] batch [10/25] time 0.153 (0.215) data 0.000 (0.061) loss 1.5967 (1.6176) acc 90.6250 (88.4375) lr 1.1874e-03 eta 0:02:23
epoch [24/50] batch [15/25] time 0.153 (0.194) data 0.000 (0.041) loss 1.0273 (1.5443) acc 96.8750 (89.3750) lr 1.1874e-03 eta 0:02:08
epoch [24/50] batch [20/25] time 0.154 (0.184) data 0.000 (0.031) loss 1.7041 (1.5451) acc 87.5000 (89.6875) lr 1.1874e-03 eta 0:02:00
epoch [24/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.025) loss 1.5791 (1.5399) acc 90.6250 (89.7500) lr 1.1253e-03 eta 0:01:55
epoch [25/50] batch [5/25] time 0.153 (0.294) data 0.000 (0.140) loss 1.4922 (1.3834) acc 93.7500 (95.0000) lr 1.1253e-03 eta 0:03:09
epoch [25/50] batch [10/25] time 0.154 (0.224) data 0.000 (0.070) loss 1.2090 (1.3511) acc 90.6250 (94.0625) lr 1.1253e-03 eta 0:02:23
epoch [25/50] batch [15/25] time 0.153 (0.200) data 0.000 (0.047) loss 2.0586 (1.4271) acc 81.2500 (92.7083) lr 1.1253e-03 eta 0:02:07
epoch [25/50] batch [20/25] time 0.153 (0.189) data 0.000 (0.035) loss 1.2861 (1.4474) acc 96.8750 (92.3438) lr 1.1253e-03 eta 0:01:58
epoch [25/50] batch [25/25] time 0.153 (0.181) data 0.000 (0.028) loss 1.7178 (1.4208) acc 81.2500 (92.1250) lr 1.0628e-03 eta 0:01:53
epoch [26/50] batch [5/25] time 0.154 (0.283) data 0.000 (0.129) loss 1.2285 (1.2896) acc 96.8750 (93.7500) lr 1.0628e-03 eta 0:02:55
epoch [26/50] batch [10/25] time 0.155 (0.219) data 0.000 (0.065) loss 1.3252 (1.3575) acc 96.8750 (93.4375) lr 1.0628e-03 eta 0:02:14
epoch [26/50] batch [15/25] time 0.154 (0.198) data 0.000 (0.043) loss 1.6172 (1.4252) acc 90.6250 (91.6667) lr 1.0628e-03 eta 0:02:00
epoch [26/50] batch [20/25] time 0.154 (0.187) data 0.000 (0.032) loss 1.5732 (1.4429) acc 90.6250 (91.5625) lr 1.0628e-03 eta 0:01:53
epoch [26/50] batch [25/25] time 0.153 (0.180) data 0.000 (0.026) loss 2.2480 (1.4823) acc 75.0000 (91.1250) lr 1.0000e-03 eta 0:01:48
epoch [27/50] batch [5/25] time 0.153 (0.278) data 0.000 (0.124) loss 1.4395 (1.6426) acc 93.7500 (86.8750) lr 1.0000e-03 eta 0:02:45
epoch [27/50] batch [10/25] time 0.154 (0.216) data 0.000 (0.062) loss 1.8965 (1.5604) acc 87.5000 (89.0625) lr 1.0000e-03 eta 0:02:07
epoch [27/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.041) loss 1.2939 (1.4841) acc 96.8750 (90.6250) lr 1.0000e-03 eta 0:01:53
epoch [27/50] batch [20/25] time 0.153 (0.184) data 0.000 (0.031) loss 1.6162 (1.5212) acc 84.3750 (89.2188) lr 1.0000e-03 eta 0:01:46
epoch [27/50] batch [25/25] time 0.153 (0.178) data 0.000 (0.025) loss 1.3086 (1.5128) acc 87.5000 (89.1250) lr 9.3721e-04 eta 0:01:42
epoch [28/50] batch [5/25] time 0.154 (0.259) data 0.000 (0.105) loss 1.1533 (1.4236) acc 93.7500 (91.2500) lr 9.3721e-04 eta 0:02:27
epoch [28/50] batch [10/25] time 0.153 (0.206) data 0.000 (0.053) loss 1.4512 (1.4170) acc 90.6250 (91.5625) lr 9.3721e-04 eta 0:01:56
epoch [28/50] batch [15/25] time 0.153 (0.189) data 0.000 (0.035) loss 1.8008 (1.4350) acc 81.2500 (91.4583) lr 9.3721e-04 eta 0:01:45
epoch [28/50] batch [20/25] time 0.153 (0.180) data 0.000 (0.026) loss 1.9746 (1.5405) acc 84.3750 (89.5312) lr 9.3721e-04 eta 0:01:39
epoch [28/50] batch [25/25] time 0.153 (0.175) data 0.000 (0.021) loss 1.7793 (1.5841) acc 84.3750 (89.1250) lr 8.7467e-04 eta 0:01:36
epoch [29/50] batch [5/25] time 0.153 (0.271) data 0.000 (0.116) loss 1.4111 (1.3822) acc 87.5000 (90.0000) lr 8.7467e-04 eta 0:02:27
epoch [29/50] batch [10/25] time 0.153 (0.212) data 0.000 (0.058) loss 1.4199 (1.3464) acc 100.0000 (92.5000) lr 8.7467e-04 eta 0:01:54
epoch [29/50] batch [15/25] time 0.154 (0.193) data 0.000 (0.039) loss 1.0986 (1.3641) acc 100.0000 (92.2917) lr 8.7467e-04 eta 0:01:43
epoch [29/50] batch [20/25] time 0.152 (0.183) data 0.000 (0.029) loss 1.4189 (1.4184) acc 90.6250 (91.4062) lr 8.7467e-04 eta 0:01:36
epoch [29/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.023) loss 1.4844 (1.4889) acc 87.5000 (90.6250) lr 8.1262e-04 eta 0:01:32
epoch [30/50] batch [5/25] time 0.153 (0.271) data 0.000 (0.117) loss 1.8867 (1.7482) acc 81.2500 (84.3750) lr 8.1262e-04 eta 0:02:21
epoch [30/50] batch [10/25] time 0.155 (0.213) data 0.000 (0.059) loss 1.7715 (1.6774) acc 81.2500 (86.2500) lr 8.1262e-04 eta 0:01:49
epoch [30/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.039) loss 1.5166 (1.6072) acc 90.6250 (87.5000) lr 8.1262e-04 eta 0:01:38
epoch [30/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.029) loss 1.6289 (1.5396) acc 90.6250 (89.3750) lr 8.1262e-04 eta 0:01:32
epoch [30/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.024) loss 1.5947 (1.5641) acc 87.5000 (89.2500) lr 7.5131e-04 eta 0:01:28
epoch [31/50] batch [5/25] time 0.154 (0.270) data 0.000 (0.115) loss 1.2637 (1.3289) acc 84.3750 (90.0000) lr 7.5131e-04 eta 0:02:13
epoch [31/50] batch [10/25] time 0.154 (0.212) data 0.000 (0.058) loss 1.8516 (1.4249) acc 81.2500 (89.3750) lr 7.5131e-04 eta 0:01:43
epoch [31/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.039) loss 1.3916 (1.4084) acc 93.7500 (91.0417) lr 7.5131e-04 eta 0:01:33
epoch [31/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.029) loss 1.3174 (1.3533) acc 93.7500 (92.3438) lr 7.5131e-04 eta 0:01:27
epoch [31/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.023) loss 1.4639 (1.3881) acc 93.7500 (91.5000) lr 6.9098e-04 eta 0:01:23
epoch [32/50] batch [5/25] time 0.153 (0.259) data 0.000 (0.104) loss 1.6182 (1.4316) acc 87.5000 (91.2500) lr 6.9098e-04 eta 0:02:01
epoch [32/50] batch [10/25] time 0.153 (0.206) data 0.000 (0.052) loss 2.0332 (1.4956) acc 84.3750 (91.5625) lr 6.9098e-04 eta 0:01:35
epoch [32/50] batch [15/25] time 0.153 (0.189) data 0.000 (0.035) loss 1.1484 (1.4479) acc 96.8750 (91.4583) lr 6.9098e-04 eta 0:01:26
epoch [32/50] batch [20/25] time 0.158 (0.181) data 0.000 (0.026) loss 1.2285 (1.4479) acc 96.8750 (91.2500) lr 6.9098e-04 eta 0:01:22
epoch [32/50] batch [25/25] time 0.157 (0.176) data 0.000 (0.021) loss 1.5488 (1.4470) acc 90.6250 (90.8750) lr 6.3188e-04 eta 0:01:19
epoch [33/50] batch [5/25] time 0.154 (0.258) data 0.000 (0.103) loss 1.7734 (1.7773) acc 87.5000 (83.7500) lr 6.3188e-04 eta 0:01:54
epoch [33/50] batch [10/25] time 0.153 (0.206) data 0.000 (0.052) loss 1.7461 (1.5840) acc 87.5000 (87.5000) lr 6.3188e-04 eta 0:01:30
epoch [33/50] batch [15/25] time 0.153 (0.188) data 0.000 (0.034) loss 1.8955 (1.6290) acc 90.6250 (87.9167) lr 6.3188e-04 eta 0:01:21
epoch [33/50] batch [20/25] time 0.153 (0.179) data 0.000 (0.026) loss 1.1875 (1.5598) acc 96.8750 (89.2188) lr 6.3188e-04 eta 0:01:17
epoch [33/50] batch [25/25] time 0.153 (0.174) data 0.000 (0.021) loss 1.0195 (1.5526) acc 96.8750 (89.1250) lr 5.7422e-04 eta 0:01:14
epoch [34/50] batch [5/25] time 0.155 (0.269) data 0.000 (0.115) loss 1.6172 (1.5691) acc 84.3750 (86.8750) lr 5.7422e-04 eta 0:01:53
epoch [34/50] batch [10/25] time 0.155 (0.212) data 0.000 (0.057) loss 1.3916 (1.5603) acc 90.6250 (87.1875) lr 5.7422e-04 eta 0:01:27
epoch [34/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.038) loss 1.3760 (1.6094) acc 93.7500 (87.7083) lr 5.7422e-04 eta 0:01:18
epoch [34/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.029) loss 1.4775 (1.5978) acc 90.6250 (87.8125) lr 5.7422e-04 eta 0:01:14
epoch [34/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.023) loss 1.4072 (1.5554) acc 87.5000 (88.3750) lr 5.1825e-04 eta 0:01:10
epoch [35/50] batch [5/25] time 0.154 (0.254) data 0.000 (0.100) loss 1.7227 (1.5648) acc 81.2500 (88.1250) lr 5.1825e-04 eta 0:01:40
epoch [35/50] batch [10/25] time 0.154 (0.204) data 0.000 (0.050) loss 1.0586 (1.4483) acc 100.0000 (90.6250) lr 5.1825e-04 eta 0:01:19
epoch [35/50] batch [15/25] time 0.153 (0.187) data 0.000 (0.033) loss 1.0312 (1.4090) acc 96.8750 (91.0417) lr 5.1825e-04 eta 0:01:12
epoch [35/50] batch [20/25] time 0.153 (0.179) data 0.000 (0.025) loss 1.4150 (1.4294) acc 93.7500 (90.7812) lr 5.1825e-04 eta 0:01:07
epoch [35/50] batch [25/25] time 0.154 (0.174) data 0.000 (0.020) loss 1.6211 (1.4625) acc 93.7500 (90.6250) lr 4.6417e-04 eta 0:01:05
epoch [36/50] batch [5/25] time 0.155 (0.266) data 0.000 (0.111) loss 1.3164 (1.3012) acc 90.6250 (93.7500) lr 4.6417e-04 eta 0:01:38
epoch [36/50] batch [10/25] time 0.153 (0.210) data 0.000 (0.056) loss 1.8770 (1.4372) acc 84.3750 (91.5625) lr 4.6417e-04 eta 0:01:16
epoch [36/50] batch [15/25] time 0.153 (0.191) data 0.000 (0.037) loss 1.7168 (1.4298) acc 90.6250 (91.8750) lr 4.6417e-04 eta 0:01:08
epoch [36/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.028) loss 1.2617 (1.3732) acc 96.8750 (93.4375) lr 4.6417e-04 eta 0:01:04
epoch [36/50] batch [25/25] time 0.152 (0.176) data 0.000 (0.022) loss 1.6934 (1.4257) acc 87.5000 (92.3750) lr 4.1221e-04 eta 0:01:01
epoch [37/50] batch [5/25] time 0.154 (0.275) data 0.000 (0.119) loss 1.3848 (1.5029) acc 93.7500 (89.3750) lr 4.1221e-04 eta 0:01:34
epoch [37/50] batch [10/25] time 0.152 (0.214) data 0.000 (0.060) loss 1.2520 (1.4110) acc 96.8750 (91.5625) lr 4.1221e-04 eta 0:01:12
epoch [37/50] batch [15/25] time 0.153 (0.195) data 0.000 (0.040) loss 1.3164 (1.4169) acc 87.5000 (91.4583) lr 4.1221e-04 eta 0:01:05
epoch [37/50] batch [20/25] time 0.154 (0.184) data 0.000 (0.030) loss 1.3477 (1.4348) acc 93.7500 (91.5625) lr 4.1221e-04 eta 0:01:00
epoch [37/50] batch [25/25] time 0.154 (0.178) data 0.000 (0.024) loss 1.4590 (1.4450) acc 93.7500 (91.3750) lr 3.6258e-04 eta 0:00:57
epoch [38/50] batch [5/25] time 0.154 (0.273) data 0.000 (0.118) loss 1.3809 (1.5838) acc 96.8750 (89.3750) lr 3.6258e-04 eta 0:01:27
epoch [38/50] batch [10/25] time 0.153 (0.213) data 0.000 (0.059) loss 1.6074 (1.4579) acc 84.3750 (89.6875) lr 3.6258e-04 eta 0:01:07
epoch [38/50] batch [15/25] time 0.154 (0.193) data 0.000 (0.039) loss 1.0898 (1.4370) acc 96.8750 (90.6250) lr 3.6258e-04 eta 0:00:59
epoch [38/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.030) loss 1.3457 (1.4660) acc 87.5000 (90.0000) lr 3.6258e-04 eta 0:00:55
epoch [38/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.024) loss 1.1836 (1.4600) acc 93.7500 (90.1250) lr 3.1545e-04 eta 0:00:53
epoch [39/50] batch [5/25] time 0.153 (0.261) data 0.000 (0.107) loss 1.8379 (1.3043) acc 84.3750 (93.7500) lr 3.1545e-04 eta 0:01:17
epoch [39/50] batch [10/25] time 0.154 (0.208) data 0.000 (0.054) loss 1.4023 (1.4234) acc 90.6250 (91.8750) lr 3.1545e-04 eta 0:01:00
epoch [39/50] batch [15/25] time 0.158 (0.190) data 0.000 (0.036) loss 0.9893 (1.3954) acc 93.7500 (91.6667) lr 3.1545e-04 eta 0:00:54
epoch [39/50] batch [20/25] time 0.154 (0.181) data 0.000 (0.027) loss 1.3301 (1.4090) acc 93.7500 (91.4062) lr 3.1545e-04 eta 0:00:50
epoch [39/50] batch [25/25] time 0.154 (0.175) data 0.000 (0.022) loss 1.8457 (1.4196) acc 90.6250 (91.1250) lr 2.7103e-04 eta 0:00:48
epoch [40/50] batch [5/25] time 0.154 (0.271) data 0.000 (0.116) loss 1.6680 (1.6207) acc 84.3750 (87.5000) lr 2.7103e-04 eta 0:01:13
epoch [40/50] batch [10/25] time 0.153 (0.212) data 0.000 (0.058) loss 1.4756 (1.5041) acc 90.6250 (90.3125) lr 2.7103e-04 eta 0:00:56
epoch [40/50] batch [15/25] time 0.154 (0.192) data 0.000 (0.039) loss 1.3105 (1.4888) acc 90.6250 (90.4167) lr 2.7103e-04 eta 0:00:50
epoch [40/50] batch [20/25] time 0.155 (0.183) data 0.000 (0.029) loss 1.7734 (1.4910) acc 87.5000 (90.4688) lr 2.7103e-04 eta 0:00:46
epoch [40/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.023) loss 1.4453 (1.5198) acc 87.5000 (89.7500) lr 2.2949e-04 eta 0:00:44
epoch [41/50] batch [5/25] time 0.154 (0.256) data 0.000 (0.102) loss 0.8760 (1.2871) acc 100.0000 (93.1250) lr 2.2949e-04 eta 0:01:02
epoch [41/50] batch [10/25] time 0.153 (0.204) data 0.000 (0.051) loss 1.0928 (1.3449) acc 93.7500 (91.8750) lr 2.2949e-04 eta 0:00:49
epoch [41/50] batch [15/25] time 0.152 (0.187) data 0.000 (0.034) loss 1.2031 (1.3365) acc 90.6250 (92.5000) lr 2.2949e-04 eta 0:00:43
epoch [41/50] batch [20/25] time 0.152 (0.178) data 0.000 (0.026) loss 1.7832 (1.3526) acc 84.3750 (92.0312) lr 2.2949e-04 eta 0:00:41
epoch [41/50] batch [25/25] time 0.153 (0.173) data 0.000 (0.021) loss 1.8594 (1.3989) acc 84.3750 (91.8750) lr 1.9098e-04 eta 0:00:38
epoch [42/50] batch [5/25] time 0.153 (0.271) data 0.000 (0.116) loss 1.5723 (1.5529) acc 90.6250 (90.6250) lr 1.9098e-04 eta 0:00:59
epoch [42/50] batch [10/25] time 0.153 (0.212) data 0.000 (0.058) loss 2.3906 (1.5034) acc 81.2500 (90.6250) lr 1.9098e-04 eta 0:00:45
epoch [42/50] batch [15/25] time 0.157 (0.193) data 0.000 (0.039) loss 1.6006 (1.5331) acc 87.5000 (90.2083) lr 1.9098e-04 eta 0:00:40
epoch [42/50] batch [20/25] time 0.154 (0.183) data 0.000 (0.029) loss 1.5596 (1.5140) acc 87.5000 (90.1562) lr 1.9098e-04 eta 0:00:37
epoch [42/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.023) loss 1.2158 (1.5477) acc 90.6250 (89.6250) lr 1.5567e-04 eta 0:00:35
epoch [43/50] batch [5/25] time 0.153 (0.259) data 0.000 (0.105) loss 1.1914 (1.4287) acc 93.7500 (90.6250) lr 1.5567e-04 eta 0:00:50
epoch [43/50] batch [10/25] time 0.153 (0.206) data 0.000 (0.052) loss 1.1992 (1.3751) acc 93.7500 (91.8750) lr 1.5567e-04 eta 0:00:39
epoch [43/50] batch [15/25] time 0.152 (0.188) data 0.000 (0.035) loss 1.3672 (1.3632) acc 93.7500 (91.4583) lr 1.5567e-04 eta 0:00:34
epoch [43/50] batch [20/25] time 0.153 (0.179) data 0.000 (0.026) loss 1.4609 (1.4068) acc 96.8750 (91.4062) lr 1.5567e-04 eta 0:00:32
epoch [43/50] batch [25/25] time 0.155 (0.175) data 0.000 (0.021) loss 1.5986 (1.3961) acc 90.6250 (91.5000) lr 1.2369e-04 eta 0:00:30
epoch [44/50] batch [5/25] time 0.154 (0.259) data 0.000 (0.102) loss 1.5039 (1.5414) acc 90.6250 (90.0000) lr 1.2369e-04 eta 0:00:43
epoch [44/50] batch [10/25] time 0.153 (0.206) data 0.000 (0.051) loss 1.4492 (1.4241) acc 93.7500 (92.5000) lr 1.2369e-04 eta 0:00:34
epoch [44/50] batch [15/25] time 0.153 (0.189) data 0.000 (0.034) loss 1.2803 (1.4137) acc 90.6250 (92.2917) lr 1.2369e-04 eta 0:00:30
epoch [44/50] batch [20/25] time 0.154 (0.180) data 0.000 (0.026) loss 1.3574 (1.4016) acc 90.6250 (91.8750) lr 1.2369e-04 eta 0:00:27
epoch [44/50] batch [25/25] time 0.154 (0.175) data 0.000 (0.020) loss 1.5596 (1.4285) acc 87.5000 (91.2500) lr 9.5173e-05 eta 0:00:26
epoch [45/50] batch [5/25] time 0.154 (0.271) data 0.000 (0.117) loss 1.2822 (1.2840) acc 90.6250 (92.5000) lr 9.5173e-05 eta 0:00:39
epoch [45/50] batch [10/25] time 0.154 (0.213) data 0.000 (0.059) loss 1.2852 (1.4492) acc 93.7500 (90.9375) lr 9.5173e-05 eta 0:00:29
epoch [45/50] batch [15/25] time 0.153 (0.193) data 0.000 (0.039) loss 1.5127 (1.3579) acc 90.6250 (92.2917) lr 9.5173e-05 eta 0:00:26
epoch [45/50] batch [20/25] time 0.153 (0.183) data 0.000 (0.029) loss 1.3594 (1.3999) acc 96.8750 (92.1875) lr 9.5173e-05 eta 0:00:23
epoch [45/50] batch [25/25] time 0.154 (0.177) data 0.000 (0.023) loss 1.6523 (1.4525) acc 87.5000 (91.2500) lr 7.0224e-05 eta 0:00:22
epoch [46/50] batch [5/25] time 0.154 (0.269) data 0.000 (0.113) loss 1.1299 (1.3961) acc 93.7500 (91.2500) lr 7.0224e-05 eta 0:00:32
epoch [46/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.057) loss 1.1836 (1.3514) acc 93.7500 (91.8750) lr 7.0224e-05 eta 0:00:24
epoch [46/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.038) loss 1.2852 (1.3827) acc 96.8750 (91.2500) lr 7.0224e-05 eta 0:00:21
epoch [46/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.028) loss 2.2656 (1.4543) acc 84.3750 (90.7812) lr 7.0224e-05 eta 0:00:19
epoch [46/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.2803 (1.4495) acc 93.7500 (91.1250) lr 4.8943e-05 eta 0:00:17
epoch [47/50] batch [5/25] time 0.152 (0.268) data 0.000 (0.114) loss 1.3086 (1.3676) acc 93.7500 (92.5000) lr 4.8943e-05 eta 0:00:25
epoch [47/50] batch [10/25] time 0.153 (0.210) data 0.000 (0.057) loss 1.6074 (1.3572) acc 84.3750 (91.8750) lr 4.8943e-05 eta 0:00:18
epoch [47/50] batch [15/25] time 0.152 (0.191) data 0.000 (0.038) loss 1.1465 (1.4398) acc 93.7500 (91.2500) lr 4.8943e-05 eta 0:00:16
epoch [47/50] batch [20/25] time 0.152 (0.181) data 0.000 (0.029) loss 1.1074 (1.4474) acc 100.0000 (91.4062) lr 4.8943e-05 eta 0:00:14
epoch [47/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.9980 (1.4591) acc 81.2500 (91.5000) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [5/25] time 0.153 (0.269) data 0.000 (0.114) loss 1.2900 (1.4436) acc 90.6250 (89.3750) lr 3.1417e-05 eta 0:00:18
epoch [48/50] batch [10/25] time 0.153 (0.211) data 0.000 (0.057) loss 1.4736 (1.4448) acc 90.6250 (90.6250) lr 3.1417e-05 eta 0:00:13
epoch [48/50] batch [15/25] time 0.153 (0.192) data 0.000 (0.038) loss 1.5713 (1.4768) acc 90.6250 (89.7917) lr 3.1417e-05 eta 0:00:11
epoch [48/50] batch [20/25] time 0.152 (0.182) data 0.000 (0.029) loss 1.4678 (1.4704) acc 90.6250 (89.8438) lr 3.1417e-05 eta 0:00:10
epoch [48/50] batch [25/25] time 0.153 (0.176) data 0.000 (0.023) loss 1.4297 (1.4762) acc 93.7500 (90.5000) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [5/25] time 0.154 (0.293) data 0.000 (0.139) loss 1.6758 (1.4592) acc 90.6250 (91.8750) lr 1.7713e-05 eta 0:00:13
epoch [49/50] batch [10/25] time 0.153 (0.223) data 0.000 (0.069) loss 1.4121 (1.4502) acc 93.7500 (91.8750) lr 1.7713e-05 eta 0:00:08
epoch [49/50] batch [15/25] time 0.152 (0.200) data 0.000 (0.046) loss 1.0439 (1.3847) acc 96.8750 (92.7083) lr 1.7713e-05 eta 0:00:06
epoch [49/50] batch [20/25] time 0.153 (0.188) data 0.000 (0.035) loss 1.0400 (1.3350) acc 93.7500 (93.2812) lr 1.7713e-05 eta 0:00:05
epoch [49/50] batch [25/25] time 0.153 (0.181) data 0.000 (0.028) loss 1.4404 (1.3719) acc 87.5000 (92.5000) lr 7.8853e-06 eta 0:00:04
epoch [50/50] batch [5/25] time 0.154 (0.275) data 0.000 (0.120) loss 1.0391 (1.4771) acc 96.8750 (89.3750) lr 7.8853e-06 eta 0:00:05
epoch [50/50] batch [10/25] time 0.153 (0.214) data 0.000 (0.060) loss 1.3887 (1.4461) acc 93.7500 (89.6875) lr 7.8853e-06 eta 0:00:03
epoch [50/50] batch [15/25] time 0.153 (0.194) data 0.000 (0.040) loss 1.0527 (1.4560) acc 96.8750 (90.0000) lr 7.8853e-06 eta 0:00:01
epoch [50/50] batch [20/25] time 0.152 (0.183) data 0.000 (0.030) loss 1.1172 (1.4113) acc 96.8750 (90.7812) lr 7.8853e-06 eta 0:00:00
epoch [50/50] batch [25/25] time 0.153 (0.177) data 0.000 (0.024) loss 0.9448 (1.4017) acc 96.8750 (91.2500) lr 1.9733e-06 eta 0:00:00
Checkpoint saved to output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3/prompt_learner/model.pth.tar-50
Finish training
Deploy the last-epoch model
Evaluate on the *test* set
  0%|          | 0/4 [00:00<?, ?it/s] 25%|██▌       | 1/4 [00:03<00:11,  3.92s/it] 50%|█████     | 2/4 [00:05<00:04,  2.28s/it] 75%|███████▌  | 3/4 [00:06<00:01,  1.76s/it]100%|██████████| 4/4 [00:06<00:00,  1.13s/it]100%|██████████| 4/4 [00:06<00:00,  1.62s/it]
=> result
* total: 1,549
* correct: 1,521
* accuracy: 98.2%
* error: 1.8%
* macro_f1: 96.5%
Elapsed: 0:03:54
Run this job and save the output to output_0509_4/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
Setting fixed seed: 1
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
resume: 
root: /data/yht/data/cl/data/
seed: 1
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1
RESUME: 
SEED: 1
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_1.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     916
---------  ----------
['ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X ibis.', 'X X X X inline skate.', 'X X X X joshua tree.', 'X X X X kangaroo.', 'X X X X ketch.', 'X X X X lamp.', 'X X X X laptop.', 'X X X X llama.', 'X X X X lobster.', 'X X X X lotus.', 'X X X X mandolin.', 'X X X X mayfly.', 'X X X X menorah.', 'X X X X metronome.', 'X X X X minaret.', 'X X X X nautilus.', 'X X X X octopus.', 'X X X X okapi.', 'X X X X pagoda.', 'X X X X panda.', 'X X X X pigeon.', 'X X X X pizza.', 'X X X X platypus.', 'X X X X pyramid.', 'X X X X revolver.', 'X X X X rhino.', 'X X X X rooster.', 'X X X X saxophone.', 'X X X X schooner.', 'X X X X scissors.', 'X X X X scorpion.', 'X X X X sea horse.', 'X X X X snoopy.', 'X X X X soccer ball.', 'X X X X stapler.', 'X X X X starfish.', 'X X X X stegosaurus.', 'X X X X stop sign.', 'X X X X strawberry.', 'X X X X sunflower.', 'X X X X tick.', 'X X X X trilobite.', 'X X X X umbrella.', 'X X X X watch.', 'X X X X water lilly.', 'X X X X wheelchair.', 'X X X X wild cat.', 'X X X X windsor chair.', 'X X X X wrench.', 'X X X X yin yang.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed1/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:03<00:03,  3.68s/it]100%|██████████| 2/2 [00:04<00:00,  2.11s/it]100%|██████████| 2/2 [00:04<00:00,  2.41s/it]
=> result
* total: 916
* correct: 866
* accuracy: 94.5%
* error: 5.5%
* macro_f1: 94.2%
Run this job and save the output to output_0509_4/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
Setting fixed seed: 2
***************
** Arguments **
***************
backbone: 
config_file: configs/trainers/TCP/vit_b16_ep100_ctxv1.yaml
dataset_config_file: configs/datasets/caltech101.yaml
eval_only: True
head: 
load_epoch: 50
model_dir: output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
no_train: False
opts: ['TRAINER.COOP.N_CTX', '4', 'TRAINER.COOP.CSC', 'False', 'TRAINER.COOP.CLASS_TOKEN_POSITION', 'end', 'DATASET.NUM_SHOTS', '16', 'DATASET.SUBSAMPLE_CLASSES', 'new']
output_dir: output_0509_4/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
resume: 
root: /data/yht/data/cl/data/
seed: 2
source_domains: None
target_domains: None
trainer: TCP
transforms: None
************
** Config **
************
DATALOADER:
  K_TRANSFORMS: 1
  NUM_WORKERS: 8
  RETURN_IMG0: False
  TEST:
    BATCH_SIZE: 500
    SAMPLER: SequentialSampler
  TRAIN_U:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAME_AS_X: True
    SAMPLER: RandomSampler
  TRAIN_X:
    BATCH_SIZE: 32
    N_DOMAIN: 0
    N_INS: 16
    SAMPLER: RandomSampler
DATASET:
  ALL_AS_UNLABELED: False
  CIFAR_C_LEVEL: 1
  CIFAR_C_TYPE: 
  NAME: Caltech101
  NUM_LABELED: -1
  NUM_SHOTS: 16
  ROOT: /data/yht/data/cl/data/
  SOURCE_DOMAINS: ()
  STL10_FOLD: -1
  SUBSAMPLE_CLASSES: new
  TARGET_DOMAINS: ()
  VAL_PERCENT: 0.1
INPUT:
  COLORJITTER_B: 0.4
  COLORJITTER_C: 0.4
  COLORJITTER_H: 0.1
  COLORJITTER_S: 0.4
  CROP_PADDING: 4
  CUTOUT_LEN: 16
  CUTOUT_N: 1
  GB_K: 21
  GB_P: 0.5
  GN_MEAN: 0.0
  GN_STD: 0.15
  INTERPOLATION: bicubic
  NO_TRANSFORM: False
  PIXEL_MEAN: [0.48145466, 0.4578275, 0.40821073]
  PIXEL_STD: [0.26862954, 0.26130258, 0.27577711]
  RANDAUGMENT_M: 10
  RANDAUGMENT_N: 2
  RGS_P: 0.2
  RRCROP_SCALE: (0.08, 1.0)
  SIZE: (224, 224)
  TRANSFORMS: ('random_resized_crop', 'random_flip', 'normalize')
LOSS:
  ALPHA: 0.0
  GM: False
  LAMBDA: 1.0
  NAME: 
  T: 1.0
MODEL:
  BACKBONE:
    NAME: ViT-B/16
    PRETRAINED: True
  HEAD:
    ACTIVATION: relu
    BN: True
    DROPOUT: 0.0
    HIDDEN_LAYERS: ()
    NAME: 
  INIT_WEIGHTS: 
OPTIM:
  ADAM_BETA1: 0.9
  ADAM_BETA2: 0.999
  BASE_LR_MULT: 0.1
  EPS: 0.001
  GAMMA: 0.1
  LR: 0.002
  LR_SCHEDULER: cosine
  MAX_EPOCH: 50
  MOMENTUM: 0.9
  NAME: adam
  NEW_LAYERS: ()
  RMSPROP_ALPHA: 0.99
  SGD_DAMPNING: 0
  SGD_NESTEROV: False
  STAGED_LR: False
  STEPSIZE: (-1,)
  WARMUP_CONS_LR: 1e-05
  WARMUP_EPOCH: 1
  WARMUP_MIN_LR: 1e-05
  WARMUP_RECOUNT: True
  WARMUP_TYPE: constant
  WEIGHT_DECAY: 0.0005
OUTPUT_DIR: output_0509_4/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2
RESUME: 
SEED: 2
TEST:
  COMPUTE_CMAT: False
  EVALUATOR: Classification
  FINAL_MODEL: last_step
  NO_TEST: False
  PER_CLASS_RESULT: False
  SPLIT: test
TRAIN:
  CHECKPOINT_FREQ: 0
  COUNT_ITER: train_x
  PRINT_FREQ: 5
TRAINER:
  CDAC:
    CLASS_LR_MULTI: 10
    P_THRESH: 0.95
    RAMPUP_COEF: 30
    RAMPUP_ITRS: 1000
    STRONG_TRANSFORMS: ()
    TOPK_MATCH: 5
  COCOOP:
    CTX_INIT: False
    N_CTX: 16
    PREC: fp16
  COOP:
    ALPHA: 1.0
    CLASS_TOKEN_POSITION: end
    CSC: False
    CTX_INIT: False
    N_CTX: 4
    PREC: fp16
    W: 1.0
  CROSSGRAD:
    ALPHA_D: 0.5
    ALPHA_F: 0.5
    EPS_D: 1.0
    EPS_F: 1.0
  DAEL:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DAELDG:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 0.5
  DDAIG:
    ALPHA: 0.5
    CLAMP: False
    CLAMP_MAX: 1.0
    CLAMP_MIN: -1.0
    G_ARCH: 
    LMDA: 0.3
    WARMUP: 0
  DOMAINMIX:
    ALPHA: 1.0
    BETA: 1.0
    TYPE: crossdomain
  ENTMIN:
    LMDA: 0.001
  FIXMATCH:
    CONF_THRE: 0.95
    STRONG_TRANSFORMS: ()
    WEIGHT_U: 1.0
  M3SDA:
    LMDA: 0.5
    N_STEP_F: 4
  MCD:
    N_STEP_F: 4
  MEANTEACHER:
    EMA_ALPHA: 0.999
    RAMPUP: 5
    WEIGHT_U: 1.0
  MIXMATCH:
    MIXUP_BETA: 0.75
    RAMPUP: 20000
    TEMP: 2.0
    WEIGHT_U: 100.0
  MME:
    LMDA: 0.1
  NAME: TCP
  SE:
    CONF_THRE: 0.95
    EMA_ALPHA: 0.999
    RAMPUP: 300
USE_CUDA: True
VERBOSE: True
VERSION: 1
Collecting env info ...
** System info **
PyTorch version: 1.8.1+cu111
Is debug build: False
CUDA used to build PyTorch: 11.1
ROCM used to build PyTorch: N/A

OS: Ubuntu 16.04.7 LTS (x86_64)
GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.12) 5.4.0 20160609
Clang version: Could not collect
CMake version: Could not collect

Python version: 3.8 (64-bit runtime)
Is CUDA available: True
CUDA runtime version: Could not collect
GPU models and configuration: 
GPU 0: NVIDIA GeForce RTX 3090
GPU 1: NVIDIA GeForce RTX 3090
GPU 2: NVIDIA GeForce RTX 3090
GPU 3: NVIDIA GeForce RTX 3090
GPU 4: NVIDIA GeForce RTX 3090
GPU 5: NVIDIA GeForce RTX 3090
GPU 6: NVIDIA GeForce RTX 3090
GPU 7: NVIDIA GeForce RTX 3090

Nvidia driver version: 470.57.02
cuDNN version: Probably one of the following:
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_adv_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_cnn_train.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_infer.so.8
/usr/local/cuda-11.2/targets/x86_64-linux/lib/libcudnn_ops_train.so.8
HIP runtime version: N/A
MIOpen runtime version: N/A

Versions of relevant libraries:
[pip3] fast-pytorch-kmeans==0.2.0.1
[pip3] numpy==1.24.4
[pip3] pytorch-model-summary==0.1.2
[pip3] torch==1.8.1+cu111
[pip3] torchaudio==0.8.1
[pip3] torchvision==0.9.1+cu111
[conda] fast-pytorch-kmeans       0.2.0.1                   <pip>
[conda] numpy                     1.24.4                    <pip>
[conda] pytorch-model-summary     0.1.2                     <pip>
[conda] torch                     1.8.1+cu111               <pip>
[conda] torchaudio                0.8.1                     <pip>
[conda] torchvision               0.9.1+cu111               <pip>
        Pillow (10.1.0)

Loading trainer: TCP
Loading dataset: Caltech101
Reading split from /data/yht/data/cl/data/caltech-101/split_zhou_Caltech101.json
Loading preprocessed few-shot data from /data/yht/data/cl/data/caltech-101/split_fewshot/shot_16-seed_2.pkl
SUBSAMPLE NEW CLASSES!
Building transform_train
+ random resized crop (size=(224, 224), scale=(0.08, 1.0))
+ random flip
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
Building transform_test
+ resize the smaller edge to 224
+ 224x224 center crop
+ to torch tensor of range [0, 1]
+ normalization (mean=[0.48145466, 0.4578275, 0.40821073], std=[0.26862954, 0.26130258, 0.27577711])
---------  ----------
Dataset    Caltech101
# classes  50
# train_x  800
# val      200
# test     916
---------  ----------
['ibis', 'inline_skate', 'joshua_tree', 'kangaroo', 'ketch', 'lamp', 'laptop', 'llama', 'lobster', 'lotus', 'mandolin', 'mayfly', 'menorah', 'metronome', 'minaret', 'nautilus', 'octopus', 'okapi', 'pagoda', 'panda', 'pigeon', 'pizza', 'platypus', 'pyramid', 'revolver', 'rhino', 'rooster', 'saxophone', 'schooner', 'scissors', 'scorpion', 'sea_horse', 'snoopy', 'soccer_ball', 'stapler', 'starfish', 'stegosaurus', 'stop_sign', 'strawberry', 'sunflower', 'tick', 'trilobite', 'umbrella', 'watch', 'water_lilly', 'wheelchair', 'wild_cat', 'windsor_chair', 'wrench', 'yin_yang']
Loading CLIP (backbone: ViT-B/16)
Building custom CLIP
Initializing a generic context
Initial context: "X X X X"
Number of context words (tokens): 4
['X X X X ibis.', 'X X X X inline skate.', 'X X X X joshua tree.', 'X X X X kangaroo.', 'X X X X ketch.', 'X X X X lamp.', 'X X X X laptop.', 'X X X X llama.', 'X X X X lobster.', 'X X X X lotus.', 'X X X X mandolin.', 'X X X X mayfly.', 'X X X X menorah.', 'X X X X metronome.', 'X X X X minaret.', 'X X X X nautilus.', 'X X X X octopus.', 'X X X X okapi.', 'X X X X pagoda.', 'X X X X panda.', 'X X X X pigeon.', 'X X X X pizza.', 'X X X X platypus.', 'X X X X pyramid.', 'X X X X revolver.', 'X X X X rhino.', 'X X X X rooster.', 'X X X X saxophone.', 'X X X X schooner.', 'X X X X scissors.', 'X X X X scorpion.', 'X X X X sea horse.', 'X X X X snoopy.', 'X X X X soccer ball.', 'X X X X stapler.', 'X X X X starfish.', 'X X X X stegosaurus.', 'X X X X stop sign.', 'X X X X strawberry.', 'X X X X sunflower.', 'X X X X tick.', 'X X X X trilobite.', 'X X X X umbrella.', 'X X X X watch.', 'X X X X water lilly.', 'X X X X wheelchair.', 'X X X X wild cat.', 'X X X X windsor chair.', 'X X X X wrench.', 'X X X X yin yang.']
Turning off gradients in both the image and the text encoder
prompt_learner.ctx
prompt_learner.ctx_visual
prompt_learner.meta_net.linear1.weight
prompt_learner.meta_net.linear1.bias
prompt_learner.meta_net.linear2.weight
prompt_learner.meta_net.linear2.bias
prompt_learner.meta_vis.linear1.weight
prompt_learner.meta_vis.linear1.bias
prompt_learner.meta_vis.linear2.weight
prompt_learner.meta_vis.linear2.bias
prompt_learner.meta_vises.0.linear1.weight
prompt_learner.meta_vises.0.linear1.bias
prompt_learner.meta_vises.0.linear2.weight
prompt_learner.meta_vises.0.linear2.bias
prompt_learner.meta_vises.1.linear1.weight
prompt_learner.meta_vises.1.linear1.bias
prompt_learner.meta_vises.1.linear2.weight
prompt_learner.meta_vises.1.linear2.bias
prompt_learner.meta_vises.2.linear1.weight
prompt_learner.meta_vises.2.linear1.bias
prompt_learner.meta_vises.2.linear2.weight
prompt_learner.meta_vises.2.linear2.bias
prompt_learner.meta_vises.3.linear1.weight
prompt_learner.meta_vises.3.linear1.bias
prompt_learner.meta_vises.3.linear2.weight
prompt_learner.meta_vises.3.linear2.bias
prompt_learner.meta_vises.4.linear1.weight
prompt_learner.meta_vises.4.linear1.bias
prompt_learner.meta_vises.4.linear2.weight
prompt_learner.meta_vises.4.linear2.bias
prompt_learner.meta_vises.5.linear1.weight
prompt_learner.meta_vises.5.linear1.bias
prompt_learner.meta_vises.5.linear2.weight
prompt_learner.meta_vises.5.linear2.bias
prompt_learner.VPT_fc.weight
torch.Size([50, 512])
Loading evaluator: Classification
['prompt_learner']
Loading weights to prompt_learner from "output_0509_4/base2new/train_base/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed2/prompt_learner/model.pth.tar-50" (epoch = 50)
Evaluate on the *test* set
  0%|          | 0/2 [00:00<?, ?it/s] 50%|█████     | 1/2 [00:03<00:03,  3.46s/it]100%|██████████| 2/2 [00:04<00:00,  2.02s/it]100%|██████████| 2/2 [00:04<00:00,  2.28s/it]
=> result
* total: 916
* correct: 868
* accuracy: 94.8%
* error: 5.2%
* macro_f1: 94.9%
Run this job and save the output to output_0509_4/base2new/test_new/caltech101/shots_16_1.0/TCP/vit_b16_ep100_ctxv1/seed3
